{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Neuroelectrophysiology object model, data exploration, and analysis in Python.</p>"},{"location":"#overview","title":"Overview","text":"<p>Nelpy (Neuroelectrophysiology) is an open source package for analysis of neuroelectrophysiology data. Nelpy defines a number of data objects to make it easier to work with electrophysiology (ephys) data, and although it was originally designed for use with extracellular recorded data, it can be used much more broadly. Nelpy is intended to make interactive data analysis and exploration of these ephys data easy, by providing several convenience functions and common visualizations that operate directly on the nelpy objects.</p> <p>More specifically, the functionality of this package includes:</p> <ul> <li>several container objects (<code>SpikeTrain</code>, <code>BinnedSpikeTrain</code>, <code>AnalogSignal</code>, <code>EpochArray</code>, ...) with nice human-readable <code>__repr__</code> methods</li> <li>powerful ways to interact with the data in the container objects</li> <li>hidden Markov model analysis of neural activity</li> <li>basic data exploration and visualization operating directly on the core nelpy objects</li> <li>and much more</li> </ul>"},{"location":"#support","title":"Support","text":"<p>This work was supported by the National Science Foundation (CBET-1351692 and IOS-1550994) and the Human Frontiers Science Program (RGY0088). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>"},{"location":"#quick-examples","title":"Quick examples","text":"<p>Let's give it a try. Create a <code>SpikeTrainArray</code>:</p> <pre><code>\n    import nelpy as nel  # main nelpy imports\n    import nelpy.plotting as npl  # optional plotting imports\n    spike_times = np.array([1, 2, 4, 5, 10])\n    st = nel.SpikeTrainArray(spike_times)\n</code></pre> <p>Do something:</p> <pre><code>\n    &gt;&gt;&gt; print(st.n_spikes) # print out how many spikes there are in st\n    5\n\n    &gt;&gt;&gt; print(st.supportn_spikes) # print out the underlying EpochArray on which st is defined\n    &lt;EpochArray at 0x1d4812c7550: 1 epoch&gt; of duration 9 seconds\n\n    &gt;&gt;&gt; npl.raster(st) # plots the spike raster\n</code></pre> <p>As a more representative example of what nelpy can be used for, consider the estimation of place fields (spatial tuning curves) of CA1 units while an animal runs on a linear track.</p> <p>Estimating the place fields can be a complicated affair, and roughly involves the following steps:</p> <ol> <li>assume we have position data and spike data available</li> <li>linearize the environment (and position data), if desired</li> <li>estimate the running velocity from the position data</li> <li>smooth the velocity estimates, since numerical differentiation is inherently noisy (and our measurements are imprecise)</li> <li>identify epochs where the animal was running, and where the animal was resting</li> <li>count the number of spikes from each unit, in each spatial bin of the environment, during run behavior</li> <li>determine how long the animal spent in each spatial bin (while running)</li> <li>estimate a firing rate within each spatial bin, by normalizing the number of observed spikes by the time spent in that spatial bin</li> <li>visualize the estimated tuning curves, and evaluate how well the tuning curves can be used to decode the animal's position</li> <li>...</li> </ol> <p></p> <p>Nelpy makes it easy to do all of the above, to interact with the ephys data, and to visualize the results.</p> <p>To see the full code that was used to generate the figures above, take a look at the linear track example analysis.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>The best way to get started with using <code>nelpy</code> is probably to take a look at the tutorials (a work-in-progress) and example analyses.</p> <p>The tutorials are still pretty bare-bones, but will hopefully be expanded soon!</p>"},{"location":"#installation","title":"Installation","text":"<p>The easiest way to install nelpy is to use <code>pip</code>. From the terminal, run:</p> <pre><code>    $ pip install nelpy\n</code></pre> <p>Alternatively, you can install the latest version of nelpy by running the following commands:</p> <pre><code>    $ git clone https://github.com/nelpy/nelpy.git\n    $ cd nelpy\n    $ pip install .\n</code></pre> <p>If you would like to modify the code, then replace the last command with</p> <pre><code>    $ pip install -e .\n</code></pre> <p>A weak prerequisite for installing nelpy is a modified version of hmmlearn. This requirement is weak, in the sense that installation will complete successfully without it, and most of nelpy can also be used without any problems. However, as soon as any of the hidden Markov model (HMM) functions are used, you will get an error if the correct version of <code>hmmlearn</code> is not installed. To make things easier, there is a handy 64-bit Windows wheel in the hmmlearn directory of this repository. Installation on Linux/Unix should be almost trivial.</p>"},{"location":"#related-work-and-inspiration","title":"Related work and inspiration","text":"<p>Nelpy drew heavy inspiration from the <code>python-vdmlab</code> package (renamed to <code>nept</code>) from the van der Meer lab at Dartmouth College (https://github.com/vandermeerlab), which was created by Emily Irvine (https://github.com/emirvine). It is also inspired by the neuralensemble.org NEO project (http://neo.readthedocs.io).</p> <p>Short history: Etienne A started the nelpy project for two main reasons, namely</p> <ol> <li>he wanted / needed a <code>BinnedSpikeTrain</code> object for hidden Markov model analysis that wasn't (at the time) avaialable in <code>neo</code> or <code>python-vdmlab</code>, and</li> <li>he fundamentally wanted to add \"support\" attributes to all the container objects. Here \"support\" should be understood in the mathematical sense of \"domain of definition\", whereas the mathematical support technically would not include some elements for which the function maps to zero. This is critical for spike trains, for example, where it is important to differentiate \"no spike at time t\" from \"no record at time t\".</li> </ol>"},{"location":"#scope-of-this-work","title":"Scope of this work","text":"<p>The nelpy object model is expected to be quite similar to the python-vdmlab object model, which in turn has significant overlap with neuralensemble.org's neo model. However, the nelpy object model extends the former by making binned data first class citizens, and by changing the API for indexing and extracting subsets of data, as well as making \"functional support\" an integral part of the model. It (nelpy) is currently simpler and less comprehensive than neo, and specifically lacks in terms of physical units and complex object hierarchies and nonlinear relationships. However, nelpy again makes binned data a core object, and nelpy further aims to add additional analysis code including filtering, smoothing, position analysis, subsampling, interpolation, spike rate estimation, spike generation / synthesis, ripple detection, Bayesian decoding, and so on. In short, nelpy is more than just an object model, but the nelpy core is designed to be a flexible, readable, yet powerful object model for neuroelectrophysiology.</p>"},{"location":"#where","title":"Where","text":"<p>===================   ========================================================  download             https://pypi.python.org/pypi/nelpy  tutorials            https://github.com/nelpy/tutorials  example analyses     https://github.com/nelpy/example-analyses  docs                 nelpy.github.io/nelpy/  code                 https://github.com/nelpy/nelpy ===================   ========================================================</p>"},{"location":"#cite","title":"Cite","text":"<p>If you use nelpy in your research, please cite it:</p> <pre><code>    @misc{nelpy17,\n      author =   {Etienne Ackermann},\n      title =    {Nelpy: Neuroelectrophysiology object model, data exploration, and analysis in Python},\n      howpublished = {\\url{https://github.com/nelpy/nelpy/}},\n      year = {2017--2018}\n    }\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Nelpy is distributed under the MIT license. See the LICENSE file for details.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nelpy<ul> <li> all</li> <li>analysis<ul> <li> ergodic</li> <li> hmm_sparsity</li> <li> replay</li> </ul> </li> <li> decoding</li> <li> estimators</li> <li> filtering</li> <li> formatters</li> <li> hmmutils</li> <li>io<ul> <li> brian</li> <li> hc11</li> <li> hc18</li> <li> hc3</li> <li> matlab</li> <li> miniscopy</li> <li> neo</li> <li> neuralynx</li> </ul> </li> <li> ipynb</li> <li> min</li> <li>plotting<ul> <li> colormaps</li> <li> colors</li> <li> core</li> <li> decoding</li> <li> graph</li> <li> helpers</li> <li> miscplot</li> <li> palettes</li> <li> rcmod</li> <li> scalebar</li> <li> utils</li> </ul> </li> <li> preprocessing</li> <li> scoring</li> <li>synthesis<ul> <li> poisson</li> </ul> </li> <li> utils</li> <li>utils_<ul> <li> decorators</li> <li> metrics</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/analysis/","title":"Analysis API Reference","text":""},{"location":"reference/analysis/#nelpy.analysis--nelpyanalysis","title":"nelpy.analysis","text":"<p>This is the nelpy analysis sub-package.</p> <p>nelpy.analysis provides several commonly used analyses.</p>"},{"location":"reference/analysis/#nelpy.analysis.column_cycle_array","title":"<code>column_cycle_array(posterior, amt=None)</code>","text":"<p>Cycle each column of the posterior matrix by a random or specified amount.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>amt</code> <code>array - like or None</code> <p>Amount to cycle each column. If None, random cycling is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Cycled posterior matrix.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def column_cycle_array(posterior, amt=None):\n    \"\"\"\n    Cycle each column of the posterior matrix by a random or specified amount.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    amt : array-like or None, optional\n        Amount to cycle each column. If None, random cycling is used.\n\n    Returns\n    -------\n    out : np.ndarray\n        Cycled posterior matrix.\n    \"\"\"\n    out = copy.deepcopy(posterior)\n    rows, cols = posterior.shape\n\n    if amt is None:\n        for col in range(cols):\n            if np.isnan(np.sum(posterior[:, col])):\n                continue\n            else:\n                out[:, col] = np.roll(posterior[:, col], np.random.randint(1, rows))\n    else:\n        if len(amt) == cols:\n            for col in range(cols):\n                if np.isnan(np.sum(posterior[:, col])):\n                    continue\n                else:\n                    out[:, col] = np.roll(posterior[:, col], int(amt[col]))\n        else:\n            raise TypeError(\"amt does not seem to be the correct shape!\")\n    return out\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.fmpt","title":"<code>fmpt(P)</code>","text":"<p>Calculates the matrix of first mean passage times for an ergodic transition probability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <p>an ergodic Markov transition probability matrix</p> required <p>Returns:</p> Name Type Description <code>M</code> <code>array(kxk)</code> <p>elements are the expected value for the number of intervals required for  a chain starting in state i to first enter state j If i=j then this is the recurrence time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n&gt;&gt;&gt; fm = fmpt(p)\n&gt;&gt;&gt; fm\narray([[ 2.5       ,  4.        ,  3.33333333],\n       [ 2.66666667,  5.        ,  2.66666667],\n       [ 3.33333333,  4.        ,  2.5       ]])\nThus, if it is raining today in Oz we can expect a nice day to come\nalong in another 4 days, on average, and snow to hit in 3.33 days. We can\nexpect another rainy day in 2.5 days. If it is nice today in Oz, we would\nexperience a change in the weather (either rain or snow) in 2.67 days from\ntoday. (That wicked witch can only die once so I reckon that is the\nultimate absorbing state).\n</code></pre> <p>Notes ----- Uses formulation (and examples on p. 218) in Kemeny and Snell (1976) [1]_ References</p> <p>.. [1] Kemeny, John, G. and J. Laurie Snell (1976) Finite Markov    Chains. Springer-Verlag. Berlin</p> Source code in <code>nelpy/analysis/ergodic.py</code> <pre><code>def fmpt(P):\n    \"\"\"\n    Calculates the matrix of first mean passage times for an\n    ergodic transition probability matrix.\n    Parameters\n    ----------\n    P    : array (kxk)\n           an ergodic Markov transition probability matrix\n    Returns\n    -------\n    M    : array (kxk)\n           elements are the expected value for the number of intervals\n           required for  a chain starting in state i to first enter state j\n           If i=j then this is the recurrence time.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n    &gt;&gt;&gt; fm = fmpt(p)\n    &gt;&gt;&gt; fm\n    array([[ 2.5       ,  4.        ,  3.33333333],\n           [ 2.66666667,  5.        ,  2.66666667],\n           [ 3.33333333,  4.        ,  2.5       ]])\n    Thus, if it is raining today in Oz we can expect a nice day to come\n    along in another 4 days, on average, and snow to hit in 3.33 days. We can\n    expect another rainy day in 2.5 days. If it is nice today in Oz, we would\n    experience a change in the weather (either rain or snow) in 2.67 days from\n    today. (That wicked witch can only die once so I reckon that is the\n    ultimate absorbing state).\n\n    Notes -----\n    Uses formulation (and examples on p. 218) in Kemeny and Snell (1976) [1]_\n    References\n    ----------\n\n    .. [1] Kemeny, John, G. and J. Laurie Snell (1976) Finite Markov\n       Chains. Springer-Verlag. Berlin\n    \"\"\"\n    P = np.asarray(P)\n    A = np.zeros_like(P)\n    ss = steady_state(P)\n    k = ss.shape[0]\n    for i in range(k):\n        A[:, i] = ss.flatten()\n    A = A.T\n    identity_matrix = np.identity(k)\n    Z = la.inv(identity_matrix - P + A)\n    E = np.ones_like(Z)\n    D = np.diag(1.0 / np.diag(A))\n    Zdg = np.diag(np.diag(Z))\n    M = (identity_matrix - Z + np.multiply(E, Zdg)) @ D\n    return M\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.get_significant_events","title":"<code>get_significant_events(scores, shuffled_scores, q=95)</code>","text":"<p>Return the significant events based on percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>Scores for each event.</p> required <code>shuffled_scores</code> <code>ndarray</code> <p>Shuffled scores for each event and shuffle.</p> required <code>q</code> <code>float</code> <p>Percentile to compute (default is 95).</p> <code>95</code> <p>Returns:</p> Name Type Description <code>sig_event_idx</code> <code>ndarray</code> <p>Indices of significant events.</p> <code>pvalues</code> <code>ndarray</code> <p>Monte Carlo p-values for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def get_significant_events(scores, shuffled_scores, q=95):\n    \"\"\"\n    Return the significant events based on percentiles.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        Scores for each event.\n    shuffled_scores : np.ndarray\n        Shuffled scores for each event and shuffle.\n    q : float, optional\n        Percentile to compute (default is 95).\n\n    Returns\n    -------\n    sig_event_idx : np.ndarray\n        Indices of significant events.\n    pvalues : np.ndarray\n        Monte Carlo p-values for each event.\n    \"\"\"\n\n    n, _ = shuffled_scores.shape\n    r = np.sum(shuffled_scores &gt;= scores, axis=0)\n    pvalues = (r + 1) / (n + 1)\n\n    sig_event_idx = np.argwhere(\n        scores &gt; np.percentile(shuffled_scores, axis=0, q=q)\n    ).squeeze()\n\n    return np.atleast_1d(sig_event_idx), np.atleast_1d(pvalues)\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.linregress_array","title":"<code>linregress_array(posterior)</code>","text":"<p>Perform linear regression on the posterior matrix, and return the slope, intercept, and R^2 value.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <p>Returns:</p> Name Type Description <code>slope</code> <code>float</code> <p>Slope of the best-fit line.</p> <code>intercept</code> <code>float</code> <p>Intercept of the best-fit line.</p> <code>r2</code> <code>float</code> <p>R^2 value of the fit.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def linregress_array(posterior):\n    \"\"\"\n    Perform linear regression on the posterior matrix, and return the slope, intercept, and R^2 value.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n\n    Returns\n    -------\n    slope : float\n        Slope of the best-fit line.\n    intercept : float\n        Intercept of the best-fit line.\n    r2 : float\n        R^2 value of the fit.\n    \"\"\"\n\n    mode_pth = get_mode_pth_from_array(posterior)\n\n    y = mode_pth\n    x = np.arange(len(y))\n    x = x[~np.isnan(y)]\n    y = y[~np.isnan(y)]\n\n    if len(y) &gt; 0:\n        slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)\n        return slope, intercept, rvalue**2\n    else:\n        return np.nan, np.nan, np.nan\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.linregress_bst","title":"<code>linregress_bst(bst, tuningcurve)</code>","text":"<p>Perform linear regression on all the events in bst, and return the slopes, intercepts, and R^2 values.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for decoding.</p> required <p>Returns:</p> Name Type Description <code>slopes</code> <code>ndarray</code> <p>Slopes for each event.</p> <code>intercepts</code> <code>ndarray</code> <p>Intercepts for each event.</p> <code>r2values</code> <code>ndarray</code> <p>R^2 values for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def linregress_bst(bst, tuningcurve):\n    \"\"\"\n    Perform linear regression on all the events in bst, and return the slopes, intercepts, and R^2 values.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve for decoding.\n\n    Returns\n    -------\n    slopes : np.ndarray\n        Slopes for each event.\n    intercepts : np.ndarray\n        Intercepts for each event.\n    r2values : np.ndarray\n        R^2 values for each event.\n    \"\"\"\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    slopes = np.zeros(bst.n_epochs)\n    intercepts = np.zeros(bst.n_epochs)\n    r2values = np.zeros(bst.n_epochs)\n    for idx in range(bst.n_epochs):\n        y = mode_pth[bdries[idx] : bdries[idx + 1]]\n        x = np.arange(bdries[idx], bdries[idx + 1], step=1)\n        x = x[~np.isnan(y)]\n        y = y[~np.isnan(y)]\n\n        if len(y) &gt; 0:\n            slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)\n            slopes[idx] = slope\n            intercepts[idx] = intercept\n            r2values[idx] = rvalue**2\n        else:\n            slopes[idx] = np.nan\n            intercepts[idx] = np.nan\n            r2values[idx] = np.nan  #\n    #     if bst.n_epochs == 1:\n    #         return np.asscalar(slopes), np.asscalar(intercepts), np.asscalar(r2values)\n    return slopes, intercepts, r2values\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.linregress_ting","title":"<code>linregress_ting(bst, tuningcurve, n_shuffles=250)</code>","text":"<p>Perform linear regression on all the events in bst, and return the R^2 values.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for decoding.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution (default is 250).</p> <code>250</code> <p>Returns:</p> Name Type Description <code>r2values</code> <code>ndarray</code> <p>R^2 values for each event.</p> <code>r2values_shuffled</code> <code>ndarray</code> <p>Shuffled R^2 values for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def linregress_ting(bst, tuningcurve, n_shuffles=250):\n    \"\"\"\n    Perform linear regression on all the events in bst, and return the R^2 values.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve for decoding.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution (default is 250).\n\n    Returns\n    -------\n    r2values : np.ndarray\n        R^2 values for each event.\n    r2values_shuffled : np.ndarray\n        Shuffled R^2 values for each event and shuffle.\n    \"\"\"\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    #     bdries = np.insert(np.cumsum(bst.lengths), 0, 0)\n    r2values = np.zeros(bst.n_epochs)\n    r2values_shuffled = np.zeros((n_shuffles, bst.n_epochs))\n    for idx in range(bst.n_epochs):\n        y = mode_pth[bdries[idx] : bdries[idx + 1]]\n        x = np.arange(bdries[idx], bdries[idx + 1], step=1)\n        x = x[~np.isnan(y)]\n        y = y[~np.isnan(y)]\n\n        if len(y) &gt; 0:\n            slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)\n            r2values[idx] = rvalue**2\n        else:\n            r2values[idx] = np.nan  #\n        for ss in range(n_shuffles):\n            if len(y) &gt; 0:\n                slope, intercept, rvalue, pvalue, stderr = stats.linregress(\n                    np.random.permutation(x), y\n                )\n                r2values_shuffled[ss, idx] = rvalue**2\n            else:\n                r2values_shuffled[ss, idx] = (\n                    np.nan\n                )  # event contained NO decoded activity... unlikely or even impossible with current code\n\n    #     sig_idx = np.argwhere(r2values[0,:] &gt; np.percentile(r2values, q=q, axis=0))\n    #     np.argwhere(((R2[1:,:] &gt;= R2[0,:]).sum(axis=0))/(R2.shape[0]-1)&lt;0.05) # equivalent to above\n    if n_shuffles &gt; 0:\n        return r2values, r2values_shuffled\n    return r2values\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.pooled_time_swap_bst","title":"<code>pooled_time_swap_bst(bst)</code>","text":"<p>Time swap on BinnedSpikeTrainArray, swapping within entire bst.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to swap.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Time-swapped spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def pooled_time_swap_bst(bst):\n    \"\"\"\n    Time swap on BinnedSpikeTrainArray, swapping within entire bst.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to swap.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Time-swapped spike train array.\n    \"\"\"\n    out = copy.deepcopy(bst)  # should this be deep? YES! Oh my goodness, yes!\n    shuffled = np.random.permutation(bst.n_bins)\n    out._data = out._data[:, shuffled]\n    return out\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.score_hmm_logprob_cumulative","title":"<code>score_hmm_logprob_cumulative(bst, hmm, normalize=False)</code>","text":"<p>Score events in a BinnedSpikeTrainArray by computing the cumulative log probability under the model.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>normalize</code> <code>bool</code> <p>If True, log probabilities will be normalized by their sequence lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>ndarray</code> <p>Cumulative log probabilities for each event in bst.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_logprob_cumulative(bst, hmm, normalize=False):\n    \"\"\"\n    Score events in a BinnedSpikeTrainArray by computing the cumulative log probability under the model.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    normalize : bool, optional\n        If True, log probabilities will be normalized by their sequence lengths.\n\n    Returns\n    -------\n    logprob : np.ndarray\n        Cumulative log probabilities for each event in bst.\n    \"\"\"\n\n    logprob = np.atleast_1d(hmm._cum_score_per_bin(bst))\n    if normalize:\n        cumlengths = []\n        for evt in bst.lengths:\n            cumlengths.extend(np.arange(1, evt + 1).tolist())\n        cumlengths = np.array(cumlengths)\n        logprob = np.atleast_1d(logprob) / cumlengths\n\n    return logprob\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.score_hmm_time_resolved","title":"<code>score_hmm_time_resolved(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (time-resolved).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_time_resolved(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (time-resolved).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    hmm_shuffled = copy.deepcopy(hmm)\n    Lbraw = score_hmm_logprob_cumulative(bst=bst, hmm=hmm, normalize=normalize)\n\n    # per event, compute L(:b|raw) - L(:b-1|raw)\n    Lb = copy.deepcopy(Lbraw)\n\n    cumLengths = np.cumsum(bst.lengths)\n    cumLengths = np.insert(cumLengths, 0, 0)\n\n    for ii in range(bst.n_epochs):\n        LE = cumLengths[ii]\n        RE = cumLengths[ii + 1]\n        Lb[LE + 1 : RE] -= Lbraw[LE : RE - 1]\n\n    n_bins = bst.n_bins\n    shuffled = np.zeros((n_shuffles, n_bins))\n    for ii in range(n_shuffles):\n        hmm_shuffled.transmat_ = shuffle_transmat(hmm_shuffled.transmat_)\n        Lbtmat = score_hmm_logprob_cumulative(\n            bst=bst, hmm=hmm_shuffled, normalize=normalize\n        )\n\n        # per event, compute L(:b|tmat) - L(:b-1|raw)\n        NL = copy.deepcopy(Lbtmat)\n        for jj in range(bst.n_epochs):\n            LE = cumLengths[jj]\n            RE = cumLengths[jj + 1]\n            NL[LE + 1 : RE] -= Lbraw[LE : RE - 1]\n\n        shuffled[ii, :] = NL\n\n    scores = Lb\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.steady_state","title":"<code>steady_state(P)</code>","text":"<p>Calculates the steady state probability vector for a regular Markov transition matrix P</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <pre><code>   an ergodic Markov transition probability matrix\n</code></pre> required <p>Returns:</p> Name Type Description <code>implicit</code> <code>array(kx1)</code> <p>steady state distribution</p> <p>Examples:</p> <p>Taken from Kemeny and Snell. [1]_ Land of Oz example where the states are Rain, Nice and Snow - so there is 25 percent chance that if it rained in Oz today, it will snow tomorrow, while if it snowed today in Oz there is a 50 percent chance of snow again tomorrow and a 25 percent chance of a nice day (nice, like when the witch with the monkeys is melting).</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n&gt;&gt;&gt; steady_state(p)\narray([[ 0.4],\n       [ 0.2],\n       [ 0.4]])\nThus, the long run distribution for Oz is to have 40 percent of the\ndays classified as Rain, 20 percent as Nice, and 40 percent as Snow\n(states are mutually exclusive).\n</code></pre> Source code in <code>nelpy/analysis/ergodic.py</code> <pre><code>def steady_state(P):\n    \"\"\"\n    Calculates the steady state probability vector for a regular Markov\n    transition matrix P\n    Parameters\n    ----------\n    P        : array (kxk)\n               an ergodic Markov transition probability matrix\n    Returns\n    -------\n    implicit : array (kx1)\n               steady state distribution\n    Examples\n    --------\n    Taken from Kemeny and Snell. [1]_ Land of Oz example where the states are\n    Rain, Nice and Snow - so there is 25 percent chance that if it\n    rained in Oz today, it will snow tomorrow, while if it snowed today in\n    Oz there is a 50 percent chance of snow again tomorrow and a 25\n    percent chance of a nice day (nice, like when the witch with the monkeys\n    is melting).\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n    &gt;&gt;&gt; steady_state(p)\n    array([[ 0.4],\n           [ 0.2],\n           [ 0.4]])\n    Thus, the long run distribution for Oz is to have 40 percent of the\n    days classified as Rain, 20 percent as Nice, and 40 percent as Snow\n    (states are mutually exclusive).\n    \"\"\"\n\n    v, d = la.eig(np.transpose(P))\n\n    # for a regular P maximum eigenvalue will be 1\n    mv = max(v.real)  # Use real part for comparison\n    # find its position\n    i = v.real.tolist().index(mv)\n\n    # normalize eigenvector corresponding to the eigenvalue 1\n    # Take real part to avoid complex warning\n    eigenvector = d[:, i].real\n    return eigenvector / np.sum(eigenvector)\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.time_swap_array","title":"<code>time_swap_array(posterior)</code>","text":"<p>Time swap.</p> <p>Note: it is often possible to simply shuffle the time bins, and not the actual data, for computational efficiency. Still, this function works as expected.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Time-swapped posterior matrix.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def time_swap_array(posterior):\n    \"\"\"\n    Time swap.\n\n    Note: it is often possible to simply shuffle the time bins, and not the actual data, for computational\n    efficiency. Still, this function works as expected.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n\n    Returns\n    -------\n    out : np.ndarray\n        Time-swapped posterior matrix.\n    \"\"\"\n    out = copy.deepcopy(posterior)\n    rows, cols = posterior.shape\n\n    colidx = np.arange(cols)\n    shuffle_cols = np.random.permutation(colidx)\n    out = out[:, shuffle_cols]\n\n    return out\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.trajectory_score_array","title":"<code>trajectory_score_array(posterior, slope=None, intercept=None, w=None, weights=None, normalize=False)</code>","text":"<p>Compute the trajectory score for a given posterior matrix and line parameters.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>slope</code> <code>float</code> <p>Slope of the line. If None, estimated from the data.</p> <code>None</code> <code>intercept</code> <code>float</code> <p>Intercept of the line. If None, estimated from the data.</p> <code>None</code> <code>w</code> <code>int</code> <p>Half band width for calculating the trajectory score. Default is 0.</p> <code>None</code> <code>weights</code> <code>array - like</code> <p>Weights for the band around the line (not yet implemented).</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the score by the number of non-NaN bins.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>Trajectory score for the event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def trajectory_score_array(\n    posterior, slope=None, intercept=None, w=None, weights=None, normalize=False\n):\n    \"\"\"\n    Compute the trajectory score for a given posterior matrix and line parameters.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    slope : float, optional\n        Slope of the line. If None, estimated from the data.\n    intercept : float, optional\n        Intercept of the line. If None, estimated from the data.\n    w : int, optional\n        Half band width for calculating the trajectory score. Default is 0.\n    weights : array-like, optional\n        Weights for the band around the line (not yet implemented).\n    normalize : bool, optional\n        If True, normalize the score by the number of non-NaN bins.\n\n    Returns\n    -------\n    score : float\n        Trajectory score for the event.\n    \"\"\"\n\n    rows, cols = posterior.shape\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n    if slope is None or intercept is None:\n        slope, intercept, _ = linregress_array(posterior=posterior)\n\n    x = np.arange(cols)\n    line_y = np.round((slope * x + intercept))  # in position bin #s\n\n    # idea: cycle each column so that the top w rows are the band surrounding the regression line\n\n    if np.isnan(slope):  # this will happen if we have 0 or only 1 decoded bins\n        return np.nan\n    else:\n        temp = column_cycle_array(posterior, -line_y + w)\n\n    if normalize:\n        num_non_nan_bins = round(np.nansum(posterior))\n    else:\n        num_non_nan_bins = 1\n\n    return np.nansum(temp[: 2 * w + 1, :]) / num_non_nan_bins\n</code></pre>"},{"location":"reference/analysis/#nelpy.analysis.trajectory_score_bst","title":"<code>trajectory_score_bst(bst, tuningcurve, w=None, n_shuffles=250, weights=None, normalize=False)</code>","text":"<p>Compute the trajectory scores from Davidson et al. for each event in the BinnedSpikeTrainArray.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve to decode events in bst.</p> required <code>w</code> <code>int</code> <p>Half band width for calculating the trajectory score. Default is 0.</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>weights</code> <code>array - like</code> <p>Weights for the band around the line (not yet implemented).</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the score by the number of non-NaN bins.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Trajectory scores for each event.</p> <code>scores_time_swap</code> <code>ndarray</code> <p>Shuffled scores using time swap.</p> <code>scores_col_cycle</code> <code>ndarray</code> <p>Shuffled scores using column cycle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def trajectory_score_bst(\n    bst, tuningcurve, w=None, n_shuffles=250, weights=None, normalize=False\n):\n    \"\"\"\n    Compute the trajectory scores from Davidson et al. for each event in the BinnedSpikeTrainArray.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve to decode events in bst.\n    w : int, optional\n        Half band width for calculating the trajectory score. Default is 0.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    weights : array-like, optional\n        Weights for the band around the line (not yet implemented).\n    normalize : bool, optional\n        If True, normalize the score by the number of non-NaN bins.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Trajectory scores for each event.\n    scores_time_swap : np.ndarray\n        Shuffled scores using time swap.\n    scores_col_cycle : np.ndarray\n        Shuffled scores using column cycle.\n    \"\"\"\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    # idea: cycle each column so that the top w rows are the band\n    # surrounding the regression line\n\n    scores = np.zeros(bst.n_epochs)\n    if n_shuffles &gt; 0:\n        scores_time_swap = np.zeros((n_shuffles, bst.n_epochs))\n        scores_col_cycle = np.zeros((n_shuffles, bst.n_epochs))\n\n    for idx in range(bst.n_epochs):\n        posterior_array = posterior[:, bdries[idx] : bdries[idx + 1]]\n        scores[idx] = trajectory_score_array(\n            posterior=posterior_array, w=w, normalize=normalize\n        )\n        for shflidx in range(n_shuffles):\n            # time swap:\n\n            posterior_ts = time_swap_array(posterior_array)\n            posterior_cs = column_cycle_array(posterior_array)\n            scores_time_swap[shflidx, idx] = trajectory_score_array(\n                posterior=posterior_ts, w=w, normalize=normalize\n            )\n            scores_col_cycle[shflidx, idx] = trajectory_score_array(\n                posterior=posterior_cs, w=w, normalize=normalize\n            )\n\n    if n_shuffles &gt; 0:\n        return scores, scores_time_swap, scores_col_cycle\n    return scores\n</code></pre>"},{"location":"reference/auxiliary/","title":"Auxiliary API Reference","text":"<p>nelpy auxiliary objects</p> <p><code>nelpy</code> is a neuroelectrophysiology object model and data analysis suite.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.DirectionalTuningCurve1D","title":"<code>DirectionalTuningCurve1D</code>","text":"<p>               Bases: <code>TuningCurve1D</code></p> <p>Directional tuning curves (1-dimensional) of multiple units.</p> <p>Parameters:</p> Name Type Description Default <code>bst_l2r</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array for left-to-right direction.</p> required <code>bst_r2l</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array for right-to-left direction.</p> required <code>bst_combined</code> <code>BinnedSpikeTrainArray</code> <p>Combined binned spike train array.</p> required <code>extern</code> <code>array - like</code> <p>External correlates (e.g., position).</p> required <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian smoothing.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Truncation parameter for smoothing.</p> <code>None</code> <code>n_extern</code> <code>int</code> <p>Number of bins for external correlates.</p> <code>None</code> <code>transform_func</code> <code>callable</code> <p>Function to transform external correlates.</p> <code>None</code> <code>minbgrate</code> <code>float</code> <p>Minimum background firing rate.</p> <code>None</code> <code>extmin</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>extmax</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>extlabels</code> <code>list</code> <p>Labels for external correlates.</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>Unit IDs.</p> <code>None</code> <code>unit_labels</code> <code>list</code> <p>Unit labels.</p> <code>None</code> <code>unit_tags</code> <code>list</code> <p>Unit tags.</p> <code>None</code> <code>label</code> <code>str</code> <p>Label for the tuning curve.</p> <code>None</code> <code>min_peakfiringrate</code> <code>float</code> <p>Minimum peak firing rate.</p> <code>None</code> <code>max_avgfiringrate</code> <code>float</code> <p>Maximum average firing rate.</p> <code>None</code> <code>unimodal</code> <code>bool</code> <p>If True, enforce unimodality.</p> <code>False</code> <code>empty</code> <code>bool</code> <p>If True, create an empty DirectionalTuningCurve1D.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>All attributes of TuningCurve1D, plus direction-specific attributes.</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>class DirectionalTuningCurve1D(TuningCurve1D):\n    \"\"\"\n    Directional tuning curves (1-dimensional) of multiple units.\n\n    Parameters\n    ----------\n    bst_l2r : BinnedSpikeTrainArray\n        Binned spike train array for left-to-right direction.\n    bst_r2l : BinnedSpikeTrainArray\n        Binned spike train array for right-to-left direction.\n    bst_combined : BinnedSpikeTrainArray\n        Combined binned spike train array.\n    extern : array-like\n        External correlates (e.g., position).\n    sigma : float, optional\n        Standard deviation for Gaussian smoothing.\n    truncate : float, optional\n        Truncation parameter for smoothing.\n    n_extern : int, optional\n        Number of bins for external correlates.\n    transform_func : callable, optional\n        Function to transform external correlates.\n    minbgrate : float, optional\n        Minimum background firing rate.\n    extmin, extmax : float, optional\n        Extent of the external correlates.\n    extlabels : list, optional\n        Labels for external correlates.\n    unit_ids : list, optional\n        Unit IDs.\n    unit_labels : list, optional\n        Unit labels.\n    unit_tags : list, optional\n        Unit tags.\n    label : str, optional\n        Label for the tuning curve.\n    min_peakfiringrate : float, optional\n        Minimum peak firing rate.\n    max_avgfiringrate : float, optional\n        Maximum average firing rate.\n    unimodal : bool, optional\n        If True, enforce unimodality.\n    empty : bool, optional\n        If True, create an empty DirectionalTuningCurve1D.\n\n    Attributes\n    ----------\n    All attributes of TuningCurve1D, plus direction-specific attributes.\n    \"\"\"\n\n    __attributes__ = [\"_unit_ids_l2r\", \"_unit_ids_r2l\"]\n    __attributes__.extend(TuningCurve1D.__attributes__)\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def __init__(\n        self,\n        *,\n        bst_l2r,\n        bst_r2l,\n        bst_combined,\n        extern,\n        sigma=None,\n        truncate=None,\n        n_extern=None,\n        transform_func=None,\n        minbgrate=None,\n        extmin=0,\n        extmax=1,\n        extlabels=None,\n        unit_ids=None,\n        unit_labels=None,\n        unit_tags=None,\n        label=None,\n        empty=False,\n        min_peakfiringrate=None,\n        max_avgfiringrate=None,\n        unimodal=False,\n    ):\n        \"\"\"\n\n        If sigma is nonzero, then smoothing is applied.\n\n        We always require bst and extern, and then some combination of\n            (1) bin edges, transform_func*\n            (2) n_extern, transform_func*\n            (3) n_extern, x_min, x_max, transform_func*\n\n            transform_func operates on extern and returns a value that TuninCurve1D can interpret. If no transform is specified, the identity operator is assumed.\n        \"\"\"\n        # TODO: input validation\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        # self._bst_combined = bst_combined\n        self._extern = extern\n\n        if min_peakfiringrate is None:\n            min_peakfiringrate = 1.5  # Hz minimum peak firing rate\n\n        if max_avgfiringrate is None:\n            max_avgfiringrate = 10  # Hz maximum average firing rate\n\n        if minbgrate is None:\n            minbgrate = 0.01  # Hz minimum background firing rate\n\n        if n_extern is not None:\n            if extmin is not None and extmax is not None:\n                self._bins = np.linspace(extmin, extmax, n_extern + 1)\n            else:\n                raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        self._min_peakfiringrate = min_peakfiringrate\n        self._max_avgfiringrate = max_avgfiringrate\n        self._unimodal = unimodal\n        self._unit_ids = bst_combined.unit_ids\n        self._unit_labels = bst_combined.unit_labels\n        self._unit_tags = bst_combined.unit_tags  # no input validation yet\n        self.label = label\n\n        if transform_func is None:\n            self.trans_func = self._trans_func\n\n        # left to right:\n        self._bst = bst_l2r\n        # compute occupancy\n        self._occupancy = self._compute_occupancy()\n        # compute ratemap (in Hz)\n        self._ratemap = self._compute_ratemap()\n        # normalize firing rate by occupancy\n        self._ratemap = self._normalize_firing_rate_by_occupancy()\n        # enforce minimum background firing rate\n        self._ratemap[self._ratemap &lt; minbgrate] = (\n            minbgrate  # background firing rate of 0.01 Hz\n        )\n        if sigma is not None:\n            if sigma &gt; 0:\n                self.smooth(sigma=sigma, truncate=truncate, inplace=True)\n        # store l2r ratemap\n        ratemap_l2r = self.ratemap.copy()\n\n        # right to left:\n        self._bst = bst_r2l\n        # compute occupancy\n        self._occupancy = self._compute_occupancy()\n        # compute ratemap (in Hz)\n        self._ratemap = self._compute_ratemap()\n        # normalize firing rate by occupancy\n        self._ratemap = self._normalize_firing_rate_by_occupancy()\n        # enforce minimum background firing rate\n        self._ratemap[self._ratemap &lt; minbgrate] = (\n            minbgrate  # background firing rate of 0.01 Hz\n        )\n        if sigma is not None:\n            if sigma &gt; 0:\n                self.smooth(sigma=sigma, truncate=truncate, inplace=True)\n        # store r2l ratemap\n        ratemap_r2l = self.ratemap.copy()\n\n        # combined (non-directional):\n        self._bst = bst_combined\n        # compute occupancy\n        self._occupancy = self._compute_occupancy()\n        # compute ratemap (in Hz)\n        self._ratemap = self._compute_ratemap()\n        # normalize firing rate by occupancy\n        self._ratemap = self._normalize_firing_rate_by_occupancy()\n        # enforce minimum background firing rate\n        self._ratemap[self._ratemap &lt; minbgrate] = (\n            minbgrate  # background firing rate of 0.01 Hz\n        )\n        if sigma is not None:\n            if sigma &gt; 0:\n                self.smooth(sigma=sigma, truncate=truncate, inplace=True)\n        # store combined ratemap\n        # ratemap = self.ratemap\n\n        # determine unit membership:\n        l2r_unit_ids = self.restrict_units(ratemap_l2r)\n        r2l_unit_ids = self.restrict_units(ratemap_r2l)\n\n        common_unit_ids = list(r2l_unit_ids.intersection(l2r_unit_ids))\n        l2r_only_unit_ids = list(l2r_unit_ids.difference(common_unit_ids))\n        r2l_only_unit_ids = list(r2l_unit_ids.difference(common_unit_ids))\n\n        # update ratemap with directional tuning curves\n        for unit_id in l2r_only_unit_ids:\n            unit_idx = self.unit_ids.index(unit_id)\n            # print('replacing', self._ratemap[unit_idx, :])\n            # print('with', ratemap_l2r[unit_idx, :])\n            self._ratemap[unit_idx, :] = ratemap_l2r[unit_idx, :]\n        for unit_id in r2l_only_unit_ids:\n            unit_idx = self.unit_ids.index(unit_id)\n            self._ratemap[unit_idx, :] = ratemap_r2l[unit_idx, :]\n\n        self._unit_ids_l2r = l2r_only_unit_ids\n        self._unit_ids_r2l = r2l_only_unit_ids\n\n        # optionally detach _bst and _extern to save space when pickling, for example\n        self._detach()\n\n    def restrict_units(self, ratemap=None):\n        if ratemap is None:\n            ratemap = self.ratemap\n\n        # enforce minimum peak firing rate\n        unit_ids_to_keep = set(\n            np.asanyarray(self.unit_ids)[\n                np.argwhere(ratemap.max(axis=1) &gt; self._min_peakfiringrate)\n                .squeeze()\n                .tolist()\n            ]\n        )\n        # enforce maximum average firing rate\n        unit_ids_to_keep = unit_ids_to_keep.intersection(\n            set(\n                np.asanyarray(self.unit_ids)[\n                    np.argwhere(ratemap.mean(axis=1) &lt; self._max_avgfiringrate)\n                    .squeeze()\n                    .tolist()\n                ]\n            )\n        )\n        # remove multimodal units\n        if self._unimodal:\n            raise NotImplementedError(\n                \"restriction to unimodal cells not yet implemented!\"\n            )\n            # placecellidx = placecellidx.intersection(set(unimodal_cells))\n\n        return unit_ids_to_keep\n\n    @property\n    def unit_ids_l2r(self):\n        return self._unit_ids_l2r\n\n    @property\n    def unit_ids_r2l(self):\n        return self._unit_ids_r2l\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.ResultsContainer","title":"<code>ResultsContainer</code>","text":"<p>               Bases: <code>object</code></p> <p>Extremely simple namespace for passing around and pickling data.</p> <p>This container is used to group, store, and load results from analyses or experiments.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>Description of the results container.</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to store in the container.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>description</code> <code>str</code> <p>Description of the results container.</p> <code>_index</code> <code>int</code> <p>Internal index for iteration.</p> Source code in <code>nelpy/auxiliary/_results.py</code> <pre><code>class ResultsContainer(object):\n    \"\"\"\n    Extremely simple namespace for passing around and pickling data.\n\n    This container is used to group, store, and load results from analyses or experiments.\n\n    Parameters\n    ----------\n    description : str, optional\n        Description of the results container.\n    **kwargs :\n        Additional attributes to store in the container.\n\n    Attributes\n    ----------\n    description : str\n        Description of the results container.\n    _index : int\n        Internal index for iteration.\n    \"\"\"\n\n    def __init__(self, *args, description=None, **kwargs):\n        \"\"\"\n        Initialize a ResultsContainer.\n\n        Parameters\n        ----------\n        description : str, optional\n            Description of the results container.\n        **kwargs :\n            Additional attributes to store in the container.\n        \"\"\"\n        kwargs[\"description\"] = description\n\n        if len(args) &gt; 0:\n            raise NotImplementedError(\"only keyword arguments accepted!\")\n\n        for key, val in kwargs.items():\n            setattr(self, key, val)\n\n        self._index = 0\n\n    # def __init__(self, *args, description=None, **kwargs):\n    #     kwargs['description'] = description\n\n    #     # BEGIN very hacky code to get *args names; might break!\n    #     # see http://stackoverflow.com/questions/2749796/how-to-get-the-original-variable-name-of-variable-passed-to-a-function\n    #     frame = inspect.currentframe()\n    #     frame = inspect.getouterframes(frame)[1]\n    #     string = inspect.getframeinfo(frame[0]).code_context[0].strip()\n    #     _args = string[string.find('(') + 1:-1].split(',')\n\n    #     names = []\n    #     for i in _args:\n    #         if i.find('=') != -1:\n    #             pass\n    #             # names.append(i.split('=')[1].strip())\n    #         else:\n    #             names.append(i)\n\n    #     for ii, arg in enumerate(args):\n    #         setattr(self, names[ii], arg)\n    #     # END very hacky code to get *args names; might break!\n\n    #     for key, val in kwargs.items():\n    #         setattr(self, key, val)\n\n    #     self.description = None\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the ResultsContainer.\n\n        Returns\n        -------\n        repr_str : str\n            String representation of the ResultsContainer, including address, number of objects, and description.\n        \"\"\"\n        if self.isempty:\n            return \"&lt;Empty ResultsContainer&gt;\"\n        if self.n_objects == 1:\n            n_str = \" (\" + str(self.n_objects) + \" object)\"\n        else:\n            n_str = \" (\" + str(self.n_objects) + \" objects)\"\n        address_str = \" at \" + str(hex(id(self)))\n        descr_str = \"\"\n        if self.description is not None:\n            descr_str = \"\\n**Description:** \" + str(self.description)\n        return \"&lt;ResultsContainer\" + address_str + n_str + \"&gt;\" + descr_str\n\n    # def __iter__(self):\n    #     self._index = 0\n    #     return self\n\n    # def __next__(self):\n    #     index = self._index\n\n    #     if index &gt; self.n_objects - 1:\n    #         raise StopIteration\n    #     self._index += 1\n    #     return self[index]\n\n    # def __getitem__(self, idx):\n    #     keys = [key for key in self.__dict__ if key not in ['description', '_index']]\n    #     keys.sort()\n    #     return keys[idx]\n\n    @property\n    def isempty(self):\n        \"\"\"Empty ResultsContainer.\"\"\"\n        return self.n_objects == 0\n\n    @property\n    def n_objects(self):\n        \"\"\"(int) Number of objects in ResultsContainer.\"\"\"\n        return len(list(self.__dict__.values())) - 2\n\n    @property\n    def keys(self):\n        \"\"\"(list) Key names in ResultsContainer.\"\"\"\n        keys = [key for key in self.__dict__ if key not in [\"description\", \"_index\"]]\n        keys.sort()\n        return keys\n\n    def save_pkl(self, fname, zip=True, overwrite=False):\n        \"\"\"Write pickled data to disk, possible compressing.\"\"\"\n        if os.path.isfile(fname):\n            # file exists\n            if overwrite:\n                pass\n            else:\n                print('File \"{}\" already exists! Aborting...'.format(fname))\n                return\n        if zip:\n            save_large_file_without_zip = False\n            with gzip.open(fname, \"wb\") as fid:\n                try:\n                    pickle.dump(self, fid)\n                except OverflowError:\n                    print(\n                        \"writing to disk using protocol=4, which supports file sizes &gt; 4 GiB, and ignoring zip=True (zip is not supported for large files yet)\"\n                    )\n                    save_large_file_without_zip = True\n\n            if save_large_file_without_zip:\n                with open(fname, \"wb\") as fid:\n                    pickle.dump(self, fid, protocol=4)\n        else:\n            with open(fname, \"wb\") as fid:\n                try:\n                    pickle.dump(self, fid)\n                except OverflowError:\n                    print(\n                        \"writing to disk using protocol=4, which supports file sizes &gt; 4 GiB\"\n                    )\n                    pickle.dump(self, fid, protocol=4)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.ResultsContainer.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>Empty ResultsContainer.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.ResultsContainer.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>(list) Key names in ResultsContainer.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.ResultsContainer.n_objects","title":"<code>n_objects</code>  <code>property</code>","text":"<p>(int) Number of objects in ResultsContainer.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.ResultsContainer.save_pkl","title":"<code>save_pkl(fname, zip=True, overwrite=False)</code>","text":"<p>Write pickled data to disk, possible compressing.</p> Source code in <code>nelpy/auxiliary/_results.py</code> <pre><code>def save_pkl(self, fname, zip=True, overwrite=False):\n    \"\"\"Write pickled data to disk, possible compressing.\"\"\"\n    if os.path.isfile(fname):\n        # file exists\n        if overwrite:\n            pass\n        else:\n            print('File \"{}\" already exists! Aborting...'.format(fname))\n            return\n    if zip:\n        save_large_file_without_zip = False\n        with gzip.open(fname, \"wb\") as fid:\n            try:\n                pickle.dump(self, fid)\n            except OverflowError:\n                print(\n                    \"writing to disk using protocol=4, which supports file sizes &gt; 4 GiB, and ignoring zip=True (zip is not supported for large files yet)\"\n                )\n                save_large_file_without_zip = True\n\n        if save_large_file_without_zip:\n            with open(fname, \"wb\") as fid:\n                pickle.dump(self, fid, protocol=4)\n    else:\n        with open(fname, \"wb\") as fid:\n            try:\n                pickle.dump(self, fid)\n            except OverflowError:\n                print(\n                    \"writing to disk using protocol=4, which supports file sizes &gt; 4 GiB\"\n                )\n                pickle.dump(self, fid, protocol=4)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.Session","title":"<code>Session</code>","text":"<p>Nelpy session with common clock.</p> Source code in <code>nelpy/auxiliary/_session.py</code> <pre><code>class Session:\n    \"\"\"Nelpy session with common clock.\"\"\"\n\n    __attributes__ = [\"_animal\", \"_label\", \"_st\", \"_extern\", \"_mua\"]\n\n    def __init__(\n        self, animal=None, st=None, extern=None, mua=None, label=None, empty=False\n    ):\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        self._animal = animal\n        self._extern = extern\n        self._st = st\n        self._mua = mua\n        self.label = label\n\n    @property\n    def animal(self):\n        return self._animal\n\n    @property\n    def extern(self):\n        return self._extern\n\n    @property\n    def st(self):\n        return self._st\n\n    @property\n    def mua(self):\n        return self._mua\n\n    @property\n    def label(self):\n        \"\"\"Label pertaining to the source of the spike train.\"\"\"\n        if self._label is None:\n            warnings.warn(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.Session.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label pertaining to the source of the spike train.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D","title":"<code>TuningCurve1D</code>","text":"<p>Tuning curves (1-dimensional) of multiple units.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array for tuning curve estimation.</p> <code>None</code> <code>extern</code> <code>array - like</code> <p>External correlates (e.g., position).</p> <code>None</code> <code>ratemap</code> <code>ndarray</code> <p>Precomputed rate map.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian smoothing.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Truncation parameter for smoothing.</p> <code>None</code> <code>n_extern</code> <code>int</code> <p>Number of bins for external correlates.</p> <code>None</code> <code>transform_func</code> <code>callable</code> <p>Function to transform external correlates.</p> <code>None</code> <code>minbgrate</code> <code>float</code> <p>Minimum background firing rate.</p> <code>None</code> <code>extmin</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>extmax</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>extlabels</code> <code>list</code> <p>Labels for external correlates.</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>Unit IDs.</p> <code>None</code> <code>unit_labels</code> <code>list</code> <p>Unit labels.</p> <code>None</code> <code>unit_tags</code> <code>list</code> <p>Unit tags.</p> <code>None</code> <code>label</code> <code>str</code> <p>Label for the tuning curve.</p> <code>None</code> <code>min_duration</code> <code>float</code> <p>Minimum duration for occupancy.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>If True, create an empty TuningCurve1D.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>ratemap</code> <code>ndarray</code> <p>The 1D rate map.</p> <code>occupancy</code> <code>ndarray</code> <p>Occupancy map.</p> <code>unit_ids</code> <code>list</code> <p>Unit IDs.</p> <code>unit_labels</code> <code>list</code> <p>Unit labels.</p> <code>unit_tags</code> <code>list</code> <p>Unit tags.</p> <code>label</code> <code>str</code> <p>Label for the tuning curve.</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>class TuningCurve1D:\n    \"\"\"\n    Tuning curves (1-dimensional) of multiple units.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray, optional\n        Binned spike train array for tuning curve estimation.\n    extern : array-like, optional\n        External correlates (e.g., position).\n    ratemap : np.ndarray, optional\n        Precomputed rate map.\n    sigma : float, optional\n        Standard deviation for Gaussian smoothing.\n    truncate : float, optional\n        Truncation parameter for smoothing.\n    n_extern : int, optional\n        Number of bins for external correlates.\n    transform_func : callable, optional\n        Function to transform external correlates.\n    minbgrate : float, optional\n        Minimum background firing rate.\n    extmin, extmax : float, optional\n        Extent of the external correlates.\n    extlabels : list, optional\n        Labels for external correlates.\n    unit_ids : list, optional\n        Unit IDs.\n    unit_labels : list, optional\n        Unit labels.\n    unit_tags : list, optional\n        Unit tags.\n    label : str, optional\n        Label for the tuning curve.\n    min_duration : float, optional\n        Minimum duration for occupancy.\n    empty : bool, optional\n        If True, create an empty TuningCurve1D.\n\n    Attributes\n    ----------\n    ratemap : np.ndarray\n        The 1D rate map.\n    occupancy : np.ndarray\n        Occupancy map.\n    unit_ids : list\n        Unit IDs.\n    unit_labels : list\n        Unit labels.\n    unit_tags : list\n        Unit tags.\n    label : str\n        Label for the tuning curve.\n    \"\"\"\n\n    __attributes__ = [\n        \"_ratemap\",\n        \"_occupancy\",\n        \"_unit_ids\",\n        \"_unit_labels\",\n        \"_unit_tags\",\n        \"_label\",\n    ]\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def __init__(\n        self,\n        *,\n        bst=None,\n        extern=None,\n        ratemap=None,\n        sigma=None,\n        truncate=None,\n        n_extern=None,\n        transform_func=None,\n        minbgrate=None,\n        extmin=0,\n        extmax=1,\n        extlabels=None,\n        unit_ids=None,\n        unit_labels=None,\n        unit_tags=None,\n        label=None,\n        min_duration=None,\n        empty=False,\n    ):\n        \"\"\"\n\n        If sigma is nonzero, then smoothing is applied.\n\n        We always require bst and extern, and then some combination of\n            (1) bin edges, transform_func*\n            (2) n_extern, transform_func*\n            (3) n_extern, x_min, x_max, transform_func*\n\n            transform_func operates on extern and returns a value that TuninCurve1D can interpret. If no transform is specified, the identity operator is assumed.\n        \"\"\"\n        # TODO: input validation\n        if not empty:\n            if ratemap is None:\n                assert bst is not None, (\n                    \"bst must be specified or ratemap must be specified!\"\n                )\n                assert extern is not None, (\n                    \"extern must be specified or ratemap must be specified!\"\n                )\n            else:\n                assert bst is None, \"ratemap and bst cannot both be specified!\"\n                assert extern is None, \"ratemap and extern cannot both be specified!\"\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        if ratemap is not None:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._init_from_ratemap(\n                ratemap=ratemap,\n                extmin=extmin,\n                extmax=extmax,\n                extlabels=extlabels,\n                unit_ids=unit_ids,\n                unit_labels=unit_labels,\n                unit_tags=unit_tags,\n                label=label,\n            )\n            return\n\n        self._bst = bst\n        self._extern = extern\n\n        if minbgrate is None:\n            minbgrate = 0.01  # Hz minimum background firing rate\n\n        if n_extern is not None:\n            if extmin is not None and extmax is not None:\n                self._bins = np.linspace(extmin, extmax, n_extern + 1)\n            else:\n                raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        if min_duration is None:\n            min_duration = 0\n\n        self._min_duration = min_duration\n\n        self._unit_ids = bst.unit_ids\n        self._unit_labels = bst.unit_labels\n        self._unit_tags = bst.unit_tags  # no input validation yet\n        self.label = label\n\n        if transform_func is None:\n            self.trans_func = self._trans_func\n\n        # compute occupancy\n        self._occupancy = self._compute_occupancy()\n        # compute ratemap (in Hz)\n        self._ratemap = self._compute_ratemap()\n        # normalize firing rate by occupancy\n        self._ratemap = self._normalize_firing_rate_by_occupancy()\n        # enforce minimum background firing rate\n        self._ratemap[self._ratemap &lt; minbgrate] = (\n            minbgrate  # background firing rate of 0.01 Hz\n        )\n\n        if sigma is not None:\n            if sigma &gt; 0:\n                self.smooth(sigma=sigma, truncate=truncate, inplace=True)\n\n        # optionally detach _bst and _extern to save space when pickling, for example\n        self._detach()\n\n    @property\n    def is2d(self):\n        return False\n\n    def spatial_information(self):\n        \"\"\"Compute the spatial information...\n\n        The specificity index examines the amount of information\n        (in bits) that a single spike conveys about the animal's\n        location (i.e., how well cell firing predicts the animal's\n        location).The spatial information content of cell discharge was\n        calculated using the formula:\n            information content = \\Sum P_i(R_i/R)log_2(R_i/R)\n        where i is the bin number, P_i, is the probability for occupancy\n        of bin i, R_i, is the mean firing rate for bin i, and R is the\n        overall mean firing rate.\n\n        In order to account for the effects of low firing rates (with\n        fewer spikes there is a tendency toward higher information\n        content) or random bursts of firing, the spike firing\n        time-series was randomly offset in time from the rat location\n        time-series, and the information content was calculated. A\n        distribution of the information content based on 100 such random\n        shifts was obtained and was used to compute a standardized score\n        (Zscore) of information content for that cell. While the\n        distribution is not composed of independent samples, it was\n        nominally normally distributed, and a Z value of 2.29 was chosen\n        as a cut-off for significance (the equivalent of a one-tailed\n        t-test with P = 0.01 under a normal distribution).\n\n        Reference(s)\n        ------------\n        Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n            and Skaggs, W. E. (1994). \"Spatial information content and\n            reliability of hippocampal CA1 neurons: effects of visual\n            input\", Hippocampus, 4(4), 410-421.\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        si : array of shape (n_units,)\n            spatial information (in bits) per unit\n        sparsity: array of shape (n_units,)\n            sparsity (in percent) for each unit\n        \"\"\"\n\n        return utils.spatial_information(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def information_rate(self):\n        \"\"\"Compute the information rate...\"\"\"\n        return utils.information_rate(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def spatial_selectivity(self):\n        \"\"\"Compute the spatial selectivity...\"\"\"\n        return utils.spatial_selectivity(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def spatial_sparsity(self):\n        \"\"\"Compute the firing sparsity...\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        si : array of shape (n_units,)\n            spatial information (in bits) per unit\n        sparsity: array of shape (n_units,)\n            sparsity (in percent) for each unit\n        \"\"\"\n        return utils.spatial_sparsity(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def _init_from_ratemap(\n        self,\n        ratemap,\n        occupancy=None,\n        extmin=0,\n        extmax=1,\n        extlabels=None,\n        unit_ids=None,\n        unit_labels=None,\n        unit_tags=None,\n        label=None,\n    ):\n        \"\"\"Initialize a TuningCurve1D object from a ratemap.\n\n        Parameters\n        ----------\n        ratemap : array\n            Array of shape (n_units, n_extern)\n\n        Returns\n        -------\n\n        \"\"\"\n        n_units, n_extern = ratemap.shape\n\n        if occupancy is None:\n            # assume uniform occupancy\n            self._occupancy = np.ones(n_extern)\n\n        if extmin is None:\n            extmin = 0\n        if extmax is None:\n            extmax = extmin + 1\n\n        self._bins = np.linspace(extmin, extmax, n_extern + 1)\n        self._ratemap = ratemap\n\n        # inherit unit IDs if available, otherwise initialize to default\n        if unit_ids is None:\n            unit_ids = list(range(1, n_units + 1))\n\n        unit_ids = np.array(unit_ids, ndmin=1)  # standardize unit_ids\n\n        # if unit_labels is empty, default to unit_ids\n        if unit_labels is None:\n            unit_labels = unit_ids\n\n        unit_labels = np.array(unit_labels, ndmin=1)  # standardize\n\n        self._unit_ids = unit_ids\n        self._unit_labels = unit_labels\n        self._unit_tags = unit_tags  # no input validation yet\n        if label is not None:\n            self.label = label\n\n        return self\n\n    def mean(self, *, axis=None):\n        \"\"\"Returns the mean of firing rate (in Hz).\n        Parameters\n        ----------\n        axis : int, optional\n            When axis is None, the global mean firing rate is returned.\n            When axis is 0, the mean firing rates across units, as a\n            function of the external correlate (e.g. position) are\n            returned.\n            When axis is 1, the mean firing rate for each unit is\n            returned.\n        Returns\n        -------\n        mean :\n        \"\"\"\n        means = np.mean(self.ratemap, axis=axis).squeeze()\n        if means.size == 1:\n            return np.asarray(means).item()\n        return means\n\n    def max(self, *, axis=None):\n        \"\"\"Returns the mean of firing rate (in Hz).\n        Parameters\n        ----------\n        axis : int, optional\n            When axis is None, the global mean firing rate is returned.\n            When axis is 0, the mean firing rates across units, as a\n            function of the external correlate (e.g. position) are\n            returned.\n            When axis is 1, the mean firing rate for each unit is\n            returned.\n        Returns\n        -------\n        mean :\n        \"\"\"\n        maxes = np.max(self.ratemap, axis=axis).squeeze()\n        if maxes.size == 1:\n            return np.asarray(maxes).item()\n        return maxes\n\n    def min(self, *, axis=None):\n        \"\"\"Returns the mean of firing rate (in Hz).\n        Parameters\n        ----------\n        axis : int, optional\n            When axis is None, the global mean firing rate is returned.\n            When axis is 0, the mean firing rates across units, as a\n            function of the external correlate (e.g. position) are\n            returned.\n            When axis is 1, the mean firing rate for each unit is\n            returned.\n        Returns\n        -------\n        mean :\n        \"\"\"\n        mins = np.min(self.ratemap, axis=axis).squeeze()\n        if mins.size == 1:\n            return np.asarray(mins).item()\n        return mins\n\n    @property\n    def ratemap(self):\n        return self._ratemap\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        return len(self.bins) - 1\n\n    @property\n    def occupancy(self):\n        return self._occupancy\n\n    @property\n    def bins(self):\n        \"\"\"External correlate bins.\"\"\"\n        return self._bins\n\n    @property\n    def bin_centers(self):\n        \"\"\"External correlate bin centers.\"\"\"\n        return (self.bins + (self.bins[1] - self.bins[0]) / 2)[:-1]\n\n    def _trans_func(self, extern, at):\n        \"\"\"Default transform function to map extern into numerical bins\"\"\"\n\n        _, ext = extern.asarray(at=at)\n\n        return np.atleast_1d(ext)\n\n    def _compute_occupancy(self):\n        # Make sure that self._bst_centers fall within not only the support\n        # of extern, but also within the extreme sample times; otherwise,\n        # interpolation will yield NaNs at the extremes. Indeed, when we have\n        # sample times within a support epoch, we can assume that the signal\n        # stayed roughly constant for that one sample duration.\n\n        if self._bst._bin_centers[0] &lt; self._extern.time[0]:\n            self._extern = copy.copy(self._extern)\n            self._extern.time[0] = self._bst._bin_centers[0]\n            self._extern._interp = None\n            # raise ValueError('interpolated sample requested before first sample of extern!')\n        if self._bst._bin_centers[-1] &gt; self._extern.time[-1]:\n            self._extern = copy.copy(self._extern)\n            self._extern.time[-1] = self._bst._bin_centers[-1]\n            self._extern._interp = None\n            # raise ValueError('interpolated sample requested after last sample of extern!')\n\n        ext = self.trans_func(self._extern, at=self._bst.bin_centers)\n\n        xmin = self.bins[0]\n        xmax = self.bins[-1]\n        occupancy, _ = np.histogram(ext, bins=self.bins, range=(xmin, xmax))\n        # xbins = (bins + xmax/n_xbins)[:-1] # for plotting\n        return occupancy\n\n    def _compute_ratemap(self, min_duration=None):\n        if min_duration is None:\n            min_duration = self._min_duration\n\n        ext = self.trans_func(self._extern, at=self._bst.bin_centers)\n\n        ext_bin_idx = np.squeeze(np.digitize(ext, self.bins, right=True))\n        # make sure that all the events fit between extmin and extmax:\n        # TODO: this might rather be a warning, but it's a pretty serious warning...\n        if ext_bin_idx.max() &gt; self.n_bins:\n            raise ValueError(\"ext values greater than 'ext_max'\")\n        if ext_bin_idx.min() == 0:\n            raise ValueError(\"ext values less than 'ext_min'\")\n\n        ratemap = np.zeros((self.n_units, self.n_bins))\n\n        for tt, bidx in enumerate(ext_bin_idx):\n            ratemap[:, bidx - 1] += self._bst.data[:, tt]\n\n        # apply minimum observation duration\n        for uu in range(self.n_units):\n            ratemap[uu][self.occupancy * self._bst.ds &lt; min_duration] = 0\n\n        return ratemap / self._bst.ds\n\n    def normalize(self, inplace=False):\n        if not inplace:\n            out = copy.deepcopy(self)\n        else:\n            out = self\n        if self.n_units &gt; 1:\n            per_unit_max = np.max(out.ratemap, axis=1)[..., np.newaxis]\n            out._ratemap = self.ratemap / np.tile(per_unit_max, (1, out.n_bins))\n        else:\n            per_unit_max = np.max(out.ratemap)\n            out._ratemap = self.ratemap / np.tile(per_unit_max, out.n_bins)\n        return out\n\n    def _normalize_firing_rate_by_occupancy(self):\n        # normalize spike counts by occupancy:\n        denom = np.tile(self.occupancy, (self.n_units, 1))\n        denom[denom == 0] = 1\n        ratemap = self.ratemap / denom\n        return ratemap\n\n    @property\n    def unit_ids(self):\n        \"\"\"Unit IDs contained in the SpikeTrain.\"\"\"\n        return list(self._unit_ids)\n\n    @unit_ids.setter\n    def unit_ids(self, val):\n        if len(val) != self.n_units:\n            # print(len(val))\n            # print(self.n_units)\n            raise TypeError(\"unit_ids must be of length n_units\")\n        elif len(set(val)) &lt; len(val):\n            raise TypeError(\"duplicate unit_ids are not allowed\")\n        else:\n            try:\n                # cast to int:\n                unit_ids = [int(id) for id in val]\n            except TypeError:\n                raise TypeError(\"unit_ids must be int-like\")\n        self._unit_ids = unit_ids\n\n    @property\n    def unit_labels(self):\n        \"\"\"Labels corresponding to units contained in the SpikeTrain.\"\"\"\n        if self._unit_labels is None:\n            warnings.warn(\"unit labels have not yet been specified\")\n        return self._unit_labels\n\n    @unit_labels.setter\n    def unit_labels(self, val):\n        if len(val) != self.n_units:\n            raise TypeError(\"labels must be of length n_units\")\n        else:\n            try:\n                # cast to str:\n                labels = [str(label) for label in val]\n            except TypeError:\n                raise TypeError(\"labels must be string-like\")\n        self._unit_labels = labels\n\n    @property\n    def unit_tags(self):\n        \"\"\"Tags corresponding to units contained in the SpikeTrain\"\"\"\n        if self._unit_tags is None:\n            warnings.warn(\"unit tags have not yet been specified\")\n        return self._unit_tags\n\n    @property\n    def label(self):\n        \"\"\"Label pertaining to the source of the spike train.\"\"\"\n        if self._label is None:\n            warnings.warn(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n\n    def __add__(self, other):\n        out = copy.copy(self)\n\n        if isinstance(other, numbers.Number):\n            out._ratemap = out.ratemap + other\n        elif isinstance(other, TuningCurve1D):\n            # TODO: this should merge two TuningCurve1D objects\n            raise NotImplementedError\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: 'TuningCurve1D' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n        return out\n\n    def __sub__(self, other):\n        out = copy.copy(self)\n        out._ratemap = out.ratemap - other\n        return out\n\n    def __mul__(self, other):\n        \"\"\"overloaded * operator.\"\"\"\n        out = copy.copy(self)\n        out._ratemap = out.ratemap * other\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        \"\"\"overloaded / operator.\"\"\"\n        out = copy.copy(self)\n        out._ratemap = out.ratemap / other\n        return out\n\n    def __len__(self):\n        return self.n_units\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n        \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n        mode : {\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to \u2018constant\u2019. Default is\n            \u2018reflect\u2019\n        cval : scalar, optional\n            Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0\n        \"\"\"\n        if sigma is None:\n            sigma = 0.1  # in units of extern\n        if truncate is None:\n            truncate = 4\n        if mode is None:\n            mode = \"reflect\"\n        if cval is None:\n            cval = 0.0\n\n        # Handle sigma parameter - support both scalar and array\n        sigma_array = np.asarray(sigma)\n        if sigma_array.ndim == 0:\n            # Single sigma value\n            sigma_val = float(sigma_array)\n        else:\n            # Array of sigma values - for 1D should have length 1\n            if len(sigma_array) != 1:\n                raise ValueError(\n                    f\"sigma array length {len(sigma_array)} must equal 1 for TuningCurve1D\"\n                )\n            sigma_val = sigma_array[0]\n\n        ds = (self.bins[-1] - self.bins[0]) / self.n_bins\n        sigma = sigma_val / ds\n\n        if not inplace:\n            out = copy.deepcopy(self)\n        else:\n            out = self\n\n        if self.n_units &gt; 1:\n            out._ratemap = gaussian_filter(\n                self.ratemap, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n            )\n        else:\n            out._ratemap = gaussian_filter(\n                self.ratemap, sigma=sigma, truncate=truncate, mode=mode, cval=cval\n            )\n\n        return out\n\n    @property\n    def n_units(self):\n        \"\"\"(int) The number of units.\"\"\"\n        try:\n            return len(self._unit_ids)\n        except TypeError:  # when unit_ids is an integer\n            return 1\n        except AttributeError:\n            return 0\n\n    @property\n    def shape(self):\n        \"\"\"(tuple) The shape of the TuningCurve1D ratemap.\"\"\"\n        if self.isempty:\n            return (self.n_units, 0)\n        if len(self.ratemap.shape) == 1:\n            return (1, self.ratemap.shape[0])\n        return self.ratemap.shape\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty TuningCurve1D\" + address_str + \"&gt;\"\n        shapestr = \" with shape (%s, %s)\" % (self.shape[0], self.shape[1])\n        return \"&lt;TuningCurve1D%s&gt;%s\" % (address_str, shapestr)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) True if TuningCurve1D is empty\"\"\"\n        try:\n            return len(self.ratemap) == 0\n        except TypeError:  # TypeError should happen if ratemap = []\n            return True\n\n    def __iter__(self):\n        \"\"\"TuningCurve1D iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"TuningCurve1D iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_units - 1:\n            raise StopIteration\n        out = copy.copy(self)\n        out._ratemap = self.ratemap[index, :]\n        out._unit_ids = self.unit_ids[index]\n        out._unit_labels = self.unit_labels[index]\n        self._index += 1\n        return out\n\n    def __getitem__(self, *idx):\n        \"\"\"TuningCurve1D index access.\n\n        Accepts integers, slices, and lists\"\"\"\n\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        if self.isempty:\n            return self\n        try:\n            out = copy.copy(self)\n            out._ratemap = self.ratemap[idx, :]\n            out._unit_ids = (np.asanyarray(out._unit_ids)[idx]).tolist()\n            out._unit_labels = (np.asanyarray(out._unit_labels)[idx]).tolist()\n            return out\n        except Exception:\n            raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def _unit_subset(self, unit_list):\n        \"\"\"Return a TuningCurve1D restricted to a subset of units.\n\n        Parameters\n        ----------\n        unit_list : array-like\n            Array or list of unit_ids.\n        \"\"\"\n        unit_subset_ids = []\n        for unit in unit_list:\n            try:\n                id = self.unit_ids.index(unit)\n            except ValueError:\n                warnings.warn(\n                    \"unit_id \" + str(unit) + \" not found in TuningCurve1D; ignoring\"\n                )\n                pass\n            else:\n                unit_subset_ids.append(id)\n\n        new_unit_ids = (np.asarray(self.unit_ids)[unit_subset_ids]).tolist()\n        new_unit_labels = (np.asarray(self.unit_labels)[unit_subset_ids]).tolist()\n\n        if len(unit_subset_ids) == 0:\n            warnings.warn(\"no units remaining in requested unit subset\")\n            return TuningCurve1D(empty=True)\n\n        newtuningcurve = copy.copy(self)\n        newtuningcurve._unit_ids = new_unit_ids\n        newtuningcurve._unit_labels = new_unit_labels\n        # TODO: implement tags\n        # newtuningcurve._unit_tags =\n        newtuningcurve._ratemap = self.ratemap[unit_subset_ids, :]\n        # TODO: shall we restrict _bst as well? This will require a copy to be made...\n        # newtuningcurve._bst =\n\n        return newtuningcurve\n\n    def _get_peak_firing_order_idx(self):\n        \"\"\"Docstring goes here\n\n        ratemap has shape (n_units, n_ext)\n        \"\"\"\n        peakorder = np.argmax(self.ratemap, axis=1).argsort()\n\n        return peakorder.tolist()\n\n    def get_peak_firing_order_ids(self):\n        \"\"\"Docstring goes here\n\n        ratemap has shape (n_units, n_ext)\n        \"\"\"\n        peakorder = np.argmax(self.ratemap, axis=1).argsort()\n\n        return (np.asanyarray(self.unit_ids)[peakorder]).tolist()\n\n    def _reorder_units_by_idx(self, neworder=None, *, inplace=False):\n        \"\"\"Reorder units according to a specified order.\n\n        neworder must be list-like, of size (n_units,) and in 0,..n_units\n        and not in terms of unit_ids\n\n        Return\n        ------\n        out : reordered TuningCurve1D\n        \"\"\"\n        if neworder is None:\n            neworder = self._get_peak_firing_order_idx()\n        if inplace:\n            out = self\n        else:\n            out = copy.deepcopy(self)\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._ratemap, frm, to)\n            out._unit_ids[frm], out._unit_ids[to] = (\n                out._unit_ids[to],\n                out._unit_ids[frm],\n            )\n            out._unit_labels[frm], out._unit_labels[to] = (\n                out._unit_labels[to],\n                out._unit_labels[frm],\n            )\n            # TODO: re-build unit tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        return out\n\n    def reorder_units_by_ids(self, neworder=None, *, inplace=False):\n        \"\"\"Reorder units according to a specified order.\n\n        neworder must be list-like, of size (n_units,) and in terms of\n        unit_ids\n\n        Return\n        ------\n        out : reordered TuningCurve1D\n        \"\"\"\n        if neworder is None:\n            neworder = self.get_peak_firing_order_ids()\n        if inplace:\n            out = self\n        else:\n            out = copy.deepcopy(self)\n\n        # unit_ids = list(unit_ids)\n        neworder = [self.unit_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._ratemap, frm, to)\n            out._unit_ids[frm], out._unit_ids[to] = (\n                out._unit_ids[to],\n                out._unit_ids[frm],\n            )\n            out._unit_labels[frm], out._unit_labels[to] = (\n                out._unit_labels[to],\n                out._unit_labels[frm],\n            )\n            # TODO: re-build unit tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        return out\n\n    def reorder_units(self, inplace=False):\n        \"\"\"Convenience function to reorder units by peak firing location.\"\"\"\n        return self.reorder_units_by_ids(inplace=inplace)\n\n    def _detach(self):\n        \"\"\"Detach bst and extern from tuning curve.\"\"\"\n        self._bst = None\n        self._extern = None\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.bin_centers","title":"<code>bin_centers</code>  <code>property</code>","text":"<p>External correlate bin centers.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.bins","title":"<code>bins</code>  <code>property</code>","text":"<p>External correlate bins.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) True if TuningCurve1D is empty</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label pertaining to the source of the spike train.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.n_units","title":"<code>n_units</code>  <code>property</code>","text":"<p>(int) The number of units.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>(tuple) The shape of the TuningCurve1D ratemap.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.unit_ids","title":"<code>unit_ids</code>  <code>property</code> <code>writable</code>","text":"<p>Unit IDs contained in the SpikeTrain.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.unit_labels","title":"<code>unit_labels</code>  <code>property</code> <code>writable</code>","text":"<p>Labels corresponding to units contained in the SpikeTrain.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.unit_tags","title":"<code>unit_tags</code>  <code>property</code>","text":"<p>Tags corresponding to units contained in the SpikeTrain</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.get_peak_firing_order_ids","title":"<code>get_peak_firing_order_ids()</code>","text":"<p>Docstring goes here</p> <p>ratemap has shape (n_units, n_ext)</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def get_peak_firing_order_ids(self):\n    \"\"\"Docstring goes here\n\n    ratemap has shape (n_units, n_ext)\n    \"\"\"\n    peakorder = np.argmax(self.ratemap, axis=1).argsort()\n\n    return (np.asanyarray(self.unit_ids)[peakorder]).tolist()\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.information_rate","title":"<code>information_rate()</code>","text":"<p>Compute the information rate...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def information_rate(self):\n    \"\"\"Compute the information rate...\"\"\"\n    return utils.information_rate(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.max","title":"<code>max(*, axis=None)</code>","text":"<p>Returns the mean of firing rate (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>When axis is None, the global mean firing rate is returned. When axis is 0, the mean firing rates across units, as a function of the external correlate (e.g. position) are returned. When axis is 1, the mean firing rate for each unit is returned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mean</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def max(self, *, axis=None):\n    \"\"\"Returns the mean of firing rate (in Hz).\n    Parameters\n    ----------\n    axis : int, optional\n        When axis is None, the global mean firing rate is returned.\n        When axis is 0, the mean firing rates across units, as a\n        function of the external correlate (e.g. position) are\n        returned.\n        When axis is 1, the mean firing rate for each unit is\n        returned.\n    Returns\n    -------\n    mean :\n    \"\"\"\n    maxes = np.max(self.ratemap, axis=axis).squeeze()\n    if maxes.size == 1:\n        return np.asarray(maxes).item()\n    return maxes\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.mean","title":"<code>mean(*, axis=None)</code>","text":"<p>Returns the mean of firing rate (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>When axis is None, the global mean firing rate is returned. When axis is 0, the mean firing rates across units, as a function of the external correlate (e.g. position) are returned. When axis is 1, the mean firing rate for each unit is returned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mean</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def mean(self, *, axis=None):\n    \"\"\"Returns the mean of firing rate (in Hz).\n    Parameters\n    ----------\n    axis : int, optional\n        When axis is None, the global mean firing rate is returned.\n        When axis is 0, the mean firing rates across units, as a\n        function of the external correlate (e.g. position) are\n        returned.\n        When axis is 1, the mean firing rate for each unit is\n        returned.\n    Returns\n    -------\n    mean :\n    \"\"\"\n    means = np.mean(self.ratemap, axis=axis).squeeze()\n    if means.size == 1:\n        return np.asarray(means).item()\n    return means\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.min","title":"<code>min(*, axis=None)</code>","text":"<p>Returns the mean of firing rate (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>When axis is None, the global mean firing rate is returned. When axis is 0, the mean firing rates across units, as a function of the external correlate (e.g. position) are returned. When axis is 1, the mean firing rate for each unit is returned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mean</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def min(self, *, axis=None):\n    \"\"\"Returns the mean of firing rate (in Hz).\n    Parameters\n    ----------\n    axis : int, optional\n        When axis is None, the global mean firing rate is returned.\n        When axis is 0, the mean firing rates across units, as a\n        function of the external correlate (e.g. position) are\n        returned.\n        When axis is 1, the mean firing rate for each unit is\n        returned.\n    Returns\n    -------\n    mean :\n    \"\"\"\n    mins = np.min(self.ratemap, axis=axis).squeeze()\n    if mins.size == 1:\n        return np.asarray(mins).item()\n    return mins\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.reorder_units","title":"<code>reorder_units(inplace=False)</code>","text":"<p>Convenience function to reorder units by peak firing location.</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def reorder_units(self, inplace=False):\n    \"\"\"Convenience function to reorder units by peak firing location.\"\"\"\n    return self.reorder_units_by_ids(inplace=inplace)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.reorder_units_by_ids","title":"<code>reorder_units_by_ids(neworder=None, *, inplace=False)</code>","text":"<p>Reorder units according to a specified order.</p> <p>neworder must be list-like, of size (n_units,) and in terms of unit_ids</p> Return <p>out : reordered TuningCurve1D</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def reorder_units_by_ids(self, neworder=None, *, inplace=False):\n    \"\"\"Reorder units according to a specified order.\n\n    neworder must be list-like, of size (n_units,) and in terms of\n    unit_ids\n\n    Return\n    ------\n    out : reordered TuningCurve1D\n    \"\"\"\n    if neworder is None:\n        neworder = self.get_peak_firing_order_ids()\n    if inplace:\n        out = self\n    else:\n        out = copy.deepcopy(self)\n\n    # unit_ids = list(unit_ids)\n    neworder = [self.unit_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        utils.swap_rows(out._ratemap, frm, to)\n        out._unit_ids[frm], out._unit_ids[to] = (\n            out._unit_ids[to],\n            out._unit_ids[frm],\n        )\n        out._unit_labels[frm], out._unit_labels[to] = (\n            out._unit_labels[to],\n            out._unit_labels[frm],\n        )\n        # TODO: re-build unit tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    return out\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.smooth","title":"<code>smooth(*, sigma=None, truncate=None, inplace=False, mode=None, cval=None)</code>","text":"<p>Smooths the tuning curve with a Gaussian kernel.</p> <p>mode : {\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional     The mode parameter determines how the array borders are handled,     where cval is the value when mode is equal to \u2018constant\u2019. Default is     \u2018reflect\u2019 cval : scalar, optional     Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n    \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n    mode : {\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to \u2018constant\u2019. Default is\n        \u2018reflect\u2019\n    cval : scalar, optional\n        Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0\n    \"\"\"\n    if sigma is None:\n        sigma = 0.1  # in units of extern\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n\n    # Handle sigma parameter - support both scalar and array\n    sigma_array = np.asarray(sigma)\n    if sigma_array.ndim == 0:\n        # Single sigma value\n        sigma_val = float(sigma_array)\n    else:\n        # Array of sigma values - for 1D should have length 1\n        if len(sigma_array) != 1:\n            raise ValueError(\n                f\"sigma array length {len(sigma_array)} must equal 1 for TuningCurve1D\"\n            )\n        sigma_val = sigma_array[0]\n\n    ds = (self.bins[-1] - self.bins[0]) / self.n_bins\n    sigma = sigma_val / ds\n\n    if not inplace:\n        out = copy.deepcopy(self)\n    else:\n        out = self\n\n    if self.n_units &gt; 1:\n        out._ratemap = gaussian_filter(\n            self.ratemap, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n        )\n    else:\n        out._ratemap = gaussian_filter(\n            self.ratemap, sigma=sigma, truncate=truncate, mode=mode, cval=cval\n        )\n\n    return out\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.spatial_information","title":"<code>spatial_information()</code>","text":"<p>Compute the spatial information...</p> <p>The specificity index examines the amount of information (in bits) that a single spike conveys about the animal's location (i.e., how well cell firing predicts the animal's location).The spatial information content of cell discharge was calculated using the formula:     information content = \\Sum P_i(R_i/R)log_2(R_i/R) where i is the bin number, P_i, is the probability for occupancy of bin i, R_i, is the mean firing rate for bin i, and R is the overall mean firing rate.</p> <p>In order to account for the effects of low firing rates (with fewer spikes there is a tendency toward higher information content) or random bursts of firing, the spike firing time-series was randomly offset in time from the rat location time-series, and the information content was calculated. A distribution of the information content based on 100 such random shifts was obtained and was used to compute a standardized score (Zscore) of information content for that cell. While the distribution is not composed of independent samples, it was nominally normally distributed, and a Z value of 2.29 was chosen as a cut-off for significance (the equivalent of a one-tailed t-test with P = 0.01 under a normal distribution).</p> Reference(s) <p>Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,     and Skaggs, W. E. (1994). \"Spatial information content and     reliability of hippocampal CA1 neurons: effects of visual     input\", Hippocampus, 4(4), 410-421.</p> <p>Parameters:</p> Name Type Description Default <code>Returns</code> required <code>si</code> <code>array of shape (n_units,)</code> <p>spatial information (in bits) per unit</p> required <code>sparsity</code> <p>sparsity (in percent) for each unit</p> required Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_information(self):\n    \"\"\"Compute the spatial information...\n\n    The specificity index examines the amount of information\n    (in bits) that a single spike conveys about the animal's\n    location (i.e., how well cell firing predicts the animal's\n    location).The spatial information content of cell discharge was\n    calculated using the formula:\n        information content = \\Sum P_i(R_i/R)log_2(R_i/R)\n    where i is the bin number, P_i, is the probability for occupancy\n    of bin i, R_i, is the mean firing rate for bin i, and R is the\n    overall mean firing rate.\n\n    In order to account for the effects of low firing rates (with\n    fewer spikes there is a tendency toward higher information\n    content) or random bursts of firing, the spike firing\n    time-series was randomly offset in time from the rat location\n    time-series, and the information content was calculated. A\n    distribution of the information content based on 100 such random\n    shifts was obtained and was used to compute a standardized score\n    (Zscore) of information content for that cell. While the\n    distribution is not composed of independent samples, it was\n    nominally normally distributed, and a Z value of 2.29 was chosen\n    as a cut-off for significance (the equivalent of a one-tailed\n    t-test with P = 0.01 under a normal distribution).\n\n    Reference(s)\n    ------------\n    Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n        and Skaggs, W. E. (1994). \"Spatial information content and\n        reliability of hippocampal CA1 neurons: effects of visual\n        input\", Hippocampus, 4(4), 410-421.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    si : array of shape (n_units,)\n        spatial information (in bits) per unit\n    sparsity: array of shape (n_units,)\n        sparsity (in percent) for each unit\n    \"\"\"\n\n    return utils.spatial_information(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.spatial_selectivity","title":"<code>spatial_selectivity()</code>","text":"<p>Compute the spatial selectivity...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_selectivity(self):\n    \"\"\"Compute the spatial selectivity...\"\"\"\n    return utils.spatial_selectivity(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve1D.spatial_sparsity","title":"<code>spatial_sparsity()</code>","text":"<p>Compute the firing sparsity...</p> <p>Parameters:</p> Name Type Description Default <code>Returns</code> required <code>si</code> <code>array of shape (n_units,)</code> <p>spatial information (in bits) per unit</p> required <code>sparsity</code> <p>sparsity (in percent) for each unit</p> required Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_sparsity(self):\n    \"\"\"Compute the firing sparsity...\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    si : array of shape (n_units,)\n        spatial information (in bits) per unit\n    sparsity: array of shape (n_units,)\n        sparsity (in percent) for each unit\n    \"\"\"\n    return utils.spatial_sparsity(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D","title":"<code>TuningCurve2D</code>","text":"<p>Tuning curves (2-dimensional) of multiple units.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array for tuning curve estimation.</p> <code>None</code> <code>extern</code> <code>array - like</code> <p>External correlates (e.g., position).</p> <code>None</code> <code>ratemap</code> <code>ndarray</code> <p>Precomputed rate map.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation for Gaussian smoothing.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Truncation parameter for smoothing.</p> <code>None</code> <code>ext_nx</code> <code>int</code> <p>Number of bins in x-dimension.</p> <code>None</code> <code>ext_ny</code> <code>int</code> <p>Number of bins in y-dimension.</p> <code>None</code> <code>transform_func</code> <code>callable</code> <p>Function to transform external correlates.</p> <code>None</code> <code>minbgrate</code> <code>float</code> <p>Minimum background firing rate.</p> <code>None</code> <code>ext_xmin</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>ext_xmax</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>ext_ymin</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>ext_ymax</code> <code>float</code> <p>Extent of the external correlates.</p> <code>0</code> <code>extlabels</code> <code>list</code> <p>Labels for external correlates.</p> <code>None</code> <code>min_duration</code> <code>float</code> <p>Minimum duration for occupancy.</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>Unit IDs.</p> <code>None</code> <code>unit_labels</code> <code>list</code> <p>Unit labels.</p> <code>None</code> <code>unit_tags</code> <code>list</code> <p>Unit tags.</p> <code>None</code> <code>label</code> <code>str</code> <p>Label for the tuning curve.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>If True, create an empty TuningCurve2D.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>ratemap</code> <code>ndarray</code> <p>The 2D rate map.</p> <code>occupancy</code> <code>ndarray</code> <p>Occupancy map.</p> <code>unit_ids</code> <code>list</code> <p>Unit IDs.</p> <code>unit_labels</code> <code>list</code> <p>Unit labels.</p> <code>unit_tags</code> <code>list</code> <p>Unit tags.</p> <code>label</code> <code>str</code> <p>Label for the tuning curve.</p> <code>mask</code> <code>ndarray</code> <p>Mask for valid regions.</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>class TuningCurve2D:\n    \"\"\"\n    Tuning curves (2-dimensional) of multiple units.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray, optional\n        Binned spike train array for tuning curve estimation.\n    extern : array-like, optional\n        External correlates (e.g., position).\n    ratemap : np.ndarray, optional\n        Precomputed rate map.\n    sigma : float, optional\n        Standard deviation for Gaussian smoothing.\n    truncate : float, optional\n        Truncation parameter for smoothing.\n    ext_nx : int, optional\n        Number of bins in x-dimension.\n    ext_ny : int, optional\n        Number of bins in y-dimension.\n    transform_func : callable, optional\n        Function to transform external correlates.\n    minbgrate : float, optional\n        Minimum background firing rate.\n    ext_xmin, ext_xmax, ext_ymin, ext_ymax : float, optional\n        Extent of the external correlates.\n    extlabels : list, optional\n        Labels for external correlates.\n    min_duration : float, optional\n        Minimum duration for occupancy.\n    unit_ids : list, optional\n        Unit IDs.\n    unit_labels : list, optional\n        Unit labels.\n    unit_tags : list, optional\n        Unit tags.\n    label : str, optional\n        Label for the tuning curve.\n    empty : bool, optional\n        If True, create an empty TuningCurve2D.\n\n    Attributes\n    ----------\n    ratemap : np.ndarray\n        The 2D rate map.\n    occupancy : np.ndarray\n        Occupancy map.\n    unit_ids : list\n        Unit IDs.\n    unit_labels : list\n        Unit labels.\n    unit_tags : list\n        Unit tags.\n    label : str\n        Label for the tuning curve.\n    mask : np.ndarray\n        Mask for valid regions.\n    \"\"\"\n\n    __attributes__ = [\n        \"_ratemap\",\n        \"_occupancy\",\n        \"_unit_ids\",\n        \"_unit_labels\",\n        \"_unit_tags\",\n        \"_label\",\n        \"_mask\",\n    ]\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def __init__(\n        self,\n        *,\n        bst=None,\n        extern=None,\n        ratemap=None,\n        sigma=None,\n        truncate=None,\n        ext_nx=None,\n        ext_ny=None,\n        transform_func=None,\n        minbgrate=None,\n        ext_xmin=0,\n        ext_ymin=0,\n        ext_xmax=1,\n        ext_ymax=1,\n        extlabels=None,\n        min_duration=None,\n        unit_ids=None,\n        unit_labels=None,\n        unit_tags=None,\n        label=None,\n        empty=False,\n    ):\n        \"\"\"\n\n        NOTE: tuning curves in 2D have shapes (n_units, ny, nx) so that\n        we can plot them in an intuitive manner\n\n        If sigma is nonzero, then smoothing is applied.\n\n        We always require bst and extern, and then some combination of\n            (1) bin edges, transform_func*\n            (2) n_extern, transform_func*\n            (3) n_extern, x_min, x_max, transform_func*\n\n            transform_func operates on extern and returns a value that\n            TuninCurve2D can interpret. If no transform is specified, the\n            identity operator is assumed.\n\n        TODO: ext_xmin and ext_xmax (and same for y) should be inferred from\n        extern if not passed in explicitly.\n\n        e.g.\n            ext_xmin, ext_xmax = np.floor(pos[:,0].min()/10)*10, np.ceil(pos[:,0].max()/10)*10\n            ext_ymin, ext_ymax = np.floor(pos[:,1].min()/10)*10, np.ceil(pos[:,1].max()/10)*10\n\n        TODO: mask should be learned during constructor, or additionally\n        after-the-fact. If a mask is present, then smoothing should be applied\n        while respecting this mask. Similarly, decoding MAY be altered by\n        finding the closest point WITHIN THE MASK after doing mean decoding?\n        This way, if there's an outlier pulling us off of the track, we may\n        expect decoding accuracy to be improved.\n        \"\"\"\n        # TODO: input validation\n        if not empty:\n            if ratemap is None:\n                assert bst is not None, (\n                    \"bst must be specified or ratemap must be specified!\"\n                )\n                assert extern is not None, (\n                    \"extern must be specified or ratemap must be specified!\"\n                )\n            else:\n                assert bst is None, \"ratemap and bst cannot both be specified!\"\n                assert extern is None, \"ratemap and extern cannot both be specified!\"\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        if ratemap is not None:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._init_from_ratemap(\n                ratemap=ratemap,\n                ext_xmin=ext_xmin,\n                ext_xmax=ext_xmax,\n                ext_ymin=ext_ymin,\n                ext_ymax=ext_ymax,\n                extlabels=extlabels,\n                unit_ids=unit_ids,\n                unit_labels=unit_labels,\n                unit_tags=unit_tags,\n                label=label,\n            )\n            return\n\n        self._mask = None  # TODO: change this when we can learn a mask in __init__!\n        self._bst = bst\n        self._extern = extern\n\n        if minbgrate is None:\n            minbgrate = 0.01  # Hz minimum background firing rate\n\n        if ext_nx is not None:\n            if ext_xmin is not None and ext_xmax is not None:\n                self._xbins = np.linspace(ext_xmin, ext_xmax, ext_nx + 1)\n            else:\n                raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        if ext_ny is not None:\n            if ext_ymin is not None and ext_ymax is not None:\n                self._ybins = np.linspace(ext_ymin, ext_ymax, ext_ny + 1)\n            else:\n                raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n        if min_duration is None:\n            min_duration = 0\n\n        self._min_duration = min_duration\n        self._unit_ids = bst.unit_ids\n        self._unit_labels = bst.unit_labels\n        self._unit_tags = bst.unit_tags  # no input validation yet\n        self.label = label\n\n        if transform_func is None:\n            self.trans_func = self._trans_func\n        else:\n            self.trans_func = transform_func\n\n        # compute occupancy\n        self._occupancy = self._compute_occupancy()\n        # compute ratemap (in Hz)\n        self._ratemap = self._compute_ratemap()\n        # normalize firing rate by occupancy\n        self._ratemap = self._normalize_firing_rate_by_occupancy()\n        # enforce minimum background firing rate\n        self._ratemap[self._ratemap &lt; minbgrate] = (\n            minbgrate  # background firing rate of 0.01 Hz\n        )\n\n        # TODO: support 2D sigma\n        if sigma is not None:\n            if sigma &gt; 0:\n                self.smooth(sigma=sigma, truncate=truncate, inplace=True)\n\n        # optionally detach _bst and _extern to save space when pickling, for example\n        self._detach()\n\n    def spatial_information(self):\n        \"\"\"Compute the spatial information and firing sparsity...\n\n        The specificity index examines the amount of information\n        (in bits) that a single spike conveys about the animal's\n        location (i.e., how well cell firing predicts the animal's\n        location).The spatial information content of cell discharge was\n        calculated using the formula:\n            information content = \\\\Sum P_i(R_i/R)log_2(R_i/R)\n        where i is the bin number, P_i, is the probability for occupancy\n        of bin i, R_i, is the mean firing rate for bin i, and R is the\n        overall mean firing rate.\n\n        In order to account for the effects of low firing rates (with\n        fewer spikes there is a tendency toward higher information\n        content) or random bursts of firing, the spike firing\n        time-series was randomly offset in time from the rat location\n        time-series, and the information content was calculated. A\n        distribution of the information content based on 100 such random\n        shifts was obtained and was used to compute a standardized score\n        (Zscore) of information content for that cell. While the\n        distribution is not composed of independent samples, it was\n        nominally normally distributed, and a Z value of 2.29 was chosen\n        as a cut-off for significance (the equivalent of a one-tailed\n        t-test with P = 0.01 under a normal distribution).\n\n        Reference(s)\n        ------------\n        Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n            and Skaggs, W. E. (1994). \"Spatial information content and\n            reliability of hippocampal CA1 neurons: effects of visual\n            input\", Hippocampus, 4(4), 410-421.\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        si : array of shape (n_units,)\n            spatial information (in bits) per spike\n        \"\"\"\n\n        return utils.spatial_information(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def information_rate(self):\n        \"\"\"Compute the information rate...\"\"\"\n        return utils.information_rate(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def spatial_selectivity(self):\n        \"\"\"Compute the spatial selectivity...\"\"\"\n        return utils.spatial_selectivity(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def spatial_sparsity(self):\n        \"\"\"Compute the spatial information and firing sparsity...\n\n        The specificity index examines the amount of information\n        (in bits) that a single spike conveys about the animal's\n        location (i.e., how well cell firing predicts the animal's\n        location).The spatial information content of cell discharge was\n        calculated using the formula:\n            information content = \\Sum P_i(R_i/R)log_2(R_i/R)\n        where i is the bin number, P_i, is the probability for occupancy\n        of bin i, R_i, is the mean firing rate for bin i, and R is the\n        overall mean firing rate.\n\n        In order to account for the effects of low firing rates (with\n        fewer spikes there is a tendency toward higher information\n        content) or random bursts of firing, the spike firing\n        time-series was randomly offset in time from the rat location\n        time-series, and the information content was calculated. A\n        distribution of the information content based on 100 such random\n        shifts was obtained and was used to compute a standardized score\n        (Zscore) of information content for that cell. While the\n        distribution is not composed of independent samples, it was\n        nominally normally distributed, and a Z value of 2.29 was chosen\n        as a cut-off for significance (the equivalent of a one-tailed\n        t-test with P = 0.01 under a normal distribution).\n\n        Reference(s)\n        ------------\n        Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n            and Skaggs, W. E. (1994). \"Spatial information content and\n            reliability of hippocampal CA1 neurons: effects of visual\n            input\", Hippocampus, 4(4), 410-421.\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        si : array of shape (n_units,)\n            spatial information (in bits) per unit\n        sparsity: array of shape (n_units,)\n            sparsity (in percent) for each unit\n        \"\"\"\n        return utils.spatial_sparsity(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def _initialize_mask_from_extern(self, extern):\n        \"\"\"Attached a mask from extern.\n        TODO: improve docstring, add example.\n        Typically extern is an AnalogSignalArray or a PositionArray.\n        \"\"\"\n        xpos, ypos = extern.asarray().yvals\n        mask_x = np.digitize(xpos, self._xbins, right=True) - 1  # spatial bin numbers\n        mask_y = np.digitize(ypos, self._ybins, right=True) - 1  # spatial bin numbers\n\n        mask = np.empty((self.n_xbins, self.n_xbins))\n        mask[:] = np.nan\n        mask[mask_x, mask_y] = 1\n\n        self._mask_x = mask_x  # may not be useful or necessary to store?\n        self._mask_y = mask_y  # may not be useful or necessary to store?\n        self._mask = mask\n\n    def __add__(self, other):\n        out = copy.copy(self)\n\n        if isinstance(other, numbers.Number):\n            out._ratemap = out.ratemap + other\n        elif isinstance(other, TuningCurve2D):\n            # TODO: this should merge two TuningCurve2D objects\n            raise NotImplementedError\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: 'TuningCurve2D' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n        return out\n\n    def __sub__(self, other):\n        out = copy.copy(self)\n        out._ratemap = out.ratemap - other\n        return out\n\n    def __mul__(self, other):\n        \"\"\"overloaded * operator.\"\"\"\n        out = copy.copy(self)\n        out._ratemap = out.ratemap * other\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        \"\"\"overloaded / operator.\"\"\"\n        out = copy.copy(self)\n        out._ratemap = out.ratemap / other\n        return out\n\n    def _init_from_ratemap(\n        self,\n        ratemap,\n        occupancy=None,\n        ext_xmin=0,\n        ext_xmax=1,\n        ext_ymin=0,\n        ext_ymax=1,\n        extlabels=None,\n        unit_ids=None,\n        unit_labels=None,\n        unit_tags=None,\n        label=None,\n    ):\n        \"\"\"Initialize a TuningCurve2D object from a ratemap.\n\n        Parameters\n        ----------\n        ratemap : array\n            Array of shape (n_units, ext_nx, ext_ny)\n\n        Returns\n        -------\n\n        \"\"\"\n        n_units, ext_nx, ext_ny = ratemap.shape\n\n        if occupancy is None:\n            # assume uniform occupancy\n            self._occupancy = np.ones((ext_nx, ext_ny))\n\n        if ext_xmin is None:\n            ext_xmin = 0\n        if ext_xmax is None:\n            ext_xmax = ext_xmin + 1\n\n        if ext_ymin is None:\n            ext_ymin = 0\n        if ext_ymax is None:\n            ext_ymax = ext_ymin + 1\n\n        self._xbins = np.linspace(ext_xmin, ext_xmax, ext_nx + 1)\n        self._ybins = np.linspace(ext_ymin, ext_ymax, ext_ny + 1)\n        self._ratemap = ratemap\n\n        # inherit unit IDs if available, otherwise initialize to default\n        if unit_ids is None:\n            unit_ids = list(range(1, n_units + 1))\n\n        unit_ids = np.array(unit_ids, ndmin=1)  # standardize unit_ids\n\n        # if unit_labels is empty, default to unit_ids\n        if unit_labels is None:\n            unit_labels = unit_ids\n\n        unit_labels = np.array(unit_labels, ndmin=1)  # standardize\n\n        self._unit_ids = unit_ids\n        self._unit_labels = unit_labels\n        self._unit_tags = unit_tags  # no input validation yet\n        if label is not None:\n            self.label = label\n\n        return self\n\n    def max(self, *, axis=None):\n        \"\"\"Returns the mean of firing rate (in Hz).\n        Parameters\n        ----------\n        axis : int, optional\n            When axis is None, the global max firing rate is returned.\n            When axis is 0, the max firing rates across units, as a\n            function of the external correlate (e.g. position) are\n            returned.\n            When axis is 1, the max firing rate for each unit is\n            returned.\n        Returns\n        -------\n        max :\n        \"\"\"\n        if (axis is None) | (axis == 0):\n            maxes = np.max(self.ratemap, axis=axis)\n        elif axis == 1:\n            maxes = [\n                self.ratemap[unit_i, :, :].max()\n                for unit_i in range(self.ratemap.shape[0])\n            ]\n\n        return maxes\n\n    def min(self, *, axis=None):\n        \"\"\"Returns the min of firing rate (in Hz).\n        Parameters\n        ----------\n        axis : int, optional\n            When axis is None, the global min firing rate is returned.\n            When axis is 0, the min firing rates across units, as a\n            function of the external correlate (e.g. position) are\n            returned.\n            When axis is 1, the min firing rate for each unit is\n            returned.\n        Returns\n        -------\n        min :\n        \"\"\"\n\n        if (axis is None) | (axis == 0):\n            mins = np.min(self.ratemap, axis=axis)\n        elif axis == 1:\n            mins = [\n                self.ratemap[unit_i, :, :].min()\n                for unit_i in range(self.ratemap.shape[0])\n            ]\n\n        return mins\n\n    def mean(self, *, axis=None):\n        \"\"\"Returns the mean of firing rate (in Hz).\n        Parameters\n        ----------\n        axis : int, optional\n            When axis is None, the global mean firing rate is returned.\n            When axis is 0, the mean firing rates across units, as a\n            function of the external correlate (e.g. position) are\n            returned.\n            When axis is 1, the mean firing rate for each unit is\n            returned.\n        Returns\n        -------\n        mean :\n        \"\"\"\n\n        if (axis is None) | (axis == 0):\n            means = np.mean(self.ratemap, axis=axis)\n        elif axis == 1:\n            means = [\n                self.ratemap[unit_i, :, :].mean()\n                for unit_i in range(self.ratemap.shape[0])\n            ]\n\n        return means\n\n    def std(self, *, axis=None):\n        \"\"\"Returns the std of firing rate (in Hz).\n        Parameters\n        ----------\n        axis : int, optional\n            When axis is None, the global std firing rate is returned.\n            When axis is 0, the std firing rates across units, as a\n            function of the external correlate (e.g. position) are\n            returned.\n            When axis is 1, the std firing rate for each unit is\n            returned.\n        Returns\n        -------\n        std :\n        \"\"\"\n\n        if (axis is None) | (axis == 0):\n            stds = np.std(self.ratemap, axis=axis)\n        elif axis == 1:\n            stds = [\n                self.ratemap[unit_i, :, :].std()\n                for unit_i in range(self.ratemap.shape[0])\n            ]\n\n        return stds\n\n    def _detach(self):\n        \"\"\"Detach bst and extern from tuning curve.\"\"\"\n        self._bst = None\n        self._extern = None\n\n    @property\n    def mask(self):\n        \"\"\"(n_xbins, n_ybins) Mask for tuning curve.\"\"\"\n        return self._mask\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        return self.n_xbins * self.n_ybins\n\n    @property\n    def n_xbins(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        return len(self.xbins) - 1\n\n    @property\n    def n_ybins(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        return len(self.ybins) - 1\n\n    @property\n    def xbins(self):\n        \"\"\"External correlate bins.\"\"\"\n        return self._xbins\n\n    @property\n    def ybins(self):\n        \"\"\"External correlate bins.\"\"\"\n        return self._ybins\n\n    @property\n    def xbin_centers(self):\n        \"\"\"External correlate bin centers.\"\"\"\n        return (self.xbins + (self.xbins[1] - self.xbins[0]) / 2)[:-1]\n\n    @property\n    def ybin_centers(self):\n        \"\"\"External correlate bin centers.\"\"\"\n        return (self.ybins + (self.ybins[1] - self.ybins[0]) / 2)[:-1]\n\n    @property\n    def bin_centers(self):\n        return tuple([self.xbin_centers, self.ybin_centers])\n\n    @property\n    def bins(self):\n        \"\"\"External correlate bins.\"\"\"\n        return (self.xbins, self.ybins)\n\n    def _trans_func(self, extern, at):\n        \"\"\"Default transform function to map extern into numerical bins.\n\n        Assumes first signal is x-dim, second is y-dim.\n        \"\"\"\n\n        _, ext = extern.asarray(at=at)\n        x, y = ext[0, :], ext[1, :]\n\n        return np.atleast_1d(x), np.atleast_1d(y)\n\n    def __getitem__(self, *idx):\n        \"\"\"TuningCurve2D index access.\n\n        Accepts integers, slices, and lists\"\"\"\n\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        if self.isempty:\n            return self\n        try:\n            out = copy.copy(self)\n            out._ratemap = self.ratemap[idx, :]\n            out._unit_ids = (np.asanyarray(out._unit_ids)[idx]).tolist()\n            out._unit_labels = (np.asanyarray(out._unit_labels)[idx]).tolist()\n            return out\n        except Exception:\n            raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def _compute_occupancy(self):\n        \"\"\" \"\"\"\n\n        # Make sure that self._bst_centers fall within not only the support\n        # of extern, but also within the extreme sample times; otherwise,\n        # interpolation will yield NaNs at the extremes. Indeed, when we have\n        # sample times within a support epoch, we can assume that the signal\n        # stayed roughly constant for that one sample duration.\n\n        if self._bst._bin_centers[0] &lt; self._extern.time[0]:\n            self._extern = copy.copy(self._extern)\n            self._extern.time[0] = self._bst._bin_centers[0]\n            self._extern._interp = None\n            # raise ValueError('interpolated sample requested before first sample of extern!')\n        if self._bst._bin_centers[-1] &gt; self._extern.time[-1]:\n            self._extern = copy.copy(self._extern)\n            self._extern.time[-1] = self._bst._bin_centers[-1]\n            self._extern._interp = None\n            # raise ValueError('interpolated sample requested after last sample of extern!')\n\n        x, y = self.trans_func(self._extern, at=self._bst.bin_centers)\n\n        xmin = self.xbins[0]\n        xmax = self.xbins[-1]\n        ymin = self.ybins[0]\n        ymax = self.ybins[-1]\n\n        occupancy, _, _ = np.histogram2d(\n            x, y, bins=[self.xbins, self.ybins], range=([[xmin, xmax], [ymin, ymax]])\n        )\n\n        return occupancy\n\n    def _compute_ratemap(self, min_duration=None):\n        \"\"\"\n\n        min_duration is the min duration in seconds for a bin to be\n        considered 'valid'; if too few observations were made, then the\n        firing rate is kept at an estimate of 0. If min_duration == 0,\n        then all the spikes are used.\n        \"\"\"\n\n        if min_duration is None:\n            min_duration = self._min_duration\n\n        x, y = self.trans_func(self._extern, at=self._bst.bin_centers)\n\n        ext_bin_idx_x = np.squeeze(np.digitize(x, self.xbins, right=True))\n        ext_bin_idx_y = np.squeeze(np.digitize(y, self.ybins, right=True))\n\n        # make sure that all the events fit between extmin and extmax:\n        # TODO: this might rather be a warning, but it's a pretty serious warning...\n        if ext_bin_idx_x.max() &gt; self.n_xbins:\n            raise ValueError(\"ext values greater than 'ext_xmax'\")\n        if ext_bin_idx_x.min() == 0:\n            raise ValueError(\"ext values less than 'ext_xmin'\")\n        if ext_bin_idx_y.max() &gt; self.n_ybins:\n            raise ValueError(\"ext values greater than 'ext_ymax'\")\n        if ext_bin_idx_y.min() == 0:\n            raise ValueError(\"ext values less than 'ext_ymin'\")\n\n        ratemap = np.zeros((self.n_units, self.n_xbins, self.n_ybins))\n\n        for tt, (bidxx, bidxy) in enumerate(zip(ext_bin_idx_x, ext_bin_idx_y)):\n            ratemap[:, bidxx - 1, bidxy - 1] += self._bst.data[:, tt]\n\n        # apply minimum observation duration\n        for uu in range(self.n_units):\n            ratemap[uu][self.occupancy * self._bst.ds &lt; min_duration] = 0\n\n        return ratemap / self._bst.ds\n\n    def normalize(self, inplace=False):\n        \"\"\"Normalize firing rates. For visualization.\"\"\"\n\n        raise NotImplementedError\n\n        if not inplace:\n            out = copy.deepcopy(self)\n        else:\n            out = self\n        if self.n_units &gt; 1:\n            per_unit_max = np.max(out.ratemap, axis=1)[..., np.newaxis]\n            out._ratemap = self.ratemap / np.tile(per_unit_max, (1, out.n_bins))\n        else:\n            per_unit_max = np.max(out.ratemap)\n            out._ratemap = self.ratemap / np.tile(per_unit_max, out.n_bins)\n        return out\n\n    def _normalize_firing_rate_by_occupancy(self):\n        # normalize spike counts by occupancy:\n        denom = np.tile(self.occupancy, (self.n_units, 1, 1))\n        denom[denom == 0] = 1\n        ratemap = self.ratemap / denom\n        return ratemap\n\n    @property\n    def is2d(self):\n        return True\n\n    @property\n    def occupancy(self):\n        return self._occupancy\n\n    @property\n    def n_units(self):\n        \"\"\"(int) The number of units.\"\"\"\n        try:\n            return len(self._unit_ids)\n        except TypeError:  # when unit_ids is an integer\n            return 1\n        except AttributeError:\n            return 0\n\n    @property\n    def shape(self):\n        \"\"\"(tuple) The shape of the TuningCurve2D ratemap.\"\"\"\n        if self.isempty:\n            return (self.n_units, 0, 0)\n        if len(self.ratemap.shape) == 1:\n            return (self.ratemap.shape[0], 1, 1)\n        return self.ratemap.shape\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty TuningCurve2D\" + address_str + \"&gt;\"\n        shapestr = \" with shape (%s, %s, %s)\" % (\n            self.shape[0],\n            self.shape[1],\n            self.shape[2],\n        )\n        return \"&lt;TuningCurve2D%s&gt;%s\" % (address_str, shapestr)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) True if TuningCurve1D is empty\"\"\"\n        try:\n            return len(self.ratemap) == 0\n        except TypeError:  # TypeError should happen if ratemap = []\n            return True\n\n    @property\n    def ratemap(self):\n        return self._ratemap\n\n    def __len__(self):\n        return self.n_units\n\n    def __iter__(self):\n        \"\"\"TuningCurve2D iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"TuningCurve2D iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_units - 1:\n            raise StopIteration\n        out = copy.copy(self)\n        out._ratemap = self.ratemap[index, :]\n        out._unit_ids = self.unit_ids[index]\n        out._unit_labels = self.unit_labels[index]\n        self._index += 1\n        return out\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n        \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n        mode : {\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to \u2018constant\u2019. Default is\n            \u2018reflect\u2019\n        cval : scalar, optional\n            Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0\n        \"\"\"\n        if sigma is None:\n            sigma = 0.1  # in units of extern\n        if truncate is None:\n            truncate = 4\n        if mode is None:\n            mode = \"reflect\"\n        if cval is None:\n            cval = 0.0\n\n        # Handle sigma parameter - support both scalar and array\n        sigma_array = np.asarray(sigma)\n        if sigma_array.ndim == 0:\n            # Single sigma value - apply to both dimensions\n            sigma_x_val = sigma_y_val = float(sigma_array)\n        else:\n            # Array of sigma values\n            if len(sigma_array) != 2:\n                raise ValueError(\n                    f\"sigma array length {len(sigma_array)} must equal 2 for TuningCurve2D\"\n                )\n            sigma_x_val = sigma_array[0]\n            sigma_y_val = sigma_array[1]\n\n        ds_x = (self.xbins[-1] - self.xbins[0]) / self.n_xbins\n        ds_y = (self.ybins[-1] - self.ybins[0]) / self.n_ybins\n        sigma_x = sigma_x_val / ds_x\n        sigma_y = sigma_y_val / ds_y\n\n        if not inplace:\n            out = copy.deepcopy(self)\n        else:\n            out = self\n\n        if self.mask is None:\n            if (self.n_units &gt; 1) | (self.ratemap.shape[0] &gt; 1):\n                out._ratemap = gaussian_filter(\n                    self.ratemap,\n                    sigma=(0, sigma_x, sigma_y),\n                    truncate=truncate,\n                    mode=mode,\n                    cval=cval,\n                )\n            elif self.ratemap.shape[0] == 1:\n                out._ratemap[0, :, :] = gaussian_filter(\n                    self.ratemap[0, :, :],\n                    sigma=(sigma_x, sigma_y),\n                    truncate=truncate,\n                    mode=mode,\n                    cval=cval,\n                )\n            else:\n                raise ValueError(\"ratemap has an unexpected shape\")\n        else:  # we have a mask!\n            # smooth, dealing properly with NANs\n            # NB! see https://stackoverflow.com/questions/18697532/gaussian-filtering-a-image-with-nan-in-python\n\n            masked_ratemap = self.ratemap.copy() * self.mask\n            V = masked_ratemap.copy()\n            V[masked_ratemap != masked_ratemap] = 0\n            W = 0 * masked_ratemap.copy() + 1\n            W[masked_ratemap != masked_ratemap] = 0\n\n            if (self.n_units &gt; 1) | (self.ratemap.shape[0] &gt; 1):\n                VV = gaussian_filter(\n                    V,\n                    sigma=(0, sigma_x, sigma_y),\n                    truncate=truncate,\n                    mode=mode,\n                    cval=cval,\n                )\n                WW = gaussian_filter(\n                    W,\n                    sigma=(0, sigma_x, sigma_y),\n                    truncate=truncate,\n                    mode=mode,\n                    cval=cval,\n                )\n                Z = VV / WW\n                out._ratemap = Z * self.mask\n            else:\n                VV = gaussian_filter(\n                    V, sigma=(sigma_x, sigma_y), truncate=truncate, mode=mode, cval=cval\n                )\n                WW = gaussian_filter(\n                    W, sigma=(sigma_x, sigma_y), truncate=truncate, mode=mode, cval=cval\n                )\n                Z = VV / WW\n                out._ratemap = Z * self.mask\n\n        return out\n\n    def reorder_units_by_ids(self, neworder, *, inplace=False):\n        \"\"\"Reorder units according to a specified order.\n\n        neworder must be list-like, of size (n_units,) and in terms of\n        unit_ids\n\n        Return\n        ------\n        out : reordered TuningCurve2D\n        \"\"\"\n\n        def swap_units(arr, frm, to):\n            \"\"\"swap 'units' of a 3D np.array\"\"\"\n            arr[[frm, to], :, :] = arr[[to, frm], :, :]\n\n        if inplace:\n            out = self\n        else:\n            out = copy.deepcopy(self)\n\n        # unit_ids = list(self.unit_ids)\n\n        neworder = [self.unit_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_units(out._ratemap, frm, to)\n            out._unit_ids[frm], out._unit_ids[to] = (\n                out._unit_ids[to],\n                out._unit_ids[frm],\n            )\n            out._unit_labels[frm], out._unit_labels[to] = (\n                out._unit_labels[to],\n                out._unit_labels[frm],\n            )\n            # TODO: re-build unit tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        return out\n\n    @property\n    def unit_ids(self):\n        \"\"\"Unit IDs contained in the SpikeTrain.\"\"\"\n        return list(self._unit_ids)\n\n    @unit_ids.setter\n    def unit_ids(self, val):\n        if len(val) != self.n_units:\n            # print(len(val))\n            # print(self.n_units)\n            raise TypeError(\"unit_ids must be of length n_units\")\n        elif len(set(val)) &lt; len(val):\n            raise TypeError(\"duplicate unit_ids are not allowed\")\n        else:\n            try:\n                # cast to int:\n                unit_ids = [int(id) for id in val]\n            except TypeError:\n                raise TypeError(\"unit_ids must be int-like\")\n        self._unit_ids = unit_ids\n\n    @property\n    def unit_labels(self):\n        \"\"\"Labels corresponding to units contained in the SpikeTrain.\"\"\"\n        if self._unit_labels is None:\n            warnings.warn(\"unit labels have not yet been specified\")\n        return self._unit_labels\n\n    @unit_labels.setter\n    def unit_labels(self, val):\n        if len(val) != self.n_units:\n            raise TypeError(\"labels must be of length n_units\")\n        else:\n            try:\n                # cast to str:\n                labels = [str(label) for label in val]\n            except TypeError:\n                raise TypeError(\"labels must be string-like\")\n        self._unit_labels = labels\n\n    @property\n    def unit_tags(self):\n        \"\"\"Tags corresponding to units contained in the SpikeTrain\"\"\"\n        if self._unit_tags is None:\n            warnings.warn(\"unit tags have not yet been specified\")\n        return self._unit_tags\n\n    @property\n    def label(self):\n        \"\"\"Label pertaining to the source of the spike train.\"\"\"\n        if self._label is None:\n            warnings.warn(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.bins","title":"<code>bins</code>  <code>property</code>","text":"<p>External correlate bins.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) True if TuningCurve1D is empty</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label pertaining to the source of the spike train.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.mask","title":"<code>mask</code>  <code>property</code>","text":"<p>(n_xbins, n_ybins) Mask for tuning curve.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.n_units","title":"<code>n_units</code>  <code>property</code>","text":"<p>(int) The number of units.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.n_xbins","title":"<code>n_xbins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.n_ybins","title":"<code>n_ybins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>(tuple) The shape of the TuningCurve2D ratemap.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.unit_ids","title":"<code>unit_ids</code>  <code>property</code> <code>writable</code>","text":"<p>Unit IDs contained in the SpikeTrain.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.unit_labels","title":"<code>unit_labels</code>  <code>property</code> <code>writable</code>","text":"<p>Labels corresponding to units contained in the SpikeTrain.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.unit_tags","title":"<code>unit_tags</code>  <code>property</code>","text":"<p>Tags corresponding to units contained in the SpikeTrain</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.xbin_centers","title":"<code>xbin_centers</code>  <code>property</code>","text":"<p>External correlate bin centers.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.xbins","title":"<code>xbins</code>  <code>property</code>","text":"<p>External correlate bins.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.ybin_centers","title":"<code>ybin_centers</code>  <code>property</code>","text":"<p>External correlate bin centers.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.ybins","title":"<code>ybins</code>  <code>property</code>","text":"<p>External correlate bins.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.information_rate","title":"<code>information_rate()</code>","text":"<p>Compute the information rate...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def information_rate(self):\n    \"\"\"Compute the information rate...\"\"\"\n    return utils.information_rate(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.max","title":"<code>max(*, axis=None)</code>","text":"<p>Returns the mean of firing rate (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>When axis is None, the global max firing rate is returned. When axis is 0, the max firing rates across units, as a function of the external correlate (e.g. position) are returned. When axis is 1, the max firing rate for each unit is returned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>max</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def max(self, *, axis=None):\n    \"\"\"Returns the mean of firing rate (in Hz).\n    Parameters\n    ----------\n    axis : int, optional\n        When axis is None, the global max firing rate is returned.\n        When axis is 0, the max firing rates across units, as a\n        function of the external correlate (e.g. position) are\n        returned.\n        When axis is 1, the max firing rate for each unit is\n        returned.\n    Returns\n    -------\n    max :\n    \"\"\"\n    if (axis is None) | (axis == 0):\n        maxes = np.max(self.ratemap, axis=axis)\n    elif axis == 1:\n        maxes = [\n            self.ratemap[unit_i, :, :].max()\n            for unit_i in range(self.ratemap.shape[0])\n        ]\n\n    return maxes\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.mean","title":"<code>mean(*, axis=None)</code>","text":"<p>Returns the mean of firing rate (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>When axis is None, the global mean firing rate is returned. When axis is 0, the mean firing rates across units, as a function of the external correlate (e.g. position) are returned. When axis is 1, the mean firing rate for each unit is returned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mean</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def mean(self, *, axis=None):\n    \"\"\"Returns the mean of firing rate (in Hz).\n    Parameters\n    ----------\n    axis : int, optional\n        When axis is None, the global mean firing rate is returned.\n        When axis is 0, the mean firing rates across units, as a\n        function of the external correlate (e.g. position) are\n        returned.\n        When axis is 1, the mean firing rate for each unit is\n        returned.\n    Returns\n    -------\n    mean :\n    \"\"\"\n\n    if (axis is None) | (axis == 0):\n        means = np.mean(self.ratemap, axis=axis)\n    elif axis == 1:\n        means = [\n            self.ratemap[unit_i, :, :].mean()\n            for unit_i in range(self.ratemap.shape[0])\n        ]\n\n    return means\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.min","title":"<code>min(*, axis=None)</code>","text":"<p>Returns the min of firing rate (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>When axis is None, the global min firing rate is returned. When axis is 0, the min firing rates across units, as a function of the external correlate (e.g. position) are returned. When axis is 1, the min firing rate for each unit is returned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>min</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def min(self, *, axis=None):\n    \"\"\"Returns the min of firing rate (in Hz).\n    Parameters\n    ----------\n    axis : int, optional\n        When axis is None, the global min firing rate is returned.\n        When axis is 0, the min firing rates across units, as a\n        function of the external correlate (e.g. position) are\n        returned.\n        When axis is 1, the min firing rate for each unit is\n        returned.\n    Returns\n    -------\n    min :\n    \"\"\"\n\n    if (axis is None) | (axis == 0):\n        mins = np.min(self.ratemap, axis=axis)\n    elif axis == 1:\n        mins = [\n            self.ratemap[unit_i, :, :].min()\n            for unit_i in range(self.ratemap.shape[0])\n        ]\n\n    return mins\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.normalize","title":"<code>normalize(inplace=False)</code>","text":"<p>Normalize firing rates. For visualization.</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def normalize(self, inplace=False):\n    \"\"\"Normalize firing rates. For visualization.\"\"\"\n\n    raise NotImplementedError\n\n    if not inplace:\n        out = copy.deepcopy(self)\n    else:\n        out = self\n    if self.n_units &gt; 1:\n        per_unit_max = np.max(out.ratemap, axis=1)[..., np.newaxis]\n        out._ratemap = self.ratemap / np.tile(per_unit_max, (1, out.n_bins))\n    else:\n        per_unit_max = np.max(out.ratemap)\n        out._ratemap = self.ratemap / np.tile(per_unit_max, out.n_bins)\n    return out\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.reorder_units_by_ids","title":"<code>reorder_units_by_ids(neworder, *, inplace=False)</code>","text":"<p>Reorder units according to a specified order.</p> <p>neworder must be list-like, of size (n_units,) and in terms of unit_ids</p> Return <p>out : reordered TuningCurve2D</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def reorder_units_by_ids(self, neworder, *, inplace=False):\n    \"\"\"Reorder units according to a specified order.\n\n    neworder must be list-like, of size (n_units,) and in terms of\n    unit_ids\n\n    Return\n    ------\n    out : reordered TuningCurve2D\n    \"\"\"\n\n    def swap_units(arr, frm, to):\n        \"\"\"swap 'units' of a 3D np.array\"\"\"\n        arr[[frm, to], :, :] = arr[[to, frm], :, :]\n\n    if inplace:\n        out = self\n    else:\n        out = copy.deepcopy(self)\n\n    # unit_ids = list(self.unit_ids)\n\n    neworder = [self.unit_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        swap_units(out._ratemap, frm, to)\n        out._unit_ids[frm], out._unit_ids[to] = (\n            out._unit_ids[to],\n            out._unit_ids[frm],\n        )\n        out._unit_labels[frm], out._unit_labels[to] = (\n            out._unit_labels[to],\n            out._unit_labels[frm],\n        )\n        # TODO: re-build unit tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    return out\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.smooth","title":"<code>smooth(*, sigma=None, truncate=None, inplace=False, mode=None, cval=None)</code>","text":"<p>Smooths the tuning curve with a Gaussian kernel.</p> <p>mode : {\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional     The mode parameter determines how the array borders are handled,     where cval is the value when mode is equal to \u2018constant\u2019. Default is     \u2018reflect\u2019 cval : scalar, optional     Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n    \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n    mode : {\u2018reflect\u2019, \u2018constant\u2019, \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to \u2018constant\u2019. Default is\n        \u2018reflect\u2019\n    cval : scalar, optional\n        Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0\n    \"\"\"\n    if sigma is None:\n        sigma = 0.1  # in units of extern\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n\n    # Handle sigma parameter - support both scalar and array\n    sigma_array = np.asarray(sigma)\n    if sigma_array.ndim == 0:\n        # Single sigma value - apply to both dimensions\n        sigma_x_val = sigma_y_val = float(sigma_array)\n    else:\n        # Array of sigma values\n        if len(sigma_array) != 2:\n            raise ValueError(\n                f\"sigma array length {len(sigma_array)} must equal 2 for TuningCurve2D\"\n            )\n        sigma_x_val = sigma_array[0]\n        sigma_y_val = sigma_array[1]\n\n    ds_x = (self.xbins[-1] - self.xbins[0]) / self.n_xbins\n    ds_y = (self.ybins[-1] - self.ybins[0]) / self.n_ybins\n    sigma_x = sigma_x_val / ds_x\n    sigma_y = sigma_y_val / ds_y\n\n    if not inplace:\n        out = copy.deepcopy(self)\n    else:\n        out = self\n\n    if self.mask is None:\n        if (self.n_units &gt; 1) | (self.ratemap.shape[0] &gt; 1):\n            out._ratemap = gaussian_filter(\n                self.ratemap,\n                sigma=(0, sigma_x, sigma_y),\n                truncate=truncate,\n                mode=mode,\n                cval=cval,\n            )\n        elif self.ratemap.shape[0] == 1:\n            out._ratemap[0, :, :] = gaussian_filter(\n                self.ratemap[0, :, :],\n                sigma=(sigma_x, sigma_y),\n                truncate=truncate,\n                mode=mode,\n                cval=cval,\n            )\n        else:\n            raise ValueError(\"ratemap has an unexpected shape\")\n    else:  # we have a mask!\n        # smooth, dealing properly with NANs\n        # NB! see https://stackoverflow.com/questions/18697532/gaussian-filtering-a-image-with-nan-in-python\n\n        masked_ratemap = self.ratemap.copy() * self.mask\n        V = masked_ratemap.copy()\n        V[masked_ratemap != masked_ratemap] = 0\n        W = 0 * masked_ratemap.copy() + 1\n        W[masked_ratemap != masked_ratemap] = 0\n\n        if (self.n_units &gt; 1) | (self.ratemap.shape[0] &gt; 1):\n            VV = gaussian_filter(\n                V,\n                sigma=(0, sigma_x, sigma_y),\n                truncate=truncate,\n                mode=mode,\n                cval=cval,\n            )\n            WW = gaussian_filter(\n                W,\n                sigma=(0, sigma_x, sigma_y),\n                truncate=truncate,\n                mode=mode,\n                cval=cval,\n            )\n            Z = VV / WW\n            out._ratemap = Z * self.mask\n        else:\n            VV = gaussian_filter(\n                V, sigma=(sigma_x, sigma_y), truncate=truncate, mode=mode, cval=cval\n            )\n            WW = gaussian_filter(\n                W, sigma=(sigma_x, sigma_y), truncate=truncate, mode=mode, cval=cval\n            )\n            Z = VV / WW\n            out._ratemap = Z * self.mask\n\n    return out\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.spatial_information","title":"<code>spatial_information()</code>","text":"<p>Compute the spatial information and firing sparsity...</p> <p>The specificity index examines the amount of information (in bits) that a single spike conveys about the animal's location (i.e., how well cell firing predicts the animal's location).The spatial information content of cell discharge was calculated using the formula:     information content = \\Sum P_i(R_i/R)log_2(R_i/R) where i is the bin number, P_i, is the probability for occupancy of bin i, R_i, is the mean firing rate for bin i, and R is the overall mean firing rate.</p> <p>In order to account for the effects of low firing rates (with fewer spikes there is a tendency toward higher information content) or random bursts of firing, the spike firing time-series was randomly offset in time from the rat location time-series, and the information content was calculated. A distribution of the information content based on 100 such random shifts was obtained and was used to compute a standardized score (Zscore) of information content for that cell. While the distribution is not composed of independent samples, it was nominally normally distributed, and a Z value of 2.29 was chosen as a cut-off for significance (the equivalent of a one-tailed t-test with P = 0.01 under a normal distribution).</p> Reference(s) <p>Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,     and Skaggs, W. E. (1994). \"Spatial information content and     reliability of hippocampal CA1 neurons: effects of visual     input\", Hippocampus, 4(4), 410-421.</p> <p>Parameters:</p> Name Type Description Default <code>Returns</code> required <code>si</code> <code>array of shape (n_units,)</code> <p>spatial information (in bits) per spike</p> required Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_information(self):\n    \"\"\"Compute the spatial information and firing sparsity...\n\n    The specificity index examines the amount of information\n    (in bits) that a single spike conveys about the animal's\n    location (i.e., how well cell firing predicts the animal's\n    location).The spatial information content of cell discharge was\n    calculated using the formula:\n        information content = \\\\Sum P_i(R_i/R)log_2(R_i/R)\n    where i is the bin number, P_i, is the probability for occupancy\n    of bin i, R_i, is the mean firing rate for bin i, and R is the\n    overall mean firing rate.\n\n    In order to account for the effects of low firing rates (with\n    fewer spikes there is a tendency toward higher information\n    content) or random bursts of firing, the spike firing\n    time-series was randomly offset in time from the rat location\n    time-series, and the information content was calculated. A\n    distribution of the information content based on 100 such random\n    shifts was obtained and was used to compute a standardized score\n    (Zscore) of information content for that cell. While the\n    distribution is not composed of independent samples, it was\n    nominally normally distributed, and a Z value of 2.29 was chosen\n    as a cut-off for significance (the equivalent of a one-tailed\n    t-test with P = 0.01 under a normal distribution).\n\n    Reference(s)\n    ------------\n    Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n        and Skaggs, W. E. (1994). \"Spatial information content and\n        reliability of hippocampal CA1 neurons: effects of visual\n        input\", Hippocampus, 4(4), 410-421.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    si : array of shape (n_units,)\n        spatial information (in bits) per spike\n    \"\"\"\n\n    return utils.spatial_information(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.spatial_selectivity","title":"<code>spatial_selectivity()</code>","text":"<p>Compute the spatial selectivity...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_selectivity(self):\n    \"\"\"Compute the spatial selectivity...\"\"\"\n    return utils.spatial_selectivity(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.spatial_sparsity","title":"<code>spatial_sparsity()</code>","text":"<p>Compute the spatial information and firing sparsity...</p> <p>The specificity index examines the amount of information (in bits) that a single spike conveys about the animal's location (i.e., how well cell firing predicts the animal's location).The spatial information content of cell discharge was calculated using the formula:     information content = \\Sum P_i(R_i/R)log_2(R_i/R) where i is the bin number, P_i, is the probability for occupancy of bin i, R_i, is the mean firing rate for bin i, and R is the overall mean firing rate.</p> <p>In order to account for the effects of low firing rates (with fewer spikes there is a tendency toward higher information content) or random bursts of firing, the spike firing time-series was randomly offset in time from the rat location time-series, and the information content was calculated. A distribution of the information content based on 100 such random shifts was obtained and was used to compute a standardized score (Zscore) of information content for that cell. While the distribution is not composed of independent samples, it was nominally normally distributed, and a Z value of 2.29 was chosen as a cut-off for significance (the equivalent of a one-tailed t-test with P = 0.01 under a normal distribution).</p> Reference(s) <p>Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,     and Skaggs, W. E. (1994). \"Spatial information content and     reliability of hippocampal CA1 neurons: effects of visual     input\", Hippocampus, 4(4), 410-421.</p> <p>Parameters:</p> Name Type Description Default <code>Returns</code> required <code>si</code> <code>array of shape (n_units,)</code> <p>spatial information (in bits) per unit</p> required <code>sparsity</code> <p>sparsity (in percent) for each unit</p> required Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_sparsity(self):\n    \"\"\"Compute the spatial information and firing sparsity...\n\n    The specificity index examines the amount of information\n    (in bits) that a single spike conveys about the animal's\n    location (i.e., how well cell firing predicts the animal's\n    location).The spatial information content of cell discharge was\n    calculated using the formula:\n        information content = \\Sum P_i(R_i/R)log_2(R_i/R)\n    where i is the bin number, P_i, is the probability for occupancy\n    of bin i, R_i, is the mean firing rate for bin i, and R is the\n    overall mean firing rate.\n\n    In order to account for the effects of low firing rates (with\n    fewer spikes there is a tendency toward higher information\n    content) or random bursts of firing, the spike firing\n    time-series was randomly offset in time from the rat location\n    time-series, and the information content was calculated. A\n    distribution of the information content based on 100 such random\n    shifts was obtained and was used to compute a standardized score\n    (Zscore) of information content for that cell. While the\n    distribution is not composed of independent samples, it was\n    nominally normally distributed, and a Z value of 2.29 was chosen\n    as a cut-off for significance (the equivalent of a one-tailed\n    t-test with P = 0.01 under a normal distribution).\n\n    Reference(s)\n    ------------\n    Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n        and Skaggs, W. E. (1994). \"Spatial information content and\n        reliability of hippocampal CA1 neurons: effects of visual\n        input\", Hippocampus, 4(4), 410-421.\n\n    Parameters\n    ----------\n\n    Returns\n    -------\n    si : array of shape (n_units,)\n        spatial information (in bits) per unit\n    sparsity: array of shape (n_units,)\n        sparsity (in percent) for each unit\n    \"\"\"\n    return utils.spatial_sparsity(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurve2D.std","title":"<code>std(*, axis=None)</code>","text":"<p>Returns the std of firing rate (in Hz).</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>When axis is None, the global std firing rate is returned. When axis is 0, the std firing rates across units, as a function of the external correlate (e.g. position) are returned. When axis is 1, the std firing rate for each unit is returned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>std</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def std(self, *, axis=None):\n    \"\"\"Returns the std of firing rate (in Hz).\n    Parameters\n    ----------\n    axis : int, optional\n        When axis is None, the global std firing rate is returned.\n        When axis is 0, the std firing rates across units, as a\n        function of the external correlate (e.g. position) are\n        returned.\n        When axis is 1, the std firing rate for each unit is\n        returned.\n    Returns\n    -------\n    std :\n    \"\"\"\n\n    if (axis is None) | (axis == 0):\n        stds = np.std(self.ratemap, axis=axis)\n    elif axis == 1:\n        stds = [\n            self.ratemap[unit_i, :, :].std()\n            for unit_i in range(self.ratemap.shape[0])\n        ]\n\n    return stds\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND","title":"<code>TuningCurveND</code>","text":"<p>Tuning curves (N-dimensional) of multiple units.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array for tuning curve estimation.</p> <code>None</code> <code>extern</code> <code>array - like</code> <p>External correlates (e.g., position).</p> <code>None</code> <code>ratemap</code> <code>ndarray</code> <p>Precomputed rate map.</p> <code>None</code> <code>sigma</code> <code>float or array - like</code> <p>Standard deviation for Gaussian smoothing. If float, applied to all dimensions. If array-like, must have length equal to n_dimensions.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Truncation parameter for smoothing.</p> <code>None</code> <code>n_bins</code> <code>array - like</code> <p>Number of bins for each dimension.</p> <code>None</code> <code>transform_func</code> <code>callable</code> <p>Function to transform external correlates.</p> <code>None</code> <code>minbgrate</code> <code>float</code> <p>Minimum background firing rate.</p> <code>None</code> <code>ext_min</code> <code>array - like</code> <p>Extent of the external correlates for each dimension.</p> <code>None</code> <code>ext_max</code> <code>array - like</code> <p>Extent of the external correlates for each dimension.</p> <code>None</code> <code>extlabels</code> <code>list</code> <p>Labels for external correlates.</p> <code>None</code> <code>min_duration</code> <code>float</code> <p>Minimum duration for occupancy.</p> <code>None</code> <code>unit_ids</code> <code>list</code> <p>Unit IDs.</p> <code>None</code> <code>unit_labels</code> <code>list</code> <p>Unit labels.</p> <code>None</code> <code>unit_tags</code> <code>list</code> <p>Unit tags.</p> <code>None</code> <code>label</code> <code>str</code> <p>Label for the tuning curve.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>If True, create an empty TuningCurveND.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>ratemap</code> <code>ndarray</code> <p>The N-D rate map with shape (n_units, *n_bins).</p> <code>occupancy</code> <code>ndarray</code> <p>Occupancy map.</p> <code>unit_ids</code> <code>list</code> <p>Unit IDs.</p> <code>unit_labels</code> <code>list</code> <p>Unit labels.</p> <code>unit_tags</code> <code>list</code> <p>Unit tags.</p> <code>label</code> <code>str</code> <p>Label for the tuning curve.</p> <code>mask</code> <code>ndarray</code> <p>Mask for valid regions.</p> <code>n_dimensions</code> <code>int</code> <p>Number of dimensions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; n_units = 5  # Number of units\n&gt;&gt;&gt; spike_times = np.sort(np.random.rand(n_units, 1000))\n&gt;&gt;&gt; # Create SpikeTrainArray and bin it\n&gt;&gt;&gt; sta = nel.SpikeTrainArray(\n...     spike_times,\n...     fs=1000,\n... )\n&gt;&gt;&gt; # Bin the spike train array\n&gt;&gt;&gt; bst = sta.bin(ds=0.1)  # 100ms bins\n&gt;&gt;&gt; # Create an external AnalogSignalArray\n&gt;&gt;&gt; # For example, a random signal with 4 channels and 1000 samples at 10 Hz\n&gt;&gt;&gt; extern = nel.AnalogSignalArray(data=np.random.rand(4, 1000), fs=10.0)\n&gt;&gt;&gt; # Create a 4D tuning curve\n&gt;&gt;&gt; tuning_curve = nel.TuningCurveND(bst=bst, extern=extern, n_bins=[2, 5, 4, 3])\n&gt;&gt;&gt; tuning_curve.ratemap.shape\n(5, 2, 5, 4, 3)  # 5 units, with bins in each of the 4 dimensions\n</code></pre> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>class TuningCurveND:\n    \"\"\"\n    Tuning curves (N-dimensional) of multiple units.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray, optional\n        Binned spike train array for tuning curve estimation.\n    extern : array-like, optional\n        External correlates (e.g., position).\n    ratemap : np.ndarray, optional\n        Precomputed rate map.\n    sigma : float or array-like, optional\n        Standard deviation for Gaussian smoothing. If float, applied to all\n        dimensions. If array-like, must have length equal to n_dimensions.\n    truncate : float, optional\n        Truncation parameter for smoothing.\n    n_bins : array-like, optional\n        Number of bins for each dimension.\n    transform_func : callable, optional\n        Function to transform external correlates.\n    minbgrate : float, optional\n        Minimum background firing rate.\n    ext_min, ext_max : array-like, optional\n        Extent of the external correlates for each dimension.\n    extlabels : list, optional\n        Labels for external correlates.\n    min_duration : float, optional\n        Minimum duration for occupancy.\n    unit_ids : list, optional\n        Unit IDs.\n    unit_labels : list, optional\n        Unit labels.\n    unit_tags : list, optional\n        Unit tags.\n    label : str, optional\n        Label for the tuning curve.\n    empty : bool, optional\n        If True, create an empty TuningCurveND.\n\n    Attributes\n    ----------\n    ratemap : np.ndarray\n        The N-D rate map with shape (n_units, *n_bins).\n    occupancy : np.ndarray\n        Occupancy map.\n    unit_ids : list\n        Unit IDs.\n    unit_labels : list\n        Unit labels.\n    unit_tags : list\n        Unit tags.\n    label : str\n        Label for the tuning curve.\n    mask : np.ndarray\n        Mask for valid regions.\n    n_dimensions : int\n        Number of dimensions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; n_units = 5  # Number of units\n    &gt;&gt;&gt; spike_times = np.sort(np.random.rand(n_units, 1000))\n    &gt;&gt;&gt; # Create SpikeTrainArray and bin it\n    &gt;&gt;&gt; sta = nel.SpikeTrainArray(\n    ...     spike_times,\n    ...     fs=1000,\n    ... )\n    &gt;&gt;&gt; # Bin the spike train array\n    &gt;&gt;&gt; bst = sta.bin(ds=0.1)  # 100ms bins\n    &gt;&gt;&gt; # Create an external AnalogSignalArray\n    &gt;&gt;&gt; # For example, a random signal with 4 channels and 1000 samples at 10 Hz\n    &gt;&gt;&gt; extern = nel.AnalogSignalArray(data=np.random.rand(4, 1000), fs=10.0)\n    &gt;&gt;&gt; # Create a 4D tuning curve\n    &gt;&gt;&gt; tuning_curve = nel.TuningCurveND(bst=bst, extern=extern, n_bins=[2, 5, 4, 3])\n    &gt;&gt;&gt; tuning_curve.ratemap.shape\n    (5, 2, 5, 4, 3)  # 5 units, with bins in each of the 4 dimensions\n    \"\"\"\n\n    __attributes__ = [\n        \"_ratemap\",\n        \"_occupancy\",\n        \"_unit_ids\",\n        \"_unit_labels\",\n        \"_unit_tags\",\n        \"_label\",\n        \"_mask\",\n        \"_n_dimensions\",\n        \"_bins_list\",\n        \"_extlabels\",\n    ]\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def __init__(\n        self,\n        *,\n        bst=None,\n        extern=None,\n        ratemap=None,\n        sigma=None,\n        truncate=None,\n        n_bins=None,\n        transform_func=None,\n        minbgrate=None,\n        ext_min=None,\n        ext_max=None,\n        extlabels=None,\n        min_duration=None,\n        unit_ids=None,\n        unit_labels=None,\n        unit_tags=None,\n        label=None,\n        empty=False,\n    ):\n        \"\"\"\n        Initialize TuningCurveND object.\n\n        NOTE: tuning curves in ND have shapes (n_units, *n_bins) so that\n        the first dimension is always n_units, followed by the spatial dimensions.\n\n        If sigma is nonzero, then smoothing is applied.\n\n        We always require bst and extern, and then some combination of\n            (1) n_bins, ext_min, ext_max, transform_func*\n            (2) bin edges, transform_func*\n\n            transform_func operates on extern and returns a 2D array of shape\n            (n_dimensions, n_timepoints). If no transform is specified, the\n            identity operator is assumed.\n        \"\"\"\n        # TODO: input validation\n        if not empty:\n            if ratemap is None:\n                assert bst is not None, (\n                    \"bst must be specified or ratemap must be specified!\"\n                )\n                assert extern is not None, (\n                    \"extern must be specified or ratemap must be specified!\"\n                )\n            else:\n                assert bst is None, \"ratemap and bst cannot both be specified!\"\n                assert extern is None, \"ratemap and extern cannot both be specified!\"\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        if ratemap is not None:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._init_from_ratemap(\n                ratemap=ratemap,\n                ext_min=ext_min,\n                ext_max=ext_max,\n                extlabels=extlabels,\n                unit_ids=unit_ids,\n                unit_labels=unit_labels,\n                unit_tags=unit_tags,\n                label=label,\n            )\n            return\n\n        self._mask = None  # TODO: change this when we can learn a mask in __init__!\n        self._bst = bst\n        self._extern = extern\n\n        if minbgrate is None:\n            minbgrate = 0.01  # Hz minimum background firing rate\n\n        # Handle n_bins parameter\n        if n_bins is not None:\n            n_bins = np.asarray(n_bins, dtype=int)\n            if n_bins.ndim == 0:\n                raise ValueError(\"n_bins must be array-like for ND tuning curves\")\n            self._n_dimensions = len(n_bins)\n\n            # Set default ext_min and ext_max if not provided\n            if ext_min is None:\n                ext_min = np.zeros(self._n_dimensions)\n            if ext_max is None:\n                ext_max = np.ones(self._n_dimensions)\n\n            ext_min = np.asarray(ext_min)\n            ext_max = np.asarray(ext_max)\n\n            if len(ext_min) != self._n_dimensions or len(ext_max) != self._n_dimensions:\n                raise ValueError(\n                    \"ext_min and ext_max must have length equal to n_dimensions\"\n                )\n\n            # Create bin edges for each dimension\n            self._bins_list = []\n            for i in range(self._n_dimensions):\n                bins = np.linspace(ext_min[i], ext_max[i], n_bins[i] + 1)\n                self._bins_list.append(bins)\n        else:\n            raise NotImplementedError(\"Must specify n_bins for TuningCurveND\")\n\n        if min_duration is None:\n            min_duration = 0\n\n        self._min_duration = min_duration\n        self._unit_ids = bst.unit_ids\n        self._unit_labels = bst.unit_labels\n        self._unit_tags = bst.unit_tags  # no input validation yet\n        self.label = label\n\n        # Set extlabels\n        if extlabels is None:\n            self._extlabels = [f\"dim_{i}\" for i in range(self._n_dimensions)]\n        else:\n            if len(extlabels) != self._n_dimensions:\n                raise ValueError(\"extlabels must have length equal to n_dimensions\")\n            self._extlabels = extlabels\n\n        if transform_func is None:\n            self.trans_func = self._trans_func\n        else:\n            self.trans_func = transform_func\n\n        # compute occupancy\n        self._occupancy = self._compute_occupancy()\n        # compute ratemap (in Hz)\n        self._ratemap = self._compute_ratemap()\n        # normalize firing rate by occupancy\n        self._ratemap = self._normalize_firing_rate_by_occupancy()\n        # enforce minimum background firing rate\n        self._ratemap[self._ratemap &lt; minbgrate] = (\n            minbgrate  # background firing rate of 0.01 Hz\n        )\n\n        if sigma is not None:\n            if np.any(np.asarray(sigma) &gt; 0):\n                self.smooth(sigma=sigma, truncate=truncate, inplace=True)\n\n        # optionally detach _bst and _extern to save space when pickling, for example\n        self._detach()\n\n    def spatial_information(self):\n        \"\"\"Compute the spatial information and firing sparsity...\"\"\"\n        return utils.spatial_information(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def information_rate(self):\n        \"\"\"Compute the information rate...\"\"\"\n        return utils.information_rate(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def spatial_selectivity(self):\n        \"\"\"Compute the spatial selectivity...\"\"\"\n        return utils.spatial_selectivity(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def spatial_sparsity(self):\n        \"\"\"Compute the spatial information and firing sparsity...\"\"\"\n        return utils.spatial_sparsity(ratemap=self.ratemap, Pi=self.occupancy)\n\n    def __add__(self, other):\n        out = copy.copy(self)\n\n        if isinstance(other, numbers.Number):\n            out._ratemap = out.ratemap + other\n        elif isinstance(other, TuningCurveND):\n            # TODO: this should merge two TuningCurveND objects\n            raise NotImplementedError\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: 'TuningCurveND' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n        return out\n\n    def __sub__(self, other):\n        out = copy.copy(self)\n        out._ratemap = out.ratemap - other\n        return out\n\n    def __mul__(self, other):\n        \"\"\"overloaded * operator.\"\"\"\n        out = copy.copy(self)\n        out._ratemap = out.ratemap * other\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        \"\"\"overloaded / operator.\"\"\"\n        out = copy.copy(self)\n        out._ratemap = out.ratemap / other\n        return out\n\n    def _init_from_ratemap(\n        self,\n        ratemap,\n        occupancy=None,\n        ext_min=None,\n        ext_max=None,\n        extlabels=None,\n        unit_ids=None,\n        unit_labels=None,\n        unit_tags=None,\n        label=None,\n    ):\n        \"\"\"Initialize a TuningCurveND object from a ratemap.\n\n        Parameters\n        ----------\n        ratemap : array\n            Array of shape (n_units, *n_bins)\n        \"\"\"\n        self._n_dimensions = ratemap.ndim - 1  # subtract 1 for units dimension\n        n_units = ratemap.shape[0]\n        bin_shape = ratemap.shape[1:]\n\n        if occupancy is None:\n            # assume uniform occupancy\n            self._occupancy = np.ones(bin_shape)\n\n        if ext_min is None:\n            ext_min = np.zeros(self._n_dimensions)\n        if ext_max is None:\n            ext_max = np.ones(self._n_dimensions)\n\n        ext_min = np.asarray(ext_min)\n        ext_max = np.asarray(ext_max)\n\n        self._bins_list = []\n        for i in range(self._n_dimensions):\n            bins = np.linspace(ext_min[i], ext_max[i], bin_shape[i] + 1)\n            self._bins_list.append(bins)\n\n        self._ratemap = ratemap\n\n        # inherit unit IDs if available, otherwise initialize to default\n        if unit_ids is None:\n            unit_ids = list(range(1, n_units + 1))\n\n        unit_ids = np.array(unit_ids, ndmin=1)  # standardize unit_ids\n\n        # if unit_labels is empty, default to unit_ids\n        if unit_labels is None:\n            unit_labels = unit_ids\n\n        unit_labels = np.array(unit_labels, ndmin=1)  # standardize\n\n        self._unit_ids = unit_ids\n        self._unit_labels = unit_labels\n        self._unit_tags = unit_tags  # no input validation yet\n\n        # Set extlabels\n        if extlabels is None:\n            self._extlabels = [f\"dim_{i}\" for i in range(self._n_dimensions)]\n        else:\n            if len(extlabels) != self._n_dimensions:\n                raise ValueError(\"extlabels must have length equal to n_dimensions\")\n            self._extlabels = extlabels\n\n        if label is not None:\n            self.label = label\n\n        return self\n\n    def max(self, *, axis=None):\n        \"\"\"Returns the max of firing rate (in Hz).\"\"\"\n        if axis is None or axis == 0:\n            maxes = np.max(self.ratemap, axis=axis)\n        elif axis == 1:\n            # Flatten all spatial dimensions for per-unit calculation\n            maxes = [\n                self.ratemap[unit_i].max() for unit_i in range(self.ratemap.shape[0])\n            ]\n        else:\n            maxes = np.max(self.ratemap, axis=axis)\n\n        return maxes\n\n    def min(self, *, axis=None):\n        \"\"\"Returns the min of firing rate (in Hz).\"\"\"\n        if axis is None or axis == 0:\n            mins = np.min(self.ratemap, axis=axis)\n        elif axis == 1:\n            # Flatten all spatial dimensions for per-unit calculation\n            mins = [\n                self.ratemap[unit_i].min() for unit_i in range(self.ratemap.shape[0])\n            ]\n        else:\n            mins = np.min(self.ratemap, axis=axis)\n\n        return mins\n\n    def mean(self, *, axis=None):\n        \"\"\"Returns the mean of firing rate (in Hz).\"\"\"\n        if axis is None or axis == 0:\n            means = np.mean(self.ratemap, axis=axis)\n        elif axis == 1:\n            # Flatten all spatial dimensions for per-unit calculation\n            means = [\n                self.ratemap[unit_i].mean() for unit_i in range(self.ratemap.shape[0])\n            ]\n        else:\n            means = np.mean(self.ratemap, axis=axis)\n\n        return means\n\n    def std(self, *, axis=None):\n        \"\"\"Returns the std of firing rate (in Hz).\"\"\"\n        if axis is None or axis == 0:\n            stds = np.std(self.ratemap, axis=axis)\n        elif axis == 1:\n            # Flatten all spatial dimensions for per-unit calculation\n            stds = [\n                self.ratemap[unit_i].std() for unit_i in range(self.ratemap.shape[0])\n            ]\n        else:\n            stds = np.std(self.ratemap, axis=axis)\n\n        return stds\n\n    def _detach(self):\n        \"\"\"Detach bst and extern from tuning curve.\"\"\"\n        self._bst = None\n        self._extern = None\n\n    @property\n    def mask(self):\n        \"\"\"Mask for tuning curve.\"\"\"\n        return self._mask\n\n    @property\n    def n_bins(self):\n        \"\"\"Total number of bins (product of bins in all dimensions).\"\"\"\n        return np.prod([len(bins) - 1 for bins in self._bins_list])\n\n    @property\n    def n_bins_per_dim(self):\n        \"\"\"Number of bins for each dimension.\"\"\"\n        return [len(bins) - 1 for bins in self._bins_list]\n\n    @property\n    def bins(self):\n        \"\"\"List of bin edges for each dimension.\"\"\"\n        return self._bins_list\n\n    @property\n    def bin_centers(self):\n        \"\"\"List of bin centers for each dimension.\"\"\"\n        centers = []\n        for bins in self._bins_list:\n            center = (bins + (bins[1] - bins[0]) / 2)[:-1]\n            centers.append(center)\n        return centers\n\n    @property\n    def n_dimensions(self):\n        \"\"\"Number of dimensions.\"\"\"\n        return self._n_dimensions\n\n    @property\n    def extlabels(self):\n        \"\"\"Labels for external correlates.\"\"\"\n        return self._extlabels\n\n    # Backwards compatibility properties for 1D and 2D cases\n    @property\n    def xbins(self):\n        \"\"\"X-dimension bins (for backwards compatibility when n_dimensions &gt;= 1).\"\"\"\n        if self._n_dimensions &gt;= 1:\n            return self._bins_list[0]\n        else:\n            raise AttributeError(\"xbins not available for 0-dimensional tuning curves\")\n\n    @property\n    def ybins(self):\n        \"\"\"Y-dimension bins (for backwards compatibility when n_dimensions &gt;= 2).\"\"\"\n        if self._n_dimensions &gt;= 2:\n            return self._bins_list[1]\n        else:\n            raise AttributeError(\n                \"ybins not available for tuning curves with &lt; 2 dimensions\"\n            )\n\n    @property\n    def n_xbins(self):\n        \"\"\"Number of X-dimension bins (for backwards compatibility).\"\"\"\n        if self._n_dimensions &gt;= 1:\n            return len(self._bins_list[0]) - 1\n        else:\n            raise AttributeError(\n                \"n_xbins not available for 0-dimensional tuning curves\"\n            )\n\n    @property\n    def n_ybins(self):\n        \"\"\"Number of Y-dimension bins (for backwards compatibility).\"\"\"\n        if self._n_dimensions &gt;= 2:\n            return len(self._bins_list[1]) - 1\n        else:\n            raise AttributeError(\n                \"n_ybins not available for tuning curves with &lt; 2 dimensions\"\n            )\n\n    @property\n    def xbin_centers(self):\n        \"\"\"X-dimension bin centers (for backwards compatibility).\"\"\"\n        if self._n_dimensions &gt;= 1:\n            bins = self._bins_list[0]\n            return (bins + (bins[1] - bins[0]) / 2)[:-1]\n        else:\n            raise AttributeError(\n                \"xbin_centers not available for 0-dimensional tuning curves\"\n            )\n\n    @property\n    def ybin_centers(self):\n        \"\"\"Y-dimension bin centers (for backwards compatibility).\"\"\"\n        if self._n_dimensions &gt;= 2:\n            bins = self._bins_list[1]\n            return (bins + (bins[1] - bins[0]) / 2)[:-1]\n        else:\n            raise AttributeError(\n                \"ybin_centers not available for tuning curves with &lt; 2 dimensions\"\n            )\n\n    @property\n    def is2d(self):\n        \"\"\"Check if this is a 2D tuning curve.\"\"\"\n        return self._n_dimensions == 2\n\n    def _trans_func(self, extern, at):\n        \"\"\"Default transform function to map extern into numerical bins.\n\n        Returns\n        -------\n        coords : np.ndarray\n            Array of shape (n_dimensions, n_timepoints)\n        \"\"\"\n        _, ext = extern.asarray(at=at)\n\n        # If extern has shape (n_signals, n_timepoints), use first n_dimensions signals\n        if ext.ndim == 2 and ext.shape[0] &gt;= self._n_dimensions:\n            coords = ext[: self._n_dimensions, :]\n        # If extern is 1D and we need 1 dimension, reshape appropriately\n        elif ext.ndim == 1 and self._n_dimensions == 1:\n            coords = ext.reshape(1, -1)\n        else:\n            raise ValueError(\n                f\"extern shape {ext.shape} incompatible with {self._n_dimensions} dimensions\"\n            )\n\n        return coords\n\n    def __getitem__(self, *idx):\n        \"\"\"TuningCurveND index access. Accepts integers, slices, and lists\"\"\"\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        if self.isempty:\n            return self\n        try:\n            out = copy.copy(self)\n            out._ratemap = self.ratemap[idx]\n            out._unit_ids = (np.asanyarray(out._unit_ids)[idx]).tolist()\n            out._unit_labels = (np.asanyarray(out._unit_labels)[idx]).tolist()\n            return out\n        except Exception:\n            raise TypeError(\"unsupported subscripting type {}\".format(type(idx)))\n\n    def _compute_occupancy(self):\n        \"\"\"Compute occupancy using np.histogramdd.\"\"\"\n        # Make sure that self._bst_centers fall within not only the support\n        # of extern, but also within the extreme sample times\n        if self._bst._bin_centers[0] &lt; self._extern.time[0]:\n            self._extern = copy.copy(self._extern)\n            self._extern.time[0] = self._bst._bin_centers[0]\n            self._extern._interp = None\n\n        if self._bst._bin_centers[-1] &gt; self._extern.time[-1]:\n            self._extern = copy.copy(self._extern)\n            self._extern.time[-1] = self._bst._bin_centers[-1]\n            self._extern._interp = None\n\n        coords = self.trans_func(self._extern, at=self._bst.bin_centers)\n\n        # coords should be shape (n_dimensions, n_timepoints)\n        # transpose to (n_timepoints, n_dimensions) for histogramdd\n        coords_transposed = coords.T\n\n        # Create ranges for histogramdd\n        ranges = []\n        for bins in self._bins_list:\n            ranges.append([bins[0], bins[-1]])\n\n        occupancy, _ = np.histogramdd(\n            coords_transposed, bins=self._bins_list, range=ranges\n        )\n\n        return occupancy\n\n    def _compute_ratemap(self, min_duration=None):\n        \"\"\"Compute ratemap using N-dimensional binning.\"\"\"\n        if min_duration is None:\n            min_duration = self._min_duration\n\n        coords = self.trans_func(self._extern, at=self._bst.bin_centers)\n\n        # Get bin indices for each dimension\n        bin_indices = []\n        for i, bins in enumerate(self._bins_list):\n            indices = np.digitize(coords[i, :], bins, right=True)\n            bin_indices.append(indices)\n\n        # Check bounds for all dimensions\n        for i, (indices, bins) in enumerate(zip(bin_indices, self._bins_list)):\n            n_bins_dim = len(bins) - 1\n            if indices.max() &gt; n_bins_dim:\n                raise ValueError(f\"ext values greater than 'ext_max' in dimension {i}\")\n            if indices.min() == 0:\n                raise ValueError(f\"ext values less than 'ext_min' in dimension {i}\")\n\n        # Initialize ratemap\n        ratemap_shape = (self.n_units,) + tuple(self.n_bins_per_dim)\n        ratemap = np.zeros(ratemap_shape)\n\n        # Accumulate spikes in appropriate bins\n        for tt in range(len(self._bst.bin_centers)):\n            # Get bin coordinates for this time point\n            bin_coords = tuple(\n                idx[tt] - 1 for idx in bin_indices\n            )  # subtract 1 for 0-indexing\n\n            # Check if all coordinates are valid (within bounds)\n            if all(\n                0 &lt;= coord &lt; dim_size\n                for coord, dim_size in zip(bin_coords, self.n_bins_per_dim)\n            ):\n                # Use advanced indexing to add spike counts for all units\n                for unit_idx in range(self.n_units):\n                    ratemap[(unit_idx,) + bin_coords] += self._bst.data[unit_idx, tt]\n\n        # Apply minimum observation duration\n        for uu in range(self.n_units):\n            ratemap[uu][self.occupancy * self._bst.ds &lt; min_duration] = 0\n\n        return ratemap / self._bst.ds\n\n    def _normalize_firing_rate_by_occupancy(self):\n        \"\"\"Normalize spike counts by occupancy.\"\"\"\n        # Tile occupancy to match ratemap shape\n        occupancy_tiled = np.tile(\n            self.occupancy, (self.n_units,) + (1,) * self._n_dimensions\n        )\n        occupancy_tiled[occupancy_tiled == 0] = 1\n        ratemap = self.ratemap / occupancy_tiled\n        return ratemap\n\n    @property\n    def occupancy(self):\n        return self._occupancy\n\n    @property\n    def n_units(self):\n        \"\"\"(int) The number of units.\"\"\"\n        try:\n            return len(self._unit_ids)\n        except TypeError:  # when unit_ids is an integer\n            return 1\n        except AttributeError:\n            return 0\n\n    @property\n    def shape(self):\n        \"\"\"(tuple) The shape of the TuningCurveND ratemap.\"\"\"\n        if self.isempty:\n            return (self.n_units,) + (0,) * self._n_dimensions\n        return self.ratemap.shape\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return f\"&lt;empty TuningCurveND{address_str}&gt;\"\n        shapestr = \" with shape \" + str(self.shape)\n        return f\"&lt;TuningCurveND({self._n_dimensions}D){address_str}&gt;{shapestr}\"\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) True if TuningCurveND is empty\"\"\"\n        try:\n            return len(self.ratemap) == 0\n        except TypeError:  # TypeError should happen if ratemap = []\n            return True\n\n    @property\n    def ratemap(self):\n        return self._ratemap\n\n    def __len__(self):\n        return self.n_units\n\n    def __iter__(self):\n        \"\"\"TuningCurveND iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"TuningCurveND iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_units - 1:\n            raise StopIteration\n        out = copy.copy(self)\n        out._ratemap = self.ratemap[index][np.newaxis, ...]  # Keep unit dimension\n        out._unit_ids = [self.unit_ids[index]]\n        out._unit_labels = [self.unit_labels[index]]\n        self._index += 1\n        return out\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n        \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n        Parameters\n        ----------\n        sigma : float or array-like, optional\n            Standard deviation for Gaussian smoothing. If float, applied to all\n            dimensions. If array-like, must have length equal to n_dimensions.\n        \"\"\"\n        if sigma is None:\n            sigma = 0.1  # in units of extern\n        if truncate is None:\n            truncate = 4\n        if mode is None:\n            mode = \"reflect\"\n        if cval is None:\n            cval = 0.0\n\n        # Handle sigma parameter\n        sigma_array = np.asarray(sigma)\n        if sigma_array.ndim == 0:\n            # Single sigma value - apply to all dimensions\n            sigma_values = np.full(self._n_dimensions, float(sigma_array))\n        else:\n            # Array of sigma values\n            if len(sigma_array) != self._n_dimensions:\n                raise ValueError(\n                    f\"sigma array length {len(sigma_array)} must equal n_dimensions {self._n_dimensions}\"\n                )\n            sigma_values = sigma_array\n\n        # Convert sigma from extern units to pixel units for each dimension\n        sigma_pixels = []\n        for i, (bins, sigma_val) in enumerate(zip(self._bins_list, sigma_values)):\n            ds = (bins[-1] - bins[0]) / (len(bins) - 1)\n            sigma_pixels.append(sigma_val / ds)\n\n        # Create full sigma tuple: (0, sigma_dim0, sigma_dim1, ...)\n        full_sigma = (0,) + tuple(sigma_pixels)\n\n        if not inplace:\n            out = copy.deepcopy(self)\n        else:\n            out = self\n\n        if self.mask is None:\n            # Simple case without mask\n            out._ratemap = gaussian_filter(\n                self.ratemap,\n                sigma=full_sigma,\n                truncate=truncate,\n                mode=mode,\n                cval=cval,\n            )\n        else:\n            # Complex case with mask - handle NaNs properly\n            masked_ratemap = self.ratemap.copy() * self.mask\n            V = masked_ratemap.copy()\n            V[masked_ratemap != masked_ratemap] = 0\n            W = 0 * masked_ratemap.copy() + 1\n            W[masked_ratemap != masked_ratemap] = 0\n\n            VV = gaussian_filter(\n                V,\n                sigma=full_sigma,\n                truncate=truncate,\n                mode=mode,\n                cval=cval,\n            )\n            WW = gaussian_filter(\n                W,\n                sigma=full_sigma,\n                truncate=truncate,\n                mode=mode,\n                cval=cval,\n            )\n            Z = VV / WW\n            out._ratemap = Z * self.mask\n\n        return out\n\n    @property\n    def unit_ids(self):\n        \"\"\"Unit IDs contained in the SpikeTrain.\"\"\"\n        return list(self._unit_ids)\n\n    @unit_ids.setter\n    def unit_ids(self, val):\n        if len(val) != self.n_units:\n            raise TypeError(\"unit_ids must be of length n_units\")\n        elif len(set(val)) &lt; len(val):\n            raise TypeError(\"duplicate unit_ids are not allowed\")\n        else:\n            try:\n                # cast to int:\n                unit_ids = [int(id) for id in val]\n            except TypeError:\n                raise TypeError(\"unit_ids must be int-like\")\n        self._unit_ids = unit_ids\n\n    @property\n    def unit_labels(self):\n        \"\"\"Labels corresponding to units contained in the SpikeTrain.\"\"\"\n        if self._unit_labels is None:\n            warnings.warn(\"unit labels have not yet been specified\")\n        return self._unit_labels\n\n    @unit_labels.setter\n    def unit_labels(self, val):\n        if len(val) != self.n_units:\n            raise TypeError(\"labels must be of length n_units\")\n        else:\n            try:\n                # cast to str:\n                labels = [str(label) for label in val]\n            except TypeError:\n                raise TypeError(\"labels must be string-like\")\n        self._unit_labels = labels\n\n    @property\n    def unit_tags(self):\n        \"\"\"Tags corresponding to units contained in the SpikeTrain\"\"\"\n        if self._unit_tags is None:\n            warnings.warn(\"unit tags have not yet been specified\")\n        return self._unit_tags\n\n    @property\n    def label(self):\n        \"\"\"Label pertaining to the source of the spike train.\"\"\"\n        if self._label is None:\n            warnings.warn(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.bin_centers","title":"<code>bin_centers</code>  <code>property</code>","text":"<p>List of bin centers for each dimension.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.bins","title":"<code>bins</code>  <code>property</code>","text":"<p>List of bin edges for each dimension.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.extlabels","title":"<code>extlabels</code>  <code>property</code>","text":"<p>Labels for external correlates.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.is2d","title":"<code>is2d</code>  <code>property</code>","text":"<p>Check if this is a 2D tuning curve.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) True if TuningCurveND is empty</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label pertaining to the source of the spike train.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.mask","title":"<code>mask</code>  <code>property</code>","text":"<p>Mask for tuning curve.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>Total number of bins (product of bins in all dimensions).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.n_bins_per_dim","title":"<code>n_bins_per_dim</code>  <code>property</code>","text":"<p>Number of bins for each dimension.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.n_dimensions","title":"<code>n_dimensions</code>  <code>property</code>","text":"<p>Number of dimensions.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.n_units","title":"<code>n_units</code>  <code>property</code>","text":"<p>(int) The number of units.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.n_xbins","title":"<code>n_xbins</code>  <code>property</code>","text":"<p>Number of X-dimension bins (for backwards compatibility).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.n_ybins","title":"<code>n_ybins</code>  <code>property</code>","text":"<p>Number of Y-dimension bins (for backwards compatibility).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>(tuple) The shape of the TuningCurveND ratemap.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.unit_ids","title":"<code>unit_ids</code>  <code>property</code> <code>writable</code>","text":"<p>Unit IDs contained in the SpikeTrain.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.unit_labels","title":"<code>unit_labels</code>  <code>property</code> <code>writable</code>","text":"<p>Labels corresponding to units contained in the SpikeTrain.</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.unit_tags","title":"<code>unit_tags</code>  <code>property</code>","text":"<p>Tags corresponding to units contained in the SpikeTrain</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.xbin_centers","title":"<code>xbin_centers</code>  <code>property</code>","text":"<p>X-dimension bin centers (for backwards compatibility).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.xbins","title":"<code>xbins</code>  <code>property</code>","text":"<p>X-dimension bins (for backwards compatibility when n_dimensions &gt;= 1).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.ybin_centers","title":"<code>ybin_centers</code>  <code>property</code>","text":"<p>Y-dimension bin centers (for backwards compatibility).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.ybins","title":"<code>ybins</code>  <code>property</code>","text":"<p>Y-dimension bins (for backwards compatibility when n_dimensions &gt;= 2).</p>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.information_rate","title":"<code>information_rate()</code>","text":"<p>Compute the information rate...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def information_rate(self):\n    \"\"\"Compute the information rate...\"\"\"\n    return utils.information_rate(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.max","title":"<code>max(*, axis=None)</code>","text":"<p>Returns the max of firing rate (in Hz).</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def max(self, *, axis=None):\n    \"\"\"Returns the max of firing rate (in Hz).\"\"\"\n    if axis is None or axis == 0:\n        maxes = np.max(self.ratemap, axis=axis)\n    elif axis == 1:\n        # Flatten all spatial dimensions for per-unit calculation\n        maxes = [\n            self.ratemap[unit_i].max() for unit_i in range(self.ratemap.shape[0])\n        ]\n    else:\n        maxes = np.max(self.ratemap, axis=axis)\n\n    return maxes\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.mean","title":"<code>mean(*, axis=None)</code>","text":"<p>Returns the mean of firing rate (in Hz).</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def mean(self, *, axis=None):\n    \"\"\"Returns the mean of firing rate (in Hz).\"\"\"\n    if axis is None or axis == 0:\n        means = np.mean(self.ratemap, axis=axis)\n    elif axis == 1:\n        # Flatten all spatial dimensions for per-unit calculation\n        means = [\n            self.ratemap[unit_i].mean() for unit_i in range(self.ratemap.shape[0])\n        ]\n    else:\n        means = np.mean(self.ratemap, axis=axis)\n\n    return means\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.min","title":"<code>min(*, axis=None)</code>","text":"<p>Returns the min of firing rate (in Hz).</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def min(self, *, axis=None):\n    \"\"\"Returns the min of firing rate (in Hz).\"\"\"\n    if axis is None or axis == 0:\n        mins = np.min(self.ratemap, axis=axis)\n    elif axis == 1:\n        # Flatten all spatial dimensions for per-unit calculation\n        mins = [\n            self.ratemap[unit_i].min() for unit_i in range(self.ratemap.shape[0])\n        ]\n    else:\n        mins = np.min(self.ratemap, axis=axis)\n\n    return mins\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.smooth","title":"<code>smooth(*, sigma=None, truncate=None, inplace=False, mode=None, cval=None)</code>","text":"<p>Smooths the tuning curve with a Gaussian kernel.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float or array - like</code> <p>Standard deviation for Gaussian smoothing. If float, applied to all dimensions. If array-like, must have length equal to n_dimensions.</p> <code>None</code> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n    \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n    Parameters\n    ----------\n    sigma : float or array-like, optional\n        Standard deviation for Gaussian smoothing. If float, applied to all\n        dimensions. If array-like, must have length equal to n_dimensions.\n    \"\"\"\n    if sigma is None:\n        sigma = 0.1  # in units of extern\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n\n    # Handle sigma parameter\n    sigma_array = np.asarray(sigma)\n    if sigma_array.ndim == 0:\n        # Single sigma value - apply to all dimensions\n        sigma_values = np.full(self._n_dimensions, float(sigma_array))\n    else:\n        # Array of sigma values\n        if len(sigma_array) != self._n_dimensions:\n            raise ValueError(\n                f\"sigma array length {len(sigma_array)} must equal n_dimensions {self._n_dimensions}\"\n            )\n        sigma_values = sigma_array\n\n    # Convert sigma from extern units to pixel units for each dimension\n    sigma_pixels = []\n    for i, (bins, sigma_val) in enumerate(zip(self._bins_list, sigma_values)):\n        ds = (bins[-1] - bins[0]) / (len(bins) - 1)\n        sigma_pixels.append(sigma_val / ds)\n\n    # Create full sigma tuple: (0, sigma_dim0, sigma_dim1, ...)\n    full_sigma = (0,) + tuple(sigma_pixels)\n\n    if not inplace:\n        out = copy.deepcopy(self)\n    else:\n        out = self\n\n    if self.mask is None:\n        # Simple case without mask\n        out._ratemap = gaussian_filter(\n            self.ratemap,\n            sigma=full_sigma,\n            truncate=truncate,\n            mode=mode,\n            cval=cval,\n        )\n    else:\n        # Complex case with mask - handle NaNs properly\n        masked_ratemap = self.ratemap.copy() * self.mask\n        V = masked_ratemap.copy()\n        V[masked_ratemap != masked_ratemap] = 0\n        W = 0 * masked_ratemap.copy() + 1\n        W[masked_ratemap != masked_ratemap] = 0\n\n        VV = gaussian_filter(\n            V,\n            sigma=full_sigma,\n            truncate=truncate,\n            mode=mode,\n            cval=cval,\n        )\n        WW = gaussian_filter(\n            W,\n            sigma=full_sigma,\n            truncate=truncate,\n            mode=mode,\n            cval=cval,\n        )\n        Z = VV / WW\n        out._ratemap = Z * self.mask\n\n    return out\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.spatial_information","title":"<code>spatial_information()</code>","text":"<p>Compute the spatial information and firing sparsity...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_information(self):\n    \"\"\"Compute the spatial information and firing sparsity...\"\"\"\n    return utils.spatial_information(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.spatial_selectivity","title":"<code>spatial_selectivity()</code>","text":"<p>Compute the spatial selectivity...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_selectivity(self):\n    \"\"\"Compute the spatial selectivity...\"\"\"\n    return utils.spatial_selectivity(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.spatial_sparsity","title":"<code>spatial_sparsity()</code>","text":"<p>Compute the spatial information and firing sparsity...</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def spatial_sparsity(self):\n    \"\"\"Compute the spatial information and firing sparsity...\"\"\"\n    return utils.spatial_sparsity(ratemap=self.ratemap, Pi=self.occupancy)\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.TuningCurveND.std","title":"<code>std(*, axis=None)</code>","text":"<p>Returns the std of firing rate (in Hz).</p> Source code in <code>nelpy/auxiliary/_tuningcurve.py</code> <pre><code>def std(self, *, axis=None):\n    \"\"\"Returns the std of firing rate (in Hz).\"\"\"\n    if axis is None or axis == 0:\n        stds = np.std(self.ratemap, axis=axis)\n    elif axis == 1:\n        # Flatten all spatial dimensions for per-unit calculation\n        stds = [\n            self.ratemap[unit_i].std() for unit_i in range(self.ratemap.shape[0])\n        ]\n    else:\n        stds = np.std(self.ratemap, axis=axis)\n\n    return stds\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.load_pkl","title":"<code>load_pkl(fname, zip=True)</code>","text":"<p>Read pickled data from disk, possible decompressing.</p> Source code in <code>nelpy/auxiliary/_results.py</code> <pre><code>def load_pkl(fname, zip=True):\n    \"\"\"Read pickled data from disk, possible decompressing.\"\"\"\n    if zip:\n        try:\n            with gzip.open(fname, \"rb\") as fid:\n                res = pickle.load(fid)\n        except OSError:\n            # most likely, results were not saved using zip=True, so let's try\n            # to load without zip:\n            with open(fname, \"rb\") as fid:\n                res = pickle.load(fid)\n    else:\n        with open(fname, \"rb\") as fid:\n            res = pickle.load(fid)\n    return res\n</code></pre>"},{"location":"reference/auxiliary/#nelpy.auxiliary.save_pkl","title":"<code>save_pkl(fname, res, zip=True, overwrite=False)</code>","text":"<p>Write pickled data to disk, possible compressing.</p> Source code in <code>nelpy/auxiliary/_results.py</code> <pre><code>def save_pkl(fname, res, zip=True, overwrite=False):\n    \"\"\"Write pickled data to disk, possible compressing.\"\"\"\n    if os.path.isfile(fname):\n        # file exists\n        if overwrite:\n            pass\n        else:\n            print('File \"{}\" already exists! Aborting...'.format(fname))\n            return\n    if zip:\n        save_large_file_without_zip = False\n        with gzip.open(fname, \"wb\") as fid:\n            try:\n                pickle.dump(res, fid)\n            except OverflowError:\n                print(\n                    \"writing to disk using protocol=4, which supports file sizes &gt; 4 GiB, and ignoring zip=True (zip is not supported for large files yet)\"\n                )\n                save_large_file_without_zip = True\n\n        if save_large_file_without_zip:\n            with open(fname, \"wb\") as fid:\n                pickle.dump(res, fid, protocol=4)\n    else:\n        with open(fname, \"wb\") as fid:\n            try:\n                pickle.dump(res, fid)\n            except OverflowError:\n                print(\n                    \"writing to disk using protocol=4, which supports file sizes &gt; 4 GiB\"\n                )\n                pickle.dump(res, fid, protocol=4)\n</code></pre>"},{"location":"reference/core/","title":"Core API Reference","text":"<p>???+ info \"EventArray\"     ::: nelpy.core._eventarray</p> <p>???+ info \"AnalogSignalArray\"     ::: nelpy.core._analogsignalarray</p> <p>???+ info \"IntervalArray\"     ::: nelpy.core._intervalarray</p> <p>???+ info \"ValeventArray\"     ::: nelpy.core._valeventarray</p> <p>???+ info \"Accessors\"     ::: nelpy.core._accessors</p> <p>???+ info \"Coordinates\"     ::: nelpy.core._coordinates </p>"},{"location":"reference/decoding/","title":"Decoding API Reference","text":"<p>Bayesian encoding and decoding</p>"},{"location":"reference/decoding/#nelpy.decoding.BayesianDecoder","title":"<code>BayesianDecoder</code>","text":"<p>               Bases: <code>object</code></p> <p>Bayesian decoder for neural population activity.</p> <p>This class provides a scikit-learn-like API for Bayesian decoding using tuning curves. Supports 1D and 2D decoding, and can be extended for more complex models.</p> <p>Parameters:</p> Name Type Description Default <code>tuningcurve</code> <code>TuningCurve1D or TuningCurve2D</code> <p>Tuning curve to use for decoding.</p> <code>None</code> Source code in <code>nelpy/decoding.py</code> <pre><code>class BayesianDecoder(object):\n    \"\"\"\n    Bayesian decoder for neural population activity.\n\n    This class provides a scikit-learn-like API for Bayesian decoding using tuning curves.\n    Supports 1D and 2D decoding, and can be extended for more complex models.\n\n    Parameters\n    ----------\n    tuningcurve : TuningCurve1D or TuningCurve2D, optional\n        Tuning curve to use for decoding.\n    \"\"\"\n\n    def __init__(self, tuningcurve=None):\n        \"\"\"\n        Initialize the BayesianDecoder.\n\n        Parameters\n        ----------\n        tuningcurve : TuningCurve1D or TuningCurve2D, optional\n            Tuning curve to use for decoding.\n        \"\"\"\n        self.tuningcurve = tuningcurve\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the decoder to data X. (Stores the tuning curve if provided.)\n\n        Parameters\n        ----------\n        X : array-like or TuningCurve1D/2D\n            Training data or tuning curve.\n        y : Ignored\n        \"\"\"\n        # If X is a tuning curve, store it\n        self.tuningcurve = X\n        return self\n\n    def predict_proba(self, X, **kwargs):\n        \"\"\"\n        Predict posterior probabilities for data X.\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Data to decode.\n        Returns\n        -------\n        posterior : np.ndarray\n            Posterior probability matrix.\n        \"\"\"\n        if self.tuningcurve is None:\n            raise ValueError(\n                \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n            )\n        # Use decode1D or decode2D depending on tuning curve\n        if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n            # TuningCurve1D or TuningCurve2D object\n            ratemap = self.tuningcurve.ratemap\n        else:\n            ratemap = self.tuningcurve\n        # Try to infer 1D vs 2D\n        if ratemap.ndim == 2:\n            posterior, _, _, _ = decode1D(X, ratemap, **kwargs)\n        elif ratemap.ndim == 3:\n            posterior, _, _, _ = decode2D(X, ratemap, **kwargs)\n        else:\n            raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n        return posterior\n\n    def predict(self, X, **kwargs):\n        \"\"\"\n        Predict external variable from data X (returns mode path).\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Data to decode.\n        Returns\n        -------\n        mode_pth : np.ndarray\n            Most likely position at each time bin.\n        \"\"\"\n        if self.tuningcurve is None:\n            raise ValueError(\n                \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n            )\n        if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n            ratemap = self.tuningcurve.ratemap\n        else:\n            ratemap = self.tuningcurve\n        if ratemap.ndim == 2:\n            _, _, mode_pth, _ = decode1D(X, ratemap, **kwargs)\n        elif ratemap.ndim == 3:\n            _, _, mode_pth, _ = decode2D(X, ratemap, **kwargs)\n        else:\n            raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n        return mode_pth\n\n    def predict_asa(self, X, **kwargs):\n        \"\"\"\n        Predict analog signal array (mean path) from data X.\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Data to decode.\n        Returns\n        -------\n        asa : AnalogSignalArray or np.ndarray\n            Mean path as AnalogSignalArray if possible, else array.\n        \"\"\"\n        if self.tuningcurve is None:\n            raise ValueError(\n                \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n            )\n        if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n            ratemap = self.tuningcurve.ratemap\n        else:\n            ratemap = self.tuningcurve\n        if ratemap.ndim == 2:\n            _, _, _, mean_pth = decode1D(X, ratemap, **kwargs)\n        elif ratemap.ndim == 3:\n            _, _, _, mean_pth = decode2D(X, ratemap, **kwargs)\n        else:\n            raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n        # Try to return as AnalogSignalArray if possible, with timestamps if available\n        try:\n            from .core import AnalogSignalArray\n\n            abscissa_vals = None\n            # Try to get bin centers from X if possible\n            if hasattr(X, \"bin_centers\"):\n                abscissa_vals = X.bin_centers\n            elif hasattr(X, \"abscissa_vals\"):\n                abscissa_vals = X.abscissa_vals\n            return AnalogSignalArray(data=mean_pth, abscissa_vals=abscissa_vals)\n        except Exception:\n            return mean_pth\n\n    def __repr__(self):\n        s = \"&lt;BayesianDecoder: \"\n        if self.tuningcurve is not None:\n            s += f\"tuningcurve shape={getattr(self.tuningcurve, 'ratemap', self.tuningcurve).shape}\"\n        else:\n            s += \"no tuningcurve\"\n        s += \"&gt;\"\n        return s\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.BayesianDecoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the decoder to data X. (Stores the tuning curve if provided.)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or TuningCurve1D/2D</code> <p>Training data or tuning curve.</p> required <code>y</code> <code>Ignored</code> <code>None</code> Source code in <code>nelpy/decoding.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    Fit the decoder to data X. (Stores the tuning curve if provided.)\n\n    Parameters\n    ----------\n    X : array-like or TuningCurve1D/2D\n        Training data or tuning curve.\n    y : Ignored\n    \"\"\"\n    # If X is a tuning curve, store it\n    self.tuningcurve = X\n    return self\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.BayesianDecoder.predict","title":"<code>predict(X, **kwargs)</code>","text":"<p>Predict external variable from data X (returns mode path).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or BinnedEventArray</code> <p>Data to decode.</p> required <p>Returns:</p> Name Type Description <code>mode_pth</code> <code>ndarray</code> <p>Most likely position at each time bin.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def predict(self, X, **kwargs):\n    \"\"\"\n    Predict external variable from data X (returns mode path).\n\n    Parameters\n    ----------\n    X : array-like or BinnedEventArray\n        Data to decode.\n    Returns\n    -------\n    mode_pth : np.ndarray\n        Most likely position at each time bin.\n    \"\"\"\n    if self.tuningcurve is None:\n        raise ValueError(\n            \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n        )\n    if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n        ratemap = self.tuningcurve.ratemap\n    else:\n        ratemap = self.tuningcurve\n    if ratemap.ndim == 2:\n        _, _, mode_pth, _ = decode1D(X, ratemap, **kwargs)\n    elif ratemap.ndim == 3:\n        _, _, mode_pth, _ = decode2D(X, ratemap, **kwargs)\n    else:\n        raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n    return mode_pth\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.BayesianDecoder.predict_asa","title":"<code>predict_asa(X, **kwargs)</code>","text":"<p>Predict analog signal array (mean path) from data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or BinnedEventArray</code> <p>Data to decode.</p> required <p>Returns:</p> Name Type Description <code>asa</code> <code>AnalogSignalArray or ndarray</code> <p>Mean path as AnalogSignalArray if possible, else array.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def predict_asa(self, X, **kwargs):\n    \"\"\"\n    Predict analog signal array (mean path) from data X.\n\n    Parameters\n    ----------\n    X : array-like or BinnedEventArray\n        Data to decode.\n    Returns\n    -------\n    asa : AnalogSignalArray or np.ndarray\n        Mean path as AnalogSignalArray if possible, else array.\n    \"\"\"\n    if self.tuningcurve is None:\n        raise ValueError(\n            \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n        )\n    if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n        ratemap = self.tuningcurve.ratemap\n    else:\n        ratemap = self.tuningcurve\n    if ratemap.ndim == 2:\n        _, _, _, mean_pth = decode1D(X, ratemap, **kwargs)\n    elif ratemap.ndim == 3:\n        _, _, _, mean_pth = decode2D(X, ratemap, **kwargs)\n    else:\n        raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n    # Try to return as AnalogSignalArray if possible, with timestamps if available\n    try:\n        from .core import AnalogSignalArray\n\n        abscissa_vals = None\n        # Try to get bin centers from X if possible\n        if hasattr(X, \"bin_centers\"):\n            abscissa_vals = X.bin_centers\n        elif hasattr(X, \"abscissa_vals\"):\n            abscissa_vals = X.abscissa_vals\n        return AnalogSignalArray(data=mean_pth, abscissa_vals=abscissa_vals)\n    except Exception:\n        return mean_pth\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.BayesianDecoder.predict_proba","title":"<code>predict_proba(X, **kwargs)</code>","text":"<p>Predict posterior probabilities for data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or BinnedEventArray</code> <p>Data to decode.</p> required <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def predict_proba(self, X, **kwargs):\n    \"\"\"\n    Predict posterior probabilities for data X.\n\n    Parameters\n    ----------\n    X : array-like or BinnedEventArray\n        Data to decode.\n    Returns\n    -------\n    posterior : np.ndarray\n        Posterior probability matrix.\n    \"\"\"\n    if self.tuningcurve is None:\n        raise ValueError(\n            \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n        )\n    # Use decode1D or decode2D depending on tuning curve\n    if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n        # TuningCurve1D or TuningCurve2D object\n        ratemap = self.tuningcurve.ratemap\n    else:\n        ratemap = self.tuningcurve\n    # Try to infer 1D vs 2D\n    if ratemap.ndim == 2:\n        posterior, _, _, _ = decode1D(X, ratemap, **kwargs)\n    elif ratemap.ndim == 3:\n        posterior, _, _, _ = decode2D(X, ratemap, **kwargs)\n    else:\n        raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n    return posterior\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.Cumhist","title":"<code>Cumhist</code>","text":"<p>               Bases: <code>ndarray</code></p> <p>Cumulative histogram with interpolation support.</p> <p>Parameters:</p> Name Type Description Default <code>cumhist</code> <code>ndarray</code> <p>Cumulative histogram values.</p> required <code>bincenters</code> <code>ndarray</code> <p>Bin centers corresponding to the cumulative histogram.</p> required Source code in <code>nelpy/decoding.py</code> <pre><code>class Cumhist(np.ndarray):\n    \"\"\"\n    Cumulative histogram with interpolation support.\n\n    Parameters\n    ----------\n    cumhist : np.ndarray\n        Cumulative histogram values.\n    bincenters : np.ndarray\n        Bin centers corresponding to the cumulative histogram.\n    \"\"\"\n\n    def __new__(cls, cumhist, bincenters):\n        obj = np.asarray(cumhist).view(cls)\n        obj._bincenters = bincenters\n        return obj\n\n    def __call__(self, *val):\n        f = interpolate.interp1d(\n            x=self, y=self._bincenters, kind=\"linear\", fill_value=np.nan\n        )\n        try:\n            vals = f(*val).item()\n        except AttributeError:\n            vals = f(*val)\n\n        return vals\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.ItemGetter_iloc","title":"<code>ItemGetter_iloc</code>","text":"<p>               Bases: <code>object</code></p> <p>.iloc is primarily integer position based (from 0 to length-1 of the axis).</p> <p>Allows integer-based selection of intervals and series in event arrays. Raises IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (conforms with python/numpy slice semantics).</p> <p>Allowed inputs are:     - An integer e.g. 5     - A list or array of integers [4, 3, 0]     - A slice object with ints 1:7</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The parent object to slice.</p> required Source code in <code>nelpy/decoding.py</code> <pre><code>class ItemGetter_iloc(object):\n    \"\"\"\n    .iloc is primarily integer position based (from 0 to length-1 of the axis).\n\n    Allows integer-based selection of intervals and series in event arrays.\n    Raises IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (conforms with python/numpy slice semantics).\n\n    Allowed inputs are:\n        - An integer e.g. 5\n        - A list or array of integers [4, 3, 0]\n        - A slice object with ints 1:7\n\n    Parameters\n    ----------\n    obj : object\n        The parent object to slice.\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = self.obj._slicer[idx]\n        out = copy.copy(self.obj)\n        if isinstance(seriesslice, int):\n            seriesslice = [seriesslice]\n        out._data = out._data[seriesslice]\n        singleseries = len(out._data) == 1\n        if singleseries:\n            out._data = np.array(out._data[0], ndmin=2)\n        out._series_ids = list(\n            np.atleast_1d(np.atleast_1d(out._series_ids)[seriesslice])\n        )\n        out._series_labels = list(\n            np.atleast_1d(np.atleast_1d(out._series_labels)[seriesslice])\n        )\n        # TODO: update tags\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                out.loc = ItemGetter_loc(out)\n                out.iloc = ItemGetter_iloc(out)\n                return out\n        out = out._intervalslicer(intervalslice)\n        out.loc = ItemGetter_loc(out)\n        out.iloc = ItemGetter_iloc(out)\n        return out\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.ItemGetter_loc","title":"<code>ItemGetter_loc</code>","text":"<p>               Bases: <code>object</code></p> <p>.loc is primarily label based (that is, series_id based).</p> <p>Allows label-based selection of intervals and series in event arrays. Raises KeyError when the items are not found.</p> <p>Allowed inputs are:     - A single label, e.g. 5 or 'a' (interpreted as a label, not a position)     - A list or array of labels ['a', 'b', 'c']     - A slice object with labels 'a':'f' (both start and stop are included)</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The parent object to slice.</p> required Source code in <code>nelpy/decoding.py</code> <pre><code>class ItemGetter_loc(object):\n    \"\"\"\n    .loc is primarily label based (that is, series_id based).\n\n    Allows label-based selection of intervals and series in event arrays.\n    Raises KeyError when the items are not found.\n\n    Allowed inputs are:\n        - A single label, e.g. 5 or 'a' (interpreted as a label, not a position)\n        - A list or array of labels ['a', 'b', 'c']\n        - A slice object with labels 'a':'f' (both start and stop are included)\n\n    Parameters\n    ----------\n    obj : object\n        The parent object to slice.\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = self.obj._slicer[idx]\n\n        # first convert series slice into list\n        if isinstance(seriesslice, slice):\n            start = seriesslice.start\n            stop = seriesslice.stop\n            istep = seriesslice.step\n            try:\n                if start is None:\n                    istart = 0\n                else:\n                    istart = self.obj._series_ids.index(start)\n            except ValueError:\n                raise KeyError(\n                    \"series_id {} could not be found in BaseEventArray!\".format(start)\n                )\n            try:\n                if stop is None:\n                    istop = self.obj.n_series\n                else:\n                    istop = self.obj._series_ids.index(stop) + 1\n            except ValueError:\n                raise KeyError(\n                    \"series_id {} could not be found in BaseEventArray!\".format(stop)\n                )\n            if istep is None:\n                istep = 1\n            if istep &lt; 0:\n                istop -= 1\n                istart -= 1\n                istart, istop = istop, istart\n            series_idx_list = list(range(istart, istop, istep))\n        else:\n            series_idx_list = []\n            seriesslice = np.atleast_1d(seriesslice)\n            for series in seriesslice:\n                try:\n                    uidx = self.obj.series_ids.index(series)\n                except ValueError:\n                    raise KeyError(\n                        \"series_id {} could not be found in BaseEventArray!\".format(\n                            series\n                        )\n                    )\n                else:\n                    series_idx_list.append(uidx)\n\n        if not isinstance(series_idx_list, list):\n            series_idx_list = list(series_idx_list)\n        out = copy.copy(self.obj)\n        try:\n            out._data = out._data[series_idx_list]\n            singleseries = len(out._data) == 1\n        except AttributeError:\n            out._data = out._data[series_idx_list]\n            singleseries = len(out._data) == 1\n\n        if singleseries:\n            out._data = np.array(out._data[0], ndmin=2)\n        out._series_ids = list(\n            np.atleast_1d(np.atleast_1d(out._series_ids)[series_idx_list])\n        )\n        out._series_labels = list(\n            np.atleast_1d(np.atleast_1d(out._series_labels)[series_idx_list])\n        )\n        # TODO: update tags\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                out.loc = ItemGetter_loc(out)\n                out.iloc = ItemGetter_iloc(out)\n                return out\n        out = out._intervalslicer(intervalslice)\n        out.loc = ItemGetter_loc(out)\n        out.iloc = ItemGetter_iloc(out)\n        return out\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.cumulative_dist_decoding_error","title":"<code>cumulative_dist_decoding_error(bst, *, tuningcurve, extern, decodefunc=decode1D, transfunc=None, n_bins=None)</code>","text":"<p>Compute the cumulative distribution of decoding errors using a fixed tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all epochs to decode.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve to use for decoding.</p> required <code>extern</code> <code>object</code> <p>Query-able object of external correlates (e.g., position AnalogSignalArray).</p> required <code>decodefunc</code> <code>callable</code> <p>Decoding function to use (default is decode1D).</p> <code>decode1D</code> <code>transfunc</code> <code>callable</code> <p>Function to transform external variable (default is None).</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>Number of decoding error bins (default is 200).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>cumhist</code> <code>Cumhist</code> <p>Cumulative histogram of decoding errors.</p> <code>bincenters</code> <code>ndarray</code> <p>Bin centers for the cumulative histogram.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def cumulative_dist_decoding_error(\n    bst, *, tuningcurve, extern, decodefunc=decode1D, transfunc=None, n_bins=None\n):\n    \"\"\"\n    Compute the cumulative distribution of decoding errors using a fixed tuning curve.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all epochs to decode.\n    tuningcurve : TuningCurve1D\n        Tuning curve to use for decoding.\n    extern : object\n        Query-able object of external correlates (e.g., position AnalogSignalArray).\n    decodefunc : callable, optional\n        Decoding function to use (default is decode1D).\n    transfunc : callable, optional\n        Function to transform external variable (default is None).\n    n_bins : int, optional\n        Number of decoding error bins (default is 200).\n\n    Returns\n    -------\n    cumhist : Cumhist\n        Cumulative histogram of decoding errors.\n    bincenters : np.ndarray\n        Bin centers for the cumulative histogram.\n    \"\"\"\n\n    def _trans_func(extern, at):\n        \"\"\"Default transform function to map extern into numerical bins\"\"\"\n\n        _, ext = extern.asarray(at=at)\n\n        return ext\n\n    if transfunc is None:\n        transfunc = _trans_func\n    if n_bins is None:\n        n_bins = 200\n\n    # indices of training and validation epochs / events\n\n    max_error = tuningcurve.bins[-1] - tuningcurve.bins[0]\n\n    posterior, _, mode_pth, mean_pth = decodefunc(bst=bst, ratemap=tuningcurve)\n    target = transfunc(extern, at=bst.bin_centers)\n    hist, bins = np.histogram(\n        np.abs(target - mean_pth), bins=n_bins, range=(0, max_error)\n    )\n\n    # build cumulative error distribution\n    cumhist = np.cumsum(hist)\n    cumhist = cumhist / cumhist[-1]\n    bincenters = (bins + (bins[1] - bins[0]) / 2)[:-1]\n\n    # modify to start at (0,0):\n    cumhist = np.insert(cumhist, 0, 0)\n    bincenters = np.insert(bincenters, 0, 0)\n\n    # modify to end at (max_error,1):\n    cumhist = np.append(cumhist, 1)\n    bincenters = np.append(bincenters, max_error)\n\n    cumhist = Cumhist(cumhist, bincenters)\n\n    return cumhist, bincenters\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.cumulative_dist_decoding_error_using_xval","title":"<code>cumulative_dist_decoding_error_using_xval(bst, extern, *, decodefunc=decode1D, k=5, transfunc=None, n_extern=100, extmin=0, extmax=100, sigma=3, n_bins=None, randomize=False)</code>","text":"<p>Compute the cumulative distribution of decoding errors using k-fold cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all epochs to decode.</p> required <code>extern</code> <code>object</code> <p>Query-able object of external correlates (e.g., position AnalogSignalArray).</p> required <code>decodefunc</code> <code>callable</code> <p>Decoding function to use (default is decode1D).</p> <code>decode1D</code> <code>k</code> <code>int</code> <p>Number of folds for cross-validation (default is 5).</p> <code>5</code> <code>transfunc</code> <code>callable</code> <p>Function to transform external variable (default is None).</p> <code>None</code> <code>n_extern</code> <code>int</code> <p>Number of external bins (default is 100).</p> <code>100</code> <code>extmin</code> <code>float</code> <p>Minimum value of external variable (default is 0).</p> <code>0</code> <code>extmax</code> <code>float</code> <p>Maximum value of external variable (default is 100).</p> <code>100</code> <code>sigma</code> <code>float</code> <p>Smoothing parameter for tuning curve (default is 3).</p> <code>3</code> <code>n_bins</code> <code>int</code> <p>Number of decoding error bins (default is 200).</p> <code>None</code> <code>randomize</code> <code>bool</code> <p>If True, randomize the order of epochs (default is False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>cumhist</code> <code>Cumhist</code> <p>Cumulative histogram of decoding errors.</p> <code>bincenters</code> <code>ndarray</code> <p>Bin centers for the cumulative histogram.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def cumulative_dist_decoding_error_using_xval(\n    bst,\n    extern,\n    *,\n    decodefunc=decode1D,\n    k=5,\n    transfunc=None,\n    n_extern=100,\n    extmin=0,\n    extmax=100,\n    sigma=3,\n    n_bins=None,\n    randomize=False,\n):\n    \"\"\"\n    Compute the cumulative distribution of decoding errors using k-fold cross-validation.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all epochs to decode.\n    extern : object\n        Query-able object of external correlates (e.g., position AnalogSignalArray).\n    decodefunc : callable, optional\n        Decoding function to use (default is decode1D).\n    k : int, optional\n        Number of folds for cross-validation (default is 5).\n    transfunc : callable, optional\n        Function to transform external variable (default is None).\n    n_extern : int, optional\n        Number of external bins (default is 100).\n    extmin : float, optional\n        Minimum value of external variable (default is 0).\n    extmax : float, optional\n        Maximum value of external variable (default is 100).\n    sigma : float, optional\n        Smoothing parameter for tuning curve (default is 3).\n    n_bins : int, optional\n        Number of decoding error bins (default is 200).\n    randomize : bool, optional\n        If True, randomize the order of epochs (default is False).\n\n    Returns\n    -------\n    cumhist : Cumhist\n        Cumulative histogram of decoding errors.\n    bincenters : np.ndarray\n        Bin centers for the cumulative histogram.\n    \"\"\"\n\n    def _trans_func(extern, at):\n        \"\"\"Default transform function to map extern into numerical bins\"\"\"\n\n        _, ext = extern.asarray(at=at)\n\n        return ext\n\n    if transfunc is None:\n        transfunc = _trans_func\n\n    if n_bins is None:\n        n_bins = 200\n\n    max_error = extmax - extmin\n\n    # indices of training and validation epochs / events\n\n    hist = np.zeros(n_bins)\n    for training, validation in k_fold_cross_validation(\n        bst.n_epochs, k=k, randomize=randomize\n    ):\n        # estimate place fields using bst[training]\n        tc = auxiliary.TuningCurve1D(\n            bst=bst[training],\n            extern=extern,\n            n_extern=n_extern,\n            extmin=extmin,\n            extmax=extmax,\n            sigma=sigma,\n        )\n        # decode position using bst[validation]\n        posterior, _, mode_pth, mean_pth = decodefunc(bst[validation], tc)\n        # calculate validation error (for current fold) by comapring\n        # decoded pos v target pos\n        target = transfunc(extern, at=bst[validation].bin_centers)\n\n        histnew, bins = np.histogram(\n            np.abs(target - mean_pth), bins=n_bins, range=(0, max_error)\n        )\n        hist = hist + histnew\n\n    # build cumulative error distribution\n    cumhist = np.cumsum(hist)\n    cumhist = cumhist / cumhist[-1]\n    bincenters = (bins + (bins[1] - bins[0]) / 2)[:-1]\n\n    # modify to start at (0,0):\n    cumhist = np.insert(cumhist, 0, 0)\n    bincenters = np.insert(bincenters, 0, 0)\n\n    # modify to end at (max_error,1):\n    cumhist = np.append(cumhist, 1)\n    bincenters = np.append(bincenters, max_error)\n\n    cumhist = Cumhist(cumhist, bincenters)\n    return cumhist, bincenters\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.decode1D","title":"<code>decode1D(bst, ratemap, xmin=0, xmax=100, w=1, nospk_prior=None, _skip_empty_bins=True)</code>","text":"<p>Decode binned spike trains using a 1D ratemap (Bayesian decoding).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to decode.</p> required <code>ratemap</code> <code>array_like or TuningCurve1D</code> <p>Firing rate map with shape (n_units, n_ext), where n_ext is the number of external correlates (e.g., position bins). The rate map is in spks/second.</p> required <code>xmin</code> <code>float</code> <p>Minimum value of external variable (default is 0).</p> <code>0</code> <code>xmax</code> <code>float</code> <p>Maximum value of external variable (default is 100).</p> <code>100</code> <code>w</code> <code>int</code> <p>Window size for decoding (default is 1).</p> <code>1</code> <code>nospk_prior</code> <code>array_like or float</code> <p>Prior distribution over external correlates with shape (n_ext,). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.</p> <code>None</code> <code>_skip_empty_bins</code> <code>bool</code> <p>If True, skip bins with no spikes. If False, fill with prior.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray</code> <p>Posterior distribution with shape (n_ext, n_posterior_bins).</p> <code>cum_posterior_lengths</code> <code>ndarray</code> <p>Cumulative posterior lengths for each epoch.</p> <code>mode_pth</code> <code>ndarray</code> <p>Most likely position at each time bin.</p> <code>mean_pth</code> <code>ndarray</code> <p>Expected position at each time bin.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode1D(bst, ratemap)\n</code></pre> Source code in <code>nelpy/decoding.py</code> <pre><code>def decode1D(\n    bst, ratemap, xmin=0, xmax=100, w=1, nospk_prior=None, _skip_empty_bins=True\n):\n    \"\"\"\n    Decode binned spike trains using a 1D ratemap (Bayesian decoding).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to decode.\n    ratemap : array_like or TuningCurve1D\n        Firing rate map with shape (n_units, n_ext), where n_ext is the number of external correlates (e.g., position bins). The rate map is in spks/second.\n    xmin : float, optional\n        Minimum value of external variable (default is 0).\n    xmax : float, optional\n        Maximum value of external variable (default is 100).\n    w : int, optional\n        Window size for decoding (default is 1).\n    nospk_prior : array_like or float, optional\n        Prior distribution over external correlates with shape (n_ext,). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.\n    _skip_empty_bins : bool, optional\n        If True, skip bins with no spikes. If False, fill with prior.\n\n    Returns\n    -------\n    posterior : np.ndarray\n        Posterior distribution with shape (n_ext, n_posterior_bins).\n    cum_posterior_lengths : np.ndarray\n        Cumulative posterior lengths for each epoch.\n    mode_pth : np.ndarray\n        Most likely position at each time bin.\n    mean_pth : np.ndarray\n        Expected position at each time bin.\n\n    Examples\n    --------\n    &gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode1D(bst, ratemap)\n    \"\"\"\n\n    if w is None:\n        w = 1\n    assert float(w).is_integer(), \"w must be a positive integer!\"\n    assert w &gt; 0, \"w must be a positive integer!\"\n\n    n_units, t_bins = bst.data.shape\n    _, n_xbins = ratemap.shape\n\n    # if we pass a TuningCurve1D object, extract the ratemap and re-order\n    # units if necessary\n    if isinstance(ratemap, auxiliary.TuningCurve1D) | isinstance(\n        ratemap, auxiliary._tuningcurve.TuningCurve1D\n    ):\n        # xmin = ratemap.bins[0]\n        xmax = ratemap.bins[-1]\n        bin_centers = ratemap.bin_centers\n        # re-order units if necessary\n        ratemap = ratemap.reorder_units_by_ids(bst.unit_ids)\n        ratemap = ratemap.ratemap\n    else:\n        # xmin = 0\n        xmax = n_xbins\n        bin_centers = np.arange(n_xbins)\n\n    if nospk_prior is None:\n        nospk_prior = np.full(n_xbins, np.nan)\n    elif isinstance(nospk_prior, numbers.Number):\n        nospk_prior = np.full(n_xbins, 1.0)\n\n    assert nospk_prior.shape[0] == n_xbins, \"prior must have length {}\".format(n_xbins)\n    assert nospk_prior.size == n_xbins, (\n        \"prior must be a 1D array with length {}\".format(n_xbins)\n    )\n\n    lfx = np.log(ratemap)\n\n    eterm = -ratemap.sum(axis=0) * bst.ds * w\n\n    # if we decode using multiple bins at a time (w&gt;1) then we have to decode each epoch separately:\n\n    # first, we determine the number of bins we will decode. This requires us to scan over the epochs\n    n_bins = 0\n    cumlengths = np.cumsum(bst.lengths)\n    posterior_lengths = np.zeros(bst.n_epochs, dtype=int)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        datalen = to_idx - prev_idx\n        prev_idx = to_idx\n        posterior_lengths[ii] = np.max((1, datalen - w + 1))\n\n    n_bins = posterior_lengths.sum()\n    posterior = np.zeros((n_xbins, n_bins))\n\n    # next, we decode each epoch separately, one bin at a time\n    cum_posterior_lengths = np.insert(np.cumsum(posterior_lengths), 0, 0)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        data = bst.data[:, prev_idx:to_idx]\n        prev_idx = to_idx\n        datacum = np.cumsum(\n            data, axis=1\n        )  # ii'th data segment, with column of zeros prepended\n        datacum = np.hstack((np.zeros((n_units, 1)), datacum))\n        re = w  # right edge ptr\n        # TODO: check if datalen &lt; w and act appropriately\n        if posterior_lengths[ii] &gt; 1:  # more than one full window fits into data length\n            for tt in range(posterior_lengths[ii]):\n                obs = datacum[:, re] - datacum[:, re - w]  # spikes in window of size w\n                re += 1\n                post_idx = cum_posterior_lengths[ii] + tt\n                if obs.sum() == 0 and _skip_empty_bins:\n                    # no spikes to decode in window!\n                    posterior[:, post_idx] = nospk_prior\n                else:\n                    posterior[:, post_idx] = (\n                        np.tile(np.array(obs, ndmin=2).T, n_xbins) * lfx\n                    ).sum(axis=0) + eterm\n        else:  # only one window can fit in, and perhaps only partially. We just take all the data we can get,\n            # and ignore the scaling problem where the window size is now possibly less than bst.ds*w\n            post_idx = cum_posterior_lengths[ii]\n            obs = datacum[:, -1]  # spikes in window of size at most w\n            if obs.sum() == 0 and _skip_empty_bins:\n                # no spikes to decode in window!\n                posterior[:, post_idx] = nospk_prior\n            else:\n                posterior[:, post_idx] = (\n                    np.tile(np.array(obs, ndmin=2).T, n_xbins) * lfx\n                ).sum(axis=0) + eterm\n\n    # normalize posterior:\n    posterior = np.exp(posterior - logsumexp(posterior, axis=0))\n\n    # TODO: what was my rationale behid the following? Why not use bin centers?\n    # _, bins = np.histogram([], bins=n_xbins, range=(xmin,xmax))\n    # xbins = (bins + xmax/n_xbins)[:-1]\n\n    mode_pth = np.argmax(posterior, axis=0) * xmax / n_xbins\n    mode_pth = np.where(np.isnan(posterior.sum(axis=0)), np.nan, mode_pth)\n    mean_pth = (bin_centers * posterior.T).sum(axis=1)\n    return posterior, cum_posterior_lengths, mode_pth, mean_pth\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.decode2D","title":"<code>decode2D(bst, ratemap, xmin=0, xmax=100, ymin=0, ymax=100, w=1, nospk_prior=None, _skip_empty_bins=True)</code>","text":"<p>Decode binned spike trains using a 2D ratemap (Bayesian decoding).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to decode.</p> required <code>ratemap</code> <code>array_like or TuningCurve2D</code> <p>Firing rate map with shape (n_units, ext_nx, ext_ny), where ext_nx and ext_ny are the number of external correlates (e.g., position bins). The rate map is in spks/second.</p> required <code>xmin</code> <code>float</code> <p>Minimum x value of external variable (default is 0).</p> <code>0</code> <code>xmax</code> <code>float</code> <p>Maximum x value of external variable (default is 100).</p> <code>100</code> <code>ymin</code> <code>float</code> <p>Minimum y value of external variable (default is 0).</p> <code>0</code> <code>ymax</code> <code>float</code> <p>Maximum y value of external variable (default is 100).</p> <code>100</code> <code>w</code> <code>int</code> <p>Window size for decoding (default is 1).</p> <code>1</code> <code>nospk_prior</code> <code>array_like or float</code> <p>Prior distribution over external correlates with shape (ext_nx, ext_ny). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.</p> <code>None</code> <code>_skip_empty_bins</code> <code>bool</code> <p>If True, skip bins with no spikes. If False, fill with prior.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray</code> <p>Posterior distribution with shape (ext_nx, ext_ny, n_posterior_bins).</p> <code>cum_posterior_lengths</code> <code>ndarray</code> <p>Cumulative posterior lengths for each epoch.</p> <code>mode_pth</code> <code>ndarray</code> <p>Most likely (x, y) position at each time bin.</p> <code>mean_pth</code> <code>ndarray</code> <p>Expected (x, y) position at each time bin.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode2D(bst, ratemap)\n</code></pre> Source code in <code>nelpy/decoding.py</code> <pre><code>def decode2D(\n    bst,\n    ratemap,\n    xmin=0,\n    xmax=100,\n    ymin=0,\n    ymax=100,\n    w=1,\n    nospk_prior=None,\n    _skip_empty_bins=True,\n):\n    \"\"\"\n    Decode binned spike trains using a 2D ratemap (Bayesian decoding).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to decode.\n    ratemap : array_like or TuningCurve2D\n        Firing rate map with shape (n_units, ext_nx, ext_ny), where ext_nx and ext_ny are the number of external correlates (e.g., position bins). The rate map is in spks/second.\n    xmin : float, optional\n        Minimum x value of external variable (default is 0).\n    xmax : float, optional\n        Maximum x value of external variable (default is 100).\n    ymin : float, optional\n        Minimum y value of external variable (default is 0).\n    ymax : float, optional\n        Maximum y value of external variable (default is 100).\n    w : int, optional\n        Window size for decoding (default is 1).\n    nospk_prior : array_like or float, optional\n        Prior distribution over external correlates with shape (ext_nx, ext_ny). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.\n    _skip_empty_bins : bool, optional\n        If True, skip bins with no spikes. If False, fill with prior.\n\n    Returns\n    -------\n    posterior : np.ndarray\n        Posterior distribution with shape (ext_nx, ext_ny, n_posterior_bins).\n    cum_posterior_lengths : np.ndarray\n        Cumulative posterior lengths for each epoch.\n    mode_pth : np.ndarray\n        Most likely (x, y) position at each time bin.\n    mean_pth : np.ndarray\n        Expected (x, y) position at each time bin.\n\n    Examples\n    --------\n    &gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode2D(bst, ratemap)\n    \"\"\"\n\n    def tile_obs(obs, nx, ny):\n        n_units = len(obs)\n        out = np.zeros((n_units, nx, ny))\n        for unit in range(n_units):\n            out[unit, :, :] = obs[unit]\n        return out\n\n    if w is None:\n        w = 1\n    assert float(w).is_integer(), \"w must be a positive integer!\"\n    assert w &gt; 0, \"w must be a positive integer!\"\n\n    n_units, t_bins = bst.data.shape\n\n    xbins = None\n    ybins = None\n\n    # if we pass a TuningCurve2D object, extract the ratemap and re-order\n    # units if necessary\n    if isinstance(ratemap, auxiliary.TuningCurve2D):\n        xbins = ratemap.xbins\n        ybins = ratemap.ybins\n        xbin_centers = ratemap.xbin_centers\n        ybin_centers = ratemap.ybin_centers\n        # re-order units if necessary\n        ratemap = ratemap.reorder_units_by_ids(bst.unit_ids)\n        ratemap = ratemap.ratemap\n\n    _, n_xbins, n_ybins = ratemap.shape\n\n    if nospk_prior is None:\n        nospk_prior = np.full((n_xbins, n_ybins), np.nan)\n    elif isinstance(nospk_prior, numbers.Number):\n        nospk_prior = np.full((n_xbins, n_ybins), 1.0)\n\n    assert nospk_prior.shape == (\n        n_xbins,\n        n_ybins,\n    ), \"prior must have shape ({}, {})\".format(n_xbins, n_ybins)\n\n    lfx = np.log(ratemap)\n\n    eterm = -ratemap.sum(axis=0) * bst.ds * w\n\n    # if we decode using multiple bins at a time (w&gt;1) then we have to decode each epoch separately:\n\n    # first, we determine the number of bins we will decode. This requires us to scan over the epochs\n    n_tbins = 0\n    cumlengths = np.cumsum(bst.lengths)\n    posterior_lengths = np.zeros(bst.n_epochs, dtype=int)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        datalen = to_idx - prev_idx\n        prev_idx = to_idx\n        posterior_lengths[ii] = np.max((1, datalen - w + 1))\n\n    n_tbins = posterior_lengths.sum()\n\n    ########################################################################\n    posterior = np.zeros((n_xbins, n_ybins, n_tbins))\n\n    # next, we decode each epoch separately, one bin at a time\n    cum_posterior_lengths = np.insert(np.cumsum(posterior_lengths), 0, 0)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        data = bst.data[:, prev_idx:to_idx]\n        prev_idx = to_idx\n        datacum = np.cumsum(\n            data, axis=1\n        )  # ii'th data segment, with column of zeros prepended\n        datacum = np.hstack((np.zeros((n_units, 1)), datacum))\n        re = w  # right edge ptr\n        # TODO: check if datalen &lt; w and act appropriately\n        if posterior_lengths[ii] &gt; 1:  # more than one full window fits into data length\n            for tt in range(posterior_lengths[ii]):\n                obs = datacum[:, re] - datacum[:, re - w]  # spikes in window of size w\n                re += 1\n                post_idx = cum_posterior_lengths[ii] + tt\n                if obs.sum() == 0 and not _skip_empty_bins:\n                    # no spikes to decode in window!\n                    posterior[:, :, post_idx] = nospk_prior\n                else:\n                    posterior[:, :, post_idx] = (\n                        tile_obs(obs, n_xbins, n_ybins) * lfx\n                    ).sum(axis=0) + eterm\n        else:  # only one window can fit in, and perhaps only partially. We just take all the data we can get,\n            # and ignore the scaling problem where the window size is now possibly less than bst.ds*w\n            post_idx = cum_posterior_lengths[ii]\n            obs = datacum[:, -1]  # spikes in window of size at most w\n            if obs.sum() == 0 and not _skip_empty_bins:\n                # no spikes to decode in window!\n                posterior[:, :, post_idx] = nospk_prior\n            else:\n                posterior[:, :, post_idx] = (tile_obs(obs, n_xbins, n_ybins) * lfx).sum(\n                    axis=0\n                ) + eterm\n\n    # normalize posterior:\n    # see http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n\n    for tt in range(n_tbins):\n        posterior[:, :, tt] = posterior[:, :, tt] - posterior[:, :, tt].max()\n        posterior[:, :, tt] = np.exp(posterior[:, :, tt])\n        posterior[:, :, tt] = posterior[:, :, tt] / posterior[:, :, tt].sum()\n\n    # if xbins is None:\n    #     _, bins = np.histogram([], bins=n_xbins, range=(xmin,xmax))\n    #     xbins = (bins + xmax/n_xbins)[:-1]\n    # if ybins is None:\n    #     _, bins = np.histogram([], bins=n_ybins, range=(ymin,ymax))\n    #     ybins = (bins + ymax/n_ybins)[:-1]\n\n    mode_pth = np.zeros((2, n_tbins))\n    for tt in range(n_tbins):\n        if np.any(np.isnan(posterior[:, :, tt])):\n            mode_pth[0, tt] = np.nan\n            mode_pth[0, tt] = np.nan\n        else:\n            x_, y_ = np.unravel_index(\n                np.argmax(posterior[:, :, tt]), (n_xbins, n_ybins)\n            )\n            mode_pth[0, tt] = xbins[x_]\n            mode_pth[1, tt] = ybins[y_]\n\n    expected_x = (xbin_centers * posterior.sum(axis=1).T).sum(axis=1)\n    expected_y = (ybin_centers * posterior.sum(axis=0).T).sum(axis=1)\n    mean_pth = np.vstack((expected_x, expected_y))\n\n    posterior = np.transpose(posterior, axes=[1, 0, 2])\n\n    return posterior, cum_posterior_lengths, mode_pth, mean_pth\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.get_mean_pth_from_array","title":"<code>get_mean_pth_from_array(posterior, tuningcurve=None)</code>","text":"<p>Compute the mean path (expected position) from a posterior probability matrix.</p> <p>If a tuning curve is provided, the mean is mapped back to external coordinates/units. Otherwise, the mean is in bin space.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for mapping bins to external coordinates.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mean_pth</code> <code>ndarray</code> <p>Expected position at each time bin.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def get_mean_pth_from_array(posterior, tuningcurve=None):\n    \"\"\"\n    Compute the mean path (expected position) from a posterior probability matrix.\n\n    If a tuning curve is provided, the mean is mapped back to external coordinates/units.\n    Otherwise, the mean is in bin space.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    tuningcurve : TuningCurve1D, optional\n        Tuning curve for mapping bins to external coordinates.\n\n    Returns\n    -------\n    mean_pth : np.ndarray\n        Expected position at each time bin.\n    \"\"\"\n    n_xbins = posterior.shape[0]\n\n    if tuningcurve is None:\n        xmin = 0\n        xmax = 1\n    else:\n        # TODO: this only works for TuningCurve1D currently\n        if isinstance(tuningcurve, auxiliary.TuningCurve1D):\n            xmin = tuningcurve.bins[0]\n            xmax = tuningcurve.bins[-1]\n        else:\n            raise TypeError(\"tuningcurve type not yet supported!\")\n\n    _, bins = np.histogram([], bins=n_xbins, range=(xmin, xmax))\n    xbins = (bins + xmax / n_xbins)[:-1]\n\n    mean_pth = (xbins * posterior.T).sum(axis=1)\n\n    return mean_pth\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.get_mode_pth_from_array","title":"<code>get_mode_pth_from_array(posterior, tuningcurve=None)</code>","text":"<p>Compute the mode path (most likely position) from a posterior probability matrix.</p> <p>If a tuning curve is provided, the mode is mapped back to external coordinates/units. Otherwise, the mode is in bin space.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for mapping bins to external coordinates.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mode_pth</code> <code>ndarray</code> <p>Most likely position at each time bin.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def get_mode_pth_from_array(posterior, tuningcurve=None):\n    \"\"\"\n    Compute the mode path (most likely position) from a posterior probability matrix.\n\n    If a tuning curve is provided, the mode is mapped back to external coordinates/units.\n    Otherwise, the mode is in bin space.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    tuningcurve : TuningCurve1D, optional\n        Tuning curve for mapping bins to external coordinates.\n\n    Returns\n    -------\n    mode_pth : np.ndarray\n        Most likely position at each time bin.\n    \"\"\"\n    n_xbins = posterior.shape[0]\n\n    if tuningcurve is None:\n        xmin = 0\n        xmax = n_xbins\n    else:\n        # TODO: this only works for TuningCurve1D currently\n        if isinstance(tuningcurve, auxiliary.TuningCurve1D):\n            xmin = tuningcurve.bins[0]\n            xmax = tuningcurve.bins[-1]\n        else:\n            raise TypeError(\"tuningcurve type not yet supported!\")\n\n    _, bins = np.histogram([], bins=n_xbins, range=(xmin, xmax))\n    # xbins = (bins + xmax / n_xbins)[:-1]\n\n    mode_pth = np.argmax(posterior, axis=0) * xmax / n_xbins\n    mode_pth = np.where(np.isnan(posterior.sum(axis=0)), np.nan, mode_pth)\n\n    return mode_pth\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.k_fold_cross_validation","title":"<code>k_fold_cross_validation(X, k=None, randomize=False)</code>","text":"<p>Generate K (training, validation) pairs from the items in X for cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>list or int</code> <p>List of items, list of indices, or integer number of indices.</p> required <code>k</code> <code>int or str</code> <p>Number of folds for k-fold cross-validation. 'loo' or 'LOO' for leave-one-out. Default is 5.</p> <code>None</code> <code>randomize</code> <code>bool</code> <p>If True, shuffle X before partitioning. Default is False.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>training</code> <code>list</code> <p>Training set indices.</p> <code>validation</code> <code>list</code> <p>Validation set indices.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = [i for i in range(97)]\n&gt;&gt;&gt; for training, validation in k_fold_cross_validation(X, k=5):\n...     print(training, validation)\n</code></pre> Source code in <code>nelpy/decoding.py</code> <pre><code>def k_fold_cross_validation(X, k=None, randomize=False):\n    \"\"\"\n    Generate K (training, validation) pairs from the items in X for cross-validation.\n\n    Parameters\n    ----------\n    X : list or int\n        List of items, list of indices, or integer number of indices.\n    k : int or str, optional\n        Number of folds for k-fold cross-validation. 'loo' or 'LOO' for leave-one-out. Default is 5.\n    randomize : bool, optional\n        If True, shuffle X before partitioning. Default is False.\n\n    Yields\n    ------\n    training : list\n        Training set indices.\n    validation : list\n        Validation set indices.\n\n    Examples\n    --------\n    &gt;&gt;&gt; X = [i for i in range(97)]\n    &gt;&gt;&gt; for training, validation in k_fold_cross_validation(X, k=5):\n    ...     print(training, validation)\n    \"\"\"\n    # deal with default values:\n    if isinstance(X, int):\n        X = range(X)\n    n_samples = len(X)\n    if k is None:\n        k = 5\n    elif k == \"loo\" or k == \"LOO\":\n        k = n_samples\n\n    if randomize:\n        from random import shuffle\n\n        X = list(X)\n        shuffle(X)\n    for _k_ in range(k):\n        training = [x for i, x in enumerate(X) if i % k != _k_]\n        validation = [x for i, x in enumerate(X) if i % k == _k_]\n        try:\n            yield training, validation\n        except StopIteration:\n            return\n</code></pre>"},{"location":"reference/decoding/#nelpy.decoding.rmse","title":"<code>rmse(predictions, targets)</code>","text":"<p>Calculate the root mean squared error (RMSE) between predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array_like</code> <p>Array of predicted values.</p> required <code>targets</code> <code>array_like</code> <p>Array of target values.</p> required <p>Returns:</p> Name Type Description <code>rmse</code> <code>float</code> <p>Root mean squared error of the predictions with respect to the targets.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def rmse(predictions, targets):\n    \"\"\"\n    Calculate the root mean squared error (RMSE) between predictions and targets.\n\n    Parameters\n    ----------\n    predictions : array_like\n        Array of predicted values.\n    targets : array_like\n        Array of target values.\n\n    Returns\n    -------\n    rmse : float\n        Root mean squared error of the predictions with respect to the targets.\n    \"\"\"\n    predictions = np.asanyarray(predictions)\n    targets = np.asanyarray(targets)\n    rmse = np.sqrt(np.nanmean((predictions - targets) ** 2))\n    return rmse\n</code></pre>"},{"location":"reference/estimators/","title":"Estimators API Reference","text":""},{"location":"reference/estimators/#nelpy.estimators.BayesianDecoderTemp","title":"<code>BayesianDecoderTemp</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Bayesian decoder wrapper class.</p> <p>This class implements a Bayesian decoder for neural data, supporting various estimation modes.</p> <p>Parameters:</p> Name Type Description Default <code>rate_estimator</code> <code>FiringRateEstimator</code> <p>The firing rate estimator to use.</p> <code>None</code> <code>w</code> <code>any</code> <p>Window parameter for decoding.</p> <code>None</code> <code>ratemap</code> <code>RateMap</code> <p>Precomputed rate map.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>rate_estimator</code> <code>FiringRateEstimator</code> <p>The firing rate estimator.</p> <code>ratemap</code> <code>RateMap</code> <p>The estimated or provided rate map.</p> <code>w</code> <code>any</code> <p>Window parameter.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class BayesianDecoderTemp(BaseEstimator):\n    \"\"\"\n    Bayesian decoder wrapper class.\n\n    This class implements a Bayesian decoder for neural data, supporting various estimation modes.\n\n    Parameters\n    ----------\n    rate_estimator : FiringRateEstimator, optional\n        The firing rate estimator to use.\n    w : any, optional\n        Window parameter for decoding.\n    ratemap : RateMap, optional\n        Precomputed rate map.\n\n    Attributes\n    ----------\n    rate_estimator : FiringRateEstimator\n        The firing rate estimator.\n    ratemap : RateMap\n        The estimated or provided rate map.\n    w : any\n        Window parameter.\n    \"\"\"\n\n    def __init__(self, rate_estimator=None, w=None, ratemap=None):\n        self._rate_estimator = self._validate_rate_estimator(rate_estimator)\n        self._ratemap = self._validate_ratemap(ratemap)\n        self._w = self._validate_window(w)\n\n    @property\n    def rate_estimator(self):\n        return self._rate_estimator\n\n    @property\n    def ratemap(self):\n        return self._ratemap\n\n    @property\n    def w(self):\n        return self._w\n\n    @staticmethod\n    def _validate_rate_estimator(rate_estimator):\n        if rate_estimator is None:\n            rate_estimator = FiringRateEstimator()\n        elif not isinstance(rate_estimator, FiringRateEstimator):\n            raise TypeError(\n                \"'rate_estimator' must be a nelpy FiringRateEstimator() type!\"\n            )\n        return rate_estimator\n\n    @staticmethod\n    def _validate_ratemap(ratemap):\n        if ratemap is None:\n            ratemap = NDRateMap()\n        elif not isinstance(ratemap, NDRateMap):\n            raise TypeError(\"'ratemap' must be a nelpy RateMap() type!\")\n        return ratemap\n\n    @staticmethod\n    def _validate_window(w):\n        if w is None:\n            w = DataWindow(sum=True, bin_width=1)\n        elif not isinstance(w, DataWindow):\n            raise TypeError(\"w must be a nelpy DataWindow() type!\")\n        else:\n            w = copy.copy(w)\n        if w._sum is False:\n            logging.warning(\n                \"BayesianDecoder requires DataWindow (w) to have sum=True; changing to True\"\n            )\n            w._sum = True\n        if w.bin_width is None:\n            w.bin_width = 1\n        return w\n\n    def _check_X_dt(self, X, *, lengths=None, dt=None):\n        if isinstance(X, core.BinnedEventArray):\n            if dt is not None:\n                logging.warning(\n                    \"A {} was passed in, so 'dt' will be ignored...\".format(X.type_name)\n                )\n            dt = X.ds\n            if self._w.bin_width != dt:\n                raise ValueError(\n                    \"BayesianDecoder was fit with a bin_width of {}, but is being used to predict data with a bin_width of {}\".format(\n                        self.w.bin_width, dt\n                    )\n                )\n            X, T = self.w.transform(X, lengths=lengths, sum=True)\n        else:\n            if dt is not None:\n                if self._w.bin_width != dt:\n                    raise ValueError(\n                        \"BayesianDecoder was fit with a bin_width of {}, but is being used to predict data with a bin_width of {}\".format(\n                            self.w.bin_width, dt\n                        )\n                    )\n            else:\n                dt = self._w.bin_width\n\n        return X, dt\n\n    def _check_X_y(self, X, y, *, method=\"score\", lengths=None):\n        if isinstance(X, core.BinnedEventArray):\n            if method == \"fit\":\n                self._w.bin_width = X.ds\n                logging.info(\"Updating DataWindow.bin_width from training data.\")\n            else:\n                if self._w.bin_width != X.ds:\n                    raise ValueError(\n                        \"BayesianDecoder was fit with a bin_width of {}, but is being used to predict data with a bin_width of {}\".format(\n                            self.w.bin_width, X.ds\n                        )\n                    )\n\n            X, T = self.w.transform(X, lengths=lengths, sum=True)\n\n            if isinstance(y, core.RegularlySampledAnalogSignalArray):\n                y = y(T).T\n\n        if isinstance(y, core.RegularlySampledAnalogSignalArray):\n            raise TypeError(\n                \"y can only be a RegularlySampledAnalogSignalArray if X is a BinnedEventArray.\"\n            )\n\n        assert len(X) == len(y), \"X and y must have the same number of samples!\"\n\n        return X, y\n\n    def _ratemap_permute_unit_order(self, unit_ids, inplace=False):\n        \"\"\"Permute the unit ordering.\n\n        If no order is specified, and an ordering exists from fit(), then the\n        data in X will automatically be permuted to match that registered during\n        fit().\n\n        Parameters\n        ----------\n        unit_ids : array-like, shape (n_units,)\n        \"\"\"\n        unit_ids = self._check_unit_ids(unit_ids=unit_ids)\n        if len(unit_ids) != len(self.unit_ids):\n            raise ValueError(\n                \"To re-order (permute) units, 'unit_ids' must have the same length as self._unit_ids.\"\n            )\n        self._ratemap.reorder_units_by_ids(unit_ids, inplace=inplace)\n\n    def _check_unit_ids(self, *, X=None, unit_ids=None, fit=False):\n        \"\"\"Check that unit_ids are valid (if provided), and return unit_ids.\n\n        if calling from fit(), pass in fit=True, which will skip checks against\n        self.ratemap, which doesn't exist before fitting...\n\n        \"\"\"\n\n        def a_contains_b(a, b):\n            \"\"\"Returns True iff 'b' is a subset of 'a'.\"\"\"\n            for bb in b:\n                if bb not in a:\n                    logging.warning(\"{} was not found in set\".format(bb))\n                    return False\n            return True\n\n        if isinstance(X, core.BinnedEventArray):\n            if unit_ids is not None:\n                # unit_ids were passed in, even though it's also contained in X.unit_ids\n                # 1. check that unit_ids are contained in the data:\n                if not a_contains_b(X.series_ids, unit_ids):\n                    raise ValueError(\"Some unit_ids were not contained in X!\")\n                # 2. check that unit_ids are contained in self (decoder ratemap)\n                if not fit:\n                    if not a_contains_b(self.unit_ids, unit_ids):\n                        raise ValueError(\"Some unit_ids were not contained in ratemap!\")\n            else:\n                # infer unit_ids from X\n                unit_ids = X.series_ids\n                # check that unit_ids are contained in self (decoder ratemap)\n                if not fit:\n                    if not a_contains_b(self.unit_ids, unit_ids):\n                        raise ValueError(\n                            \"Some unit_ids from X were not contained in ratemap!\"\n                        )\n        else:  # a non-nelpy X was passed, possibly X=None\n            if unit_ids is not None:\n                # 1. check that unit_ids are contained in self (decoder ratemap)\n                if not fit:\n                    if not a_contains_b(self.unit_ids, unit_ids):\n                        raise ValueError(\"Some unit_ids were not contained in ratemap!\")\n            else:  # no unit_ids were passed, only a non-nelpy X\n                if X is not None:\n                    n_samples, n_units = X.shape\n                    if not fit:\n                        if n_units &gt; self.n_units:\n                            raise ValueError(\n                                \"X contains more units than decoder! {} &gt; {}\".format(\n                                    n_units, self.n_units\n                                )\n                            )\n                        unit_ids = self.unit_ids[:n_units]\n                    else:\n                        unit_ids = np.arange(n_units)\n                else:\n                    raise NotImplementedError(\"unexpected branch reached...\")\n        return unit_ids\n\n    def _get_transformed_ratemap(self, unit_ids):\n        # first, trim ratemap to subset of units\n        ratemap = self.ratemap.loc[unit_ids]\n        # then, permute the ratemap\n        ratemap = ratemap.reorder_units_by_ids(\n            unit_ids\n        )  # maybe unneccessary, since .loc already permutes\n        return ratemap\n\n    def fit(\n        self,\n        X,\n        y,\n        *,\n        lengths=None,\n        dt=None,\n        unit_ids=None,\n        n_bins=None,\n        sample_weight=None,\n    ):\n        \"\"\"Fit Gaussian Naive Bayes according to X, y\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n                OR\n            nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n                The number of spikes in each time bin for each neuron/unit.\n        y : array-like, shape (n_samples, n_output_dims)\n            Target values.\n                OR\n            nelpy.core.RegularlySampledAnalogSignalArray\n                containing the target values corresponding to X.\n            NOTE: If X is an array-like, then y must be an array-like.\n        lengths : array-like, shape (n_epochs,), optional (default=None)\n            Lengths (in samples) of contiguous segments in (X, y).\n            .. versionadded:: x.xx\n                BayesianDecoder does not yet support *lengths*.\n        unit_ids : array-like, shape (n_units,), optional (default=None)\n            Persistent unit IDs that are used to associate units after\n            permutation. Unit IDs are inherited from nelpy.core.BinnedEventArray\n            objects, or initialized to np.arange(n_units).\n        sample_weight : array-like, shape (n_samples,), optional (default=None)\n            Weights applied to individual samples (1. for unweighted).\n            .. versionadded:: x.xx\n               BayesianDecoder does not yet support fitting with *sample_weight*.\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n\n        # TODO dt should probably come from datawindow specification, but may be overridden here!\n\n        unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids, fit=True)\n\n        # estimate the firing rate(s):\n        self.rate_estimator.fit(X=X, y=y, dt=dt, n_bins=n_bins)\n\n        # store the estimated firing rates as a rate map:\n        bin_centers = self.rate_estimator.tc_.bin_centers  # temp code FIXME\n        # bins = self.rate_estimator.tc_.bins  # temp code FIXME\n        rates = self.rate_estimator.tc_.ratemap  # temp code FIXME\n        # unit_ids = np.array(self.rate_estimator.tc_.unit_ids) #temp code FIXME\n        self.ratemap.fit(X=bin_centers, y=rates, unit_ids=unit_ids)  # temp code FIXME\n\n        X, y = self._check_X_y(\n            X, y, method=\"fit\", lengths=lengths\n        )  # can I remove this? no; it sets the bin width... but maybe we should refactor...\n        self.ratemap_ = self.ratemap.ratemap_\n\n    def predict(\n        self, X, *, output=None, mode=\"mean\", lengths=None, unit_ids=None, dt=None\n    ):\n        # if output is 'asa', then return an ASA\n        check_is_fitted(self, \"ratemap_\")\n        unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids)\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        X, dt = self._check_X_dt(X=X, lengths=lengths, dt=dt)\n\n        posterior, mean_pth = decode_bayesian_memoryless_nd(\n            X=X, ratemap=ratemap.ratemap_, dt=dt, bin_centers=ratemap.bin_centers\n        )\n\n        if output is not None:\n            raise NotImplementedError(\"output mode not implemented yet\")\n        return posterior, mean_pth\n\n    def predict_proba(self, X, *, lengths=None, unit_ids=None, dt=None):\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        return self._predict_proba_from_ratemap(X, ratemap)\n\n    def score(self, X, y, *, lengths=None, unit_ids=None, dt=None):\n        # check that unit_ids are valid\n        # THEN, transform X, y into standardized form (including trimming and permutation) and continue with scoring\n\n        check_is_fitted(self, \"ratemap_\")\n        unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids)\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        # X = self._permute_unit_order(X)\n        # X, y = self._check_X_y(X, y, method='score', unit_ids=unit_ids)\n\n        raise NotImplementedError\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        return self._score_from_ratemap(X, ratemap)\n\n    def score_samples(self, X, y, *, lengths=None, unit_ids=None, dt=None):\n        # X = self._permute_unit_order(X)\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    @property\n    def unit_ids(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap.unit_ids\n\n    @property\n    def n_units(self):\n        check_is_fitted(self, \"ratemap_\")\n        return len(self.unit_ids)\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.BayesianDecoderTemp.fit","title":"<code>fit(X, y, *, lengths=None, dt=None, unit_ids=None, n_bins=None, sample_weight=None)</code>","text":"<p>Fit Gaussian Naive Bayes according to X, y</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Training vectors, where n_samples is the number of samples and n_features is the number of features.     OR nelpy.core.BinnedEventArray / BinnedSpikeTrainArray     The number of spikes in each time bin for each neuron/unit.</p> required <code>y</code> <code>(array - like, shape(n_samples, n_output_dims))</code> <p>Target values.     OR nelpy.core.RegularlySampledAnalogSignalArray     containing the target values corresponding to X. NOTE: If X is an array-like, then y must be an array-like.</p> required <code>lengths</code> <code>(array - like, shape(n_epochs), optional(default=None))</code> <p>Lengths (in samples) of contiguous segments in (X, y). .. versionadded:: x.xx     BayesianDecoder does not yet support lengths.</p> <code>None</code> <code>unit_ids</code> <code>(array - like, shape(n_units), optional(default=None))</code> <p>Persistent unit IDs that are used to associate units after permutation. Unit IDs are inherited from nelpy.core.BinnedEventArray objects, or initialized to np.arange(n_units).</p> <code>None</code> <code>sample_weight</code> <code>(array - like, shape(n_samples), optional(default=None))</code> <p>Weights applied to individual samples (1. for unweighted). .. versionadded:: x.xx    BayesianDecoder does not yet support fitting with sample_weight.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    *,\n    lengths=None,\n    dt=None,\n    unit_ids=None,\n    n_bins=None,\n    sample_weight=None,\n):\n    \"\"\"Fit Gaussian Naive Bayes according to X, y\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Training vectors, where n_samples is the number of samples\n        and n_features is the number of features.\n            OR\n        nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n            The number of spikes in each time bin for each neuron/unit.\n    y : array-like, shape (n_samples, n_output_dims)\n        Target values.\n            OR\n        nelpy.core.RegularlySampledAnalogSignalArray\n            containing the target values corresponding to X.\n        NOTE: If X is an array-like, then y must be an array-like.\n    lengths : array-like, shape (n_epochs,), optional (default=None)\n        Lengths (in samples) of contiguous segments in (X, y).\n        .. versionadded:: x.xx\n            BayesianDecoder does not yet support *lengths*.\n    unit_ids : array-like, shape (n_units,), optional (default=None)\n        Persistent unit IDs that are used to associate units after\n        permutation. Unit IDs are inherited from nelpy.core.BinnedEventArray\n        objects, or initialized to np.arange(n_units).\n    sample_weight : array-like, shape (n_samples,), optional (default=None)\n        Weights applied to individual samples (1. for unweighted).\n        .. versionadded:: x.xx\n           BayesianDecoder does not yet support fitting with *sample_weight*.\n    Returns\n    -------\n    self : object\n\n    \"\"\"\n\n    # TODO dt should probably come from datawindow specification, but may be overridden here!\n\n    unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids, fit=True)\n\n    # estimate the firing rate(s):\n    self.rate_estimator.fit(X=X, y=y, dt=dt, n_bins=n_bins)\n\n    # store the estimated firing rates as a rate map:\n    bin_centers = self.rate_estimator.tc_.bin_centers  # temp code FIXME\n    # bins = self.rate_estimator.tc_.bins  # temp code FIXME\n    rates = self.rate_estimator.tc_.ratemap  # temp code FIXME\n    # unit_ids = np.array(self.rate_estimator.tc_.unit_ids) #temp code FIXME\n    self.ratemap.fit(X=bin_centers, y=rates, unit_ids=unit_ids)  # temp code FIXME\n\n    X, y = self._check_X_y(\n        X, y, method=\"fit\", lengths=lengths\n    )  # can I remove this? no; it sets the bin width... but maybe we should refactor...\n    self.ratemap_ = self.ratemap.ratemap_\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.FiringRateEstimator","title":"<code>FiringRateEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>FiringRateEstimator Estimate the firing rate of a spike train.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>(hist, glm - poisson, glm - binomial, glm, gvm, bars, gp)</code> <p>The estimation mode. Default is 'hist'. - 'hist': Histogram-based estimation. - 'glm-poisson': Generalized linear model with Poisson distribution. - 'glm-binomial': Generalized linear model with Binomial distribution. - 'glm': Generalized linear model. - 'gvm': Generalized von Mises. - 'bars': Bayesian adaptive regression splines. - 'gp': Gaussian process.</p> <code>'hist'</code> <p>Attributes:</p> Name Type Description <code>mode</code> <code>str</code> <p>The estimation mode.</p> <code>tc_</code> <code>TuningCurve1D or TuningCurve2D</code> <p>The estimated tuning curve.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class FiringRateEstimator(BaseEstimator):\n    \"\"\"\n    FiringRateEstimator\n    Estimate the firing rate of a spike train.\n\n    Parameters\n    ----------\n    mode : {'hist', 'glm-poisson', 'glm-binomial', 'glm', 'gvm', 'bars', 'gp'}, optional\n        The estimation mode. Default is 'hist'.\n        - 'hist': Histogram-based estimation.\n        - 'glm-poisson': Generalized linear model with Poisson distribution.\n        - 'glm-binomial': Generalized linear model with Binomial distribution.\n        - 'glm': Generalized linear model.\n        - 'gvm': Generalized von Mises.\n        - 'bars': Bayesian adaptive regression splines.\n        - 'gp': Gaussian process.\n\n    Attributes\n    ----------\n    mode : str\n        The estimation mode.\n    tc_ : TuningCurve1D or TuningCurve2D\n        The estimated tuning curve.\n    \"\"\"\n\n    def __init__(self, mode=\"hist\"):\n        \"\"\"\n        Initialize a FiringRateEstimator.\n\n        Parameters\n        ----------\n        mode : str, optional\n            The estimation mode. Default is 'hist'.\n        \"\"\"\n        if mode not in [\"hist\"]:\n            raise NotImplementedError(\n                \"mode '{}' not supported / implemented yet!\".format(mode)\n            )\n        self._mode = mode\n\n    def _check_X_y_dt(self, X, y, lengths=None, dt=None, timestamps=None, n_bins=None):\n        \"\"\"\n        Validate and standardize input data for fitting or prediction.\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Input data.\n        y : array-like or RegularlySampledAnalogSignalArray\n            Target values.\n        lengths : array-like, optional\n            Lengths of intervals.\n        dt : float, optional\n            Temporal bin size.\n        timestamps : array-like, optional\n            Timestamps for the data.\n        n_bins : int or array-like, optional\n            Number of bins for discretization.\n\n        Returns\n        -------\n        X : np.ndarray\n            Standardized input data.\n        y : np.ndarray\n            Standardized target values.\n        dt : float\n            Temporal bin size.\n        n_bins : int or array-like\n            Number of bins for discretization.\n        \"\"\"\n        if isinstance(X, core.BinnedEventArray):\n            T = X.bin_centers\n            if lengths is not None:\n                logging.warning(\n                    \"'lengths' was passed in, but will be\"\n                    \" overwritten by 'X's 'lengths' attribute\"\n                )\n            if timestamps is not None:\n                logging.warning(\n                    \"'timestamps' was passed in, but will be\"\n                    \" overwritten by 'X's 'bin_centers' attribute\"\n                )\n            if dt is not None:\n                logging.warning(\n                    \"'dt' was passed in, but will be overwritten by 'X's 'ds' attribute\"\n                )\n            if isinstance(y, core.RegularlySampledAnalogSignalArray):\n                y = y(T).T\n\n            dt = X.ds\n            lengths = X.lengths\n            X = X.data.T\n        elif isinstance(X, np.ndarray):\n            if dt is None:\n                raise ValueError(\n                    \"'dt' is a required argument when 'X' is passed in as a numpy array!\"\n                )\n            if isinstance(y, core.RegularlySampledAnalogSignalArray):\n                if timestamps is not None:\n                    y = y(timestamps).T\n                else:\n                    raise ValueError(\n                        \"'timestamps' required when passing in 'X' as a numpy array and 'y' as a nelpy RegularlySampledAnalogSignalArray!\"\n                    )\n        else:\n            raise TypeError(\n                \"'X' should be either a nelpy BinnedEventArray, or a numpy array!\"\n            )\n\n        n_samples, n_units = X.shape\n        _, n_dims = y.shape\n        print(\"{}-dimensional y passed in\".format(n_dims))\n\n        assert n_samples == len(y), (\n            \"'X' and 'y' must have the same number\"\n            \" of samples! len(X)=={} but len(y)=={}\".format(n_samples, len(y))\n        )\n        if n_bins is not None:\n            n_bins = np.atleast_1d(n_bins)\n            assert len(n_bins) == n_dims, (\n                \"'n_bins' must have one entry for each dimension in 'y'!\"\n            )\n\n        return X, y, dt, n_bins\n\n    def fit(\n        self,\n        X,\n        y,\n        lengths=None,\n        dt=None,\n        timestamps=None,\n        unit_ids=None,\n        n_bins=None,\n        sample_weight=None,\n    ):\n        \"\"\"\n        Fit the firing rate estimator to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data.\n        y : array-like\n            Target values.\n        lengths : array-like, optional\n            Lengths of intervals.\n        dt : float, optional\n            Temporal bin size.\n        timestamps : array-like, optional\n            Timestamps for the data.\n        unit_ids : array-like, optional\n            Unit identifiers.\n        n_bins : int or array-like, optional\n            Number of bins for discretization.\n        sample_weight : array-like, optional\n            Weights for each sample.\n\n        Returns\n        -------\n        self : FiringRateEstimator\n            The fitted estimator.\n        \"\"\"\n        X, y, dt, n_bins = self._check_X_y_dt(\n            X=X, y=y, lengths=lengths, dt=dt, timestamps=timestamps, n_bins=n_bins\n        )\n\n        # 1. estimate mask\n        # 2. estimate occupancy\n        # 3. compute spikes histogram\n        # 4. normalize spike histogram by occupancy\n        # 5. apply mask\n\n        # if y.n_signals == 1:\n        #     self.tc_ = TuningCurve1D(bst=X, extern=y, n_extern=100, extmin=y.min(), extmax=y.max(), sigma=2.5, min_duration=0)\n        # if y.n_signals == 2:\n        #     xmin, ymin = y.min()\n        #     xmax, ymax = y.max()\n        #     self.tc_ = TuningCurve2D(bst=X, extern=y, ext_nx=50, ext_ny=50, ext_xmin=xmin, ext_xmax=xmax, ext_ymin=ymin, ext_ymax=ymax, sigma=2.5, min_duration=0)\n\n    @property\n    def mode(self):\n        return self._mode\n\n    def predict(self, X, lengths=None):\n        \"\"\"\n        Predict firing rates for the given input data.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        rates : array-like\n            Predicted firing rates.\n        \"\"\"\n        raise NotImplementedError\n\n    def predict_proba(self, X, lengths=None):\n        \"\"\"\n        Predict firing rate probabilities for the given input data.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        probabilities : array-like\n            Predicted probabilities.\n        \"\"\"\n        raise NotImplementedError\n\n    def score(self, X, y, lengths=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        Parameters\n        ----------\n        X : array-like\n            Test samples.\n        y : array-like\n            True values for X.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        raise NotImplementedError\n\n    def score_samples(self, X, y, lengths=None):\n        \"\"\"\n        Return the per-sample accuracy on the given test data and labels.\n\n        Parameters\n        ----------\n        X : array-like\n            Test samples.\n        y : array-like\n            True values for X.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        scores : array-like\n            Per-sample accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.FiringRateEstimator.fit","title":"<code>fit(X, y, lengths=None, dt=None, timestamps=None, unit_ids=None, n_bins=None, sample_weight=None)</code>","text":"<p>Fit the firing rate estimator to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data.</p> required <code>y</code> <code>array - like</code> <p>Target values.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <code>dt</code> <code>float</code> <p>Temporal bin size.</p> <code>None</code> <code>timestamps</code> <code>array - like</code> <p>Timestamps for the data.</p> <code>None</code> <code>unit_ids</code> <code>array - like</code> <p>Unit identifiers.</p> <code>None</code> <code>n_bins</code> <code>int or array - like</code> <p>Number of bins for discretization.</p> <code>None</code> <code>sample_weight</code> <code>array - like</code> <p>Weights for each sample.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>FiringRateEstimator</code> <p>The fitted estimator.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    lengths=None,\n    dt=None,\n    timestamps=None,\n    unit_ids=None,\n    n_bins=None,\n    sample_weight=None,\n):\n    \"\"\"\n    Fit the firing rate estimator to the data.\n\n    Parameters\n    ----------\n    X : array-like\n        Input data.\n    y : array-like\n        Target values.\n    lengths : array-like, optional\n        Lengths of intervals.\n    dt : float, optional\n        Temporal bin size.\n    timestamps : array-like, optional\n        Timestamps for the data.\n    unit_ids : array-like, optional\n        Unit identifiers.\n    n_bins : int or array-like, optional\n        Number of bins for discretization.\n    sample_weight : array-like, optional\n        Weights for each sample.\n\n    Returns\n    -------\n    self : FiringRateEstimator\n        The fitted estimator.\n    \"\"\"\n    X, y, dt, n_bins = self._check_X_y_dt(\n        X=X, y=y, lengths=lengths, dt=dt, timestamps=timestamps, n_bins=n_bins\n    )\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.FiringRateEstimator.predict","title":"<code>predict(X, lengths=None)</code>","text":"<p>Predict firing rates for the given input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rates</code> <code>array - like</code> <p>Predicted firing rates.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict(self, X, lengths=None):\n    \"\"\"\n    Predict firing rates for the given input data.\n\n    Parameters\n    ----------\n    X : array-like\n        Input data.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    rates : array-like\n        Predicted firing rates.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.FiringRateEstimator.predict_proba","title":"<code>predict_proba(X, lengths=None)</code>","text":"<p>Predict firing rate probabilities for the given input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>probabilities</code> <code>array - like</code> <p>Predicted probabilities.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict_proba(self, X, lengths=None):\n    \"\"\"\n    Predict firing rate probabilities for the given input data.\n\n    Parameters\n    ----------\n    X : array-like\n        Input data.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    probabilities : array-like\n        Predicted probabilities.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.FiringRateEstimator.score","title":"<code>score(X, y, lengths=None)</code>","text":"<p>Return the mean accuracy on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Test samples.</p> required <code>y</code> <code>array - like</code> <p>True values for X.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>Mean accuracy of self.predict(X) wrt. y.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def score(self, X, y, lengths=None):\n    \"\"\"\n    Return the mean accuracy on the given test data and labels.\n\n    Parameters\n    ----------\n    X : array-like\n        Test samples.\n    y : array-like\n        True values for X.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    score : float\n        Mean accuracy of self.predict(X) wrt. y.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.FiringRateEstimator.score_samples","title":"<code>score_samples(X, y, lengths=None)</code>","text":"<p>Return the per-sample accuracy on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Test samples.</p> required <code>y</code> <code>array - like</code> <p>True values for X.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>array - like</code> <p>Per-sample accuracy of self.predict(X) wrt. y.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def score_samples(self, X, y, lengths=None):\n    \"\"\"\n    Return the per-sample accuracy on the given test data and labels.\n\n    Parameters\n    ----------\n    X : array-like\n        Test samples.\n    y : array-like\n        True values for X.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    scores : array-like\n        Per-sample accuracy of self.predict(X) wrt. y.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.ItemGetter_iloc","title":"<code>ItemGetter_iloc</code>","text":"<p>               Bases: <code>object</code></p> <p>.iloc is primarily integer position based (from 0 to length-1 of the axis).</p> <p>.iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with python/numpy slice semantics).</p> <p>Allowed inputs are:     - An integer e.g. 5     - A list or array of integers [4, 3, 0]     - A slice object with ints 1:7</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class ItemGetter_iloc(object):\n    \"\"\".iloc is primarily integer position based (from 0 to length-1\n    of the axis).\n\n    .iloc will raise IndexError if a requested indexer is\n    out-of-bounds, except slice indexers which allow out-of-bounds\n    indexing. (this conforms with python/numpy slice semantics).\n\n    Allowed inputs are:\n        - An integer e.g. 5\n        - A list or array of integers [4, 3, 0]\n        - A slice object with ints 1:7\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        unit_idx_list = idx\n        if isinstance(idx, int):\n            unit_idx_list = [idx]\n\n        return self.obj[unit_idx_list]\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.ItemGetter_loc","title":"<code>ItemGetter_loc</code>","text":"<p>               Bases: <code>object</code></p> <p>.loc is primarily label based (that is, unit_id based)</p> <p>.loc will raise KeyError when the items are not found.</p> <p>Allowed inputs are:     - A single label, e.g. 5 or 'a', (note that 5 is interpreted         as a label of the index. This use is not an integer         position along the index)     - A list or array of labels ['a', 'b', 'c']     - A slice object with labels 'a':'f', (note that contrary to         usual python slices, both the start and the stop are         included!)</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class ItemGetter_loc(object):\n    \"\"\".loc is primarily label based (that is, unit_id based)\n\n    .loc will raise KeyError when the items are not found.\n\n    Allowed inputs are:\n        - A single label, e.g. 5 or 'a', (note that 5 is interpreted\n            as a label of the index. This use is not an integer\n            position along the index)\n        - A list or array of labels ['a', 'b', 'c']\n        - A slice object with labels 'a':'f', (note that contrary to\n            usual python slices, both the start and the stop are\n            included!)\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"unit_ids\"\"\"\n        unit_idx_list = self.obj._slicer[idx]\n\n        return self.obj[unit_idx_list]\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.KeywordError","title":"<code>KeywordError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Explanation of the error.</p> required Source code in <code>nelpy/estimators.py</code> <pre><code>class KeywordError(Exception):\n    \"\"\"\n    Exception raised for errors in keyword arguments.\n\n    Parameters\n    ----------\n    message : str\n        Explanation of the error.\n    \"\"\"\n\n    def __init__(self, message):\n        \"\"\"\n        Initialize the KeywordError.\n\n        Parameters\n        ----------\n        message : str\n            Explanation of the error.\n        \"\"\"\n        self.message = message\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap","title":"<code>NDRateMap</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>NDRateMap with persistent unit_ids and firing rates in Hz for N-dimensional data.</p> <p>Parameters:</p> Name Type Description Default <code>connectivity</code> <code>(continuous, discrete, circular)</code> <p>Defines how smoothing is applied. Default is 'continuous'. - 'continuous': Continuous smoothing. - 'discrete': No smoothing is applied. - 'circular': Circular smoothing (for angular variables).</p> <code>'continuous'</code> <p>Attributes:</p> Name Type Description <code>connectivity</code> <code>str</code> <p>Smoothing mode.</p> <code>ratemap_</code> <code>ndarray</code> <p>The estimated firing rate map.</p> <code>_unit_ids</code> <code>ndarray</code> <p>Persistent unit IDs.</p> <code>_bins</code> <code>ndarray</code> <p>Bin edges for each dimension.</p> <code>_bin_centers</code> <code>ndarray</code> <p>Bin centers for each dimension.</p> <code>_mask</code> <code>ndarray</code> <p>Mask for valid regions.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class NDRateMap(BaseEstimator):\n    \"\"\"\n    NDRateMap with persistent unit_ids and firing rates in Hz for N-dimensional data.\n\n    Parameters\n    ----------\n    connectivity : {'continuous', 'discrete', 'circular'}, optional\n        Defines how smoothing is applied. Default is 'continuous'.\n        - 'continuous': Continuous smoothing.\n        - 'discrete': No smoothing is applied.\n        - 'circular': Circular smoothing (for angular variables).\n\n    Attributes\n    ----------\n    connectivity : str\n        Smoothing mode.\n    ratemap_ : np.ndarray\n        The estimated firing rate map.\n    _unit_ids : np.ndarray\n        Persistent unit IDs.\n    _bins : np.ndarray\n        Bin edges for each dimension.\n    _bin_centers : np.ndarray\n        Bin centers for each dimension.\n    _mask : np.ndarray\n        Mask for valid regions.\n    \"\"\"\n\n    def __init__(self, connectivity=\"continuous\"):\n        self.connectivity = connectivity\n\n        self._slicer = UnitSlicer(self)\n        self.loc = ItemGetter_loc(self)\n        self.iloc = ItemGetter_iloc(self)\n\n    def __repr__(self):\n        r = super().__repr__()\n        if self._is_fitted():\n            dimstr = \"\"\n            for dd in range(self.n_dims):\n                dimstr += \", n_bins_d{}={}\".format(dd + 1, self.shape[dd + 1])\n            r += \" with shape (n_units={}{})\".format(self.n_units, dimstr)\n        return r\n\n    def fit(self, X, y, dt=1, unit_ids=None):\n        \"\"\"\n        Fit firing rates to the provided data.\n\n        Parameters\n        ----------\n        X : array-like, with shape (n_dims, ), each element of which has\n            shape (n_bins_dn, ) for n=1, ..., N; N=n_dims.\n            Bin locations (centers) where ratemap is defined.\n        y : array-like, shape (n_units, n_bins_d1, ..., n_bins_dN)\n            Expected number of spikes in a temporal bin of width dt, for each of\n            the predictor bins specified in X.\n        dt : float, optional\n            Temporal bin size with which firing rate y is defined. Default is 1.\n        unit_ids : array-like, shape (n_units,), optional\n            Persistent unit IDs that are used to associate units after\n            permutation. If None, uses np.arange(n_units).\n\n        Returns\n        -------\n        self : NDRateMap\n            The fitted NDRateMap instance.\n        \"\"\"\n        n_units, n_bins, n_dims = self._check_X_y(X, y)\n\n        self.ratemap_ = y / dt\n        self._bin_centers = X\n        self._bins = np.array(n_dims * [None])\n\n        if n_dims &gt; 1:\n            for dd in range(n_dims):\n                bin_centers = np.squeeze(X[dd])\n                dx = np.median(np.diff(bin_centers))\n                bins = np.insert(\n                    bin_centers[-1] + np.diff(bin_centers) / 2,\n                    0,\n                    bin_centers[0] - dx / 2,\n                )\n                bins = np.append(bins, bins[-1] + dx)\n                self._bins[dd] = bins\n        else:\n            bin_centers = np.squeeze(X)\n            dx = np.median(np.diff(bin_centers))\n            bins = np.insert(\n                bin_centers[-1] + np.diff(bin_centers) / 2, 0, bin_centers[0] - dx / 2\n            )\n            bins = np.append(bins, bins[-1] + dx)\n            self._bins = bins\n\n        if unit_ids is not None:\n            if len(unit_ids) != n_units:\n                raise ValueError(\n                    \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                        len(unit_ids), n_units\n                    )\n                )\n            self._unit_ids = unit_ids\n        else:\n            self._unit_ids = np.arange(n_units)\n\n    def predict(self, X):\n        \"\"\"\n        Predict firing rates for the given bin locations.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to predict firing rates for.\n\n        Returns\n        -------\n        rates : array-like\n            Predicted firing rates.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def synthesize(self, X):\n        \"\"\"\n        Generate synthetic spike data based on the ratemap.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to synthesize spikes for.\n\n        Returns\n        -------\n        spikes : array-like\n            Synthetic spike data.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def __len__(self):\n        return self.n_units\n\n    def __iter__(self):\n        \"\"\"TuningCurve1D iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"TuningCurve1D iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_units - 1:\n            raise StopIteration\n        out = copy.copy(self)\n        out.ratemap_ = self.ratemap_[tuple([index])]\n        out._unit_ids = self._unit_ids[index]\n        self._index += 1\n        return out\n\n    def __getitem__(self, *idx):\n        \"\"\"\n        Access RateMap units by index.\n\n        Parameters\n        ----------\n        *idx : int, slice, or list\n            Indices of units to access.\n\n        Returns\n        -------\n        out : NDRateMap\n            Subset NDRateMap with selected units.\n        \"\"\"\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        try:\n            out = copy.copy(self)\n            out.ratemap_ = self.ratemap_[tuple([idx])]\n            out._unit_ids = list(np.array(out._unit_ids)[tuple([idx])])\n            out._slicer = UnitSlicer(out)\n            out.loc = ItemGetter_loc(out)\n            out.iloc = ItemGetter_iloc(out)\n            return out\n        except Exception:\n            raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def get_peak_firing_order_ids(self):\n        \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n        Returns\n        -------\n        unit_ids : array-like\n            The permutaiton of unit_ids such that after reordering, the peak\n            firing locations are ordered along the RateMap.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\n                \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n            )\n        peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n        return np.array(self.unit_ids)[peakorder]\n\n    def reorder_units_by_ids(self, unit_ids, inplace=False):\n        \"\"\"Permute the unit ordering.\n\n        #TODO\n        If no order is specified, and an ordering exists from fit(), then the\n        data in X will automatically be permuted to match that registered during\n        fit().\n\n        Parameters\n        ----------\n        unit_ids : array-like, shape (n_units,)\n\n        Returns\n        -------\n        out : reordered RateMap\n        \"\"\"\n\n        def swap_units(arr, frm, to):\n            \"\"\"swap 'units' of a 3D np.array\"\"\"\n            arr[(frm, to), :] = arr[(to, frm), :]\n\n        self._validate_unit_ids(unit_ids)\n        if len(unit_ids) != len(self._unit_ids):\n            raise ValueError(\n                \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n            )\n\n        if inplace:\n            out = self\n        else:\n            out = copy.deepcopy(self)\n\n        neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_units(out.ratemap_, frm, to)\n            out._unit_ids[frm], out._unit_ids[to] = (\n                out._unit_ids[to],\n                out._unit_ids[frm],\n            )\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n        return out\n\n    def _check_X_y(self, X, y):\n        y = np.atleast_2d(y)\n\n        n_units = y.shape[0]\n        n_bins = y.shape[1:]\n        n_dims = len(n_bins)\n\n        if n_dims &gt; 1:\n            n_x_bins = tuple([len(x) for x in X])\n        else:\n            n_x_bins = tuple([len(X)])\n\n        assert n_units &gt; 0, \"n_units must be a positive integer!\"\n        assert n_x_bins == n_bins, \"X and y must have the same number of bins!\"\n\n        return n_units, n_bins, n_dims\n\n    def _validate_unit_ids(self, unit_ids):\n        self._check_unit_ids_in_ratemap(unit_ids)\n\n        if len(set(unit_ids)) != len(unit_ids):\n            raise ValueError(\"Duplicate unit_ids are not allowed.\")\n\n    def _check_unit_ids_in_ratemap(self, unit_ids):\n        for unit_id in unit_ids:\n            # NOTE: the check below allows for predict() to pass on only\n            # a subset of the units that were used during fit! So we\n            # could fit on 100 units, and then predict on only 10 of\n            # them, if we wanted.\n            if unit_id not in self.unit_ids:\n                raise ValueError(\n                    \"unit_id {} was not present during fit(); aborting...\".format(\n                        unit_id\n                    )\n                )\n\n    def _is_fitted(self):\n        try:\n            check_is_fitted(self, \"ratemap_\")\n        except Exception:  # should really be except NotFitterError\n            return False\n        return True\n\n    @property\n    def connectivity(self):\n        return self._connectivity\n\n    @connectivity.setter\n    def connectivity(self, val):\n        self._connectivity = self._validate_connectivity(val)\n\n    @staticmethod\n    def _validate_connectivity(connectivity):\n        connectivity = str(connectivity).strip().lower()\n        options = [\"continuous\", \"discrete\", \"circular\"]\n        if connectivity in options:\n            return connectivity\n        raise NotImplementedError(\n            \"connectivity '{}' is not supported yet!\".format(str(connectivity))\n        )\n\n    @property\n    def shape(self):\n        \"\"\"\n        RateMap.shape = (n_units, n_features_x, n_features_y)\n            OR\n        RateMap.shape = (n_units, n_features)\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape\n\n    @property\n    def n_dims(self):\n        check_is_fitted(self, \"ratemap_\")\n        n_dims = len(self.shape) - 1\n        return n_dims\n\n    @property\n    def is_1d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 2:\n            return True\n        return False\n\n    @property\n    def is_2d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 3:\n            return True\n        return False\n\n    @property\n    def n_units(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape[0]\n\n    @property\n    def unit_ids(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self._unit_ids\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) Number of external correlates (bins) along each dimension.\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.n_dims &gt; 1:\n            n_bins = tuple([len(x) for x in self.bin_centers])\n        else:\n            n_bins = len(self.bin_centers)\n        return n_bins\n\n    def max(self, axis=None, out=None):\n        \"\"\"\n        maximum firing rate for each unit:\n            RateMap.max()\n        maximum firing rate across units:\n            RateMap.max(axis=0)\n        \"\"\"\n        raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n            else:\n                return self.ratemap_.max(axis=1, out=out)\n        return self.ratemap_.max(axis=axis, out=out)\n\n    def min(self, axis=None, out=None):\n        raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.min(axis=1, out=out).min(axis=1, out=out)\n            else:\n                return self.ratemap_.min(axis=1, out=out)\n        return self.ratemap_.min(axis=axis, out=out)\n\n    def mean(self, axis=None, dtype=None, out=None, keepdims=False):\n        raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n        check_is_fitted(self, \"ratemap_\")\n        kwargs = {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.mean(axis=1, **kwargs).mean(axis=1, **kwargs)\n            else:\n                return self.ratemap_.mean(axis=1, **kwargs)\n        return self.ratemap_.mean(axis=axis, **kwargs)\n\n    @property\n    def bins(self):\n        return self._bins\n\n    @property\n    def bin_centers(self):\n        return self._bin_centers\n\n    @property\n    def mask(self):\n        return self._mask\n\n    @mask.setter\n    def mask(self, val):\n        # TODO: mask validation\n        raise NotImplementedError\n        self._mask = val\n\n    def plot(self, **kwargs):\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\"plot() not yet implemented for 2D RateMaps.\")\n        pad = kwargs.pop(\"pad\", None)\n        _plot_ratemap(self, pad=pad, **kwargs)\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n        \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n        mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to 'constant'. Default is\n            'reflect'\n        truncate : float\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        truncate : float, deprecated\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        cval : scalar, optional\n            Value to fill past edges of input if mode is 'constant'. Default is 0.0\n        \"\"\"\n\n        raise NotImplementedError\n\n        if sigma is None:\n            sigma = 0.1  # in units of extern\n        if truncate is None:\n            truncate = 4\n        if mode is None:\n            mode = \"reflect\"\n        if cval is None:\n            cval = 0.0\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins) along each dimension.</p>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>RateMap.shape = (n_units, n_features_x, n_features_y)     OR RateMap.shape = (n_units, n_features)</p>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.fit","title":"<code>fit(X, y, dt=1, unit_ids=None)</code>","text":"<p>Fit firing rates to the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, with shape (n_dims, ), each element of which has</code> <p>shape (n_bins_dn, ) for n=1, ..., N; N=n_dims. Bin locations (centers) where ratemap is defined.</p> required <code>y</code> <code>(array - like, shape(n_units, n_bins_d1, ..., n_bins_dN))</code> <p>Expected number of spikes in a temporal bin of width dt, for each of the predictor bins specified in X.</p> required <code>dt</code> <code>float</code> <p>Temporal bin size with which firing rate y is defined. Default is 1.</p> <code>1</code> <code>unit_ids</code> <code>(array - like, shape(n_units))</code> <p>Persistent unit IDs that are used to associate units after permutation. If None, uses np.arange(n_units).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>NDRateMap</code> <p>The fitted NDRateMap instance.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(self, X, y, dt=1, unit_ids=None):\n    \"\"\"\n    Fit firing rates to the provided data.\n\n    Parameters\n    ----------\n    X : array-like, with shape (n_dims, ), each element of which has\n        shape (n_bins_dn, ) for n=1, ..., N; N=n_dims.\n        Bin locations (centers) where ratemap is defined.\n    y : array-like, shape (n_units, n_bins_d1, ..., n_bins_dN)\n        Expected number of spikes in a temporal bin of width dt, for each of\n        the predictor bins specified in X.\n    dt : float, optional\n        Temporal bin size with which firing rate y is defined. Default is 1.\n    unit_ids : array-like, shape (n_units,), optional\n        Persistent unit IDs that are used to associate units after\n        permutation. If None, uses np.arange(n_units).\n\n    Returns\n    -------\n    self : NDRateMap\n        The fitted NDRateMap instance.\n    \"\"\"\n    n_units, n_bins, n_dims = self._check_X_y(X, y)\n\n    self.ratemap_ = y / dt\n    self._bin_centers = X\n    self._bins = np.array(n_dims * [None])\n\n    if n_dims &gt; 1:\n        for dd in range(n_dims):\n            bin_centers = np.squeeze(X[dd])\n            dx = np.median(np.diff(bin_centers))\n            bins = np.insert(\n                bin_centers[-1] + np.diff(bin_centers) / 2,\n                0,\n                bin_centers[0] - dx / 2,\n            )\n            bins = np.append(bins, bins[-1] + dx)\n            self._bins[dd] = bins\n    else:\n        bin_centers = np.squeeze(X)\n        dx = np.median(np.diff(bin_centers))\n        bins = np.insert(\n            bin_centers[-1] + np.diff(bin_centers) / 2, 0, bin_centers[0] - dx / 2\n        )\n        bins = np.append(bins, bins[-1] + dx)\n        self._bins = bins\n\n    if unit_ids is not None:\n        if len(unit_ids) != n_units:\n            raise ValueError(\n                \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                    len(unit_ids), n_units\n                )\n            )\n        self._unit_ids = unit_ids\n    else:\n        self._unit_ids = np.arange(n_units)\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.get_peak_firing_order_ids","title":"<code>get_peak_firing_order_ids()</code>","text":"<p>Get the unit_ids in order of peak firing location for 1D RateMaps.</p> <p>Returns:</p> Name Type Description <code>unit_ids</code> <code>array - like</code> <p>The permutaiton of unit_ids such that after reordering, the peak firing locations are ordered along the RateMap.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def get_peak_firing_order_ids(self):\n    \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n    Returns\n    -------\n    unit_ids : array-like\n        The permutaiton of unit_ids such that after reordering, the peak\n        firing locations are ordered along the RateMap.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    if self.is_2d:\n        raise NotImplementedError(\n            \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n        )\n    peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n    return np.array(self.unit_ids)[peakorder]\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.max","title":"<code>max(axis=None, out=None)</code>","text":"<p>maximum firing rate for each unit:     RateMap.max() maximum firing rate across units:     RateMap.max(axis=0)</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def max(self, axis=None, out=None):\n    \"\"\"\n    maximum firing rate for each unit:\n        RateMap.max()\n    maximum firing rate across units:\n        RateMap.max(axis=0)\n    \"\"\"\n    raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n    check_is_fitted(self, \"ratemap_\")\n    if axis is None:\n        if self.is_2d:\n            return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n        else:\n            return self.ratemap_.max(axis=1, out=out)\n    return self.ratemap_.max(axis=axis, out=out)\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.predict","title":"<code>predict(X)</code>","text":"<p>Predict firing rates for the given bin locations.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to predict firing rates for.</p> required <p>Returns:</p> Name Type Description <code>rates</code> <code>array - like</code> <p>Predicted firing rates.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict firing rates for the given bin locations.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to predict firing rates for.\n\n    Returns\n    -------\n    rates : array-like\n        Predicted firing rates.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.reorder_units_by_ids","title":"<code>reorder_units_by_ids(unit_ids, inplace=False)</code>","text":"<p>Permute the unit ordering.</p>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.reorder_units_by_ids--todo","title":"TODO","text":"<p>If no order is specified, and an ordering exists from fit(), then the data in X will automatically be permuted to match that registered during fit().</p> <p>Parameters:</p> Name Type Description Default <code>unit_ids</code> <code>(array - like, shape(n_units))</code> required <p>Returns:</p> Name Type Description <code>out</code> <code>reordered RateMap</code> Source code in <code>nelpy/estimators.py</code> <pre><code>def reorder_units_by_ids(self, unit_ids, inplace=False):\n    \"\"\"Permute the unit ordering.\n\n    #TODO\n    If no order is specified, and an ordering exists from fit(), then the\n    data in X will automatically be permuted to match that registered during\n    fit().\n\n    Parameters\n    ----------\n    unit_ids : array-like, shape (n_units,)\n\n    Returns\n    -------\n    out : reordered RateMap\n    \"\"\"\n\n    def swap_units(arr, frm, to):\n        \"\"\"swap 'units' of a 3D np.array\"\"\"\n        arr[(frm, to), :] = arr[(to, frm), :]\n\n    self._validate_unit_ids(unit_ids)\n    if len(unit_ids) != len(self._unit_ids):\n        raise ValueError(\n            \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n        )\n\n    if inplace:\n        out = self\n    else:\n        out = copy.deepcopy(self)\n\n    neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        swap_units(out.ratemap_, frm, to)\n        out._unit_ids[frm], out._unit_ids[to] = (\n            out._unit_ids[to],\n            out._unit_ids[frm],\n        )\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n    return out\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.smooth","title":"<code>smooth(*, sigma=None, truncate=None, inplace=False, mode=None, cval=None)</code>","text":"<p>Smooths the tuning curve with a Gaussian kernel.</p> <p>mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional     The mode parameter determines how the array borders are handled,     where cval is the value when mode is equal to 'constant'. Default is     'reflect' truncate : float     Truncate the filter at this many standard deviations. Default is 4.0. truncate : float, deprecated     Truncate the filter at this many standard deviations. Default is 4.0. cval : scalar, optional     Value to fill past edges of input if mode is 'constant'. Default is 0.0</p> Source code in <code>nelpy/estimators.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n    \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        'reflect'\n    truncate : float\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    truncate : float, deprecated\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0\n    \"\"\"\n\n    raise NotImplementedError\n\n    if sigma is None:\n        sigma = 0.1  # in units of extern\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.NDRateMap.synthesize","title":"<code>synthesize(X)</code>","text":"<p>Generate synthetic spike data based on the ratemap.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to synthesize spikes for.</p> required <p>Returns:</p> Name Type Description <code>spikes</code> <code>array - like</code> <p>Synthetic spike data.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def synthesize(self, X):\n    \"\"\"\n    Generate synthetic spike data based on the ratemap.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to synthesize spikes for.\n\n    Returns\n    -------\n    spikes : array-like\n        Synthetic spike data.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap","title":"<code>RateMap</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>RateMap with persistent unit_ids and firing rates in Hz.</p> <p>This class estimates and stores firing rate maps for neural data, supporting both 1D and 2D spatial representations.</p> <p>Parameters:</p> Name Type Description Default <code>connectivity</code> <code>(continuous, discrete, circular)</code> <p>Defines how smoothing is applied. Default is 'continuous'. - 'continuous': Continuous smoothing. - 'discrete': No smoothing is applied. - 'circular': Circular smoothing (for angular variables).</p> <code>'continuous'</code> <p>Attributes:</p> Name Type Description <code>connectivity</code> <code>str</code> <p>Smoothing mode.</p> <code>ratemap_</code> <code>ndarray</code> <p>The estimated firing rate map.</p> <code>_unit_ids</code> <code>ndarray</code> <p>Persistent unit IDs.</p> <code>_bins_x, _bins_y</code> <code>ndarray</code> <p>Bin edges for each dimension.</p> <code>_bin_centers_x, _bin_centers_y</code> <code>ndarray</code> <p>Bin centers for each dimension.</p> <code>_mask</code> <code>ndarray</code> <p>Mask for valid regions.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class RateMap(BaseEstimator):\n    \"\"\"\n    RateMap with persistent unit_ids and firing rates in Hz.\n\n    This class estimates and stores firing rate maps for neural data, supporting both 1D and 2D spatial representations.\n\n    Parameters\n    ----------\n    connectivity : {'continuous', 'discrete', 'circular'}, optional\n        Defines how smoothing is applied. Default is 'continuous'.\n        - 'continuous': Continuous smoothing.\n        - 'discrete': No smoothing is applied.\n        - 'circular': Circular smoothing (for angular variables).\n\n    Attributes\n    ----------\n    connectivity : str\n        Smoothing mode.\n    ratemap_ : np.ndarray\n        The estimated firing rate map.\n    _unit_ids : np.ndarray\n        Persistent unit IDs.\n    _bins_x, _bins_y : np.ndarray\n        Bin edges for each dimension.\n    _bin_centers_x, _bin_centers_y : np.ndarray\n        Bin centers for each dimension.\n    _mask : np.ndarray\n        Mask for valid regions.\n    \"\"\"\n\n    def __init__(self, connectivity=\"continuous\"):\n        \"\"\"\n        Initialize a RateMap object.\n\n        Parameters\n        ----------\n        connectivity : str, optional\n            Defines how smoothing is applied. If 'discrete', then no smoothing is\n            applied. Default is 'continuous'.\n        \"\"\"\n        self.connectivity = connectivity\n        self._slicer = UnitSlicer(self)\n        self.loc = ItemGetter_loc(self)\n        self.iloc = ItemGetter_iloc(self)\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the RateMap, including shape if fitted.\n\n        Returns\n        -------\n        r : str\n            String representation of the RateMap.\n        \"\"\"\n        r = super().__repr__()\n        if self._is_fitted():\n            if self.is_1d:\n                r += \" with shape (n_units={}, n_bins_x={})\".format(*self.shape)\n            else:\n                r += \" with shape (n_units={}, n_bins_x={}, n_bins_y={})\".format(\n                    *self.shape\n                )\n        return r\n\n    def fit(self, X, y, dt=1, unit_ids=None):\n        \"\"\"\n        Fit firing rates to the provided data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_bins,) or (n_bins_x, n_bins_y)\n            Bin locations (centers) where ratemap is defined.\n        y : array-like, shape (n_units, n_bins) or (n_units, n_bins_x, n_bins_y)\n            Expected number of spikes in a temporal bin of width dt, for each of\n            the predictor bins specified in X.\n        dt : float, optional\n            Temporal bin size with which firing rate y is defined. Default is 1.\n        unit_ids : array-like, shape (n_units,), optional\n            Persistent unit IDs that are used to associate units after\n            permutation. If None, uses np.arange(n_units).\n\n        Returns\n        -------\n        self : RateMap\n            The fitted RateMap instance.\n        \"\"\"\n        n_units, n_bins_x, n_bins_y = self._check_X_y(X, y)\n        if n_bins_y &gt; 0:\n            # self.ratemap_ = np.zeros((n_units, n_bins_x, n_bins_y)) #FIXME\n            self.ratemap_ = y / dt\n            bin_centers_x = np.squeeze(X[:, 0])\n            bin_centers_y = np.squeeze(X[:, 1])\n            bin_dx = np.median(np.diff(bin_centers_x))\n            bin_dy = np.median(np.diff(bin_centers_y))\n            bins_x = np.insert(\n                bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n                0,\n                bin_centers_x[0] - bin_dx / 2,\n            )\n            bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n            bins_y = np.insert(\n                bin_centers_y[:-1] + np.diff(bin_centers_y) / 2,\n                0,\n                bin_centers_y[0] - bin_dy / 2,\n            )\n            bins_y = np.append(bins_y, bins_y[-1] + bin_dy)\n            self._bins_x = bins_x\n            self._bins_y = bins_y\n            self._bin_centers_x = bin_centers_x\n            self._bin_centers_y = X[:, 1]\n        else:\n            # self.ratemap_ = np.zeros((n_units, n_bins_x)) #FIXME\n            self.ratemap_ = y / dt\n            bin_centers_x = np.squeeze(X)\n            bin_dx = np.median(np.diff(bin_centers_x))\n            bins_x = np.insert(\n                bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n                0,\n                bin_centers_x[0] - bin_dx / 2,\n            )\n            bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n            self._bins_x = bins_x\n            self._bin_centers_x = bin_centers_x\n\n        if unit_ids is not None:\n            if len(unit_ids) != n_units:\n                raise ValueError(\n                    \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                        len(unit_ids), n_units\n                    )\n                )\n            self._unit_ids = unit_ids\n        else:\n            self._unit_ids = np.arange(n_units)\n\n    def predict(self, X):\n        \"\"\"\n        Predict firing rates for the given bin locations.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to predict firing rates for.\n\n        Returns\n        -------\n        rates : array-like\n            Predicted firing rates.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def synthesize(self, X):\n        \"\"\"\n        Generate synthetic spike data based on the ratemap.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to synthesize spikes for.\n\n        Returns\n        -------\n        spikes : array-like\n            Synthetic spike data.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def __len__(self):\n        return self.n_units\n\n    def __iter__(self):\n        \"\"\"TuningCurve1D iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"TuningCurve1D iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_units - 1:\n            raise StopIteration\n        out = copy.copy(self)\n        out.ratemap_ = self.ratemap_[tuple([index])]\n        out._unit_ids = self._unit_ids[index]\n        self._index += 1\n        return out\n\n    def __getitem__(self, *idx):\n        \"\"\"\n        Access RateMap units by index.\n\n        Parameters\n        ----------\n        *idx : int, slice, or list\n            Indices of units to access.\n\n        Returns\n        -------\n        out : RateMap\n            Subset RateMap with selected units.\n        \"\"\"\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        try:\n            out = copy.copy(self)\n            out.ratemap_ = self.ratemap_[tuple([idx])]\n            out._unit_ids = list(np.array(out._unit_ids)[tuple([idx])])\n            out._slicer = UnitSlicer(out)\n            out.loc = ItemGetter_loc(out)\n            out.iloc = ItemGetter_iloc(out)\n            return out\n        except Exception:\n            raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def get_peak_firing_order_ids(self):\n        \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n        Returns\n        -------\n        unit_ids : array-like\n            The permutaiton of unit_ids such that after reordering, the peak\n            firing locations are ordered along the RateMap.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\n                \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n            )\n        peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n        return np.array(self.unit_ids)[peakorder]\n\n    def reorder_units_by_ids(self, unit_ids, inplace=False):\n        \"\"\"Permute the unit ordering.\n\n        #TODO\n        If no order is specified, and an ordering exists from fit(), then the\n        data in X will automatically be permuted to match that registered during\n        fit().\n\n        Parameters\n        ----------\n        unit_ids : array-like, shape (n_units,)\n\n        Returns\n        -------\n        out : reordered RateMap\n        \"\"\"\n\n        def swap_units(arr, frm, to):\n            \"\"\"swap 'units' of a 3D np.array\"\"\"\n            arr[(frm, to), :] = arr[(to, frm), :]\n\n        self._validate_unit_ids(unit_ids)\n        if len(unit_ids) != len(self._unit_ids):\n            raise ValueError(\n                \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n            )\n\n        if inplace:\n            out = self\n        else:\n            out = copy.deepcopy(self)\n\n        neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_units(out.ratemap_, frm, to)\n            out._unit_ids[frm], out._unit_ids[to] = (\n                out._unit_ids[to],\n                out._unit_ids[frm],\n            )\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n        return out\n\n    def _check_X_y(self, X, y):\n        X = np.atleast_1d(X)\n        y = np.atleast_2d(y)\n\n        n_units = y.shape[0]\n        n_bins_xy = y.shape[1]\n        try:\n            n_bins_yy = y.shape[2]\n        except IndexError:\n            n_bins_yy = 0\n\n        n_bins_xx = X.shape[0]\n        try:\n            n_bins_yx = X.shape[1]\n        except IndexError:\n            n_bins_yx = 0\n\n        assert n_units &gt; 0, \"n_units must be a positive integer!\"\n        assert n_bins_xx == n_bins_xy, \"X and y must have the same n_bins_x\"\n        assert n_bins_yx == n_bins_yy, \"X and y must have the same n_bins_y\"\n\n        n_bins_x = n_bins_xx\n        n_bins_y = n_bins_yy\n\n        return n_units, n_bins_x, n_bins_y\n\n    def _validate_unit_ids(self, unit_ids):\n        self._check_unit_ids_in_ratemap(unit_ids)\n\n        if len(set(unit_ids)) != len(unit_ids):\n            raise ValueError(\"Duplicate unit_ids are not allowed.\")\n\n    def _check_unit_ids_in_ratemap(self, unit_ids):\n        for unit_id in unit_ids:\n            # NOTE: the check below allows for predict() to pass on only\n            # a subset of the units that were used during fit! So we\n            # could fit on 100 units, and then predict on only 10 of\n            # them, if we wanted.\n            if unit_id not in self.unit_ids:\n                raise ValueError(\n                    \"unit_id {} was not present during fit(); aborting...\".format(\n                        unit_id\n                    )\n                )\n\n    def _is_fitted(self):\n        try:\n            check_is_fitted(self, \"ratemap_\")\n        except Exception:  # should really be except NotFitterError\n            return False\n        return True\n\n    @property\n    def connectivity(self):\n        return self._connectivity\n\n    @connectivity.setter\n    def connectivity(self, val):\n        self._connectivity = self._validate_connectivity(val)\n\n    @staticmethod\n    def _validate_connectivity(connectivity):\n        connectivity = str(connectivity).strip().lower()\n        options = [\"continuous\", \"discrete\", \"circular\"]\n        if connectivity in options:\n            return connectivity\n        raise NotImplementedError(\n            \"connectivity '{}' is not supported yet!\".format(str(connectivity))\n        )\n\n    @staticmethod\n    def _units_from_X(X):\n        \"\"\"\n        Get unit_ids from bst X, or generate them from ndarray X.\n\n        Returns\n        -------\n        n_units :\n        unit_ids :\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def T(self):\n        \"\"\"transpose the ratemap.\n        Here we transpose the x and y dims, and return a new RateMap object.\n        \"\"\"\n        if self.is_1d:\n            return self\n        out = copy.copy(self)\n        out.ratemap_ = np.transpose(out.ratemap_, axes=(0, 2, 1))\n        return out\n\n    @property\n    def shape(self):\n        \"\"\"\n        RateMap.shape = (n_units, n_features_x, n_features_y)\n            OR\n        RateMap.shape = (n_units, n_features)\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape\n\n    @property\n    def is_1d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 2:\n            return True\n        return False\n\n    @property\n    def is_2d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 3:\n            return True\n        return False\n\n    @property\n    def n_units(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape[0]\n\n    @property\n    def unit_ids(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self._unit_ids\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            return self.n_bins_x * self.n_bins_y\n        return self.n_bins_x\n\n    @property\n    def n_bins_x(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape[1]\n\n    @property\n    def n_bins_y(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_1d:\n            raise ValueError(\"RateMap is 1D; no y bins are defined.\")\n        return self.ratemap_.shape[2]\n\n    def max(self, axis=None, out=None):\n        \"\"\"\n        maximum firing rate for each unit:\n            RateMap.max()\n        maximum firing rate across units:\n            RateMap.max(axis=0)\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n            else:\n                return self.ratemap_.max(axis=1, out=out)\n        return self.ratemap_.max(axis=axis, out=out)\n\n    def min(self, axis=None, out=None):\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.min(axis=1, out=out).min(axis=1, out=out)\n            else:\n                return self.ratemap_.min(axis=1, out=out)\n        return self.ratemap_.min(axis=axis, out=out)\n\n    def mean(self, axis=None, dtype=None, out=None, keepdims=False):\n        check_is_fitted(self, \"ratemap_\")\n        kwargs = {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.mean(axis=1, **kwargs).mean(axis=1, **kwargs)\n            else:\n                return self.ratemap_.mean(axis=1, **kwargs)\n        return self.ratemap_.mean(axis=axis, **kwargs)\n\n    @property\n    def bins(self):\n        if self.is_1d:\n            return self._bins_x\n        return np.vstack((self._bins_x, self._bins_y))\n\n    @property\n    def bins_x(self):\n        return self._bins_x\n\n    @property\n    def bins_y(self):\n        if self.is_2d:\n            return self._bins_y\n        else:\n            raise ValueError(\"only valid for 2D RateMap() objects.\")\n\n    @property\n    def bin_centers(self):\n        if self.is_1d:\n            return self._bin_centers_x\n        return np.vstack((self._bin_centers_x, self._bin_centers_y))\n\n    @property\n    def bin_centers_x(self):\n        return self._bin_centers_x\n\n    @property\n    def bin_centers_y(self):\n        if self.is_2d:\n            return self._bin_centers_y\n        else:\n            raise ValueError(\"only valid for 2D RateMap() objects.\")\n\n    @property\n    def mask(self):\n        return self._mask\n\n    @mask.setter\n    def mask(self, val):\n        # TODO: mask validation\n        raise NotImplementedError\n        self._mask = val\n\n    def plot(self, **kwargs):\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\"plot() not yet implemented for 2D RateMaps.\")\n        pad = kwargs.pop(\"pad\", None)\n        _plot_ratemap(self, pad=pad, **kwargs)\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n        \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n        mode : {\u2018reflect\u2019, 'constant', \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to 'constant'. Default is\n            \u2018reflect\u2019\n        truncate : float\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        truncate : float, deprecated\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        cval : scalar, optional\n            Value to fill past edges of input if mode is 'constant'. Default is 0.0\n        \"\"\"\n\n        if sigma is None:\n            sigma = 0.1  # in units of extern\n        if truncate is None:\n            truncate = 4\n        if mode is None:\n            mode = \"reflect\"\n        if cval is None:\n            cval = 0.0\n\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.T","title":"<code>T</code>  <code>property</code>","text":"<p>transpose the ratemap. Here we transpose the x and y dims, and return a new RateMap object.</p>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.n_bins_x","title":"<code>n_bins_x</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.n_bins_y","title":"<code>n_bins_y</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>RateMap.shape = (n_units, n_features_x, n_features_y)     OR RateMap.shape = (n_units, n_features)</p>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.fit","title":"<code>fit(X, y, dt=1, unit_ids=None)</code>","text":"<p>Fit firing rates to the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_bins) or (n_bins_x, n_bins_y))</code> <p>Bin locations (centers) where ratemap is defined.</p> required <code>y</code> <code>(array - like, shape(n_units, n_bins) or (n_units, n_bins_x, n_bins_y))</code> <p>Expected number of spikes in a temporal bin of width dt, for each of the predictor bins specified in X.</p> required <code>dt</code> <code>float</code> <p>Temporal bin size with which firing rate y is defined. Default is 1.</p> <code>1</code> <code>unit_ids</code> <code>(array - like, shape(n_units))</code> <p>Persistent unit IDs that are used to associate units after permutation. If None, uses np.arange(n_units).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RateMap</code> <p>The fitted RateMap instance.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(self, X, y, dt=1, unit_ids=None):\n    \"\"\"\n    Fit firing rates to the provided data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_bins,) or (n_bins_x, n_bins_y)\n        Bin locations (centers) where ratemap is defined.\n    y : array-like, shape (n_units, n_bins) or (n_units, n_bins_x, n_bins_y)\n        Expected number of spikes in a temporal bin of width dt, for each of\n        the predictor bins specified in X.\n    dt : float, optional\n        Temporal bin size with which firing rate y is defined. Default is 1.\n    unit_ids : array-like, shape (n_units,), optional\n        Persistent unit IDs that are used to associate units after\n        permutation. If None, uses np.arange(n_units).\n\n    Returns\n    -------\n    self : RateMap\n        The fitted RateMap instance.\n    \"\"\"\n    n_units, n_bins_x, n_bins_y = self._check_X_y(X, y)\n    if n_bins_y &gt; 0:\n        # self.ratemap_ = np.zeros((n_units, n_bins_x, n_bins_y)) #FIXME\n        self.ratemap_ = y / dt\n        bin_centers_x = np.squeeze(X[:, 0])\n        bin_centers_y = np.squeeze(X[:, 1])\n        bin_dx = np.median(np.diff(bin_centers_x))\n        bin_dy = np.median(np.diff(bin_centers_y))\n        bins_x = np.insert(\n            bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n            0,\n            bin_centers_x[0] - bin_dx / 2,\n        )\n        bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n        bins_y = np.insert(\n            bin_centers_y[:-1] + np.diff(bin_centers_y) / 2,\n            0,\n            bin_centers_y[0] - bin_dy / 2,\n        )\n        bins_y = np.append(bins_y, bins_y[-1] + bin_dy)\n        self._bins_x = bins_x\n        self._bins_y = bins_y\n        self._bin_centers_x = bin_centers_x\n        self._bin_centers_y = X[:, 1]\n    else:\n        # self.ratemap_ = np.zeros((n_units, n_bins_x)) #FIXME\n        self.ratemap_ = y / dt\n        bin_centers_x = np.squeeze(X)\n        bin_dx = np.median(np.diff(bin_centers_x))\n        bins_x = np.insert(\n            bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n            0,\n            bin_centers_x[0] - bin_dx / 2,\n        )\n        bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n        self._bins_x = bins_x\n        self._bin_centers_x = bin_centers_x\n\n    if unit_ids is not None:\n        if len(unit_ids) != n_units:\n            raise ValueError(\n                \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                    len(unit_ids), n_units\n                )\n            )\n        self._unit_ids = unit_ids\n    else:\n        self._unit_ids = np.arange(n_units)\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.get_peak_firing_order_ids","title":"<code>get_peak_firing_order_ids()</code>","text":"<p>Get the unit_ids in order of peak firing location for 1D RateMaps.</p> <p>Returns:</p> Name Type Description <code>unit_ids</code> <code>array - like</code> <p>The permutaiton of unit_ids such that after reordering, the peak firing locations are ordered along the RateMap.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def get_peak_firing_order_ids(self):\n    \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n    Returns\n    -------\n    unit_ids : array-like\n        The permutaiton of unit_ids such that after reordering, the peak\n        firing locations are ordered along the RateMap.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    if self.is_2d:\n        raise NotImplementedError(\n            \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n        )\n    peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n    return np.array(self.unit_ids)[peakorder]\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.max","title":"<code>max(axis=None, out=None)</code>","text":"<p>maximum firing rate for each unit:     RateMap.max() maximum firing rate across units:     RateMap.max(axis=0)</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def max(self, axis=None, out=None):\n    \"\"\"\n    maximum firing rate for each unit:\n        RateMap.max()\n    maximum firing rate across units:\n        RateMap.max(axis=0)\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    if axis is None:\n        if self.is_2d:\n            return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n        else:\n            return self.ratemap_.max(axis=1, out=out)\n    return self.ratemap_.max(axis=axis, out=out)\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.predict","title":"<code>predict(X)</code>","text":"<p>Predict firing rates for the given bin locations.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to predict firing rates for.</p> required <p>Returns:</p> Name Type Description <code>rates</code> <code>array - like</code> <p>Predicted firing rates.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict firing rates for the given bin locations.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to predict firing rates for.\n\n    Returns\n    -------\n    rates : array-like\n        Predicted firing rates.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.reorder_units_by_ids","title":"<code>reorder_units_by_ids(unit_ids, inplace=False)</code>","text":"<p>Permute the unit ordering.</p>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.reorder_units_by_ids--todo","title":"TODO","text":"<p>If no order is specified, and an ordering exists from fit(), then the data in X will automatically be permuted to match that registered during fit().</p> <p>Parameters:</p> Name Type Description Default <code>unit_ids</code> <code>(array - like, shape(n_units))</code> required <p>Returns:</p> Name Type Description <code>out</code> <code>reordered RateMap</code> Source code in <code>nelpy/estimators.py</code> <pre><code>def reorder_units_by_ids(self, unit_ids, inplace=False):\n    \"\"\"Permute the unit ordering.\n\n    #TODO\n    If no order is specified, and an ordering exists from fit(), then the\n    data in X will automatically be permuted to match that registered during\n    fit().\n\n    Parameters\n    ----------\n    unit_ids : array-like, shape (n_units,)\n\n    Returns\n    -------\n    out : reordered RateMap\n    \"\"\"\n\n    def swap_units(arr, frm, to):\n        \"\"\"swap 'units' of a 3D np.array\"\"\"\n        arr[(frm, to), :] = arr[(to, frm), :]\n\n    self._validate_unit_ids(unit_ids)\n    if len(unit_ids) != len(self._unit_ids):\n        raise ValueError(\n            \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n        )\n\n    if inplace:\n        out = self\n    else:\n        out = copy.deepcopy(self)\n\n    neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        swap_units(out.ratemap_, frm, to)\n        out._unit_ids[frm], out._unit_ids[to] = (\n            out._unit_ids[to],\n            out._unit_ids[frm],\n        )\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n    return out\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.smooth","title":"<code>smooth(*, sigma=None, truncate=None, inplace=False, mode=None, cval=None)</code>","text":"<p>Smooths the tuning curve with a Gaussian kernel.</p> <p>mode : {\u2018reflect\u2019, 'constant', \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional     The mode parameter determines how the array borders are handled,     where cval is the value when mode is equal to 'constant'. Default is     \u2018reflect\u2019 truncate : float     Truncate the filter at this many standard deviations. Default is 4.0. truncate : float, deprecated     Truncate the filter at this many standard deviations. Default is 4.0. cval : scalar, optional     Value to fill past edges of input if mode is 'constant'. Default is 0.0</p> Source code in <code>nelpy/estimators.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n    \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n    mode : {\u2018reflect\u2019, 'constant', \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        \u2018reflect\u2019\n    truncate : float\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    truncate : float, deprecated\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.1  # in units of extern\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.RateMap.synthesize","title":"<code>synthesize(X)</code>","text":"<p>Generate synthetic spike data based on the ratemap.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to synthesize spikes for.</p> required <p>Returns:</p> Name Type Description <code>spikes</code> <code>array - like</code> <p>Synthetic spike data.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def synthesize(self, X):\n    \"\"\"\n    Generate synthetic spike data based on the ratemap.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to synthesize spikes for.\n\n    Returns\n    -------\n    spikes : array-like\n        Synthetic spike data.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.UnitSlicer","title":"<code>UnitSlicer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>nelpy/estimators.py</code> <pre><code>class UnitSlicer(object):\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, *args):\n        \"\"\"units ids\"\"\"\n        # by default, keep all units\n        unitslice = slice(None, None, None)\n        if isinstance(*args, int):\n            unitslice = args[0]\n        else:\n            slices = np.s_[args]\n            slices = slices[0]\n            unitslice = slices\n\n        if isinstance(unitslice, slice):\n            start = unitslice.start\n            stop = unitslice.stop\n            istep = unitslice.step\n\n            try:\n                if start is None:\n                    istart = 0\n                else:\n                    istart = list(self.obj.unit_ids).index(start)\n            except ValueError:\n                raise KeyError(\n                    \"unit_id {} could not be found in RateMap!\".format(start)\n                )\n\n            try:\n                if stop is None:\n                    istop = self.obj.n_units\n                else:\n                    istop = list(self.obj.unit_ids).index(stop) + 1\n            except ValueError:\n                raise KeyError(\"unit_id {} could not be found in RateMap!\".format(stop))\n            if istep is None:\n                istep = 1\n            if istep &lt; 0:\n                istop -= 1\n                istart -= 1\n                istart, istop = istop, istart\n            unit_idx_list = list(range(istart, istop, istep))\n        else:\n            unit_idx_list = []\n            unitslice = np.atleast_1d(unitslice)\n            for unit in unitslice:\n                try:\n                    uidx = list(self.obj.unit_ids).index(unit)\n                except ValueError:\n                    raise KeyError(\n                        \"unit_id {} could not be found in RateMap!\".format(unit)\n                    )\n                else:\n                    unit_idx_list.append(uidx)\n\n        return unit_idx_list\n</code></pre>"},{"location":"reference/estimators/#nelpy.estimators.decode_bayesian_memoryless_nd","title":"<code>decode_bayesian_memoryless_nd(X, *, ratemap, bin_centers, dt=1)</code>","text":"<p>Memoryless Bayesian decoding (supports multidimensional decoding).</p> <p>Decode binned spike counts (e.g. from a BinnedSpikeTrainArray) to an external correlate (e.g. position), using a memoryless Bayesian decoder and a previously estimated ratemap.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy array with shape (n_samples, n_features),</code> <p>where the features are generally putative units / cells, and where each sample represents spike counts in a singleton data window.</p> required <code>ratemap</code> <code>array-like of shape (n_units, n_bins_d1, ..., n_bins_dN)</code> <p>Expected number of spikes for each unit, within each bin, along each dimension.</p> required <code>bin_centers</code> <code>array-like with shape (n_dims, ), where each element is also</code> <p>an array-like with shape (n_bins_dn, ) containing the bin centers for the particular dimension.</p> required <code>dt</code> <code>(float, optional(default=1))</code> <p>Temporal bin width corresponding to X, in seconds.</p> <p>NOTE: generally it is assumed that ratemap will be given in Hz (that is, it has dt=1). If ratemap has a different unit, then dt might have to be adjusted to compensate for this. This can get tricky / confusing, so the recommended approach is always to construct ratemap with dt=1, and then to use the data-specific dt here when decoding.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>posterior</code> <code>numpy array of shape (n_samples, n_bins_d1, ..., n_bins_dN)</code> <p>Posterior probabilities for each voxel.</p> <code>expected_pth</code> <code>numpy array of shape (n_samples, n_dims)</code> <p>Expected (posterior-averaged) decoded trajectory.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def decode_bayesian_memoryless_nd(X, *, ratemap, bin_centers, dt=1):\n    \"\"\"Memoryless Bayesian decoding (supports multidimensional decoding).\n\n    Decode binned spike counts (e.g. from a BinnedSpikeTrainArray) to an\n    external correlate (e.g. position), using a memoryless Bayesian decoder and\n    a previously estimated ratemap.\n\n    Parameters\n    ----------\n    X : numpy array with shape (n_samples, n_features),\n        where the features are generally putative units / cells, and where\n        each sample represents spike counts in a singleton data window.\n    ratemap : array-like of shape (n_units, n_bins_d1, ..., n_bins_dN)\n        Expected number of spikes for each unit, within each bin, along each\n        dimension.\n    bin_centers : array-like with shape (n_dims, ), where each element is also\n        an array-like with shape (n_bins_dn, ) containing the bin centers for\n        the particular dimension.\n    dt : float, optional (default=1)\n        Temporal bin width corresponding to X, in seconds.\n\n        NOTE: generally it is assumed that ratemap will be given in Hz (that is,\n        it has dt=1). If ratemap has a different unit, then dt might have to be\n        adjusted to compensate for this. This can get tricky / confusing, so the\n        recommended approach is always to construct ratemap with dt=1, and then\n        to use the data-specific dt here when decoding.\n\n    Returns\n    -------\n    posterior : numpy array of shape (n_samples, n_bins_d1, ..., n_bins_dN)\n        Posterior probabilities for each voxel.\n    expected_pth : numpy array of shape (n_samples, n_dims)\n        Expected (posterior-averaged) decoded trajectory.\n    \"\"\"\n\n    def tile_obs(obs, *n_bins):\n        n_units = len(obs)\n        out = np.zeros((n_units, *n_bins))\n        for unit in range(n_units):\n            out[unit, :] = obs[unit]\n        return out\n\n    n_samples, n_features = X.shape\n    n_units = ratemap.shape[0]\n    n_bins = np.atleast_1d(ratemap.shape[1:])\n    n_dims = len(n_bins)\n\n    assert n_features == n_units, \"X has {} units, whereas ratemap has {}\".format(\n        n_features, n_units\n    )\n\n    lfx = np.log(ratemap)\n    eterm = -ratemap.sum(axis=0) * dt\n\n    posterior = np.empty((n_samples, *n_bins))\n    posterior[:] = np.nan\n\n    # decode each sample / bin separately\n    for tt in range(n_samples):\n        obs = X[tt]\n        if obs.sum() &gt; 0:\n            posterior[tt] = (tile_obs(obs, *n_bins) * lfx).sum(axis=0) + eterm\n\n    # normalize posterior:\n    posterior = np.exp(\n        posterior\n        - logsumexp(posterior, axis=tuple(np.arange(1, n_dims + 1)), keepdims=True)\n    )\n\n    if n_dims &gt; 1:\n        expected = []\n        for dd in range(1, n_dims + 1):\n            axes = tuple(set(np.arange(1, n_dims + 1)) - set([dd]))\n            expected.append(\n                (bin_centers[dd - 1] * posterior.sum(axis=axes)).sum(axis=1)\n            )\n        expected_pth = np.vstack(expected).T\n    else:\n        expected_pth = (bin_centers * posterior).sum(axis=1)\n\n    return posterior, expected_pth\n</code></pre>"},{"location":"reference/filtering/","title":"Filtering API Reference","text":"<p>This module implements filtering functionailty for core nelpy objects.</p>"},{"location":"reference/filtering/#nelpy.filtering.getsos","title":"<code>getsos(*, fs, fl=None, fh=None, bandstop=False, gpass=None, gstop=None, ftype='cheby2')</code>","text":"<p>Return second-order sections representation of the IIR filter.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>The sampling frequency (Hz).</p> required <code>fl</code> <code>float</code> <p>Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.</p> <code>None</code> <code>fh</code> <code>float</code> <p>Upper cut-off frequency (in Hz), 0 or None to ignore. Default is None.</p> <code>None</code> <code>bandstop</code> <code>boolean</code> <p>If False, passband is between fl and fh. If True, stopband is between fl and fh. Default is False.</p> <code>False</code> <code>gpass</code> <code>float</code> <p>The maximum loss in the passband (dB). Default is 0.1 dB.</p> <code>None</code> <code>gstop</code> <code>float</code> <p>The minimum attenuation in the stopband (dB). Default is 30 dB.</p> <code>None</code> <code>ftype</code> <code>str</code> <p>The type of IIR filter to design:     - Butterworth   : 'butter'     - Chebyshev I   : 'cheby1'     - Chebyshev II  : 'cheby2' (Default)     - Cauer/elliptic: 'ellip'     - Bessel/Thomson: 'bessel'</p> <code>'cheby2'</code> <p>Returns:</p> Name Type Description <code>sos</code> <code>ndarray</code> <p>Second-order sections representation of the IIR filter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from scipy import signal\n&gt;&gt;&gt;\n&gt;&gt;&gt; sos = getsos(...)\n&gt;&gt;&gt; w, h = signal.sosfreqz(sos, worN=1500)\n&gt;&gt;&gt; db = 20 * np.log10(np.abs(h))\n&gt;&gt;&gt; freq = w * fs / (2 * np.pi)\n&gt;&gt;&gt; plt.subplot(2, 1, 1)\n&gt;&gt;&gt; plt.ylabel(\"Gain [dB]\")\n&gt;&gt;&gt; plt.plot(freq, db)\n&gt;&gt;&gt; plt.subplot(2, 1, 2)\n&gt;&gt;&gt; plt.plot(freq, np.angle(h))\n&gt;&gt;&gt; plt.ylabel(\"Phase [rad]\")\n</code></pre> <p>Although not currently supported, filters can be stacked as well, as follows:</p> <pre><code>&gt;&gt;&gt; sos = np.vstack((nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n             nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n             nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n             nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=1, ftype='butter')))\n</code></pre> Source code in <code>nelpy/filtering.py</code> <pre><code>def getsos(\n    *, fs, fl=None, fh=None, bandstop=False, gpass=None, gstop=None, ftype=\"cheby2\"\n):\n    \"\"\"Return second-order sections representation of the IIR filter.\n\n    Parameters\n    ----------\n    fs : float\n        The sampling frequency (Hz).\n    fl : float, optional\n        Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.\n    fh : float, optional\n        Upper cut-off frequency (in Hz), 0 or None to ignore. Default is None.\n    bandstop : boolean, optional\n        If False, passband is between fl and fh. If True, stopband is between\n        fl and fh. Default is False.\n    gpass : float, optional\n        The maximum loss in the passband (dB). Default is 0.1 dB.\n    gstop : float, optional\n        The minimum attenuation in the stopband (dB). Default is 30 dB.\n    ftype : str, optional\n        The type of IIR filter to design:\n            - Butterworth   : 'butter'\n            - Chebyshev I   : 'cheby1'\n            - Chebyshev II  : 'cheby2' (Default)\n            - Cauer/elliptic: 'ellip'\n            - Bessel/Thomson: 'bessel'\n\n    Returns\n    -------\n    sos : ndarray\n        Second-order sections representation of the IIR filter.\n\n    Examples\n    -------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from scipy import signal\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; sos = getsos(...)\n    &gt;&gt;&gt; w, h = signal.sosfreqz(sos, worN=1500)\n    &gt;&gt;&gt; db = 20 * np.log10(np.abs(h))\n    &gt;&gt;&gt; freq = w * fs / (2 * np.pi)\n    &gt;&gt;&gt; plt.subplot(2, 1, 1)\n    &gt;&gt;&gt; plt.ylabel(\"Gain [dB]\")\n    &gt;&gt;&gt; plt.plot(freq, db)\n    &gt;&gt;&gt; plt.subplot(2, 1, 2)\n    &gt;&gt;&gt; plt.plot(freq, np.angle(h))\n    &gt;&gt;&gt; plt.ylabel(\"Phase [rad]\")\n\n    Although not currently supported, filters can be stacked as well, as follows:\n    &gt;&gt;&gt; sos = np.vstack((nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n                 nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n                 nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n                 nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=1, ftype='butter')))\n\n    \"\"\"\n\n    try:\n        assert fh &lt; fs, \"fh must be less than sampling rate!\"\n    except TypeError:\n        pass\n    try:\n        assert fl &lt; fh, \"fl must be less than fh!\"\n    except TypeError:\n        pass\n\n    if gpass is None:\n        gpass = 0.1  # max loss in passband, dB\n    if gstop is None:\n        gstop = 30  # min attenuation in stopband (dB)\n\n    try:\n        if np.isinf(fh):\n            fh = None\n    except TypeError:\n        pass\n    if fl == 0:\n        fl = None\n\n    # Handle cutoff frequencies\n    fso2 = fs / 2.0\n    if (fl is None) and (fh is None):\n        raise ValueError(\"Nonsensical all-pass filter requested...\")\n    elif fl is None:  # lowpass\n        wp = fh / fso2\n        ws = 1.4 * fh / fso2\n    elif fh is None:  # highpass\n        wp = fl / fso2\n        ws = 0.8 * fl / fso2\n    else:  # bandpass\n        wp = [fl / fso2, fh / fso2]\n        ws = [0.8 * fl / fso2, 1.4 * fh / fso2]\n    if bandstop:  # notch / bandstop filter\n        wp, ws = ws, wp\n\n    sos = sig.iirdesign(wp, ws, gpass=gpass, gstop=gstop, ftype=ftype, output=\"sos\")\n\n    return sos\n</code></pre>"},{"location":"reference/filtering/#nelpy.filtering.sosfiltfilt","title":"<code>sosfiltfilt(timeseries, *, fl=None, fh=None, fs=None, inplace=False, bandstop=False, gpass=None, gstop=None, ftype=None, buffer_len=None, overlap_len=None, parallel=True, **kwargs)</code>","text":"<p>Apply a zero-phase digital filter using second-order sections.</p> <p>This function applies a forward and backward digital filter to a signal using second-order sections (SOS) representation. The result has zero phase distortion and the same shape as the input.</p> <p>Parameters:</p> Name Type Description Default <code>timeseries</code> <code>nelpy.RegularlySampledAnalogSignalArray (preferred), ndarray, or list</code> <p>Object or data to filter. Can be a NumPy array or a Nelpy AnalogSignalArray.</p> required <code>fs</code> <code>float, optional only if RegularlySampledAnalogSignalArray is passed</code> <p>The sampling frequency (Hz). Obtained from the input timeseries.</p> <code>None</code> <code>fl</code> <code>float</code> <p>Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.</p> <code>None</code> <code>fh</code> <code>float</code> <p>Upper cut-off frequency (in Hz), np.inf or None to ignore. Default is None.</p> <code>None</code> <code>bandstop</code> <code>boolean</code> <p>If False, passband is between fl and fh. If True, stopband is between fl and fh. Default is False.</p> <code>False</code> <code>gpass</code> <code>float</code> <p>The maximum loss in the passband (dB). Default is 0.1 dB.</p> <code>None</code> <code>gstop</code> <code>float</code> <p>The minimum attenuation in the stopband (dB). Default is 30 dB.</p> <code>None</code> <code>ftype</code> <code>str</code> <p>The type of IIR filter to design:     - Butterworth   : 'butter'     - Chebyshev I   : 'cheby1'     - Chebyshev II  : 'cheby2' (Default)     - Cauer/elliptic: 'ellip'     - Bessel/Thomson: 'bessel'</p> <code>None</code> <code>buffer_len</code> <code>int</code> <p>How much data to process at a time. Default is 2**22 = 4194304 samples.</p> <code>None</code> <code>overlap_len</code> <code>int</code> <p>How much data we add to the end of each chunk to smooth out filter transients.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True, modifies the input in place. Default is False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>If True, uses multiprocessing for parallel filtering. Default is True.</p> <code>True</code> <code>kwargs</code> <code>optional</code> <p>Other keyword arguments are passed to scipy.signal's iirdesign method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>out</code> <code>nelpy.RegularlySampledAnalogSignalArray, ndarray, or list</code> <p>Same output type as input timeseries.</p> <code>WARNING</code> <code>The data type of the output object is the same as that of the input.</code> <code>Thus it is highly recommended to have your input data be floats before calling</code> <code>this function. If the input is an RSASA, you do not need to worry because</code> <code>the underlying data are already floats.</code> See Also <p>scipy.signal.sosfilt : Apply a digital filter forward in time. scipy.signal.filtfilt : Zero-phase filtering for transfer function and FIR filters.</p> Notes <p>This function is similar to <code>scipy.signal.filtfilt</code>, but uses the second-order sections (SOS) representation for improved numerical stability, especially for high-order filters.</p> <p>Examples:</p> <p>Filter a noisy sine wave (NumPy array):</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.signal import butter\n&gt;&gt;&gt; from nelpy.filtering import sosfiltfilt\n&gt;&gt;&gt; np.random.seed(0)\n&gt;&gt;&gt; t = np.linspace(0, 1, 1000, endpoint=False)\n&gt;&gt;&gt; x = np.sin(2 * np.pi * 5 * t) + 0.5 * np.random.randn(t.size)\n&gt;&gt;&gt; sos = butter(4, 10, \"low\", fs=1000, output=\"sos\")\n&gt;&gt;&gt; y = sosfiltfilt(x, fs=1000, fl=None, fh=10)\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; plt.plot(t, x, label=\"Noisy signal\")\n&gt;&gt;&gt; plt.plot(t, y, label=\"Filtered signal\")\n&gt;&gt;&gt; plt.legend()\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Filter a Nelpy AnalogSignalArray:</p> <pre><code>&gt;&gt;&gt; from nelpy import AnalogSignalArray\n&gt;&gt;&gt; # Create a 2-channel signal\n&gt;&gt;&gt; data = np.vstack([np.sin(2 * np.pi * 5 * t), np.cos(2 * np.pi * 5 * t)])\n&gt;&gt;&gt; asa = AnalogSignalArray(data=data, abscissa_vals=t, fs=1000)\n&gt;&gt;&gt; filtered_asa = sosfiltfilt(asa, fl=None, fh=10)\n&gt;&gt;&gt; print(filtered_asa.data.shape)\n(2, 1000)\n&gt;&gt;&gt; # Plot the first channel\n&gt;&gt;&gt; plt.plot(t, asa.data[0], label=\"Original\")\n&gt;&gt;&gt; plt.plot(t, filtered_asa.data[0], label=\"Filtered\")\n&gt;&gt;&gt; plt.legend()\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/filtering.py</code> <pre><code>def sosfiltfilt(\n    timeseries,\n    *,\n    fl=None,\n    fh=None,\n    fs=None,\n    inplace=False,\n    bandstop=False,\n    gpass=None,\n    gstop=None,\n    ftype=None,\n    buffer_len=None,\n    overlap_len=None,\n    parallel=True,\n    **kwargs,\n):\n    \"\"\"\n    Apply a zero-phase digital filter using second-order sections.\n\n    This function applies a forward and backward digital filter to a signal using\n    second-order sections (SOS) representation. The result has zero phase distortion\n    and the same shape as the input.\n\n    Parameters\n    ----------\n    timeseries : nelpy.RegularlySampledAnalogSignalArray (preferred), ndarray, or list\n        Object or data to filter. Can be a NumPy array or a Nelpy AnalogSignalArray.\n    fs : float, optional only if RegularlySampledAnalogSignalArray is passed\n        The sampling frequency (Hz). Obtained from the input timeseries.\n    fl : float, optional\n        Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.\n    fh : float, optional\n        Upper cut-off frequency (in Hz), np.inf or None to ignore. Default is None.\n    bandstop : boolean, optional\n        If False, passband is between fl and fh. If True, stopband is between\n        fl and fh. Default is False.\n    gpass : float, optional\n        The maximum loss in the passband (dB). Default is 0.1 dB.\n    gstop : float, optional\n        The minimum attenuation in the stopband (dB). Default is 30 dB.\n    ftype : str, optional\n        The type of IIR filter to design:\n            - Butterworth   : 'butter'\n            - Chebyshev I   : 'cheby1'\n            - Chebyshev II  : 'cheby2' (Default)\n            - Cauer/elliptic: 'ellip'\n            - Bessel/Thomson: 'bessel'\n    buffer_len : int, optional\n        How much data to process at a time. Default is 2**22 = 4194304 samples.\n    overlap_len : int, optional\n        How much data we add to the end of each chunk to smooth out filter\n        transients.\n    inplace : bool, optional\n        If True, modifies the input in place. Default is False.\n    parallel : bool, optional\n        If True, uses multiprocessing for parallel filtering. Default is True.\n    kwargs : optional\n        Other keyword arguments are passed to scipy.signal's iirdesign method\n\n    Returns\n    -------\n    out : nelpy.RegularlySampledAnalogSignalArray, ndarray, or list\n        Same output type as input timeseries.\n\n    WARNING : The data type of the output object is the same as that of the input.\n    Thus it is highly recommended to have your input data be floats before calling\n    this function. If the input is an RSASA, you do not need to worry because\n    the underlying data are already floats.\n\n    See Also\n    --------\n    scipy.signal.sosfilt : Apply a digital filter forward in time.\n    scipy.signal.filtfilt : Zero-phase filtering for transfer function and FIR filters.\n\n    Notes\n    -----\n    This function is similar to `scipy.signal.filtfilt`, but uses the second-order sections\n    (SOS) representation for improved numerical stability, especially for high-order filters.\n\n    Examples\n    --------\n    Filter a noisy sine wave (NumPy array):\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy.signal import butter\n    &gt;&gt;&gt; from nelpy.filtering import sosfiltfilt\n    &gt;&gt;&gt; np.random.seed(0)\n    &gt;&gt;&gt; t = np.linspace(0, 1, 1000, endpoint=False)\n    &gt;&gt;&gt; x = np.sin(2 * np.pi * 5 * t) + 0.5 * np.random.randn(t.size)\n    &gt;&gt;&gt; sos = butter(4, 10, \"low\", fs=1000, output=\"sos\")\n    &gt;&gt;&gt; y = sosfiltfilt(x, fs=1000, fl=None, fh=10)\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; plt.plot(t, x, label=\"Noisy signal\")\n    &gt;&gt;&gt; plt.plot(t, y, label=\"Filtered signal\")\n    &gt;&gt;&gt; plt.legend()\n    &gt;&gt;&gt; plt.show()\n\n    Filter a Nelpy AnalogSignalArray:\n\n    &gt;&gt;&gt; from nelpy import AnalogSignalArray\n    &gt;&gt;&gt; # Create a 2-channel signal\n    &gt;&gt;&gt; data = np.vstack([np.sin(2 * np.pi * 5 * t), np.cos(2 * np.pi * 5 * t)])\n    &gt;&gt;&gt; asa = AnalogSignalArray(data=data, abscissa_vals=t, fs=1000)\n    &gt;&gt;&gt; filtered_asa = sosfiltfilt(asa, fl=None, fh=10)\n    &gt;&gt;&gt; print(filtered_asa.data.shape)\n    (2, 1000)\n    &gt;&gt;&gt; # Plot the first channel\n    &gt;&gt;&gt; plt.plot(t, asa.data[0], label=\"Original\")\n    &gt;&gt;&gt; plt.plot(t, filtered_asa.data[0], label=\"Filtered\")\n    &gt;&gt;&gt; plt.legend()\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # make sure that fs is specified, unless AnalogSignalArray is passed in\n    if isinstance(timeseries, (np.ndarray, list)):\n        if fs is None:\n            raise ValueError(\"Sampling frequency, fs, must be specified!\")\n    elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n        if fs is None:\n            fs = timeseries.fs\n    else:\n        raise TypeError(\"Unsupported input type!\")\n\n    try:\n        assert fh &lt; fs, \"fh must be less than sampling rate!\"\n    except TypeError:\n        pass\n    try:\n        assert fl &lt; fh, \"fl must be less than fh!\"\n    except TypeError:\n        pass\n\n    if inplace:\n        out = timeseries\n    else:\n        out = deepcopy(timeseries)\n    if overlap_len is None:\n        overlap_len = int(fs * 2)\n    if buffer_len is None:\n        buffer_len = 4194304\n    if gpass is None:\n        gpass = 0.1  # max loss in passband, dB\n    if gstop is None:\n        gstop = 30  # min attenuation in stopband (dB)\n    if ftype is None:\n        ftype = \"cheby2\"\n\n    try:\n        if np.isinf(fh):\n            fh = None\n    except TypeError:\n        pass\n    if fl == 0:\n        fl = None\n\n    # Handle cutoff frequencies\n    fso2 = fs / 2.0\n    if (fl is None) and (fh is None):\n        raise ValueError(\"Nonsensical all-pass filter requested...\")\n    elif fl is None:  # lowpass\n        wp = fh / fso2\n        ws = 1.4 * fh / fso2\n    elif fh is None:  # highpass\n        wp = fl / fso2\n        ws = 0.8 * fl / fso2\n    else:  # bandpass\n        wp = [fl / fso2, fh / fso2]\n        ws = [0.8 * fl / fso2, 1.4 * fh / fso2]\n    if bandstop:  # notch / bandstop filter\n        wp, ws = ws, wp\n\n    sos = sig.iirdesign(\n        wp, ws, gpass=gpass, gstop=gstop, ftype=ftype, output=\"sos\", **kwargs\n    )\n\n    # Prepare input and output data structures\n    # Output array lives in shared memory and will reduce overhead from pickling/de-pickling\n    # data if we're doing parallelized filtering\n    if isinstance(timeseries, (np.ndarray, list)):\n        temp_array = np.array(timeseries)\n        dims = temp_array.shape\n        if len(temp_array.shape) &gt; 2:\n            raise NotImplementedError(\n                \"Filtering for &gt;2D ndarray or list is not implemented\"\n            )\n        shared_array_base = Array(ctypes.c_double, temp_array.size, lock=False)\n        shared_array_out = np.ctypeslib.as_array(shared_array_base)\n        # Force input and output arrays to be 2D (N x T) where N is number of signals\n        # and T is number of time points\n        if len(temp_array.squeeze().shape) == 1:\n            shared_array_out = np.ctypeslib.as_array(shared_array_base).reshape(\n                (1, temp_array.size)\n            )\n            input_asarray = temp_array.reshape((1, temp_array.size))\n        else:\n            shared_array_out = np.ctypeslib.as_array(shared_array_base).reshape(dims)\n            input_asarray = temp_array\n    elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n        dims = timeseries._data.shape\n        shared_array_base = Array(\n            ctypes.c_double, timeseries._data_rowsig.size, lock=False\n        )\n        shared_array_out = np.ctypeslib.as_array(shared_array_base).reshape(dims)\n        input_asarray = timeseries._data\n\n    # Embedded function to avoid pickling data but need global to make this function\n    # module-visible (required by multiprocessing). I know, a bit of a hack\n    global filter_chunk\n\n    def filter_chunk(it):\n        \"\"\"The function that performs the chunked filtering\"\"\"\n\n        try:\n            start, stop, buffer_len, overlap_len, buff_st_idx = it\n            buff_nd_idx = int(min(stop, buff_st_idx + buffer_len))\n            chk_st_idx = int(max(start, buff_st_idx - overlap_len))\n            chk_nd_idx = int(min(stop, buff_nd_idx + overlap_len))\n            rel_st_idx = int(buff_st_idx - chk_st_idx)\n            rel_nd_idx = int(buff_nd_idx - chk_st_idx)\n            this_y_chk = sig.sosfiltfilt(\n                sos, input_asarray[:, chk_st_idx:chk_nd_idx], axis=1\n            )\n            shared_array_out[:, buff_st_idx:buff_nd_idx] = this_y_chk[\n                :, rel_st_idx:rel_nd_idx\n            ]\n        except ValueError:\n            raise ValueError(\n                (\n                    \"Some epochs were too short to filter. Try dropping those first,\"\n                    \" filtering, and then inserting them back in\"\n                )\n            )\n\n    # Do the actual parallellized filtering\n    if (\n        sys.platform.startswith(\"linux\") or sys.platform.startswith(\"darwin\")\n    ) and parallel:\n        pool = Pool(processes=cpu_count())\n        if isinstance(timeseries, (np.ndarray, list)):\n            # ignore epochs (information not contained in list or array) so filter directly\n            start, stop = 0, input_asarray.shape[1]\n            pool.map(\n                filter_chunk,\n                zip(\n                    repeat(start),\n                    repeat(stop),\n                    repeat(buffer_len),\n                    repeat(overlap_len),\n                    range(start, stop, buffer_len),\n                ),\n                chunksize=1,\n            )\n        elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n            fei = np.insert(\n                np.cumsum(timeseries.lengths), 0, 0\n            )  # filter epoch indices, fei\n            for ii in range(len(fei) - 1):  # filter within epochs\n                start, stop = fei[ii], fei[ii + 1]\n                pool.map(\n                    filter_chunk,\n                    zip(\n                        repeat(start),\n                        repeat(stop),\n                        repeat(buffer_len),\n                        repeat(overlap_len),\n                        range(start, stop, buffer_len),\n                    ),\n                    chunksize=1,\n                )\n        pool.close()\n        pool.join()\n    # No easy parallelized filtering for other OSes\n    else:\n        if isinstance(timeseries, (np.ndarray, list)):\n            # ignore epochs (information not contained in list or array) so filter directly\n            start, stop = 0, input_asarray.shape[1]\n            iterator = zip(\n                repeat(start),\n                repeat(stop),\n                repeat(buffer_len),\n                repeat(overlap_len),\n                range(start, stop, buffer_len),\n            )\n            for item in iterator:\n                filter_chunk(item)\n        elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n            fei = np.insert(\n                np.cumsum(timeseries.lengths), 0, 0\n            )  # filter epoch indices, fei\n            for ii in range(len(fei) - 1):  # filter within epochs\n                start, stop = fei[ii], fei[ii + 1]\n                iterator = zip(\n                    repeat(start),\n                    repeat(stop),\n                    repeat(buffer_len),\n                    repeat(overlap_len),\n                    range(start, stop, buffer_len),\n                )\n                for item in iterator:\n                    filter_chunk(item)\n\n    if isinstance(timeseries, np.ndarray):\n        out[:] = np.reshape(shared_array_out, dims)\n    elif isinstance(timeseries, list):\n        out[:] = np.reshape(shared_array_out, dims).tolist()\n    elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n        out._data[:] = shared_array_out\n\n    return out\n</code></pre>"},{"location":"reference/hmmutils/","title":"HMM Utils API Reference","text":"<p>nelpy.hmmutils contains helper functions and wrappers for working with hmmlearn.</p>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM","title":"<code>PoissonHMM</code>","text":"<p>               Bases: <code>PoissonHMM</code></p> <p>Nelpy extension of PoissonHMM: Hidden Markov Model with independent Poisson emissions.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of states.</p> required <code>startprob_prior</code> <code>(array, shape(n_components))</code> <p>Initial state occupation prior distribution.</p> required <code>transmat_prior</code> <code>(array, shape(n_components, n_components))</code> <p>Matrix of prior transition probabilities between states.</p> required <code>algorithm</code> <code>string, one of the :data:`base.DECODER_ALGORITHMS`</code> <p>Decoder algorithm.</p> required <code>random_state</code> <p>A random number generator instance.</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>Maximum number of iterations to perform.</p> <code>None</code> <code>tol</code> <code>float</code> <p>Convergence threshold. EM will stop if the gain in log-likelihood is below this value.</p> required <code>verbose</code> <code>bool</code> <p>When <code>True</code> per-iteration convergence reports are printed to :data:<code>sys.stderr</code>. You can diagnose convergence via the :attr:<code>monitor_</code> attribute.</p> <code>False</code> <code>params</code> <code>string</code> <p>Controls which parameters are updated in the training process.  Can contain any combination of 's' for startprob, 't' for transmat, 'm' for means and 'c' for covars. Defaults to all parameters.</p> <code>None</code> <code>init_params</code> <code>string</code> <p>Controls which parameters are initialized prior to training.  Can contain any combination of 's' for startprob, 't' for transmat, 'm' for means and 'c' for covars. Defaults to all parameters.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_features</code> <code>int</code> <p>Dimensionality of the (independent) Poisson emissions.</p> <code>monitor_</code> <code>ConvergenceMonitor</code> <p>Monitor object used to check the convergence of EM.</p> <code>transmat_</code> <code>(array, shape(n_components, n_components))</code> <p>Matrix of transition probabilities between states.</p> <code>startprob_</code> <code>(array, shape(n_components))</code> <p>Initial state occupation distribution.</p> <code>means_</code> <code>(array, shape(n_components, n_features))</code> <p>Mean parameters for each state.</p> <code>extern_</code> <code>(array, shape(n_components, n_extern))</code> <p>Augmented mapping from state space to external variables.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.hmmutils import PoissonHMM\n&gt;&gt;&gt; PoissonHMM(n_components=2)...\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>class PoissonHMM(PHMM):\n    \"\"\"Nelpy extension of PoissonHMM: Hidden Markov Model with\n    independent Poisson emissions.\n\n    Parameters\n    ----------\n    n_components : int\n        Number of states.\n\n    startprob_prior : array, shape (n_components, )\n        Initial state occupation prior distribution.\n\n    transmat_prior : array, shape (n_components, n_components)\n        Matrix of prior transition probabilities between states.\n\n    algorithm : string, one of the :data:`base.DECODER_ALGORITHMS`\n        Decoder algorithm.\n\n    random_state: RandomState or an int seed (0 by default)\n        A random number generator instance.\n\n    n_iter : int, optional\n        Maximum number of iterations to perform.\n\n    tol : float, optional\n        Convergence threshold. EM will stop if the gain in log-likelihood\n        is below this value.\n\n    verbose : bool, optional\n        When ``True`` per-iteration convergence reports are printed\n        to :data:`sys.stderr`. You can diagnose convergence via the\n        :attr:`monitor_` attribute.\n\n    params : string, optional\n        Controls which parameters are updated in the training\n        process.  Can contain any combination of 's' for startprob,\n        't' for transmat, 'm' for means and 'c' for covars. Defaults\n        to all parameters.\n\n    init_params : string, optional\n        Controls which parameters are initialized prior to\n        training.  Can contain any combination of 's' for\n        startprob, 't' for transmat, 'm' for means and 'c' for covars.\n        Defaults to all parameters.\n\n    Attributes\n    ----------\n    n_features : int\n        Dimensionality of the (independent) Poisson emissions.\n\n    monitor_ : ConvergenceMonitor\n        Monitor object used to check the convergence of EM.\n\n    transmat_ : array, shape (n_components, n_components)\n        Matrix of transition probabilities between states.\n\n    startprob_ : array, shape (n_components, )\n        Initial state occupation distribution.\n\n    means_ : array, shape (n_components, n_features)\n        Mean parameters for each state.\n\n    extern_ : array, shape (n_components, n_extern)\n        Augmented mapping from state space to external variables.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.hmmutils import PoissonHMM\n    &gt;&gt;&gt; PoissonHMM(n_components=2)...\n\n    \"\"\"\n\n    __attributes__ = [\"_fs\", \"_ds\", \"_unit_ids\", \"_unit_labels\", \"_unit_tags\"]\n\n    def __init__(\n        self,\n        *,\n        n_components,\n        n_iter=None,\n        init_params=None,\n        params=None,\n        random_state=None,\n        verbose=False,\n    ):\n        # assign default parameter values\n        if n_iter is None:\n            n_iter = 50\n        if init_params is None:\n            init_params = \"stm\"\n        if params is None:\n            params = \"stm\"\n\n        # TODO: I don't understand why super().__init__ does not work?\n        PHMM.__init__(\n            self,\n            n_components=n_components,\n            n_iter=n_iter,\n            init_params=init_params,\n            params=params,\n            random_state=random_state,\n            verbose=verbose,\n        )\n\n        # initialize BinnedSpikeTrain attributes\n        for attrib in self.__attributes__:\n            exec(\"self.\" + attrib + \" = None\")\n\n        self._extern_ = None\n        self._ds = None\n        # self._extern_map = None\n\n        # create shortcuts to super() methods that are overridden in\n        # this class\n        self._fit = PHMM.fit\n        self._score = PHMM.score\n        self._score_samples = PHMM.score_samples\n        self._predict = PHMM.predict\n        self._predict_proba = PHMM.predict_proba\n        self._decode = PHMM.decode\n\n        self._sample = PHMM.sample\n\n    def __repr__(self):\n        try:\n            rep = super().__repr__()\n        except Exception:\n            warn(\n                \"couldn't access super().__repr__;\"\n                \" upgrade dependencies to resolve this issue.\"\n            )\n            rep = \"PoissonHMM\"\n        if self._extern_ is not None:\n            fit_ext = \"True\"\n        else:\n            fit_ext = \"False\"\n        try:\n            fit = \"False\"\n            if self.means_ is not None:\n                fit = \"True\"\n        except AttributeError:\n            fit = \"False\"\n        fitstr = \"; fit=\" + fit + \", fit_ext=\" + fit_ext\n        return \"nelpy.\" + rep + fitstr\n\n    @property\n    def extern_(self):\n        \"\"\"\n        Mapping from states to external variables (e.g., position).\n\n        Returns\n        -------\n        np.ndarray or None\n            Array of shape (n_components, n_extern) containing the mapping\n            from states to external variables. Returns None if no mapping\n            has been learned yet.\n\n        Examples\n        --------\n        &gt;&gt;&gt; hmm.fit_ext(bst, position_data)\n        &gt;&gt;&gt; extern_map = hmm.extern_\n        &gt;&gt;&gt; print(f\"State 0 maps to position bin {np.argmax(extern_map[0])}\")\n        \"\"\"\n        if self._extern_ is not None:\n            return self._extern_\n        else:\n            warn(\"no state &lt;--&gt; external mapping has been learnt yet!\")\n            return None\n\n    def _get_order_from_transmat(self, start_state=None):\n        \"\"\"Determine a state ordering based on the transition matrix.\n\n        This is a greedy approach, starting at the a priori most probable\n        state, and moving to the next most probable state according to\n        the transition matrix, and so on.\n\n        Parameters\n        ----------\n        start_state : int, optional\n            Initial state to begin from. Defaults to the most probable\n            a priori state.\n\n        Returns\n        -------\n        new_order : list\n            List of states in transmat order.\n        \"\"\"\n\n        # unless specified, start in the a priori most probable state\n        if start_state is None:\n            start_state = np.argmax(self.startprob_)\n\n        new_order = [start_state]\n        num_states = self.transmat_.shape[0]\n        rem_states = np.arange(0, start_state).tolist()\n        rem_states.extend(np.arange(start_state + 1, num_states).tolist())\n        cs = start_state  # current state\n\n        for ii in np.arange(0, num_states - 1):\n            # find largest transition to set of remaining states\n            nstilde = np.argmax(self.transmat_[cs, rem_states])\n            ns = rem_states[nstilde]\n            # remove selected state from list of remaining states\n            rem_states.remove(ns)\n            cs = ns\n            new_order.append(cs)\n\n        return new_order\n\n    @property\n    def unit_ids(self):\n        \"\"\"\n        List of unit IDs associated with the model.\n\n        Returns\n        -------\n        list\n            List of unit IDs.\n        \"\"\"\n        return self._unit_ids\n\n    @property\n    def unit_labels(self):\n        \"\"\"\n        List of unit labels associated with the model.\n\n        Returns\n        -------\n        list\n            List of unit labels.\n        \"\"\"\n        return self._unit_labels\n\n    @property\n    def means(self):\n        \"\"\"\n        Observation matrix (mean firing rates for each state and unit).\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (n_components, n_units) containing the mean parameters for each state.\n        \"\"\"\n        return self.means_\n\n    @property\n    def transmat(self):\n        \"\"\"\n        Transition probability matrix.\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (n_components, n_components) where A[i, j] = Pr(S_{t+1}=j | S_t=i).\n        \"\"\"\n        return self.transmat_\n\n    @property\n    def startprob(self):\n        \"\"\"\n        Prior distribution over states.\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (n_components,) representing the initial state probabilities.\n        \"\"\"\n        return self.startprob_\n\n    def get_state_order(self, method=None, start_state=None):\n        \"\"\"\n        Return a state ordering, optionally using augmented data.\n\n        Parameters\n        ----------\n        method : {'transmat', 'mode', 'mean'}, optional\n            Method to use for ordering states. 'transmat' (default) uses the transition matrix.\n            'mode' or 'mean' use the external mapping (requires self._extern_).\n        start_state : int, optional\n            Initial state to begin from (used only if method is 'transmat').\n\n        Returns\n        -------\n        neworder : list\n            List of state indices in the new order.\n\n        Notes\n        -----\n        Both 'mode' and 'mean' assume that _extern_ is in sorted order; this is not verified explicitly.\n\n        Examples\n        --------\n        &gt;&gt;&gt; order = hmm.get_state_order(method=\"transmat\")\n        &gt;&gt;&gt; order = hmm.get_state_order(method=\"mode\")\n        \"\"\"\n        if method is None:\n            method = \"transmat\"\n\n        neworder = []\n\n        if method == \"transmat\":\n            return self._get_order_from_transmat(start_state=start_state)\n        elif method == \"mode\":\n            if self._extern_ is not None:\n                neworder = self._extern_.argmax(axis=1).argsort()\n            else:\n                raise Exception(\n                    \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n                )\n        elif method == \"mean\":\n            if self._extern_ is not None:\n                (\n                    np.tile(np.arange(self._extern_.shape[1]), (self.n_components, 1))\n                    * self._extern_\n                ).sum(axis=1).argsort()\n                neworder = self._extern_.argmax(axis=1).argsort()\n            else:\n                raise Exception(\n                    \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n                )\n        else:\n            raise NotImplementedError(\n                \"ordering method '\" + str(method) + \"' not supported!\"\n            )\n        return neworder\n\n    def _reorder_units_by_ids(self, neworder):\n        \"\"\"\n        Reorder unit_ids to match that of a BinnedSpikeTrain.\n\n        WARNING! Modifies self.means_ in-place.\n\n        Parameters\n        ----------\n        neworder : list or array-like\n            List of unit IDs specifying the new order. Must be of size (n_units,).\n\n        Returns\n        -------\n        self : PoissonHMM\n            The reordered PoissonHMM instance.\n\n        Examples\n        --------\n        &gt;&gt;&gt; hmm._reorder_units_by_ids([3, 1, 2, 0])\n        \"\"\"\n        neworder = [self.unit_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_cols(self.means_, frm, to)\n            self._unit_ids[frm], self._unit_ids[to] = (\n                self._unit_ids[to],\n                self._unit_ids[frm],\n            )\n            self._unit_labels[frm], self._unit_labels[to] = (\n                self._unit_labels[to],\n                self._unit_labels[frm],\n            )\n            # TODO: re-build unit tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        return self\n\n    def reorder_states(self, neworder):\n        \"\"\"\n        Reorder internal HMM states according to a specified order.\n\n        Parameters\n        ----------\n        neworder : list or array-like\n            List of state indices specifying the new order. Must be of size (n_components,).\n\n        Examples\n        --------\n        &gt;&gt;&gt; hmm.reorder_states([2, 0, 1])\n        \"\"\"\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_cols(self.transmat_, frm, to)\n            swap_rows(self.transmat_, frm, to)\n            swap_rows(self.means_, frm, to)\n            if self._extern_ is not None:\n                swap_rows(self._extern_, frm, to)\n            self.startprob_[frm], self.startprob_[to] = (\n                self.startprob_[to],\n                self.startprob_[frm],\n            )\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    def assume_attributes(self, binnedSpikeTrainArray):\n        \"\"\"\n        Assume subset of attributes from a BinnedSpikeTrainArray.\n\n        This is used primarily to enable the sampling of sequences after a model has been fit.\n\n        Parameters\n        ----------\n        binnedSpikeTrainArray : BinnedSpikeTrainArray\n            The BinnedSpikeTrainArray instance from which to copy attributes.\n        \"\"\"\n        if self._ds is not None:\n            warn(\"PoissonHMM(BinnedSpikeTrain) attributes already exist.\")\n        for attrib in self.__attributes__:\n            exec(\"self.\" + attrib + \" = binnedSpikeTrainArray.\" + attrib)\n        self._unit_ids = copy.copy(binnedSpikeTrainArray.unit_ids)\n        self._unit_labels = copy.copy(binnedSpikeTrainArray.unit_labels)\n        self._unit_tags = copy.copy(binnedSpikeTrainArray.unit_tags)\n\n    def _has_same_unit_id_order(self, unit_ids):\n        \"\"\"\n        Check if the provided unit_ids are in the same order as the model's unit_ids.\n\n        Parameters\n        ----------\n        unit_ids : list or array-like\n            List of unit IDs to compare.\n\n        Returns\n        -------\n        bool\n            True if the unit_ids are in the same order, False otherwise.\n\n        Raises\n        ------\n        TypeError\n            If the number of unit_ids does not match.\n        \"\"\"\n        if self._unit_ids is None:\n            return True\n        if len(unit_ids) != len(self.unit_ids):\n            raise TypeError(\"Incorrect number of unit_ids encountered!\")\n        for ii, unit_id in enumerate(unit_ids):\n            if unit_id != self.unit_ids[ii]:\n                return False\n        return True\n\n    def _sliding_window_array(self, bst, w=1):\n        \"\"\"\n        Returns an unwrapped data array by sliding w bins one bin at a time.\n\n        If w==1, then bins are non-overlapping.\n\n        Parameters\n        ----------\n        bst : BinnedSpikeTrainArray\n            Input with data array of shape (n_units, n_bins).\n        w : int, optional\n            Window size (number of bins). Default is 1.\n\n        Returns\n        -------\n        unwrapped : np.ndarray\n            New data array of shape (n_sliding_bins, n_units).\n        lengths : np.ndarray\n            Array of shape (n_sliding_bins,) indicating the lengths of each window.\n\n        Raises\n        ------\n        NotImplementedError\n            If bst is not a BinnedSpikeTrainArray.\n        AssertionError\n            If w is not a positive integer.\n\n        Examples\n        --------\n        &gt;&gt;&gt; unwrapped, lengths = hmm._sliding_window_array(bst, w=3)\n        \"\"\"\n        if w is None:\n            w = 1\n        assert float(w).is_integer(), \"w must be a positive integer!\"\n        assert w &gt; 0, \"w must be a positive integer!\"\n\n        if not isinstance(bst, BinnedSpikeTrainArray):\n            raise NotImplementedError(\n                \"support for other datatypes not yet implemented!\"\n            )\n\n        # potentially re-organize internal observation matrix to be\n        # compatible with BinnedSpikeTrainArray\n        if not self._has_same_unit_id_order(bst.unit_ids):\n            self._reorder_units_by_ids(bst.unit_ids)\n\n        if w == 1:\n            return bst.data.T, bst.lengths\n\n        n_units, t_bins = bst.data.shape\n\n        # if we decode using multiple bins at a time (w&gt;1) then we have to decode each epoch separately:\n\n        # first, we determine the number of bins we will decode. This requires us to scan over the epochs\n        n_bins = 0\n        cumlengths = np.cumsum(bst.lengths)\n        lengths = np.zeros(bst.n_epochs, dtype=np.int)\n        prev_idx = 0\n        for ii, to_idx in enumerate(cumlengths):\n            datalen = to_idx - prev_idx\n            prev_idx = to_idx\n            lengths[ii] = np.max((1, datalen - w + 1))\n\n        n_bins = lengths.sum()\n\n        unwrapped = np.zeros((n_units, n_bins))\n\n        # next, we decode each epoch separately, one bin at a time\n        cum_lengths = np.insert(np.cumsum(lengths), 0, 0)\n\n        prev_idx = 0\n        for ii, to_idx in enumerate(cumlengths):\n            data = bst.data[:, prev_idx:to_idx]\n            prev_idx = to_idx\n            datacum = np.cumsum(\n                data, axis=1\n            )  # ii'th data segment, with column of zeros prepended\n            datacum = np.hstack((np.zeros((n_units, 1)), datacum))\n            re = w  # right edge ptr\n            # TODO: check if datalen &lt; w and act appropriately\n            if lengths[ii] &gt; 1:  # more than one full window fits into data length\n                for tt in range(lengths[ii]):\n                    obs = (\n                        datacum[:, re] - datacum[:, re - w]\n                    )  # spikes in window of size w\n                    re += 1\n                    post_idx = lengths[ii] + tt\n                    unwrapped[:, post_idx] = obs\n            else:  # only one window can fit in, and perhaps only partially. We just take all the data we can get,\n                # and ignore the scaling problem where the window size is now possibly less than bst.ds*w\n                post_idx = cum_lengths[ii]\n                obs = datacum[:, -1]  # spikes in window of size at most w\n                unwrapped[:, post_idx] = obs\n\n        return unwrapped.T, lengths\n\n    def decode(self, X, lengths=None, w=None, algorithm=None):\n        \"\"\"\n        Find the most likely state sequence corresponding to ``X``.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n            WARNING: Each decoding window is assumed to be similar in size to those used during training.\n            If not, the tuning curves have to be scaled appropriately!\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n        algorithm : str, optional\n            Decoder algorithm to be used (see DECODER_ALGORITHMS).\n\n        Returns\n        -------\n        logprob : float or list of float\n            Log probability of the produced state sequence.\n        state_sequence : np.ndarray or list of np.ndarray\n            Labels for each sample from ``X`` obtained via the given decoder algorithm.\n        centers : np.ndarray or list of np.ndarray\n            Time-centers of all bins contained in ``X``.\n\n        See Also\n        --------\n        score_samples : Compute the log probability under the model and posteriors.\n        score : Compute the log probability under the model.\n\n        Examples\n        --------\n        &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(bst)\n        &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(X, algorithm=\"viterbi\")\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            return self._decode(self, X=X, lengths=lengths, algorithm=algorithm), None\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            state_sequences = []\n            centers = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                logprob, state_sequence = self._decode(\n                    self, windowed_arr, lengths=lengths, algorithm=algorithm\n                )\n                logprobs.append(logprob)\n                state_sequences.append(state_sequence)\n                centers.append(seq.centers)\n            return logprobs, state_sequences, centers\n\n    def _decode_from_lambda_only(self, X, lengths=None):\n        \"\"\"\n        Decode using the observation (lambda) matrix only (i.e., pure memoryless decoding).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n            WARNING: Each decoding window is assumed to be similar in size to those used during training.\n            If not, the tuning curves have to be scaled appropriately!\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n\n        Returns\n        -------\n        posteriors : list of np.ndarray\n            State-membership probabilities for each sample in ``X``; one array for each sequence in X.\n        state_sequences : list of np.ndarray\n            Labels for each sample from ``X``; one array for each sequence in X.\n\n        Examples\n        --------\n        &gt;&gt;&gt; posteriors, state_sequences = hmm._decode_from_lambda_only(bst)\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            raise NotImplementedError(\"Not yet implemented!\")\n        else:\n            # we have a BinnedSpikeTrainArray\n            ratemap = copy.deepcopy(self.means_.T)\n            # make sure X and ratemap have same unit_id ordering!\n            neworder = [self.unit_ids.index(x) for x in X.unit_ids]\n            oldorder = list(range(len(neworder)))\n            for oi, ni in enumerate(neworder):\n                frm = oldorder.index(ni)\n                to = oi\n                swap_rows(ratemap, frm, to)\n                oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n            posteriors = []\n            state_sequences = []\n            for seq in X:\n                posteriors_, cumlengths, mode_pth, mean_pth = decode1D(\n                    bst=seq, ratemap=ratemap\n                )\n                # nanlocs = np.argwhere(np.isnan(mode_pth))\n                # state_sequences_ = mode_pth.astype(int)\n                state_sequences_ = mode_pth\n                posteriors.append(posteriors_)\n                state_sequences.append(state_sequences_)\n\n            return posteriors, state_sequences\n\n    def predict_proba(self, X, lengths=None, w=None, returnLengths=False):\n        \"\"\"\n        Compute the posterior probability for each state in the model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n        returnLengths : bool, optional\n            If True, also return the lengths array.\n\n        Returns\n        -------\n        posteriors : np.ndarray\n            Array of shape (n_components, n_samples) with state-membership probabilities for each sample from ``X``.\n        lengths : np.ndarray, optional\n            Returned if returnLengths is True; array of sequence lengths.\n\n        Examples\n        --------\n        &gt;&gt;&gt; posteriors = hmm.predict_proba(bst)\n        &gt;&gt;&gt; posteriors, lengths = hmm.predict_proba(bst, returnLengths=True)\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            print(\"we have a \" + str(type(X)))\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            if returnLengths:\n                return np.transpose(\n                    self._predict_proba(self, X, lengths=lengths)\n                ), lengths\n            return np.transpose(self._predict_proba(self, X, lengths=lengths))\n        else:\n            # we have a BinnedSpikeTrainArray\n            windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n            if returnLengths:\n                return np.transpose(\n                    self._predict_proba(self, windowed_arr, lengths=lengths)\n                ), lengths\n            return np.transpose(\n                self._predict_proba(self, windowed_arr, lengths=lengths)\n            )\n\n    def predict(self, X, lengths=None, w=None):\n        \"\"\"\n        Find the most likely state sequence corresponding to ``X``.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n\n        Returns\n        -------\n        state_sequence : np.ndarray or list of np.ndarray\n            Labels for each sample from ``X``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; state_seq = hmm.predict(bst)\n        &gt;&gt;&gt; state_seq = hmm.predict(X)\n        \"\"\"\n        _, state_sequences, centers = self.decode(X=X, lengths=lengths, w=w)\n        return state_sequences\n\n    def sample(self, n_samples=1, random_state=None):\n        \"\"\"\n        Generate random samples from the model.\n\n        Parameters\n        ----------\n        n_samples : int\n            Number of samples to generate.\n        random_state : RandomState or int, optional\n            A random number generator instance or seed. If None, the object's random_state is used.\n\n        Returns\n        -------\n        X : np.ndarray\n            Feature matrix of shape (n_samples, n_features).\n        state_sequence : np.ndarray\n            State sequence produced by the model.\n\n        Examples\n        --------\n        &gt;&gt;&gt; X, states = hmm.sample(n_samples=100)\n        \"\"\"\n        return self._sample(self, n_samples=n_samples, random_state=random_state)\n\n    def score_samples(self, X, lengths=None, w=None):\n        \"\"\"Compute the log probability under the model and compute posteriors.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Feature matrix of individual samples.\n            OR\n            nelpy.BinnedSpikeTrainArray\n        lengths : array-like of integers, shape (n_sequences, ), optional\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``. This is not used when X is\n            a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n            automatically inferred.\n\n        Returns\n        -------\n        logprob : float\n            Log likelihood of ``X``; one scalar for each sequence in X.\n\n        posteriors : array, shape (n_components, n_samples)\n            State-membership probabilities for each sample in ``X``;\n            one array for each sequence in X.\n\n        See Also\n        --------\n        score : Compute the log probability under the model.\n        decode : Find most likely state sequence corresponding to ``X``.\n        \"\"\"\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            logprobs, posteriors = self._score_samples(self, X, lengths=lengths)\n            return (\n                logprobs,\n                posteriors,\n            )  # .T why does this transpose affect hmm.predict_proba!!!????\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            posteriors = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                logprob, posterior = self._score_samples(\n                    self, X=windowed_arr, lengths=lengths\n                )\n                logprobs.append(logprob)\n                posteriors.append(posterior.T)\n            return logprobs, posteriors\n\n    def score(self, X, lengths=None, w=None):\n        \"\"\"Compute the log probability under the model.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Feature matrix of individual samples.\n            OR\n            nelpy.BinnedSpikeTrainArray\n        lengths : array-like of integers, shape (n_sequences, ), optional\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``. This is not used when X is\n            a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n            automatically inferred.\n\n        Returns\n        -------\n        logprob : float, or list of floats\n            Log likelihood of ``X``; one scalar for each sequence in X.\n\n        See Also\n        --------\n        score_samples : Compute the log probability under the model and\n            posteriors.\n        decode : Find most likely state sequence corresponding to ``X``.\n        \"\"\"\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            return self._score(self, X, lengths=lengths)\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                logprob = self._score(self, X=windowed_arr, lengths=lengths)\n                logprobs.append(logprob)\n        return logprobs\n\n    def _cum_score_per_bin(self, X, lengths=None, w=None):\n        \"\"\"Compute the log probability under the model, cumulatively for each bin per event.\"\"\"\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            return self._score(self, X, lengths=lengths)\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                n_bins, _ = windowed_arr.shape\n                for ii in range(1, n_bins + 1):\n                    logprob = self._score(self, X=windowed_arr[:ii, :])\n                    logprobs.append(logprob)\n        return logprobs\n\n    def fit(self, X, lengths=None, w=None):\n        \"\"\"Estimate model parameters using nelpy objects.\n\n        An initialization step is performed before entering the\n        EM-algorithm. If you want to avoid this step for a subset of\n        the parameters, pass proper ``init_params`` keyword argument\n        to estimator's constructor.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_units)\n            Feature matrix of individual samples.\n            OR\n            nelpy.BinnedSpikeTrainArray\n        lengths : array-like of integers, shape (n_sequences, )\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``. This is not used when X is\n            a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n            automatically inferred.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            self._fit(self, X, lengths=lengths)\n        else:\n            # we have a BinnedSpikeTrainArray\n            windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n            self._fit(self, windowed_arr, lengths=lengths)\n            # adopt unit_ids, unit_labels, etc. from BinnedSpikeTrain\n            self.assume_attributes(X)\n        return self\n\n    def fit_ext(\n        self,\n        X,\n        ext,\n        n_extern=None,\n        lengths=None,\n        save=True,\n        w=None,\n        normalize=True,\n        normalize_by_occupancy=True,\n    ):\n        \"\"\"Learn a mapping from the internal state space, to an external\n        augmented space (e.g. position).\n\n        Returns a row-normalized version of (n_states, n_ext), that\n        is, a distribution over external bins for each state.\n\n        X : BinnedSpikeTrainArray\n\n        ext : array-like\n            array of external correlates (n_bins, )\n        n_extern : int\n            number of extern variables, with range 0,.. n_extern-1\n        save : bool\n            stores extern in PoissonHMM if true, discards it if not\n        w:\n        normalize : bool\n            If True, then normalize each state to have a distribution over ext.\n        occupancy : array of bin counts\n            Default is all ones (uniform).\n\n        self.extern_ of size (n_components, n_extern)\n        \"\"\"\n\n        if n_extern is None:\n            n_extern = len(unique(ext))\n            ext_map = np.arange(n_extern)\n            for ii, ele in enumerate(unique(ext)):\n                ext_map[ele] = ii\n        else:\n            ext_map = np.arange(n_extern)\n\n        # idea: here, ext can be anything, and n_extern should be range\n        # we can e.g., define extern correlates {leftrun, rightrun} and\n        # fit the mapping. This is not expected to be good at all for\n        # most states, but it could allow us to identify a state or two\n        # for which there *might* be a strong predictive relationship.\n        # In this way, the binning, etc. should be done external to this\n        # function, but it might still make sense to encapsulate it as\n        # a helper function inside PoissonHMM?\n\n        # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n        # x0=0; xl=100; n_extern=50\n        # xx_left = np.linspace(x0,xl,n_extern+1)\n        # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n        # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n        extern = np.zeros((self.n_components, n_extern))\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n        else:\n            # we have a BinnedSpikeTrainArray\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n\n        posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n        if len(posteriors) != len(ext):\n            raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n        for ii, posterior in enumerate(posteriors):\n            if not np.isnan(ext[ii]):\n                extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n        if normalize_by_occupancy:\n            occupancy, _ = np.histogram(ext, bins=n_extern, range=[0, n_extern])\n            occupancy[occupancy == 0] = 1\n            occupancy = np.atleast_2d(occupancy)\n        else:\n            occupancy = 1\n\n        extern = extern / occupancy\n\n        if normalize:\n            # normalize extern tuning curves:\n            rowsum = np.tile(extern.sum(axis=1), (n_extern, 1)).T\n            rowsum = np.where(np.isclose(rowsum, 0), 1, rowsum)\n            extern = extern / rowsum\n\n        if save:\n            self._extern_ = extern\n            # self._extern_map = ext_map\n\n        return extern\n\n    def fit_ext2(self, X, ext, n_extern=None, lengths=None, w=None):\n        \"\"\"Learn a mapping from the internal state space, to an external\n        augmented space (e.g. position).\n\n        Returns a column-normalized version of (n_states, n_ext), that\n        is, a distribution over states for each extern bin.\n\n        X : BinnedSpikeTrainArray\n\n        ext : array-like\n            array of external correlates (n_bins, )\n        n_extern : int\n            number of extern variables, with range 0,.. n_extern-1\n\n        save : bool\n            stores extern in PoissonHMM if true, discards it if not\n\n        self.extern_ of size (n_components, n_extern)\n        \"\"\"\n\n        ext_map = np.arange(n_extern)\n        if n_extern is None:\n            n_extern = len(unique(ext))\n            for ii, ele in enumerate(unique(ext)):\n                ext_map[ele] = ii\n\n        # idea: here, ext can be anything, and n_extern should be range\n        # we can e.g., define extern correlates {leftrun, rightrun} and\n        # fit the mapping. This is not expexted to be good at all for\n        # most states, but it could allow us to identify a state or two\n        # for which there *might* be a strong predictive relationship.\n        # In this way, the binning, etc. should be done external to this\n        # function, but it might still make sense to encapsulate it as\n        # a helper function inside PoissonHMM?\n\n        # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n        # x0=0; xl=100; n_extern=50\n        # xx_left = np.linspace(x0,xl,n_extern+1)\n        # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n        # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n        extern = np.zeros((self.n_components, n_extern))\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n        else:\n            # we have a BinnedSpikeTrainArray\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n        posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n        if len(posteriors) != len(ext):\n            raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n        for ii, posterior in enumerate(posteriors):\n            if not np.isnan(ext[ii]):\n                extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n        # normalize extern tuning curves:\n        colsum = np.tile(extern.sum(axis=0), (self.n_components, 1))\n        colsum = np.where(np.isclose(colsum, 0), 1, colsum)\n        extern = extern / colsum\n\n        return extern\n\n    def decode_ext(self, X, lengths=None, w=None, ext_shape=None):\n        \"\"\"\n        Find memoryless most likely state sequence corresponding to ``X``,\n        (that is, the symbol-by-symbol MAP sequence) and then map those\n        states to an associated external representation (e.g., position).\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        lengths : array-like of integers, shape (n_sequences, ), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n        ext_shape : tuple, optional\n            Shape of the external variables.\n\n        Returns\n        -------\n        ext_posteriors : np.ndarray\n            Array of shape (n_extern, n_samples) with state-membership probabilities for each sample in ``X``.\n        bdries : np.ndarray\n            Array of bin boundaries.\n        mode_pth : np.ndarray\n            Most likely external variable sequence (mode path).\n        mean_pth : np.ndarray\n            Mean external variable sequence (mean path).\n\n        Examples\n        --------\n        For 1D external variables:\n        &gt;&gt;&gt; posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(\n        ...     bst_no_ripple, ext_shape=(vtc.n_bins,)\n        ... )\n        &gt;&gt;&gt; mean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0])\n\n        For 2D external variables:\n        &gt;&gt;&gt; posterior_, bdries_, mode_pth_, mean_pth_ = hmm.decode_ext(\n        ...     bst, ext_shape=(ext_nx, ext_ny)\n        ... )\n        &gt;&gt;&gt; mean_pth_[0, :] = vtc2d.xbins[0] + mean_pth_[0, :] * (\n        ...     vtc2d.xbins[-1] - vtc2d.xbins[0]\n        ... )\n        &gt;&gt;&gt; mean_pth_[1, :] = vtc2d.ybins[0] + mean_pth_[1, :] * (\n        ...     vtc2d.ybins[-1] - vtc2d.ybins[0]\n        ... )\n        \"\"\"\n\n        _, n_extern = self._extern_.shape\n\n        if ext_shape is None:\n            ext_shape = n_extern\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            raise NotImplementedError(\"not implemented yet.\")\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n        else:\n            # we have a BinnedSpikeTrainArray\n            pass\n        if len(ext_shape) == 1:\n            # do old style decoding\n            # TODO: this can be improved to be like the 2D case!\n            state_posteriors, lengths = self.predict_proba(\n                X=X, lengths=lengths, w=w, returnLengths=True\n            )\n            # fixy = np.mean(self._extern_ * np.arange(n_extern), axis=1)\n            # mean_pth = np.sum(state_posteriors.T*fixy, axis=1) # range 0 to 1\n            ext_posteriors = np.dot(\n                (self._extern_ * np.arange(n_extern)).T, state_posteriors\n            )\n            # normalize ext_posterior distributions:\n            ext_posteriors = ext_posteriors / ext_posteriors.sum(axis=0)\n            mean_pth = (\n                ext_posteriors.T * np.atleast_2d(np.linspace(0, 1, n_extern))\n            ).sum(axis=1)\n            mode_pth = (\n                np.argmax(ext_posteriors, axis=0) / n_extern\n            )  # range 0 to n_extern\n\n        elif len(ext_shape) == 2:\n            ext_posteriors = np.zeros((ext_shape[0], ext_shape[1], X.n_bins))\n            # get posterior distribution over states, of size (num_States, n_extern)\n            state_posteriors, lengths = self.predict_proba(\n                X=X, lengths=lengths, w=w, returnLengths=True\n            )\n            # for each bin, compute the distribution in the external domain\n            for bb in range(X.n_bins):\n                ext_posteriors[:, :, bb] = np.reshape(\n                    (self._extern_ * state_posteriors[:, [bb]]).sum(axis=0), ext_shape\n                )\n            # now compute mean and mode paths\n            expected_x = np.sum(\n                (\n                    ext_posteriors.sum(axis=1)\n                    * np.atleast_2d(np.linspace(0, 1, ext_shape[0])).T\n                ),\n                axis=0,\n            )\n            expected_y = np.sum(\n                (\n                    ext_posteriors.sum(axis=0)\n                    * np.atleast_2d(np.linspace(0, 1, ext_shape[1])).T\n                ),\n                axis=0,\n            )\n            mean_pth = np.vstack((expected_x, expected_y))\n\n            mode_pth = np.zeros((2, X.n_bins))\n            for tt in range(X.n_bins):\n                if np.any(np.isnan(ext_posteriors[:, :, tt])):\n                    mode_pth[0, tt] = np.nan\n                    mode_pth[0, tt] = np.nan\n                else:\n                    x_, y_ = np.unravel_index(\n                        np.argmax(ext_posteriors[:, :, tt]),\n                        (ext_shape[0], ext_shape[1]),\n                    )\n                    mode_pth[0, tt] = x_ / ext_shape[0]\n                    mode_pth[1, tt] = y_ / ext_shape[1]\n\n            ext_posteriors = np.transpose(ext_posteriors, axes=[1, 0, 2])\n        else:\n            raise TypeError(\"shape not currently supported!\")\n\n        bdries = np.cumsum(lengths)\n\n        return ext_posteriors, bdries, mode_pth, mean_pth\n\n    def _plot_external(\n        self,\n        *,\n        figsize=(3, 5),\n        sharey=True,\n        labelstates=None,\n        ec=None,\n        fillcolor=None,\n        lw=None,\n    ):\n        \"\"\"plot the externally associated state&lt;--&gt;extern mapping\n\n        WARNING! This function is not complete, and hence 'private',\n        and may be moved somewhere else later on.\n        \"\"\"\n\n        if labelstates is None:\n            labelstates = [1, self.n_components]\n        if ec is None:\n            ec = \"k\"\n        if fillcolor is None:\n            fillcolor = \"gray\"\n        if lw is None:\n            lw = 1.5\n\n        fig, axes = subplots(self.n_components, 1, figsize=figsize, sharey=sharey)\n\n        xvals = np.arange(len(self._extern_.T[:, 0]))\n\n        for state, ax in enumerate(axes):\n            ax.fill_between(xvals, 0, self._extern_.T[:, state], color=fillcolor)\n            ax.plot(xvals, self._extern_.T[:, state], color=ec, lw=lw)\n            if state + 1 in labelstates:\n                ax.set_ylabel(str(state + 1), rotation=0, y=-0.1)\n            ax.set_xticklabels([])\n            ax.set_yticklabels([])\n            ax.spines[\"right\"].set_visible(False)\n            ax.spines[\"top\"].set_visible(False)\n            ax.spines[\"bottom\"].set_visible(False)\n            ax.spines[\"left\"].set_visible(False)\n            plotting.utils.no_yticks(ax)\n            plotting.utils.no_xticks(ax)\n        # fig.suptitle('normalized place fields sorted by peak location (left) and mean location (right)', y=0.92, fontsize=14)\n        # ax.set_xticklabels(['0','20', '40', '60', '80', '100'])\n        ax.set_xlabel(\"external variable\")\n        fig.text(\n            0.02, 0.5, \"normalized state distribution\", va=\"center\", rotation=\"vertical\"\n        )\n\n        return fig, ax\n\n    def estimate_model_quality(self, bst, *, n_shuffles=1000, k_folds=5, verbose=False):\n        \"\"\"Estimate the HMM 'model quality' associated with the set of events in bst.\n\n        TODO: finish docstring, and do some more consistency checking...\n\n        Params\n        ======\n\n        Returns\n        =======\n\n        quality :\n        scores :\n        shuffled :\n\n        \"\"\"\n        n_states = self.n_components\n        quality, scores, shuffles = estimate_model_quality(\n            bst=bst,\n            n_states=n_states,\n            n_shuffles=n_shuffles,\n            k_folds=k_folds,\n            verbose=False,\n        )\n\n        return quality, scores, shuffles\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.extern_","title":"<code>extern_</code>  <code>property</code>","text":"<p>Mapping from states to external variables (e.g., position).</p> <p>Returns:</p> Type Description <code>ndarray or None</code> <p>Array of shape (n_components, n_extern) containing the mapping from states to external variables. Returns None if no mapping has been learned yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hmm.fit_ext(bst, position_data)\n&gt;&gt;&gt; extern_map = hmm.extern_\n&gt;&gt;&gt; print(f\"State 0 maps to position bin {np.argmax(extern_map[0])}\")\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.means","title":"<code>means</code>  <code>property</code>","text":"<p>Observation matrix (mean firing rates for each state and unit).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_components, n_units) containing the mean parameters for each state.</p>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.startprob","title":"<code>startprob</code>  <code>property</code>","text":"<p>Prior distribution over states.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_components,) representing the initial state probabilities.</p>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.transmat","title":"<code>transmat</code>  <code>property</code>","text":"<p>Transition probability matrix.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_components, n_components) where A[i, j] = Pr(S_{t+1}=j | S_t=i).</p>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.unit_ids","title":"<code>unit_ids</code>  <code>property</code>","text":"<p>List of unit IDs associated with the model.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unit IDs.</p>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.unit_labels","title":"<code>unit_labels</code>  <code>property</code>","text":"<p>List of unit labels associated with the model.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unit labels.</p>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.assume_attributes","title":"<code>assume_attributes(binnedSpikeTrainArray)</code>","text":"<p>Assume subset of attributes from a BinnedSpikeTrainArray.</p> <p>This is used primarily to enable the sampling of sequences after a model has been fit.</p> <p>Parameters:</p> Name Type Description Default <code>binnedSpikeTrainArray</code> <code>BinnedSpikeTrainArray</code> <p>The BinnedSpikeTrainArray instance from which to copy attributes.</p> required Source code in <code>nelpy/hmmutils.py</code> <pre><code>def assume_attributes(self, binnedSpikeTrainArray):\n    \"\"\"\n    Assume subset of attributes from a BinnedSpikeTrainArray.\n\n    This is used primarily to enable the sampling of sequences after a model has been fit.\n\n    Parameters\n    ----------\n    binnedSpikeTrainArray : BinnedSpikeTrainArray\n        The BinnedSpikeTrainArray instance from which to copy attributes.\n    \"\"\"\n    if self._ds is not None:\n        warn(\"PoissonHMM(BinnedSpikeTrain) attributes already exist.\")\n    for attrib in self.__attributes__:\n        exec(\"self.\" + attrib + \" = binnedSpikeTrainArray.\" + attrib)\n    self._unit_ids = copy.copy(binnedSpikeTrainArray.unit_ids)\n    self._unit_labels = copy.copy(binnedSpikeTrainArray.unit_labels)\n    self._unit_tags = copy.copy(binnedSpikeTrainArray.unit_tags)\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.decode","title":"<code>decode(X, lengths=None, w=None, algorithm=None)</code>","text":"<p>Find the most likely state sequence corresponding to <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray. WARNING: Each decoding window is assumed to be similar in size to those used during training. If not, the tuning curves have to be scaled appropriately!</p> required <code>lengths</code> <code>array-like of int, shape (n_sequences,)</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <code>algorithm</code> <code>str</code> <p>Decoder algorithm to be used (see DECODER_ALGORITHMS).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>float or list of float</code> <p>Log probability of the produced state sequence.</p> <code>state_sequence</code> <code>np.ndarray or list of np.ndarray</code> <p>Labels for each sample from <code>X</code> obtained via the given decoder algorithm.</p> <code>centers</code> <code>np.ndarray or list of np.ndarray</code> <p>Time-centers of all bins contained in <code>X</code>.</p> See Also <p>score_samples : Compute the log probability under the model and posteriors. score : Compute the log probability under the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(bst)\n&gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(X, algorithm=\"viterbi\")\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def decode(self, X, lengths=None, w=None, algorithm=None):\n    \"\"\"\n    Find the most likely state sequence corresponding to ``X``.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        WARNING: Each decoding window is assumed to be similar in size to those used during training.\n        If not, the tuning curves have to be scaled appropriately!\n    lengths : array-like of int, shape (n_sequences,), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n    algorithm : str, optional\n        Decoder algorithm to be used (see DECODER_ALGORITHMS).\n\n    Returns\n    -------\n    logprob : float or list of float\n        Log probability of the produced state sequence.\n    state_sequence : np.ndarray or list of np.ndarray\n        Labels for each sample from ``X`` obtained via the given decoder algorithm.\n    centers : np.ndarray or list of np.ndarray\n        Time-centers of all bins contained in ``X``.\n\n    See Also\n    --------\n    score_samples : Compute the log probability under the model and posteriors.\n    score : Compute the log probability under the model.\n\n    Examples\n    --------\n    &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(bst)\n    &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(X, algorithm=\"viterbi\")\n    \"\"\"\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        return self._decode(self, X=X, lengths=lengths, algorithm=algorithm), None\n    else:\n        # we have a BinnedSpikeTrainArray\n        logprobs = []\n        state_sequences = []\n        centers = []\n        for seq in X:\n            windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n            logprob, state_sequence = self._decode(\n                self, windowed_arr, lengths=lengths, algorithm=algorithm\n            )\n            logprobs.append(logprob)\n            state_sequences.append(state_sequence)\n            centers.append(seq.centers)\n        return logprobs, state_sequences, centers\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.decode_ext","title":"<code>decode_ext(X, lengths=None, w=None, ext_shape=None)</code>","text":"<p>Find memoryless most likely state sequence corresponding to <code>X</code>, (that is, the symbol-by-symbol MAP sequence) and then map those states to an associated external representation (e.g., position).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features) or BinnedSpikeTrainArray)</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <code>ext_shape</code> <code>tuple</code> <p>Shape of the external variables.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ext_posteriors</code> <code>ndarray</code> <p>Array of shape (n_extern, n_samples) with state-membership probabilities for each sample in <code>X</code>.</p> <code>bdries</code> <code>ndarray</code> <p>Array of bin boundaries.</p> <code>mode_pth</code> <code>ndarray</code> <p>Most likely external variable sequence (mode path).</p> <code>mean_pth</code> <code>ndarray</code> <p>Mean external variable sequence (mean path).</p> <p>Examples:</p> <p>For 1D external variables:</p> <pre><code>&gt;&gt;&gt; posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(\n...     bst_no_ripple, ext_shape=(vtc.n_bins,)\n... )\n&gt;&gt;&gt; mean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0])\n</code></pre> <p>For 2D external variables:</p> <pre><code>&gt;&gt;&gt; posterior_, bdries_, mode_pth_, mean_pth_ = hmm.decode_ext(\n...     bst, ext_shape=(ext_nx, ext_ny)\n... )\n&gt;&gt;&gt; mean_pth_[0, :] = vtc2d.xbins[0] + mean_pth_[0, :] * (\n...     vtc2d.xbins[-1] - vtc2d.xbins[0]\n... )\n&gt;&gt;&gt; mean_pth_[1, :] = vtc2d.ybins[0] + mean_pth_[1, :] * (\n...     vtc2d.ybins[-1] - vtc2d.ybins[0]\n... )\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def decode_ext(self, X, lengths=None, w=None, ext_shape=None):\n    \"\"\"\n    Find memoryless most likely state sequence corresponding to ``X``,\n    (that is, the symbol-by-symbol MAP sequence) and then map those\n    states to an associated external representation (e.g., position).\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n    lengths : array-like of integers, shape (n_sequences, ), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n    ext_shape : tuple, optional\n        Shape of the external variables.\n\n    Returns\n    -------\n    ext_posteriors : np.ndarray\n        Array of shape (n_extern, n_samples) with state-membership probabilities for each sample in ``X``.\n    bdries : np.ndarray\n        Array of bin boundaries.\n    mode_pth : np.ndarray\n        Most likely external variable sequence (mode path).\n    mean_pth : np.ndarray\n        Mean external variable sequence (mean path).\n\n    Examples\n    --------\n    For 1D external variables:\n    &gt;&gt;&gt; posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(\n    ...     bst_no_ripple, ext_shape=(vtc.n_bins,)\n    ... )\n    &gt;&gt;&gt; mean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0])\n\n    For 2D external variables:\n    &gt;&gt;&gt; posterior_, bdries_, mode_pth_, mean_pth_ = hmm.decode_ext(\n    ...     bst, ext_shape=(ext_nx, ext_ny)\n    ... )\n    &gt;&gt;&gt; mean_pth_[0, :] = vtc2d.xbins[0] + mean_pth_[0, :] * (\n    ...     vtc2d.xbins[-1] - vtc2d.xbins[0]\n    ... )\n    &gt;&gt;&gt; mean_pth_[1, :] = vtc2d.ybins[0] + mean_pth_[1, :] * (\n    ...     vtc2d.ybins[-1] - vtc2d.ybins[0]\n    ... )\n    \"\"\"\n\n    _, n_extern = self._extern_.shape\n\n    if ext_shape is None:\n        ext_shape = n_extern\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        raise NotImplementedError(\"not implemented yet.\")\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n    else:\n        # we have a BinnedSpikeTrainArray\n        pass\n    if len(ext_shape) == 1:\n        # do old style decoding\n        # TODO: this can be improved to be like the 2D case!\n        state_posteriors, lengths = self.predict_proba(\n            X=X, lengths=lengths, w=w, returnLengths=True\n        )\n        # fixy = np.mean(self._extern_ * np.arange(n_extern), axis=1)\n        # mean_pth = np.sum(state_posteriors.T*fixy, axis=1) # range 0 to 1\n        ext_posteriors = np.dot(\n            (self._extern_ * np.arange(n_extern)).T, state_posteriors\n        )\n        # normalize ext_posterior distributions:\n        ext_posteriors = ext_posteriors / ext_posteriors.sum(axis=0)\n        mean_pth = (\n            ext_posteriors.T * np.atleast_2d(np.linspace(0, 1, n_extern))\n        ).sum(axis=1)\n        mode_pth = (\n            np.argmax(ext_posteriors, axis=0) / n_extern\n        )  # range 0 to n_extern\n\n    elif len(ext_shape) == 2:\n        ext_posteriors = np.zeros((ext_shape[0], ext_shape[1], X.n_bins))\n        # get posterior distribution over states, of size (num_States, n_extern)\n        state_posteriors, lengths = self.predict_proba(\n            X=X, lengths=lengths, w=w, returnLengths=True\n        )\n        # for each bin, compute the distribution in the external domain\n        for bb in range(X.n_bins):\n            ext_posteriors[:, :, bb] = np.reshape(\n                (self._extern_ * state_posteriors[:, [bb]]).sum(axis=0), ext_shape\n            )\n        # now compute mean and mode paths\n        expected_x = np.sum(\n            (\n                ext_posteriors.sum(axis=1)\n                * np.atleast_2d(np.linspace(0, 1, ext_shape[0])).T\n            ),\n            axis=0,\n        )\n        expected_y = np.sum(\n            (\n                ext_posteriors.sum(axis=0)\n                * np.atleast_2d(np.linspace(0, 1, ext_shape[1])).T\n            ),\n            axis=0,\n        )\n        mean_pth = np.vstack((expected_x, expected_y))\n\n        mode_pth = np.zeros((2, X.n_bins))\n        for tt in range(X.n_bins):\n            if np.any(np.isnan(ext_posteriors[:, :, tt])):\n                mode_pth[0, tt] = np.nan\n                mode_pth[0, tt] = np.nan\n            else:\n                x_, y_ = np.unravel_index(\n                    np.argmax(ext_posteriors[:, :, tt]),\n                    (ext_shape[0], ext_shape[1]),\n                )\n                mode_pth[0, tt] = x_ / ext_shape[0]\n                mode_pth[1, tt] = y_ / ext_shape[1]\n\n        ext_posteriors = np.transpose(ext_posteriors, axes=[1, 0, 2])\n    else:\n        raise TypeError(\"shape not currently supported!\")\n\n    bdries = np.cumsum(lengths)\n\n    return ext_posteriors, bdries, mode_pth, mean_pth\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.estimate_model_quality","title":"<code>estimate_model_quality(bst, *, n_shuffles=1000, k_folds=5, verbose=False)</code>","text":"<p>Estimate the HMM 'model quality' associated with the set of events in bst.</p> <p>TODO: finish docstring, and do some more consistency checking...</p>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.estimate_model_quality--params","title":"Params","text":""},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.estimate_model_quality--returns","title":"Returns","text":"<p>quality : scores : shuffled :</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def estimate_model_quality(self, bst, *, n_shuffles=1000, k_folds=5, verbose=False):\n    \"\"\"Estimate the HMM 'model quality' associated with the set of events in bst.\n\n    TODO: finish docstring, and do some more consistency checking...\n\n    Params\n    ======\n\n    Returns\n    =======\n\n    quality :\n    scores :\n    shuffled :\n\n    \"\"\"\n    n_states = self.n_components\n    quality, scores, shuffles = estimate_model_quality(\n        bst=bst,\n        n_states=n_states,\n        n_shuffles=n_shuffles,\n        k_folds=k_folds,\n        verbose=False,\n    )\n\n    return quality, scores, shuffles\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.fit","title":"<code>fit(X, lengths=None, w=None)</code>","text":"<p>Estimate model parameters using nelpy objects.</p> <p>An initialization step is performed before entering the EM-algorithm. If you want to avoid this step for a subset of the parameters, pass proper <code>init_params</code> keyword argument to estimator's constructor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_units))</code> <p>Feature matrix of individual samples. OR nelpy.BinnedSpikeTrainArray</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. This is not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lenghts are automatically inferred.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns self.</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def fit(self, X, lengths=None, w=None):\n    \"\"\"Estimate model parameters using nelpy objects.\n\n    An initialization step is performed before entering the\n    EM-algorithm. If you want to avoid this step for a subset of\n    the parameters, pass proper ``init_params`` keyword argument\n    to estimator's constructor.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_units)\n        Feature matrix of individual samples.\n        OR\n        nelpy.BinnedSpikeTrainArray\n    lengths : array-like of integers, shape (n_sequences, )\n        Lengths of the individual sequences in ``X``. The sum of\n        these should be ``n_samples``. This is not used when X is\n        a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n        automatically inferred.\n\n    Returns\n    -------\n    self : object\n        Returns self.\n    \"\"\"\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        self._fit(self, X, lengths=lengths)\n    else:\n        # we have a BinnedSpikeTrainArray\n        windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n        self._fit(self, windowed_arr, lengths=lengths)\n        # adopt unit_ids, unit_labels, etc. from BinnedSpikeTrain\n        self.assume_attributes(X)\n    return self\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.fit_ext","title":"<code>fit_ext(X, ext, n_extern=None, lengths=None, save=True, w=None, normalize=True, normalize_by_occupancy=True)</code>","text":"<p>Learn a mapping from the internal state space, to an external augmented space (e.g. position).</p> <p>Returns a row-normalized version of (n_states, n_ext), that is, a distribution over external bins for each state.</p> <p>X : BinnedSpikeTrainArray</p> <p>ext : array-like     array of external correlates (n_bins, ) n_extern : int     number of extern variables, with range 0,.. n_extern-1 save : bool     stores extern in PoissonHMM if true, discards it if not w: normalize : bool     If True, then normalize each state to have a distribution over ext. occupancy : array of bin counts     Default is all ones (uniform).</p> <p>self.extern_ of size (n_components, n_extern)</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def fit_ext(\n    self,\n    X,\n    ext,\n    n_extern=None,\n    lengths=None,\n    save=True,\n    w=None,\n    normalize=True,\n    normalize_by_occupancy=True,\n):\n    \"\"\"Learn a mapping from the internal state space, to an external\n    augmented space (e.g. position).\n\n    Returns a row-normalized version of (n_states, n_ext), that\n    is, a distribution over external bins for each state.\n\n    X : BinnedSpikeTrainArray\n\n    ext : array-like\n        array of external correlates (n_bins, )\n    n_extern : int\n        number of extern variables, with range 0,.. n_extern-1\n    save : bool\n        stores extern in PoissonHMM if true, discards it if not\n    w:\n    normalize : bool\n        If True, then normalize each state to have a distribution over ext.\n    occupancy : array of bin counts\n        Default is all ones (uniform).\n\n    self.extern_ of size (n_components, n_extern)\n    \"\"\"\n\n    if n_extern is None:\n        n_extern = len(unique(ext))\n        ext_map = np.arange(n_extern)\n        for ii, ele in enumerate(unique(ext)):\n            ext_map[ele] = ii\n    else:\n        ext_map = np.arange(n_extern)\n\n    # idea: here, ext can be anything, and n_extern should be range\n    # we can e.g., define extern correlates {leftrun, rightrun} and\n    # fit the mapping. This is not expected to be good at all for\n    # most states, but it could allow us to identify a state or two\n    # for which there *might* be a strong predictive relationship.\n    # In this way, the binning, etc. should be done external to this\n    # function, but it might still make sense to encapsulate it as\n    # a helper function inside PoissonHMM?\n\n    # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n    # x0=0; xl=100; n_extern=50\n    # xx_left = np.linspace(x0,xl,n_extern+1)\n    # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n    # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n    extern = np.zeros((self.n_components, n_extern))\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n    else:\n        # we have a BinnedSpikeTrainArray\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n\n    posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n    if len(posteriors) != len(ext):\n        raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n    for ii, posterior in enumerate(posteriors):\n        if not np.isnan(ext[ii]):\n            extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n    if normalize_by_occupancy:\n        occupancy, _ = np.histogram(ext, bins=n_extern, range=[0, n_extern])\n        occupancy[occupancy == 0] = 1\n        occupancy = np.atleast_2d(occupancy)\n    else:\n        occupancy = 1\n\n    extern = extern / occupancy\n\n    if normalize:\n        # normalize extern tuning curves:\n        rowsum = np.tile(extern.sum(axis=1), (n_extern, 1)).T\n        rowsum = np.where(np.isclose(rowsum, 0), 1, rowsum)\n        extern = extern / rowsum\n\n    if save:\n        self._extern_ = extern\n        # self._extern_map = ext_map\n\n    return extern\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.fit_ext2","title":"<code>fit_ext2(X, ext, n_extern=None, lengths=None, w=None)</code>","text":"<p>Learn a mapping from the internal state space, to an external augmented space (e.g. position).</p> <p>Returns a column-normalized version of (n_states, n_ext), that is, a distribution over states for each extern bin.</p> <p>X : BinnedSpikeTrainArray</p> <p>ext : array-like     array of external correlates (n_bins, ) n_extern : int     number of extern variables, with range 0,.. n_extern-1</p> <p>save : bool     stores extern in PoissonHMM if true, discards it if not</p> <p>self.extern_ of size (n_components, n_extern)</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def fit_ext2(self, X, ext, n_extern=None, lengths=None, w=None):\n    \"\"\"Learn a mapping from the internal state space, to an external\n    augmented space (e.g. position).\n\n    Returns a column-normalized version of (n_states, n_ext), that\n    is, a distribution over states for each extern bin.\n\n    X : BinnedSpikeTrainArray\n\n    ext : array-like\n        array of external correlates (n_bins, )\n    n_extern : int\n        number of extern variables, with range 0,.. n_extern-1\n\n    save : bool\n        stores extern in PoissonHMM if true, discards it if not\n\n    self.extern_ of size (n_components, n_extern)\n    \"\"\"\n\n    ext_map = np.arange(n_extern)\n    if n_extern is None:\n        n_extern = len(unique(ext))\n        for ii, ele in enumerate(unique(ext)):\n            ext_map[ele] = ii\n\n    # idea: here, ext can be anything, and n_extern should be range\n    # we can e.g., define extern correlates {leftrun, rightrun} and\n    # fit the mapping. This is not expexted to be good at all for\n    # most states, but it could allow us to identify a state or two\n    # for which there *might* be a strong predictive relationship.\n    # In this way, the binning, etc. should be done external to this\n    # function, but it might still make sense to encapsulate it as\n    # a helper function inside PoissonHMM?\n\n    # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n    # x0=0; xl=100; n_extern=50\n    # xx_left = np.linspace(x0,xl,n_extern+1)\n    # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n    # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n    extern = np.zeros((self.n_components, n_extern))\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n    else:\n        # we have a BinnedSpikeTrainArray\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n    posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n    if len(posteriors) != len(ext):\n        raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n    for ii, posterior in enumerate(posteriors):\n        if not np.isnan(ext[ii]):\n            extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n    # normalize extern tuning curves:\n    colsum = np.tile(extern.sum(axis=0), (self.n_components, 1))\n    colsum = np.where(np.isclose(colsum, 0), 1, colsum)\n    extern = extern / colsum\n\n    return extern\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.get_state_order","title":"<code>get_state_order(method=None, start_state=None)</code>","text":"<p>Return a state ordering, optionally using augmented data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>(transmat, mode, mean)</code> <p>Method to use for ordering states. 'transmat' (default) uses the transition matrix. 'mode' or 'mean' use the external mapping (requires self.extern).</p> <code>'transmat'</code> <code>start_state</code> <code>int</code> <p>Initial state to begin from (used only if method is 'transmat').</p> <code>None</code> <p>Returns:</p> Name Type Description <code>neworder</code> <code>list</code> <p>List of state indices in the new order.</p> Notes <p>Both 'mode' and 'mean' assume that extern is in sorted order; this is not verified explicitly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; order = hmm.get_state_order(method=\"transmat\")\n&gt;&gt;&gt; order = hmm.get_state_order(method=\"mode\")\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def get_state_order(self, method=None, start_state=None):\n    \"\"\"\n    Return a state ordering, optionally using augmented data.\n\n    Parameters\n    ----------\n    method : {'transmat', 'mode', 'mean'}, optional\n        Method to use for ordering states. 'transmat' (default) uses the transition matrix.\n        'mode' or 'mean' use the external mapping (requires self._extern_).\n    start_state : int, optional\n        Initial state to begin from (used only if method is 'transmat').\n\n    Returns\n    -------\n    neworder : list\n        List of state indices in the new order.\n\n    Notes\n    -----\n    Both 'mode' and 'mean' assume that _extern_ is in sorted order; this is not verified explicitly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; order = hmm.get_state_order(method=\"transmat\")\n    &gt;&gt;&gt; order = hmm.get_state_order(method=\"mode\")\n    \"\"\"\n    if method is None:\n        method = \"transmat\"\n\n    neworder = []\n\n    if method == \"transmat\":\n        return self._get_order_from_transmat(start_state=start_state)\n    elif method == \"mode\":\n        if self._extern_ is not None:\n            neworder = self._extern_.argmax(axis=1).argsort()\n        else:\n            raise Exception(\n                \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n            )\n    elif method == \"mean\":\n        if self._extern_ is not None:\n            (\n                np.tile(np.arange(self._extern_.shape[1]), (self.n_components, 1))\n                * self._extern_\n            ).sum(axis=1).argsort()\n            neworder = self._extern_.argmax(axis=1).argsort()\n        else:\n            raise Exception(\n                \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n            )\n    else:\n        raise NotImplementedError(\n            \"ordering method '\" + str(method) + \"' not supported!\"\n        )\n    return neworder\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.predict","title":"<code>predict(X, lengths=None, w=None)</code>","text":"<p>Find the most likely state sequence corresponding to <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.</p> required <code>lengths</code> <code>array-like of int, shape (n_sequences,)</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>state_sequence</code> <code>np.ndarray or list of np.ndarray</code> <p>Labels for each sample from <code>X</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; state_seq = hmm.predict(bst)\n&gt;&gt;&gt; state_seq = hmm.predict(X)\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def predict(self, X, lengths=None, w=None):\n    \"\"\"\n    Find the most likely state sequence corresponding to ``X``.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n    lengths : array-like of int, shape (n_sequences,), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n\n    Returns\n    -------\n    state_sequence : np.ndarray or list of np.ndarray\n        Labels for each sample from ``X``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; state_seq = hmm.predict(bst)\n    &gt;&gt;&gt; state_seq = hmm.predict(X)\n    \"\"\"\n    _, state_sequences, centers = self.decode(X=X, lengths=lengths, w=w)\n    return state_sequences\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.predict_proba","title":"<code>predict_proba(X, lengths=None, w=None, returnLengths=False)</code>","text":"<p>Compute the posterior probability for each state in the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.</p> required <code>lengths</code> <code>array-like of int, shape (n_sequences,)</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <code>returnLengths</code> <code>bool</code> <p>If True, also return the lengths array.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>posteriors</code> <code>ndarray</code> <p>Array of shape (n_components, n_samples) with state-membership probabilities for each sample from <code>X</code>.</p> <code>lengths</code> <code>(ndarray, optional)</code> <p>Returned if returnLengths is True; array of sequence lengths.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; posteriors = hmm.predict_proba(bst)\n&gt;&gt;&gt; posteriors, lengths = hmm.predict_proba(bst, returnLengths=True)\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def predict_proba(self, X, lengths=None, w=None, returnLengths=False):\n    \"\"\"\n    Compute the posterior probability for each state in the model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n    lengths : array-like of int, shape (n_sequences,), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n    returnLengths : bool, optional\n        If True, also return the lengths array.\n\n    Returns\n    -------\n    posteriors : np.ndarray\n        Array of shape (n_components, n_samples) with state-membership probabilities for each sample from ``X``.\n    lengths : np.ndarray, optional\n        Returned if returnLengths is True; array of sequence lengths.\n\n    Examples\n    --------\n    &gt;&gt;&gt; posteriors = hmm.predict_proba(bst)\n    &gt;&gt;&gt; posteriors, lengths = hmm.predict_proba(bst, returnLengths=True)\n    \"\"\"\n    if not isinstance(X, BinnedSpikeTrainArray):\n        print(\"we have a \" + str(type(X)))\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        if returnLengths:\n            return np.transpose(\n                self._predict_proba(self, X, lengths=lengths)\n            ), lengths\n        return np.transpose(self._predict_proba(self, X, lengths=lengths))\n    else:\n        # we have a BinnedSpikeTrainArray\n        windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n        if returnLengths:\n            return np.transpose(\n                self._predict_proba(self, windowed_arr, lengths=lengths)\n            ), lengths\n        return np.transpose(\n            self._predict_proba(self, windowed_arr, lengths=lengths)\n        )\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.reorder_states","title":"<code>reorder_states(neworder)</code>","text":"<p>Reorder internal HMM states according to a specified order.</p> <p>Parameters:</p> Name Type Description Default <code>neworder</code> <code>list or array - like</code> <p>List of state indices specifying the new order. Must be of size (n_components,).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; hmm.reorder_states([2, 0, 1])\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def reorder_states(self, neworder):\n    \"\"\"\n    Reorder internal HMM states according to a specified order.\n\n    Parameters\n    ----------\n    neworder : list or array-like\n        List of state indices specifying the new order. Must be of size (n_components,).\n\n    Examples\n    --------\n    &gt;&gt;&gt; hmm.reorder_states([2, 0, 1])\n    \"\"\"\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        swap_cols(self.transmat_, frm, to)\n        swap_rows(self.transmat_, frm, to)\n        swap_rows(self.means_, frm, to)\n        if self._extern_ is not None:\n            swap_rows(self._extern_, frm, to)\n        self.startprob_[frm], self.startprob_[to] = (\n            self.startprob_[to],\n            self.startprob_[frm],\n        )\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.sample","title":"<code>sample(n_samples=1, random_state=None)</code>","text":"<p>Generate random samples from the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>1</code> <code>random_state</code> <code>RandomState or int</code> <p>A random number generator instance or seed. If None, the object's random_state is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> <code>state_sequence</code> <code>ndarray</code> <p>State sequence produced by the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X, states = hmm.sample(n_samples=100)\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def sample(self, n_samples=1, random_state=None):\n    \"\"\"\n    Generate random samples from the model.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples to generate.\n    random_state : RandomState or int, optional\n        A random number generator instance or seed. If None, the object's random_state is used.\n\n    Returns\n    -------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    state_sequence : np.ndarray\n        State sequence produced by the model.\n\n    Examples\n    --------\n    &gt;&gt;&gt; X, states = hmm.sample(n_samples=100)\n    \"\"\"\n    return self._sample(self, n_samples=n_samples, random_state=random_state)\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.score","title":"<code>score(X, lengths=None, w=None)</code>","text":"<p>Compute the log probability under the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Feature matrix of individual samples. OR nelpy.BinnedSpikeTrainArray</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. This is not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lenghts are automatically inferred.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>float, or list of floats</code> <p>Log likelihood of <code>X</code>; one scalar for each sequence in X.</p> See Also <p>score_samples : Compute the log probability under the model and     posteriors. decode : Find most likely state sequence corresponding to <code>X</code>.</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def score(self, X, lengths=None, w=None):\n    \"\"\"Compute the log probability under the model.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix of individual samples.\n        OR\n        nelpy.BinnedSpikeTrainArray\n    lengths : array-like of integers, shape (n_sequences, ), optional\n        Lengths of the individual sequences in ``X``. The sum of\n        these should be ``n_samples``. This is not used when X is\n        a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n        automatically inferred.\n\n    Returns\n    -------\n    logprob : float, or list of floats\n        Log likelihood of ``X``; one scalar for each sequence in X.\n\n    See Also\n    --------\n    score_samples : Compute the log probability under the model and\n        posteriors.\n    decode : Find most likely state sequence corresponding to ``X``.\n    \"\"\"\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        return self._score(self, X, lengths=lengths)\n    else:\n        # we have a BinnedSpikeTrainArray\n        logprobs = []\n        for seq in X:\n            windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n            logprob = self._score(self, X=windowed_arr, lengths=lengths)\n            logprobs.append(logprob)\n    return logprobs\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.PoissonHMM.score_samples","title":"<code>score_samples(X, lengths=None, w=None)</code>","text":"<p>Compute the log probability under the model and compute posteriors.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Feature matrix of individual samples. OR nelpy.BinnedSpikeTrainArray</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. This is not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lenghts are automatically inferred.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>float</code> <p>Log likelihood of <code>X</code>; one scalar for each sequence in X.</p> <code>posteriors</code> <code>(array, shape(n_components, n_samples))</code> <p>State-membership probabilities for each sample in <code>X</code>; one array for each sequence in X.</p> See Also <p>score : Compute the log probability under the model. decode : Find most likely state sequence corresponding to <code>X</code>.</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def score_samples(self, X, lengths=None, w=None):\n    \"\"\"Compute the log probability under the model and compute posteriors.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix of individual samples.\n        OR\n        nelpy.BinnedSpikeTrainArray\n    lengths : array-like of integers, shape (n_sequences, ), optional\n        Lengths of the individual sequences in ``X``. The sum of\n        these should be ``n_samples``. This is not used when X is\n        a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n        automatically inferred.\n\n    Returns\n    -------\n    logprob : float\n        Log likelihood of ``X``; one scalar for each sequence in X.\n\n    posteriors : array, shape (n_components, n_samples)\n        State-membership probabilities for each sample in ``X``;\n        one array for each sequence in X.\n\n    See Also\n    --------\n    score : Compute the log probability under the model.\n    decode : Find most likely state sequence corresponding to ``X``.\n    \"\"\"\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        logprobs, posteriors = self._score_samples(self, X, lengths=lengths)\n        return (\n            logprobs,\n            posteriors,\n        )  # .T why does this transpose affect hmm.predict_proba!!!????\n    else:\n        # we have a BinnedSpikeTrainArray\n        logprobs = []\n        posteriors = []\n        for seq in X:\n            windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n            logprob, posterior = self._score_samples(\n                self, X=windowed_arr, lengths=lengths\n            )\n            logprobs.append(logprob)\n            posteriors.append(posterior.T)\n        return logprobs, posteriors\n</code></pre>"},{"location":"reference/hmmutils/#nelpy.hmmutils.estimate_model_quality","title":"<code>estimate_model_quality(bst, *, hmm=None, n_states=None, n_shuffles=1000, k_folds=5, mode='timeswap-pooled', verbose=False)</code>","text":"<p>Estimate the HMM 'model quality' associated with the set of events in bst.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>The binned spike train array containing the events to evaluate.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>An existing HMM model to use. If None, a new model is fit for each fold.</p> <code>None</code> <code>n_states</code> <code>int</code> <p>Number of hidden states in the HMM. If None and hmm is provided, uses hmm.n_components.</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles to perform for the null distribution. Default is 1000.</p> <code>1000</code> <code>k_folds</code> <code>int</code> <p>Number of cross-validation folds. Default is 5.</p> <code>5</code> <code>mode</code> <code>(timeswap - pooled, timeswap - within - event, temporal - within - event)</code> <p>Shuffling mode to use for generating the null distribution. Default is 'timeswap-pooled'.</p> <code>'timeswap-pooled'</code> <code>verbose</code> <code>bool</code> <p>If True, print progress information. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>quality</code> <code>float</code> <p>Z-score of the model quality compared to the shuffled null distribution.</p> <code>scores</code> <code>ndarray</code> <p>Array of log-likelihood scores for each fold.</p> <code>shuffled</code> <code>ndarray</code> <p>Array of log-likelihood scores for each shuffle and fold.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.hmmutils import estimate_model_quality\n&gt;&gt;&gt; quality, scores, shuffled = estimate_model_quality(\n...     bst, n_states=3, n_shuffles=100, k_folds=5\n... )\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def estimate_model_quality(\n    bst,\n    *,\n    hmm=None,\n    n_states=None,\n    n_shuffles=1000,\n    k_folds=5,\n    mode=\"timeswap-pooled\",\n    verbose=False,\n):\n    \"\"\"\n    Estimate the HMM 'model quality' associated with the set of events in bst.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        The binned spike train array containing the events to evaluate.\n    hmm : PoissonHMM, optional\n        An existing HMM model to use. If None, a new model is fit for each fold.\n    n_states : int, optional\n        Number of hidden states in the HMM. If None and hmm is provided, uses hmm.n_components.\n    n_shuffles : int, optional\n        Number of shuffles to perform for the null distribution. Default is 1000.\n    k_folds : int, optional\n        Number of cross-validation folds. Default is 5.\n    mode : {'timeswap-pooled', 'timeswap-within-event', 'temporal-within-event'}, optional\n        Shuffling mode to use for generating the null distribution. Default is 'timeswap-pooled'.\n    verbose : bool, optional\n        If True, print progress information. Default is False.\n\n    Returns\n    -------\n    quality : float\n        Z-score of the model quality compared to the shuffled null distribution.\n    scores : np.ndarray\n        Array of log-likelihood scores for each fold.\n    shuffled : np.ndarray\n        Array of log-likelihood scores for each shuffle and fold.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.hmmutils import estimate_model_quality\n    &gt;&gt;&gt; quality, scores, shuffled = estimate_model_quality(\n    ...     bst, n_states=3, n_shuffles=100, k_folds=5\n    ... )\n    \"\"\"\n    from scipy.stats import zmap\n\n    from .decoding import k_fold_cross_validation\n\n    if hmm:\n        if not n_states:\n            n_states = hmm.n_components\n\n    X = [ii for ii in range(bst.n_epochs)]\n\n    scores = np.zeros(bst.n_epochs)\n    shuffled = np.zeros((bst.n_epochs, n_shuffles))\n\n    if mode == \"timeswap-pooled\":\n        # shuffle data coherently, pooled over all events:\n        shuffle_func = replay.pooled_time_swap_bst\n    elif mode == \"timeswap-within-event\":\n        # shuffle data coherently within events:\n        shuffle_func = replay.time_swap_bst\n    elif mode == \"temporal-within-event\":\n        shuffle_func = replay.incoherent_shuffle_bst\n    else:\n        raise NotImplementedError\n\n    for kk, (training, validation) in enumerate(k_fold_cross_validation(X, k=k_folds)):\n        if verbose:\n            print(\"  fold {}/{}\".format(kk + 1, k_folds))\n\n        PBEs_train = bst[training]\n        PBEs_test = bst[validation]\n\n        # train HMM on all training PBEs\n        hmm = PoissonHMM(n_components=n_states, verbose=False)\n        hmm.fit(PBEs_train)\n\n        # compute scores_hmm (log likelihoods) of validation set:\n        scores[validation] = hmm.score(PBEs_test)\n\n        for nn in range(n_shuffles):\n            # shuffle data:\n            bst_test_shuffled = shuffle_func(PBEs_test)\n\n            # score validation set with shuffled-data HMM\n            shuffled[validation, nn] = hmm.score(bst_test_shuffled)\n\n    quality = zmap(scores.mean(), shuffled.mean(axis=0))\n\n    return quality, scores, shuffled\n</code></pre>"},{"location":"reference/plotting/","title":"Plotting API Reference","text":""},{"location":"reference/plotting/#nelpy.plotting--nelpyplotting","title":"nelpy.plotting","text":"<p>The nelpy.plotting sub-package provides a variety of plotting functions and tools for visualizing data in nelpy, including raster plots, tuning curves, color utilities, colormaps, and more. It includes convenience wrappers for matplotlib and plotting functions that work directly with nelpy objects.</p> Main Features <ul> <li>Plotting functions for nelpy objects (e.g., rasterplot, epochplot, imagesc)</li> <li>Color palettes and colormaps for scientific visualization</li> <li>Utilities for figure management and aesthetics</li> <li>Context and style management for publication-quality figures</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import nelpy.plotting as npl\n&gt;&gt;&gt; npl.rasterplot(...)\n&gt;&gt;&gt; npl.plot_tuning_curves1D(...)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.FigureManager","title":"<code>FigureManager</code>","text":"<p>               Bases: <code>object</code></p> <p>Figure context manager for creating, displaying, and saving figures.</p> <p>See http://stackoverflow.com/questions/12594148/skipping-execution-of-with-block but I was unable to get a solution so far...</p> <p>See http://stackoverflow.com/questions/11195140/break-or-exit-out-of-with-statement for additional inspiration for making nested context managers...</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename without an extension. If an extension is present, AND if formats is empty, then the filename extension will be used.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, figure will be saved to disk.</p> <code>False</code> <code>show</code> <code>bool</code> <p>If True, figure will be shown.</p> <code>False</code> <code>nrows</code> <code>int</code> <p>Number of subplot rows.</p> <code>1</code> <code>ncols</code> <code>int</code> <p>Number of subplot columns.</p> <code>1</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches (width, height).</p> <code>(8, 3)</code> <code>tight_layout</code> <code>bool</code> <p>If True, use tight layout.</p> <code>False</code> <code>formats</code> <code>list</code> <p>List of formats to export. Defaults to ['pdf', 'png']</p> <code>None</code> <code>dpi</code> <code>float</code> <p>Resolution of the figure in dots per inch (DPI).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print additional output to screen.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>If True, file will be overwritten.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to plt.figure().</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with FigureManager(filename=\"myfig\", save=True, show=False) as (fig, ax):\n...     ax.plot([1, 2, 3], [4, 5, 6])\n</code></pre> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>class FigureManager(object):\n    \"\"\"\n    Figure context manager for creating, displaying, and saving figures.\n\n    See http://stackoverflow.com/questions/12594148/skipping-execution-of-with-block\n    but I was unable to get a solution so far...\n\n    See http://stackoverflow.com/questions/11195140/break-or-exit-out-of-with-statement\n    for additional inspiration for making nested context managers...\n\n    Parameters\n    ----------\n    filename : str, optional\n        Filename without an extension. If an extension is present,\n        AND if formats is empty, then the filename extension will be used.\n    save : bool, optional\n        If True, figure will be saved to disk.\n    show : bool, optional\n        If True, figure will be shown.\n    nrows : int, optional\n        Number of subplot rows.\n    ncols : int, optional\n        Number of subplot columns.\n    figsize : tuple, optional\n        Figure size in inches (width, height).\n    tight_layout : bool, optional\n        If True, use tight layout.\n    formats : list, optional\n        List of formats to export. Defaults to ['pdf', 'png']\n    dpi : float, optional\n        Resolution of the figure in dots per inch (DPI).\n    verbose : bool, optional\n        If True, print additional output to screen.\n    overwrite : bool, optional\n        If True, file will be overwritten.\n    **kwargs : dict\n        Additional keyword arguments passed to plt.figure().\n\n    Examples\n    --------\n    &gt;&gt;&gt; with FigureManager(filename=\"myfig\", save=True, show=False) as (fig, ax):\n    ...     ax.plot([1, 2, 3], [4, 5, 6])\n    \"\"\"\n\n    class Break(Exception):\n        \"\"\"Exception to break out of the context manager block.\"\"\"\n\n        pass\n\n    def __init__(\n        self,\n        *,\n        filename=None,\n        save=False,\n        show=False,\n        nrows=1,\n        ncols=1,\n        figsize=(8, 3),\n        tight_layout=False,\n        formats=None,\n        dpi=None,\n        verbose=True,\n        overwrite=False,\n        **kwargs,\n    ):\n        self.nrows = nrows\n        self.ncols = ncols\n        self.figsize = figsize\n        self.tight_layout = tight_layout\n        self.dpi = dpi\n        self.kwargs = kwargs\n\n        self.filename = filename\n        self.show = show\n        self.save = save\n        self.formats = formats\n        self.dpi = dpi\n        self.verbose = verbose\n        self.overwrite = overwrite\n\n        if self.show or self.save:\n            self.skip = False\n        else:\n            self.skip = True\n\n    def __enter__(self):\n        \"\"\"\n        Enter the context manager, creating the figure and axes.\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            The created figure.\n        ax : matplotlib.axes.Axes or numpy.ndarray\n            The created axes (single or array, depending on nrows/ncols).\n        \"\"\"\n        if not self.skip:\n            self.fig = plt.figure(figsize=self.figsize, dpi=self.dpi, **self.kwargs)\n            self.fig.npl_gs = gridspec.GridSpec(nrows=self.nrows, ncols=self.ncols)\n\n            self.ax = np.array([self.fig.add_subplot(ss) for ss in self.fig.npl_gs])\n            # self.fig, self.ax = plt.subplots(nrows=self.nrows,\n            #                                  ncols=self.ncols,\n            #                                  figsize=self.figsize,\n            #                                  tight_layout=self.tight_layout,\n            #                                  dpi=self.dpi,\n            #                                  **self.kwargs)\n            if len(self.ax) == 1:\n                self.ax = self.ax[0]\n\n            if self.tight_layout:\n                self.fig.npl_gs.tight_layout(self.fig)\n\n            # gs1.tight_layout(fig, rect=[0, 0.03, 1, 0.95])\n            if self.fig != plt.gcf():\n                self.clear()\n                raise RuntimeError(\"Figure does not match active mpl figure\")\n            return self.fig, self.ax\n        return -1, -1\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"\n        Exit the context manager, saving and/or showing the figure if requested.\n\n        Parameters\n        ----------\n        exc_type : type\n            Exception type, if any.\n        exc_value : Exception\n            Exception value, if any.\n        traceback : traceback\n            Traceback object, if any.\n        \"\"\"\n        if self.skip:\n            return True\n        if not exc_type:\n            if self.save:\n                assert self.filename is not None, \"filename has to be specified!\"\n                savefig(\n                    name=self.filename,\n                    fig=self.fig,\n                    formats=self.formats,\n                    dpi=self.dpi,\n                    verbose=self.verbose,\n                    overwrite=self.overwrite,\n                )\n\n            if self.show:\n                plt.show(self.fig)\n            self.clear()\n        else:\n            self.clear()\n            return False\n\n    def clear(self):\n        \"\"\"\n        Close the figure and clean up references.\n        \"\"\"\n        plt.close(self.fig)\n        del self.ax\n        del self.fig\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.FigureManager.Break","title":"<code>Break</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception to break out of the context manager block.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>class Break(Exception):\n    \"\"\"Exception to break out of the context manager block.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.FigureManager.clear","title":"<code>clear()</code>","text":"<p>Close the figure and clean up references.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Close the figure and clean up references.\n    \"\"\"\n    plt.close(self.fig)\n    del self.ax\n    del self.fig\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.add_scalebar","title":"<code>add_scalebar(ax, *, matchx=False, matchy=False, sizex=None, sizey=None, labelx=None, labely=None, hidex=True, hidey=True, ec='k', **kwargs)</code>","text":"<p>Add scalebars to axes, matching the size to the ticks of the plot and optionally hiding the x and y axes.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axis to attach scalebars to.</p> required <code>matchx</code> <code>bool</code> <p>If True, set size of x scalebar to spacing between ticks. Default is False.</p> <code>False</code> <code>matchy</code> <code>bool</code> <p>If True, set size of y scalebar to spacing between ticks. Default is False.</p> <code>False</code> <code>sizex</code> <code>float</code> <p>Size of x scalebar. Used if matchx is False.</p> <code>None</code> <code>sizey</code> <code>float</code> <p>Size of y scalebar. Used if matchy is False.</p> <code>None</code> <code>labelx</code> <code>str</code> <p>Label for x scalebar.</p> <code>None</code> <code>labely</code> <code>str</code> <p>Label for y scalebar.</p> <code>None</code> <code>hidex</code> <code>bool</code> <p>If True, hide x-axis of parent. Default is True.</p> <code>True</code> <code>hidey</code> <code>bool</code> <p>If True, hide y-axis of parent. Default is True.</p> <code>True</code> <code>ec</code> <code>color</code> <p>Edge color of the scalebar. Default is 'k'.</p> <code>'k'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to AnchoredScaleBar.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis containing the scalebar object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; add_scalebar(ax, sizex=1, labelx=\"1 s\")\n</code></pre> Source code in <code>nelpy/plotting/scalebar.py</code> <pre><code>def add_scalebar(\n    ax,\n    *,\n    matchx=False,\n    matchy=False,\n    sizex=None,\n    sizey=None,\n    labelx=None,\n    labely=None,\n    hidex=True,\n    hidey=True,\n    ec=\"k\",\n    **kwargs,\n):\n    \"\"\"\n    Add scalebars to axes, matching the size to the ticks of the plot and optionally hiding the x and y axes.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        The axis to attach scalebars to.\n    matchx : bool, optional\n        If True, set size of x scalebar to spacing between ticks. Default is False.\n    matchy : bool, optional\n        If True, set size of y scalebar to spacing between ticks. Default is False.\n    sizex : float, optional\n        Size of x scalebar. Used if matchx is False.\n    sizey : float, optional\n        Size of y scalebar. Used if matchy is False.\n    labelx : str, optional\n        Label for x scalebar.\n    labely : str, optional\n        Label for y scalebar.\n    hidex : bool, optional\n        If True, hide x-axis of parent. Default is True.\n    hidey : bool, optional\n        If True, hide y-axis of parent. Default is True.\n    ec : color, optional\n        Edge color of the scalebar. Default is 'k'.\n    **kwargs : dict\n        Additional arguments passed to AnchoredScaleBar.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis containing the scalebar object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; add_scalebar(ax, sizex=1, labelx=\"1 s\")\n    \"\"\"\n\n    # determine which type op scalebar to plot:\n    # [(horizontal, vertical, both), (matchx, matchy), (labelx, labely)]\n    #\n    # matchx AND sizex ==&gt; error\n    # matchy AND sizey ==&gt; error\n    #\n    # matchx == True ==&gt; determine sizex\n    # matchy == True ==&gt; determine sizey\n    #\n    # if sizex ==&gt; horizontal\n    # if sizey ==&gt; vertical\n    # if sizex and sizey ==&gt; both\n    #\n    # at this point we fully know which type the scalebar is\n    #\n    # labelx is None ==&gt; determine from size\n    # labely is None ==&gt; determine from size\n    #\n    # NOTE: to force label empty, use labelx = ' '\n    #\n\n    # TODO: add logic for inverted axes:\n    # yinverted = ax.yaxis_inverted()\n    # xinverted = ax.xaxis_inverted()\n\n    def f(axis):\n        tick_locations = axis.get_majorticklocs()\n        return len(tick_locations) &gt; 1 and (tick_locations[1] - tick_locations[0])\n\n    if matchx and sizex:\n        raise ValueError(\"matchx and sizex cannot both be specified\")\n    if matchy and sizey:\n        raise ValueError(\"matchy and sizey cannot both be specified\")\n\n    if matchx:\n        sizex = f(ax.xaxis)\n    if matchy:\n        sizey = f(ax.yaxis)\n\n    if not sizex and not sizey:\n        raise ValueError(\"sizex and sizey cannot both be zero\")\n\n    kwargs[\"sizex\"] = sizex\n    kwargs[\"sizey\"] = sizey\n\n    if sizex:\n        sbtype = \"horizontal\"\n        if labelx is None:\n            labelx = str(sizex)\n    if sizey:\n        sbtype = \"vertical\"\n        if labely is None:\n            labely = str(sizey)\n    if sizex and sizey:\n        sbtype = \"both\"\n\n    kwargs[\"labelx\"] = labelx\n    kwargs[\"labely\"] = labely\n    kwargs[\"ec\"] = ec\n\n    if sbtype == \"both\":\n        # draw horizontal component:\n        kwargs[\"labely\"] = \" \"  # necessary to correct center alignment\n        kwargs[\"ec\"] = None  # necessary to correct possible artifact\n        sbx = AnchoredScaleBar(ax.transData, xfirst=True, **kwargs)\n\n        # draw vertical component:\n        kwargs[\"ec\"] = ec\n        kwargs[\"labelx\"] = \" \"\n        kwargs[\"labely\"] = labely\n        sby = AnchoredScaleBar(ax.transData, xfirst=False, **kwargs)\n        ax.add_artist(sbx)\n        ax.add_artist(sby)\n    else:\n        sb = AnchoredScaleBar(ax.transData, **kwargs)\n        ax.add_artist(sb)\n\n    if hidex:\n        ax.xaxis.set_visible(False)\n    if hidey:\n        ax.yaxis.set_visible(False)\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.add_simple_scalebar","title":"<code>add_simple_scalebar(text, ax=None, xy=None, length=None, orientation='v', rotation_text=None, xytext=None, **kwargs)</code>","text":"<p>Add a simple horizontal or vertical scalebar with a label to an axis.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The label for the scalebar.</p> required <code>ax</code> <code>Axes</code> <p>Axis to add the scalebar to. If None, uses current axis.</p> <code>None</code> <code>xy</code> <code>tuple of float</code> <p>Starting (x, y) position for the scalebar.</p> <code>None</code> <code>length</code> <code>float</code> <p>Length of the scalebar. Default is 10.</p> <code>None</code> <code>orientation</code> <code>(v, h, vert, horz)</code> <p>Orientation of the scalebar. 'v' or 'vert' for vertical, 'h' or 'horz' for horizontal. Default is 'v'.</p> <code>'v'</code> <code>rotation_text</code> <code>int or str</code> <p>Rotation of the label text. Default is 0.</p> <code>None</code> <code>xytext</code> <code>tuple of float</code> <p>Position for the label text. If None, automatically determined.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's annotate.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; add_simple_scalebar(\"10 s\", ax=ax, xy=(0, 0), length=10, orientation=\"h\")\n</code></pre> Source code in <code>nelpy/plotting/scalebar.py</code> <pre><code>def add_simple_scalebar(\n    text,\n    ax=None,\n    xy=None,\n    length=None,\n    orientation=\"v\",\n    rotation_text=None,\n    xytext=None,\n    **kwargs,\n):\n    \"\"\"\n    Add a simple horizontal or vertical scalebar with a label to an axis.\n\n    Parameters\n    ----------\n    text : str\n        The label for the scalebar.\n    ax : matplotlib.axes.Axes, optional\n        Axis to add the scalebar to. If None, uses current axis.\n    xy : tuple of float\n        Starting (x, y) position for the scalebar.\n    length : float, optional\n        Length of the scalebar. Default is 10.\n    orientation : {'v', 'h', 'vert', 'horz'}, optional\n        Orientation of the scalebar. 'v' or 'vert' for vertical, 'h' or 'horz' for horizontal. Default is 'v'.\n    rotation_text : int or str, optional\n        Rotation of the label text. Default is 0.\n    xytext : tuple of float, optional\n        Position for the label text. If None, automatically determined.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's annotate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; add_simple_scalebar(\"10 s\", ax=ax, xy=(0, 0), length=10, orientation=\"h\")\n    \"\"\"\n    if rotation_text is None:\n        rotation_text = 0\n    if rotation_text == \"vert\" or rotation_text == \"v\":\n        rotation_text = 90\n    if rotation_text == \"horz\" or rotation_text == \"h\":\n        rotation_text = 0\n    if orientation is None:\n        orientation = 0\n    if orientation == \"vert\" or orientation == \"v\":\n        orientation = 90\n    if orientation == \"horz\" or orientation == \"h\":\n        orientation = 0\n\n    if length is None:\n        length = 10\n\n    if ax is None:\n        ax = plt.gca()\n\n    #     if va is None:\n    #         if rotation_text == 90:\n    #             va = 'bottom'\n    #         else:\n    #             va = 'baseline'\n\n    if orientation == 0:\n        ax.hlines(xy[1], xy[0], xy[0] + length, lw=2, zorder=1000)\n    else:\n        ax.vlines(xy[0], xy[1], xy[1] + length, lw=2, zorder=1000)\n        xytext = (xy[0] + 3, xy[1] + length / 2)\n        ax.annotate(\n            text, xy=xytext, rotation=rotation_text, va=\"center\", zorder=1000, **kwargs\n        )\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.axes_style","title":"<code>axes_style(style=None, rc=None)</code>","text":"<p>Return a parameter dict for the aesthetic style of the plots.</p> <p>This affects things like the color of the axes, whether a grid is enabled by default, and other aesthetic elements.</p> <p>This function returns an object that can be used in a <code>with</code> statement to temporarily change the style parameters.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn style dictionaries. This only updates parameters that are considered part of the style definition.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>style_object</code> <code>_AxesStyle</code> <p>An object that can be used as a context manager to temporarily set style.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; st = axes_style(\"whitegrid\")\n&gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; with axes_style(\"white\"):\n...     f, ax = plt.subplots()\n...     ax.plot([0, 1], [0, 1])\n</code></pre> See Also <p>set_style : set the matplotlib parameters for a seaborn theme plotting_context : return a parameter dict to scale plot elements color_palette : define the color palette for a plot</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def axes_style(style=None, rc=None):\n    \"\"\"\n    Return a parameter dict for the aesthetic style of the plots.\n\n    This affects things like the color of the axes, whether a grid is\n    enabled by default, and other aesthetic elements.\n\n    This function returns an object that can be used in a ``with`` statement\n    to temporarily change the style parameters.\n\n    Parameters\n    ----------\n    style : dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}\n        A dictionary of parameters or the name of a preconfigured set.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        style dictionaries. This only updates parameters that are\n        considered part of the style definition.\n\n    Returns\n    -------\n    style_object : _AxesStyle\n        An object that can be used as a context manager to temporarily set style.\n\n    Examples\n    --------\n    &gt;&gt;&gt; st = axes_style(\"whitegrid\")\n    &gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; with axes_style(\"white\"):\n    ...     f, ax = plt.subplots()\n    ...     ax.plot([0, 1], [0, 1])\n\n    See Also\n    --------\n    set_style : set the matplotlib parameters for a seaborn theme\n    plotting_context : return a parameter dict to scale plot elements\n    color_palette : define the color palette for a plot\n    \"\"\"\n    if style is None:\n        style_dict = {k: mpl.rcParams[k] for k in _style_keys}\n\n    elif isinstance(style, dict):\n        style_dict = style\n\n    else:\n        styles = [\"white\", \"dark\", \"whitegrid\", \"darkgrid\", \"ticks\"]\n        if style not in styles:\n            raise ValueError(\"style must be one of %s\" % \", \".join(styles))\n\n        # Define colors here\n        dark_gray = \".15\"\n        light_gray = \".8\"\n\n        # Common parameters\n        style_dict = {\n            \"figure.facecolor\": \"white\",\n            \"text.color\": dark_gray,\n            \"axes.labelcolor\": dark_gray,\n            \"legend.frameon\": False,\n            \"legend.numpoints\": 1,\n            \"legend.scatterpoints\": 1,\n            \"xtick.direction\": \"out\",\n            \"ytick.direction\": \"out\",\n            \"xtick.color\": dark_gray,\n            \"ytick.color\": dark_gray,\n            \"axes.axisbelow\": True,\n            \"lines.linewidth\": 1.75,\n            \"image.cmap\": \"Greys\",\n            \"font.family\": [\"sans-serif\"],\n            \"font.sans-serif\": [\n                \"DejaVu Sans\",\n                \"Arial\",\n                \"Liberation Sans\",\n                \"Bitstream Vera Sans\",\n                \"sans-serif\",\n            ],\n            \"grid.linestyle\": \"-\",\n            \"lines.solid_capstyle\": \"round\",\n        }\n\n        # Set grid on or off\n        if \"grid\" in style:\n            style_dict.update(\n                {\n                    \"axes.grid\": True,\n                }\n            )\n        else:\n            style_dict.update(\n                {\n                    \"axes.grid\": False,\n                }\n            )\n\n        # Set the color of the background, spines, and grids\n        if style.startswith(\"dark\"):\n            style_dict.update(\n                {\n                    \"axes.facecolor\": \"#EAEAF2\",\n                    \"axes.edgecolor\": \"white\",\n                    \"axes.linewidth\": 0,\n                    \"grid.color\": \"white\",\n                }\n            )\n\n        elif style == \"whitegrid\":\n            style_dict.update(\n                {\n                    \"axes.facecolor\": \"white\",\n                    \"axes.edgecolor\": light_gray,\n                    \"axes.linewidth\": 1,\n                    \"grid.color\": light_gray,\n                }\n            )\n\n        elif style in [\"white\", \"ticks\"]:\n            style_dict.update(\n                {\n                    \"axes.facecolor\": \"white\",\n                    \"axes.edgecolor\": dark_gray,\n                    \"axes.linewidth\": 1.25,\n                    \"grid.color\": light_gray,\n                }\n            )\n\n        # Show or hide the axes ticks\n        if style == \"ticks\":\n            style_dict.update(\n                {\n                    \"xtick.major.size\": 6,\n                    \"ytick.major.size\": 6,\n                    \"xtick.minor.size\": 3,\n                    \"ytick.minor.size\": 3,\n                }\n            )\n        else:\n            style_dict.update(\n                {\n                    \"xtick.major.size\": 0,\n                    \"ytick.major.size\": 0,\n                    \"xtick.minor.size\": 0,\n                    \"ytick.minor.size\": 0,\n                }\n            )\n\n    # Override these settings with the provided rc dictionary\n    if rc is not None:\n        rc = {k: v for k, v in rc.items() if k in _style_keys}\n        style_dict.update(rc)\n\n    # Wrap in an _AxesStyle object so this can be used in a with statement\n    style_object = _AxesStyle(style_dict)\n\n    return style_object\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.colorline","title":"<code>colorline(x, y, cmap=None, cm_range=(0, 0.7), **kwargs)</code>","text":"<p>Plot a trajectory of (x, y) points with a colormap along the path.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>X coordinates of the trajectory.</p> required <code>y</code> <code>array - like</code> <p>Y coordinates of the trajectory.</p> required <code>cmap</code> <code>Colormap</code> <p>Colormap to use for coloring the line. Defaults to plt.cm.Blues_r.</p> <code>None</code> <code>cm_range</code> <code>tuple of float</code> <p>Range of the colormap to use (min, max). Defaults to (0, 0.7).</p> <code>(0, 0.7)</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the plot (e.g., ax, lw).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>lc</code> <code>LineCollection</code> <p>The colored line collection added to the axis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; colorline(x, y, cmap=plt.cm.viridis)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def colorline(x, y, cmap=None, cm_range=(0, 0.7), **kwargs):\n    \"\"\"\n    Plot a trajectory of (x, y) points with a colormap along the path.\n\n    Parameters\n    ----------\n    x : array-like\n        X coordinates of the trajectory.\n    y : array-like\n        Y coordinates of the trajectory.\n    cmap : matplotlib.colors.Colormap, optional\n        Colormap to use for coloring the line. Defaults to plt.cm.Blues_r.\n    cm_range : tuple of float, optional\n        Range of the colormap to use (min, max). Defaults to (0, 0.7).\n    **kwargs : dict\n        Additional keyword arguments passed to the plot (e.g., ax, lw).\n\n    Returns\n    -------\n    lc : matplotlib.collections.LineCollection\n        The colored line collection added to the axis.\n\n    Examples\n    --------\n    &gt;&gt;&gt; colorline(x, y, cmap=plt.cm.viridis)\n    \"\"\"\n\n    # plt.plot(x, y, '-k', zorder=1)\n    # plt.scatter(x, y, s=40, c=plt.cm.RdBu(np.linspace(0,1,40)), zorder=2, edgecolor='k')\n\n    assert len(cm_range) == 2, \"cm_range must have (min, max)\"\n    assert len(x) == len(y), \"x and y must have the same number of elements!\"\n\n    ax = kwargs.get(\"ax\", plt.gca())\n    lw = kwargs.get(\"lw\", 2)\n    if cmap is None:\n        cmap = plt.cm.Blues_r\n\n    t = np.linspace(cm_range[0], cm_range[1], len(x))\n\n    points = np.array([x, y]).T.reshape(-1, 1, 2)\n    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n\n    lc = LineCollection(segments, cmap=cmap, norm=plt.Normalize(0, 1), zorder=50)\n    lc.set_array(t)\n    lc.set_linewidth(lw)\n\n    ax.add_collection(lc)\n\n    return lc\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.decode_and_plot_events1D","title":"<code>decode_and_plot_events1D(*, bst, tc, raster=True, st=None, st_order='track', evt_subset=None, **kwargs)</code>","text":"<p>Decode and plot 1D events with optional raster plot overlay.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>The binned spike train array to decode.</p> required <code>tc</code> <code>TuningCurve1D</code> <p>The tuning curve used for decoding.</p> required <code>raster</code> <code>bool</code> <p>Whether to include a raster plot (default is True).</p> <code>True</code> <code>st</code> <code>SpikeTrainArray</code> <p>The spike train array for raster plotting.</p> <code>None</code> <code>st_order</code> <code>str or array - like</code> <p>Order of units for raster plot. Options: 'track', 'first', 'random', or array of unit ids.</p> <code>'track'</code> <code>evt_subset</code> <code>list</code> <p>List of integer indices for event selection. If not sorted, will be sorted.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for plotting.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The figure containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = decode_and_plot_events1D(bst=bst, tc=tc, st=st)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/decoding.py</code> <pre><code>def decode_and_plot_events1D(\n    *, bst, tc, raster=True, st=None, st_order=\"track\", evt_subset=None, **kwargs\n):\n    \"\"\"\n    Decode and plot 1D events with optional raster plot overlay.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        The binned spike train array to decode.\n    tc : TuningCurve1D\n        The tuning curve used for decoding.\n    raster : bool, optional\n        Whether to include a raster plot (default is True).\n    st : SpikeTrainArray, optional\n        The spike train array for raster plotting.\n    st_order : str or array-like, optional\n        Order of units for raster plot. Options: 'track', 'first', 'random', or array of unit ids.\n    evt_subset : list, optional\n        List of integer indices for event selection. If not sorted, will be sorted.\n    **kwargs\n        Additional keyword arguments for plotting.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The figure containing the plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = decode_and_plot_events1D(bst=bst, tc=tc, st=st)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # TODO: add **kwargs\n    #   fig size, cmap, raster lw, raster color, other axes props, ...\n\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    unit_ids = set(bst.unit_ids)\n    unit_ids = unit_ids.intersection(st.unit_ids)\n    unit_ids = unit_ids.intersection(tc.unit_ids)\n\n    bst = bst._unit_subset(unit_ids)\n    st = st._unit_subset(unit_ids)\n    tc = tc._unit_subset(unit_ids)\n\n    if evt_subset is None:\n        evt_subset = np.arange(bst.n_epochs)\n    evt_subset = list(evt_subset)\n    if not is_sorted(evt_subset):\n        evt_subset.sort()\n    bst = bst[evt_subset]\n\n    # now that the bst has potentially been restricted by evt_subset, we trim down the spike train as well:\n    st = st[bst.support]\n    st = collapse_time(st)\n\n    if st_order == \"track\":\n        new_order = tc.get_peak_firing_order_ids()\n    elif st_order == \"first\":\n        new_order = st.get_spike_firing_order()\n    elif st_order == \"random\":\n        new_order = np.random.permutation(st.unit_ids)\n    else:\n        new_order = st_order\n    st.reorder_units_by_ids(new_order, inplace=True)\n\n    # now decode events in bst:\n    posterior, bdries, mode_pth, mean_pth = decoding.decode1D(\n        bst=bst, ratemap=tc, xmax=tc.bins[-1]\n    )\n\n    fig, ax = plt.subplots(figsize=(bst.n_bins / 5, 4))\n\n    pixel_width = 0.5\n\n    imagesc(\n        x=np.arange(bst.n_bins),\n        y=np.arange(311),\n        data=posterior,\n        cmap=plt.cm.Spectral_r,\n        ax=ax,\n    )\n    plotutils.yticks_interval(310)\n    plotutils.no_yticks(ax)\n\n    ax.vlines(\n        np.arange(bst.lengths.sum()) - pixel_width,\n        *ax.get_ylim(),\n        lw=1,\n        linestyle=\":\",\n        color=\"0.8\",\n    )\n    ax.vlines(np.cumsum(bst.lengths) - pixel_width, *ax.get_ylim(), lw=1)\n\n    ax.set_xlim(-pixel_width, bst.lengths.sum() - pixel_width)\n\n    event_centers = np.insert(np.cumsum(bst.lengths), 0, 0)\n    event_centers = event_centers[:-1] + bst.lengths / 2 - 0.5\n\n    #     ax.set_xticks([0, bst.n_bins-1])\n    #     ax.set_xticklabels([1, bst.n_bins])\n\n    ax.set_xticks(event_centers)\n    ax.set_xticklabels(evt_subset)\n    #     ax.xaxis.tick_top()\n    #     ax.xaxis.set_label_position('top')\n\n    plotutils.no_xticks(ax)\n\n    divider = make_axes_locatable(ax)\n    axRaster = divider.append_axes(\"top\", size=1.5, pad=0)\n\n    rasterplot(st, vertstack=True, ax=axRaster, lh=1.25, lw=2.5, color=\"0.1\")\n\n    axRaster.set_xlim(st.support.time.squeeze())\n    bin_edges = np.linspace(\n        st.support.time[0, 0], st.support.time[0, 1], bst.n_bins + 1\n    )\n    axRaster.vlines(bin_edges, *axRaster.get_ylim(), lw=1, linestyle=\":\", color=\"0.8\")\n    axRaster.vlines(\n        bin_edges[np.cumsum(bst.lengths)], *axRaster.get_ylim(), lw=1, color=\"0.2\"\n    )\n    plotutils.no_xticks(axRaster)\n    plotutils.no_xticklabels(axRaster)\n    plotutils.no_yticklabels(axRaster)\n    plotutils.no_yticks(axRaster)\n    ax.set_ylabel(\"position\")\n    axRaster.set_ylabel(\"units\")\n    ax.set_xlabel(\"time bins\")\n    plotutils.clear_left_right(axRaster)\n    plotutils.clear_top_bottom(axRaster)\n\n    plotutils.align_ylabels(0, ax, axRaster)\n    return fig\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.epochplot","title":"<code>epochplot(epochs, data=None, *, ax=None, height=None, fc='0.5', ec='0.5', alpha=0.5, hatch='', label=None, hc=None, **kwargs)</code>","text":"<p>Plot an EpochArray as horizontal bars (intervals) on a timeline.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>EpochArray</code> <p>The epochs to plot.</p> required <code>data</code> <code>array - like</code> <p>Data to plot on y axis; must be of size (epochs.n_epochs,).</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>height</code> <code>float</code> <p>Height of the bars. If None, uses default.</p> <code>None</code> <code>fc</code> <code>color</code> <p>Face color of the bars. Default is '0.5'.</p> <code>'0.5'</code> <code>ec</code> <code>color</code> <p>Edge color of the bars. Default is '0.5'.</p> <code>'0.5'</code> <code>alpha</code> <code>float</code> <p>Transparency of the bars. Default is 0.5.</p> <code>0.5</code> <code>hatch</code> <code>str</code> <p>Hatching pattern for the bars.</p> <code>''</code> <code>label</code> <code>str</code> <p>Label for the bars.</p> <code>None</code> <code>hc</code> <code>color</code> <p>Highlight color for the bars.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's barh.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the epoch plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import EpochArray\n&gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n&gt;&gt;&gt; epochplot(epochs)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def epochplot(\n    epochs,\n    data=None,\n    *,\n    ax=None,\n    height=None,\n    fc=\"0.5\",\n    ec=\"0.5\",\n    alpha=0.5,\n    hatch=\"\",\n    label=None,\n    hc=None,\n    **kwargs,\n):\n    \"\"\"\n    Plot an EpochArray as horizontal bars (intervals) on a timeline.\n\n    Parameters\n    ----------\n    epochs : nelpy.EpochArray\n        The epochs to plot.\n    data : array-like, optional\n        Data to plot on y axis; must be of size (epochs.n_epochs,).\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    height : float, optional\n        Height of the bars. If None, uses default.\n    fc : color, optional\n        Face color of the bars. Default is '0.5'.\n    ec : color, optional\n        Edge color of the bars. Default is '0.5'.\n    alpha : float, optional\n        Transparency of the bars. Default is 0.5.\n    hatch : str, optional\n        Hatching pattern for the bars.\n    label : str, optional\n        Label for the bars.\n    hc : color, optional\n        Highlight color for the bars.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's barh.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the epoch plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import EpochArray\n    &gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n    &gt;&gt;&gt; epochplot(epochs)\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    # do fixed-value-on-epoch plot if data is not None\n    if data is not None:\n        if epochs.n_intervals != len(data):\n            raise ValueError(\"epocharray and data must have the same length\")\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for epoch, val in zip(epochs, data):\n                ax.plot([epoch.start, epoch.stop], [val, val], \"-o\", **kwargs)\n        return ax\n\n    ymin, ymax = ax.get_ylim()\n    xmin, xmax = ax.get_xlim()\n    if height is None:\n        height = ymax - ymin\n\n    if hc is not None:\n        try:\n            hc_before = mpl.rcParams[\"hatch.color\"]\n            mpl.rcParams[\"hatch.color\"] = hc\n        except KeyError:\n            warnings.warn(\"Hatch color not supported for matplotlib &lt;2.0\")\n\n    for ii, (start, stop) in enumerate(zip(epochs.starts, epochs.stops)):\n        ax.axvspan(\n            start,\n            stop,\n            hatch=hatch,\n            facecolor=fc,\n            edgecolor=ec,\n            alpha=alpha,\n            label=label if ii == 0 else \"_nolegend_\",\n            **kwargs,\n        )\n\n    if epochs.start &lt; xmin:\n        xmin = epochs.start\n    if epochs.stop &gt; xmax:\n        xmax = epochs.stop\n    ax.set_xlim([xmin, xmax])\n\n    if hc is not None:\n        try:\n            mpl.rcParams[\"hatch.color\"] = hc_before\n        except UnboundLocalError:\n            pass\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.imagesc","title":"<code>imagesc(x=None, y=None, data=None, *, ax=None, large=False, **kwargs)</code>","text":"<p>Plot a 2D matrix or image similar to Matlab's imagesc.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>X values (columns).</p> <code>None</code> <code>y</code> <code>array - like</code> <p>Y values (rows).</p> <code>None</code> <code>data</code> <code>ndarray of shape (Nrows, Ncols)</code> <p>Matrix to visualize.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Plot in given axis; if None creates a new figure.</p> <code>None</code> <code>large</code> <code>bool</code> <p>If True, optimize for large matrices. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to imshow.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>im</code> <code>AxesImage</code> <p>The image object.</p> <p>Examples:</p> <p>Plot a simple matrix using imagesc:</p> <pre><code>&gt;&gt;&gt; x = np.linspace(-100, -10, 10)\n&gt;&gt;&gt; y = np.array([-8, -3.0])\n&gt;&gt;&gt; data = np.random.randn(y.size, x.size)\n&gt;&gt;&gt; imagesc(x, y, data)\nor\n&gt;&gt;&gt; imagesc(data)\n</code></pre> <p>Adding a colorbar:</p> <pre><code>&gt;&gt;&gt; ax, img = imagesc(data)\n&gt;&gt;&gt; from mpl_toolkits.axes_grid1 import make_axes_locatable\n&gt;&gt;&gt; divider = make_axes_locatable(ax)\n&gt;&gt;&gt; cax = divider.append_axes(\"right\", size=\"3.5%\", pad=0.1)\n&gt;&gt;&gt; cb = plt.colorbar(img, cax=cax)\n&gt;&gt;&gt; npl.utils.no_yticks(cax)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def imagesc(x=None, y=None, data=None, *, ax=None, large=False, **kwargs):\n    \"\"\"\n    Plot a 2D matrix or image similar to Matlab's imagesc.\n\n    Parameters\n    ----------\n    x : array-like, optional\n        X values (columns).\n    y : array-like, optional\n        Y values (rows).\n    data : ndarray of shape (Nrows, Ncols)\n        Matrix to visualize.\n    ax : matplotlib.axes.Axes, optional\n        Plot in given axis; if None creates a new figure.\n    large : bool, optional\n        If True, optimize for large matrices. Default is False.\n    **kwargs : dict\n        Additional keyword arguments passed to imshow.\n\n    Returns\n    -------\n    im : matplotlib.image.AxesImage\n        The image object.\n\n    Examples\n    --------\n    Plot a simple matrix using imagesc:\n\n    &gt;&gt;&gt; x = np.linspace(-100, -10, 10)\n    &gt;&gt;&gt; y = np.array([-8, -3.0])\n    &gt;&gt;&gt; data = np.random.randn(y.size, x.size)\n    &gt;&gt;&gt; imagesc(x, y, data)\n    or\n    &gt;&gt;&gt; imagesc(data)\n\n    Adding a colorbar:\n\n    &gt;&gt;&gt; ax, img = imagesc(data)\n    &gt;&gt;&gt; from mpl_toolkits.axes_grid1 import make_axes_locatable\n    &gt;&gt;&gt; divider = make_axes_locatable(ax)\n    &gt;&gt;&gt; cax = divider.append_axes(\"right\", size=\"3.5%\", pad=0.1)\n    &gt;&gt;&gt; cb = plt.colorbar(img, cax=cax)\n    &gt;&gt;&gt; npl.utils.no_yticks(cax)\n    \"\"\"\n\n    def extents(f):\n        if len(f) &gt; 1:\n            delta = f[1] - f[0]\n        else:\n            delta = 1\n        return [f[0] - delta / 2, f[-1] + delta / 2]\n\n    if ax is None:\n        ax = plt.gca()\n    if data is None:\n        if x is None:  # no args\n            raise ValueError(\n                \"Unknown input. Usage imagesc(x, y, data) or imagesc(data).\"\n            )\n        elif y is None:  # only one arg, so assume it to be data\n            data = x\n            x = np.arange(data.shape[1])\n            y = np.arange(data.shape[0])\n        else:  # x and y, but no data\n            raise ValueError(\n                \"Unknown input. Usage imagesc(x, y, data) or imagesc(data).\"\n            )\n\n    if data.ndim != 2:\n        raise TypeError(\"data must be 2 dimensional\")\n\n    if not large:\n        # Matplotlib imshow\n        image = ax.imshow(\n            data,\n            aspect=\"auto\",\n            interpolation=\"none\",\n            extent=extents(x) + extents(y),\n            origin=\"lower\",\n            **kwargs,\n        )\n    else:\n        # ModestImage imshow for large images, but 'extent' is still not working well\n        image = utils.imshow(\n            axes=ax,\n            X=data,\n            aspect=\"auto\",\n            interpolation=\"none\",\n            extent=extents(x) + extents(y),\n            origin=\"lower\",\n            **kwargs,\n        )\n\n    return ax, image\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.matshow","title":"<code>matshow(data, *, ax=None, **kwargs)</code>","text":"<p>Display a matrix in a new figure window using matplotlib's matshow.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like or BinnedSpikeTrainArray</code> <p>The matrix or nelpy object to display.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's matshow.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mat = np.random.rand(5, 5)\n&gt;&gt;&gt; matshow(mat)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def matshow(data, *, ax=None, **kwargs):\n    \"\"\"\n    Display a matrix in a new figure window using matplotlib's matshow.\n\n    Parameters\n    ----------\n    data : array-like or nelpy.BinnedSpikeTrainArray\n        The matrix or nelpy object to display.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's matshow.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mat = np.random.rand(5, 5)\n    &gt;&gt;&gt; matshow(mat)\n    \"\"\"\n\n    # Sort out default values for the parameters\n    if ax is None:\n        ax = plt.gca()\n\n    # Handle different types of input data\n    if isinstance(data, core.BinnedSpikeTrainArray):\n        # TODO: split by epoch, and plot matshows in same row, but with\n        # a small gap to indicate discontinuities. How about slicing\n        # then? Or slicing within an epoch?\n        ax.matshow(data.data, **kwargs)\n        ax.set_xlabel(\"time\")\n        ax.set_ylabel(\"unit\")\n        # warnings.warn(\"Automatic x-axis formatting not yet implemented\")\n    else:\n        raise NotImplementedError(\n            \"matshow({}) not yet supported\".format(str(type(data)))\n        )\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.overviewstrip","title":"<code>overviewstrip(epochs, *, ax=None, lw=5, solid_capstyle='butt', label=None, **kwargs)</code>","text":"<p>Plot an epoch array as a strip (like a scrollbar) to show gaps in e.g. matshow plots.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>EpochArray</code> <p>The epochs to plot as a strip.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width for the strip. Default is 5.</p> <code>5</code> <code>solid_capstyle</code> <code>str</code> <p>Cap style for the strip. Default is 'butt'.</p> <code>'butt'</code> <code>label</code> <code>str</code> <p>Label for the strip.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's plot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the overview strip.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import EpochArray\n&gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n&gt;&gt;&gt; overviewstrip(epochs)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def overviewstrip(\n    epochs, *, ax=None, lw=5, solid_capstyle=\"butt\", label=None, **kwargs\n):\n    \"\"\"\n    Plot an epoch array as a strip (like a scrollbar) to show gaps in e.g. matshow plots.\n\n    Parameters\n    ----------\n    epochs : nelpy.EpochArray\n        The epochs to plot as a strip.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    lw : float, optional\n        Line width for the strip. Default is 5.\n    solid_capstyle : str, optional\n        Cap style for the strip. Default is 'butt'.\n    label : str, optional\n        Label for the strip.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's plot.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the overview strip.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import EpochArray\n    &gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n    &gt;&gt;&gt; overviewstrip(epochs)\n    \"\"\"\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    if ax is None:\n        ax = plt.gca()\n\n    divider = make_axes_locatable(ax)\n    ax_ = divider.append_axes(\"top\", size=0.2, pad=0.05)\n\n    for epoch in epochs:\n        ax_.plot(\n            [epoch.start, epoch.stop],\n            [1, 1],\n            lw=lw,\n            solid_capstyle=solid_capstyle,\n            **kwargs,\n        )\n\n    if label is not None:\n        ax_.set_yticks([1])\n        ax_.set_yticklabels([label])\n    else:\n        ax_.set_yticks([])\n\n    utils.no_yticks(ax_)\n    utils.clear_left(ax_)\n    utils.clear_right(ax_)\n    utils.clear_top_bottom(ax_)\n\n    ax_.set_xlim(ax.get_xlim())\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.palplot","title":"<code>palplot(pal, size=1)</code>","text":"<p>Plot the values in a color palette as a horizontal array.</p> <p>Parameters:</p> Name Type Description Default <code>pal</code> <code>sequence of matplotlib colors</code> <p>Colors, i.e. as returned by nelpy.color_palette().</p> required <code>size</code> <code>float</code> <p>Scaling factor for size of plot. Default is 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.plotting.miscplot import palplot\n&gt;&gt;&gt; pal = [\"#FF0000\", \"#00FF00\", \"#0000FF\"]\n&gt;&gt;&gt; palplot(pal)\n</code></pre> Source code in <code>nelpy/plotting/miscplot.py</code> <pre><code>def palplot(pal, size=1):\n    \"\"\"\n    Plot the values in a color palette as a horizontal array.\n\n    Parameters\n    ----------\n    pal : sequence of matplotlib colors\n        Colors, i.e. as returned by nelpy.color_palette().\n    size : float, optional\n        Scaling factor for size of plot. Default is 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.plotting.miscplot import palplot\n    &gt;&gt;&gt; pal = [\"#FF0000\", \"#00FF00\", \"#0000FF\"]\n    &gt;&gt;&gt; palplot(pal)\n    \"\"\"\n    n = len(pal)\n    f, ax = plt.subplots(1, 1, figsize=(n * size, size))\n    ax.imshow(\n        np.arange(n).reshape(1, n),\n        cmap=mpl.colors.ListedColormap(list(pal)),\n        interpolation=\"nearest\",\n        aspect=\"auto\",\n    )\n    ax.set_xticks(np.arange(n) - 0.5)\n    ax.set_yticks([-0.5, 0.5])\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.plot","title":"<code>plot(obj, *args, **kwargs)</code>","text":"<p>Plot a nelpy object or array-like data using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>nelpy object or array-like</code> <p>The object or data to plot. Can be a nelpy RegularlySampledAnalogSignalArray or array-like.</p> required <code>*args</code> <code>tuple</code> <p>Additional positional arguments passed to matplotlib's plot.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's plot. Special keys:     ax : matplotlib.axes.Axes, optional         Axis to plot on. If None, uses current axis.     autoscale : bool, optional         Whether to autoscale the axis. Default is True.     xlabel : str, optional         X-axis label.     ylabel : str, optional         Y-axis label.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.core import RegularlySampledAnalogSignalArray\n&gt;&gt;&gt; obj = RegularlySampledAnalogSignalArray(...)  # your data here\n&gt;&gt;&gt; plot(obj)\n&gt;&gt;&gt; plot([1, 2, 3, 4])\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def plot(obj, *args, **kwargs):\n    \"\"\"\n    Plot a nelpy object or array-like data using matplotlib.\n\n    Parameters\n    ----------\n    obj : nelpy object or array-like\n        The object or data to plot. Can be a nelpy RegularlySampledAnalogSignalArray or array-like.\n    *args : tuple\n        Additional positional arguments passed to matplotlib's plot.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's plot. Special keys:\n            ax : matplotlib.axes.Axes, optional\n                Axis to plot on. If None, uses current axis.\n            autoscale : bool, optional\n                Whether to autoscale the axis. Default is True.\n            xlabel : str, optional\n                X-axis label.\n            ylabel : str, optional\n                Y-axis label.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.core import RegularlySampledAnalogSignalArray\n    &gt;&gt;&gt; obj = RegularlySampledAnalogSignalArray(...)  # your data here\n    &gt;&gt;&gt; plot(obj)\n    &gt;&gt;&gt; plot([1, 2, 3, 4])\n    \"\"\"\n\n    ax = kwargs.pop(\"ax\", None)\n    autoscale = kwargs.pop(\"autoscale\", True)\n    xlabel = kwargs.pop(\"xlabel\", None)\n    ylabel = kwargs.pop(\"ylabel\", None)\n\n    if ax is None:\n        ax = plt.gca()\n\n    # Patch: If obj is an EpochArray, delegate to plot_old\n    if isinstance(obj, core.EpochArray):\n        return plot_old(obj, *args, **kwargs)\n\n    if isinstance(obj, core.RegularlySampledAnalogSignalArray):\n        if obj.n_signals == 1:\n            label = kwargs.pop(\"label\", None)\n            for ii, (abscissa_vals, data) in enumerate(\n                zip(\n                    obj._intervaltime.plot_generator(),\n                    obj._intervaldata.plot_generator(),\n                )\n            ):\n                ax.plot(\n                    abscissa_vals,\n                    data.T,\n                    label=label if ii == 0 else \"_nolegend_\",\n                    *args,\n                    **kwargs,\n                )\n        elif obj.n_signals &gt; 1:\n            # TODO: intercept when any color is requested. This could happen\n            # multiple ways, such as plt.plot(x, '-r') or plt.plot(x, c='0.7')\n            # or plt.plot(x, color='red'), and maybe some others? Probably have\n            # dig into the matplotlib code to see how they parse this and do\n            # conflict resolution... Update: they use the last specified color.\n            # but I still need to know how to detect a color that was passed in\n            # the *args part, e.g., '-r'\n\n            color = kwargs.pop(\"color\", None)\n            carg = kwargs.pop(\"c\", None)\n\n            if color is not None and carg is not None:\n                # TODO: fix this so that a warning is issued, not raised\n                raise ValueError(\"saw kwargs ['c', 'color']\")\n                # raise UserWarning(\"saw kwargs ['c', 'color'] which are all aliases for 'color'.  Kept value from 'color'\")\n            if carg:\n                color = carg\n\n            if not color:\n                colors = []\n                for ii in range(obj.n_signals):\n                    (line,) = ax.plot(0, 0.5)\n                    colors.append(line.get_color())\n                    line.remove()\n\n                for ee, (abscissa_vals, data) in enumerate(\n                    zip(\n                        obj._intervaltime.plot_generator(),\n                        obj._intervaldata.plot_generator(),\n                    )\n                ):\n                    if ee &gt; 0:\n                        kwargs[\"label\"] = \"_nolegend_\"\n                    for ii, snippet in enumerate(data):\n                        ax.plot(\n                            abscissa_vals, snippet, *args, color=colors[ii], **kwargs\n                        )\n            else:\n                kwargs[\"color\"] = color\n                for ee, (abscissa_vals, data) in enumerate(\n                    zip(\n                        obj._intervaltime.plot_generator(),\n                        obj._intervaldata.plot_generator(),\n                    )\n                ):\n                    if ee &gt; 0:\n                        kwargs[\"label\"] = \"_nolegend_\"\n                    for ii, snippet in enumerate(data):\n                        ax.plot(abscissa_vals, snippet, *args, **kwargs)\n\n        if xlabel is None:\n            xlabel = obj._abscissa.label\n        if ylabel is None:\n            ylabel = obj._ordinate.label\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n    else:  # if we didn't handle it yet, just pass it through to matplotlib...\n        ax.plot(obj, *args, **kwargs)\n\n    if autoscale:\n        xmin = np.inf\n        xmax = -np.inf\n        ymin = np.inf\n        ymax = -np.inf\n        for child in ax.get_children():\n            try:\n                cxmin, cymin = np.min(child.get_xydata(), axis=0)\n                cxmax, cymax = np.max(child.get_xydata(), axis=0)\n                if cxmin &lt; xmin:\n                    xmin = cxmin\n                if cymin &lt; ymin:\n                    ymin = cymin\n                if cxmax &gt; xmax:\n                    xmax = cxmax\n                if cymax &gt; ymax:\n                    ymax = cymax\n            except Exception:\n                pass\n        ax.set_xlim(xmin, xmax)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.plot2d","title":"<code>plot2d(npl_obj, data=None, *, ax=None, mew=None, color=None, mec=None, markerfacecolor=None, **kwargs)</code>","text":"<p>Plot 2D data for nelpy objects or array-like input.</p> <p>Parameters:</p> Name Type Description Default <code>npl_obj</code> <code>nelpy object or array-like</code> <p>The object or data to plot in 2D.</p> required <code>data</code> <code>array - like</code> <p>Data to plot. If None, uses npl_obj's data.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>mew</code> <code>float</code> <p>Marker edge width.</p> <code>None</code> <code>color</code> <code>matplotlib color</code> <p>Trace color.</p> <code>None</code> <code>mec</code> <code>matplotlib color</code> <p>Marker edge color.</p> <code>None</code> <code>markerfacecolor</code> <code>matplotlib color</code> <p>Marker face color.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's plot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot2d([[0, 1], [1, 2], [2, 3]])\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def plot2d(\n    npl_obj,\n    data=None,\n    *,\n    ax=None,\n    mew=None,\n    color=None,\n    mec=None,\n    markerfacecolor=None,\n    **kwargs,\n):\n    \"\"\"\n    Plot 2D data for nelpy objects or array-like input.\n\n    Parameters\n    ----------\n    npl_obj : nelpy object or array-like\n        The object or data to plot in 2D.\n    data : array-like, optional\n        Data to plot. If None, uses npl_obj's data.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    mew : float, optional\n        Marker edge width.\n    color : matplotlib color, optional\n        Trace color.\n    mec : matplotlib color, optional\n        Marker edge color.\n    markerfacecolor : matplotlib color, optional\n        Marker face color.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's plot.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot2d([[0, 1], [1, 2], [2, 3]])\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    if mec is None:\n        mec = color\n    if markerfacecolor is None:\n        markerfacecolor = \"w\"\n\n    if isinstance(npl_obj, np.ndarray):\n        ax.plot(npl_obj, mec=mec, markerfacecolor=markerfacecolor, **kwargs)\n\n    # TODO: better solution for this? we could just iterate over the epochs and\n    # plot them but that might take up too much time since a copy is being made\n    # each iteration?\n    if isinstance(npl_obj, core.RegularlySampledAnalogSignalArray):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for segment in npl_obj:\n                if color is not None:\n                    ax.plot(\n                        segment[:, 0]._data_colsig,\n                        segment[:, 1]._data_colsig,\n                        color=color,\n                        mec=mec,\n                        markerfacecolor=\"w\",\n                        **kwargs,\n                    )\n                else:\n                    ax.plot(\n                        segment[:, 0]._data_colsig,\n                        segment[:, 1]._data_colsig,\n                        # color=color,\n                        mec=mec,\n                        markerfacecolor=\"w\",\n                        **kwargs,\n                    )\n\n    if isinstance(npl_obj, core.PositionArray):\n        xlim, ylim = npl_obj.xlim, npl_obj.ylim\n        if xlim is not None:\n            ax.set_xlim(xlim)\n        if ylim is not None:\n            ax.set_ylim(ylim)\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.plot_cum_error_dist","title":"<code>plot_cum_error_dist(*, cumhist=None, bincenters=None, bst=None, extern=None, decodefunc=None, k=None, transfunc=None, n_extern=None, n_bins=None, extmin=None, extmax=None, sigma=None, lw=None, ax=None, inset=True, inset_ax=None, color=None, **kwargs)</code>","text":"<p>Plot (and optionally compute) the cumulative distribution of decoding errors.</p> <p>Evaluated using a cross-validation procedure. See Fig 3.(b) of \"Analysis of Hippocampal Memory Replay Using Neural Population Decoding\", Fabian Kloosterman, 2012.</p> <p>Parameters:</p> Name Type Description Default <code>cumhist</code> <code>array - like</code> <p>Precomputed cumulative histogram of errors. If None, will be computed.</p> <code>None</code> <code>bincenters</code> <code>array - like</code> <p>Bin centers for the cumulative histogram. If None, will be computed.</p> <code>None</code> <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Required if cumhist and bincenters are not provided. Used for error computation.</p> <code>None</code> <code>extern</code> <code>array - like</code> <p>External variable (e.g., position) for decoding. Required if cumhist and bincenters are not provided.</p> <code>None</code> <code>decodefunc</code> <code>callable</code> <p>Decoding function to use. Defaults to decoding.decode1D.</p> <code>None</code> <code>k</code> <code>int</code> <p>Number of cross-validation folds. Default is 5.</p> <code>None</code> <code>transfunc</code> <code>callable</code> <p>Optional transformation function for the external variable.</p> <code>None</code> <code>n_extern</code> <code>int</code> <p>Number of external variable samples. Default is 100.</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>Number of bins for the error histogram. Default is 200.</p> <code>None</code> <code>extmin</code> <code>float</code> <p>Minimum value of the external variable. Default is 0.</p> <code>None</code> <code>extmax</code> <code>float</code> <p>Maximum value of the external variable. Default is 100.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Smoothing parameter. Default is 3.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width for the plot. Default is 1.5.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>inset</code> <code>bool</code> <p>Whether to include an inset plot. Default is True.</p> <code>True</code> <code>inset_ax</code> <code>Axes</code> <p>Axis for the inset plot. If None, one will be created.</p> <code>None</code> <code>color</code> <code>color</code> <p>Line color. If None, uses next color in cycle.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for plotting.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the cumulative error plot.</p> <code>inset_ax</code> <code>(Axes, optional)</code> <p>The axis with the inset plot (if inset=True).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ax, inset_ax = plot_cum_error_dist(bst=bst, extern=pos)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/decoding.py</code> <pre><code>def plot_cum_error_dist(\n    *,\n    cumhist=None,\n    bincenters=None,\n    bst=None,\n    extern=None,\n    decodefunc=None,\n    k=None,\n    transfunc=None,\n    n_extern=None,\n    n_bins=None,\n    extmin=None,\n    extmax=None,\n    sigma=None,\n    lw=None,\n    ax=None,\n    inset=True,\n    inset_ax=None,\n    color=None,\n    **kwargs,\n):\n    \"\"\"\n    Plot (and optionally compute) the cumulative distribution of decoding errors.\n\n    Evaluated using a cross-validation procedure. See Fig 3.(b) of \"Analysis of Hippocampal Memory Replay Using Neural Population Decoding\", Fabian Kloosterman, 2012.\n\n    Parameters\n    ----------\n    cumhist : array-like, optional\n        Precomputed cumulative histogram of errors. If None, will be computed.\n    bincenters : array-like, optional\n        Bin centers for the cumulative histogram. If None, will be computed.\n    bst : BinnedSpikeTrainArray, optional\n        Required if cumhist and bincenters are not provided. Used for error computation.\n    extern : array-like, optional\n        External variable (e.g., position) for decoding. Required if cumhist and bincenters are not provided.\n    decodefunc : callable, optional\n        Decoding function to use. Defaults to decoding.decode1D.\n    k : int, optional\n        Number of cross-validation folds. Default is 5.\n    transfunc : callable, optional\n        Optional transformation function for the external variable.\n    n_extern : int, optional\n        Number of external variable samples. Default is 100.\n    n_bins : int, optional\n        Number of bins for the error histogram. Default is 200.\n    extmin : float, optional\n        Minimum value of the external variable. Default is 0.\n    extmax : float, optional\n        Maximum value of the external variable. Default is 100.\n    sigma : float, optional\n        Smoothing parameter. Default is 3.\n    lw : float, optional\n        Line width for the plot. Default is 1.5.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    inset : bool, optional\n        Whether to include an inset plot. Default is True.\n    inset_ax : matplotlib.axes.Axes, optional\n        Axis for the inset plot. If None, one will be created.\n    color : color, optional\n        Line color. If None, uses next color in cycle.\n    **kwargs\n        Additional keyword arguments for plotting.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the cumulative error plot.\n    inset_ax : matplotlib.axes.Axes, optional\n        The axis with the inset plot (if inset=True).\n\n    Examples\n    --------\n    &gt;&gt;&gt; ax, inset_ax = plot_cum_error_dist(bst=bst, extern=pos)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    if lw is None:\n        lw = 1.5\n    if decodefunc is None:\n        decodefunc = decoding.decode1D\n    if k is None:\n        k = 5\n    if n_extern is None:\n        n_extern = 100\n    if n_bins is None:\n        n_bins = 200\n    if extmin is None:\n        extmin = 0\n    if extmax is None:\n        extmax = 100\n    if sigma is None:\n        sigma = 3\n\n    # Get the color from the current color cycle\n    if color is None:\n        (line,) = ax.plot(0, 0.5)\n        color = line.get_color()\n        line.remove()\n\n    # if cumhist or bincenters are NOT provided, then compute them\n    if cumhist is None or bincenters is None:\n        assert bst is not None, (\n            \"if cumhist and bincenters are not given, then bst must be provided to recompute them!\"\n        )\n        assert extern is not None, (\n            \"if cumhist and bincenters are not given, then extern must be provided to recompute them!\"\n        )\n        cumhist, bincenters = decoding.cumulative_dist_decoding_error_using_xval(\n            bst=bst,\n            extern=extern,\n            decodefunc=decoding.decode1D,\n            k=k,\n            transfunc=transfunc,\n            n_extern=n_extern,\n            extmin=extmin,\n            extmax=extmax,\n            sigma=sigma,\n            n_bins=n_bins,\n        )\n    # now plot results\n    ax.plot(bincenters, cumhist, lw=lw, color=color, **kwargs)\n    ax.set_xlim(bincenters[0], bincenters[-1])\n    ax.set_xlabel(\"error [cm]\")\n    ax.set_ylabel(\"cumulative probability\")\n\n    ax.set_ylim(0)\n\n    if inset:\n        if inset_ax is None:\n            inset_ax = inset_axes(\n                parent_axes=ax, width=\"60%\", height=\"50%\", loc=4, borderpad=2\n            )\n\n        inset_ax.plot(bincenters, cumhist, lw=lw, color=color, **kwargs)\n\n        # annotate inset\n        thresh1 = 0.7\n        inset_ax.hlines(\n            thresh1, 0, cumhist(thresh1), color=color, alpha=0.9, lw=lw, linestyle=\"--\"\n        )\n        inset_ax.vlines(\n            cumhist(thresh1), 0, thresh1, color=color, alpha=0.9, lw=lw, linestyle=\"--\"\n        )\n        inset_ax.set_xlim(0, 12 * np.ceil(cumhist(thresh1) / 10))\n\n        thresh2 = 0.5\n        inset_ax.hlines(\n            thresh2, 0, cumhist(thresh2), color=color, alpha=0.6, lw=lw, linestyle=\"--\"\n        )\n        inset_ax.vlines(\n            cumhist(thresh2), 0, thresh2, color=color, alpha=0.6, lw=lw, linestyle=\"--\"\n        )\n\n        inset_ax.set_yticks((0, thresh1, thresh2, 1))\n        inset_ax.set_ylim(0)\n\n        return ax, inset_ax\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.plot_posteriors","title":"<code>plot_posteriors(bst, tuningcurve, idx=None, w=1, bin_px_size=0.08)</code>","text":"<p>Plot posterior probabilities for decoded neural activity.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>The binned spike train array to decode.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>The tuning curve used for decoding.</p> required <code>idx</code> <code>array - like</code> <p>Indices of events to plot. If None, all events are plotted.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for decoding (default is 1).</p> <code>1</code> <code>bin_px_size</code> <code>float</code> <p>Size of each bin in pixels for the plot (default is 0.08).</p> <code>0.08</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the posterior plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ax = plot_posteriors(bst, tc)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/decoding.py</code> <pre><code>def plot_posteriors(bst, tuningcurve, idx=None, w=1, bin_px_size=0.08):\n    \"\"\"\n    Plot posterior probabilities for decoded neural activity.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        The binned spike train array to decode.\n    tuningcurve : TuningCurve1D\n        The tuning curve used for decoding.\n    idx : array-like, optional\n        Indices of events to plot. If None, all events are plotted.\n    w : int, optional\n        Window size for decoding (default is 1).\n    bin_px_size : float, optional\n        Size of each bin in pixels for the plot (default is 0.08).\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the posterior plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; ax = plot_posteriors(bst, tc)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    if idx is not None:\n        bst = bst[idx]\n    tc = tuningcurve\n\n    # decode neural activity\n    posterior, bdries, mode_pth, mean_pth = decoding.decode1D(\n        bst=bst, ratemap=tc, xmin=tc.bins[0], xmax=tc.bins[-1], w=w\n    )\n\n    pixel_width = 0.5\n\n    n_ext, n_bins = posterior.shape\n    lengths = np.diff(bdries)\n\n    plt.figure(figsize=(bin_px_size * n_bins, 2))\n    ax = plt.gca()\n\n    imagesc(\n        x=np.arange(n_bins),\n        y=np.arange(int(tc.bins[-1] + 1)),\n        data=posterior,\n        cmap=plt.cm.Spectral_r,\n        ax=ax,\n    )\n    plotutils.yticks_interval(tc.bins[-1])\n    plotutils.no_yticks(ax)\n    # plt.imshow(posterior, cmap=plt.cm.Spectral_r, interpolation='none', aspect='auto')\n    ax.vlines(\n        np.arange(lengths.sum()) - pixel_width,\n        *ax.get_ylim(),\n        lw=1,\n        linestyle=\":\",\n        color=\"0.8\",\n    )\n    ax.vlines(np.cumsum(lengths) - pixel_width, *ax.get_ylim(), lw=1)\n\n    ax.set_xlim(-pixel_width, lengths.sum() - pixel_width)\n\n    event_centers = np.insert(np.cumsum(lengths), 0, 0)\n    event_centers = event_centers[:-1] + lengths / 2 - 0.5\n\n    ax.set_xticks(event_centers)\n    if idx is not None:\n        ax.set_xticklabels(idx)\n    else:\n        ax.set_xticklabels(np.arange(bst.n_intervals))\n\n    plotutils.no_xticks(ax)\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.plot_tuning_curves1D","title":"<code>plot_tuning_curves1D(ratemap, ax=None, normalize=False, pad=None, unit_labels=None, fill=True, color=None, alpha=0.3)</code>","text":"<p>Plot 1D tuning curves for multiple units.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>TuningCurve1D or similar</code> <p>Object with .ratemap (2D array: n_units x n_ext), .bins, .bin_centers, and .unit_labels.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize each curve to its peak value.</p> <code>False</code> <code>pad</code> <code>float</code> <p>Vertical offset between curves. If None, uses mean of ratemap / 2.</p> <code>None</code> <code>unit_labels</code> <code>list</code> <p>Labels for each unit. If None, uses ratemap.unit_labels.</p> <code>None</code> <code>fill</code> <code>bool</code> <p>Whether to fill under each curve. Default is True.</p> <code>True</code> <code>color</code> <code>color or None</code> <p>Color for all curves. If None, uses default color cycle.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Transparency for the fill. Default is 0.3.</p> <code>0.3</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted tuning curves.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot_tuning_curves1D(tc)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def plot_tuning_curves1D(\n    ratemap,\n    ax=None,\n    normalize=False,\n    pad=None,\n    unit_labels=None,\n    fill=True,\n    color=None,\n    alpha=0.3,\n):\n    \"\"\"\n    Plot 1D tuning curves for multiple units.\n\n    Parameters\n    ----------\n    ratemap : auxiliary.TuningCurve1D or similar\n        Object with .ratemap (2D array: n_units x n_ext), .bins, .bin_centers, and .unit_labels.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    normalize : bool, optional\n        If True, normalize each curve to its peak value.\n    pad : float, optional\n        Vertical offset between curves. If None, uses mean of ratemap / 2.\n    unit_labels : list, optional\n        Labels for each unit. If None, uses ratemap.unit_labels.\n    fill : bool, optional\n        Whether to fill under each curve. Default is True.\n    color : color or None, optional\n        Color for all curves. If None, uses default color cycle.\n    alpha : float, optional\n        Transparency for the fill. Default is 0.3.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted tuning curves.\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot_tuning_curves1D(tc)\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    if isinstance(ratemap, auxiliary.TuningCurve1D) | isinstance(\n        ratemap, auxiliary._tuningcurve.TuningCurve1D\n    ):\n        xmin = ratemap.bins[0]\n        xmax = ratemap.bins[-1]\n        xvals = ratemap.bin_centers\n        if unit_labels is None:\n            unit_labels = ratemap.unit_labels\n        ratemap = ratemap.ratemap\n    else:\n        raise NotImplementedError\n\n    if pad is None:\n        pad = ratemap.mean() / 2\n\n    n_units, n_ext = ratemap.shape\n\n    if normalize:\n        peak_firing_rates = ratemap.max(axis=1)\n        ratemap = (ratemap.T / peak_firing_rates).T\n\n    # determine max firing rate\n    # max_firing_rate = ratemap.max()\n\n    if xvals is None:\n        xvals = np.arange(n_ext)\n    if xmin is None:\n        xmin = xvals[0]\n    if xmax is None:\n        xmax = xvals[-1]\n\n    for unit, curve in enumerate(ratemap):\n        if color is None:\n            line = ax.plot(\n                xvals, unit * pad + curve, zorder=int(10 + 2 * n_units - 2 * unit)\n            )\n        else:\n            line = ax.plot(\n                xvals,\n                unit * pad + curve,\n                zorder=int(10 + 2 * n_units - 2 * unit),\n                color=color,\n            )\n        if fill:\n            # Get the color from the current curve\n            fillcolor = line[0].get_color()\n            ax.fill_between(\n                xvals,\n                unit * pad,\n                unit * pad + curve,\n                alpha=alpha,\n                color=fillcolor,\n                zorder=int(10 + 2 * n_units - 2 * unit - 1),\n            )\n\n    ax.set_xlim(xmin, xmax)\n    if pad != 0:\n        yticks = np.arange(n_units) * pad + 0.5 * pad\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(unit_labels)\n        ax.set_xlabel(\"external variable\")\n        ax.set_ylabel(\"unit\")\n        utils.no_yticks(ax)\n        utils.clear_left(ax)\n    else:\n        if normalize:\n            ax.set_ylabel(\"normalized firing rate\")\n        else:\n            ax.set_ylabel(\"firing rate [Hz]\")\n        ax.set_ylim(0)\n\n    utils.clear_top(ax)\n    utils.clear_right(ax)\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.plotting_context","title":"<code>plotting_context(context=None, font_scale=1, rc=None)</code>","text":"<p>Return a parameter dict to scale elements of the figure.</p> <p>This affects things like the size of the labels, lines, and other elements of the plot, but not the overall style. The base context is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\", which are versions of the notebook parameters scaled by .8, 1.3, and 1.6, respectively.</p> <p>This function returns an object that can be used in a <code>with</code> statement to temporarily change the context parameters.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict, None, or one of {paper, notebook, talk, poster}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>font_scale</code> <code>float</code> <p>Separate scaling factor to independently scale the size of the font elements.</p> <code>1</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn context dictionaries. This only updates parameters that are considered part of the context definition.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>context_object</code> <code>_PlottingContext</code> <p>An object that can be used as a context manager to temporarily set context.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c = plotting_context(\"poster\")\n&gt;&gt;&gt; c = plotting_context(\"notebook\", font_scale=1.5)\n&gt;&gt;&gt; c = plotting_context(\"talk\", rc={\"lines.linewidth\": 2})\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; with plotting_context(\"paper\"):\n...     f, ax = plt.subplots()\n...     ax.plot([0, 1], [0, 1])\n</code></pre> See Also <p>set_context : set the matplotlib parameters to scale plot elements axes_style : return a dict of parameters defining a figure style color_palette : define the color palette for a plot</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def plotting_context(context=None, font_scale=1, rc=None):\n    \"\"\"\n    Return a parameter dict to scale elements of the figure.\n\n    This affects things like the size of the labels, lines, and other\n    elements of the plot, but not the overall style. The base context\n    is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\",\n    which are versions of the notebook parameters scaled by .8, 1.3, and 1.6,\n    respectively.\n\n    This function returns an object that can be used in a ``with`` statement\n    to temporarily change the context parameters.\n\n    Parameters\n    ----------\n    context : dict, None, or one of {paper, notebook, talk, poster}\n        A dictionary of parameters or the name of a preconfigured set.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        context dictionaries. This only updates parameters that are\n        considered part of the context definition.\n\n    Returns\n    -------\n    context_object : _PlottingContext\n        An object that can be used as a context manager to temporarily set context.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c = plotting_context(\"poster\")\n    &gt;&gt;&gt; c = plotting_context(\"notebook\", font_scale=1.5)\n    &gt;&gt;&gt; c = plotting_context(\"talk\", rc={\"lines.linewidth\": 2})\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; with plotting_context(\"paper\"):\n    ...     f, ax = plt.subplots()\n    ...     ax.plot([0, 1], [0, 1])\n\n    See Also\n    --------\n    set_context : set the matplotlib parameters to scale plot elements\n    axes_style : return a dict of parameters defining a figure style\n    color_palette : define the color palette for a plot\n    \"\"\"\n    if context is None:\n        context_dict = {k: mpl.rcParams[k] for k in _context_keys}\n\n    elif isinstance(context, dict):\n        context_dict = context\n\n    else:\n        contexts = [\"paper\", \"notebook\", \"talk\", \"poster\"]\n        if context not in contexts:\n            raise ValueError(\"context must be in %s\" % \", \".join(contexts))\n\n        # Set up dictionary of default parameters\n        base_context = {\n            \"figure.figsize\": np.array([8, 5.5]),\n            \"font.size\": 12,\n            \"axes.labelsize\": 11,\n            \"axes.titlesize\": 12,\n            \"xtick.labelsize\": 10,\n            \"ytick.labelsize\": 10,\n            \"legend.fontsize\": 10,\n            \"grid.linewidth\": 1,\n            \"lines.linewidth\": 1.75,\n            \"patch.linewidth\": 0.3,\n            \"lines.markersize\": 7,\n            \"lines.markeredgewidth\": 0,\n            \"xtick.major.width\": 1,\n            \"ytick.major.width\": 1,\n            \"xtick.minor.width\": 0.5,\n            \"ytick.minor.width\": 0.5,\n            \"xtick.major.pad\": 7,\n            \"ytick.major.pad\": 7,\n        }\n\n        # Scale all the parameters by the same factor depending on the context\n        scaling = dict(paper=0.8, notebook=1, talk=1.3, poster=1.6)[context]\n        context_dict = {k: v * scaling for k, v in base_context.items()}\n\n        # Now independently scale the fonts\n        font_keys = [\n            \"axes.labelsize\",\n            \"axes.titlesize\",\n            \"legend.fontsize\",\n            \"xtick.labelsize\",\n            \"ytick.labelsize\",\n            \"font.size\",\n        ]\n        font_dict = {k: context_dict[k] * font_scale for k in font_keys}\n        context_dict.update(font_dict)\n\n    # Implement hack workaround for matplotlib bug\n    # See https://github.com/mwaskom/seaborn/issues/344\n    # There is a bug in matplotlib 1.4.2 that makes points invisible when\n    # they don't have an edgewidth. It will supposedly be fixed in 1.4.3.\n    if mpl.__version__ == \"1.4.2\":\n        context_dict[\"lines.markeredgewidth\"] = 0.01\n\n    # Override these settings with the provided rc dictionary\n    if rc is not None:\n        rc = {k: v for k, v in rc.items() if k in _context_keys}\n        context_dict.update(rc)\n\n    # Wrap in a _PlottingContext object so this can be used in a with statement\n    context_object = _PlottingContext(context_dict)\n\n    return context_object\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.psdplot","title":"<code>psdplot(data, *, fs=None, window=None, nfft=None, detrend='constant', return_onesided=True, scaling='density', ax=None)</code>","text":"<p>Plot the power spectrum of a regularly-sampled time-domain signal.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>RegularlySampledAnalogSignalArray</code> <p>The input signal to analyze. Must be a 1D regularly sampled signal.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of the time series in Hz. Defaults to data.fs if available.</p> <code>None</code> <code>window</code> <code>str or tuple or array_like</code> <p>Desired window to use. See scipy.signal.get_window for options. If an array, used directly as the window. Defaults to None ('boxcar').</p> <code>None</code> <code>nfft</code> <code>int</code> <p>Length of the FFT used. If None, the length of data will be used.</p> <code>None</code> <code>detrend</code> <code>str or function</code> <p>Specifies how to detrend data prior to computing the spectrum. If a string, passed as the type argument to detrend. If a function, should return a detrended array. Defaults to 'constant'.</p> <code>'constant'</code> <code>return_onesided</code> <code>bool</code> <p>If True, return a one-sided spectrum for real data. If False, return a two-sided spectrum. For complex data, always returns two-sided spectrum.</p> <code>True</code> <code>scaling</code> <code>(density, spectrum)</code> <p>Selects between computing the power spectral density ('density', units V2/Hz) and the power spectrum ('spectrum', units V2). Defaults to 'density'.</p> <code>'density'</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, creates a new figure and axis.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted power spectrum.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.plotting.core import psdplot\n&gt;&gt;&gt; ax = psdplot(my_signal)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def psdplot(\n    data,\n    *,\n    fs=None,\n    window=None,\n    nfft=None,\n    detrend=\"constant\",\n    return_onesided=True,\n    scaling=\"density\",\n    ax=None,\n):\n    \"\"\"\n    Plot the power spectrum of a regularly-sampled time-domain signal.\n\n    Parameters\n    ----------\n    data : RegularlySampledAnalogSignalArray\n        The input signal to analyze. Must be a 1D regularly sampled signal.\n    fs : float, optional\n        Sampling frequency of the time series in Hz. Defaults to data.fs if available.\n    window : str or tuple or array_like, optional\n        Desired window to use. See scipy.signal.get_window for options. If an array, used directly as the window. Defaults to None ('boxcar').\n    nfft : int, optional\n        Length of the FFT used. If None, the length of data will be used.\n    detrend : str or function, optional\n        Specifies how to detrend data prior to computing the spectrum. If a string, passed as the type argument to detrend. If a function, should return a detrended array. Defaults to 'constant'.\n    return_onesided : bool, optional\n        If True, return a one-sided spectrum for real data. If False, return a two-sided spectrum. For complex data, always returns two-sided spectrum.\n    scaling : {'density', 'spectrum'}, optional\n        Selects between computing the power spectral density ('density', units V**2/Hz) and the power spectrum ('spectrum', units V**2). Defaults to 'density'.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, creates a new figure and axis.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted power spectrum.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.plotting.core import psdplot\n    &gt;&gt;&gt; ax = psdplot(my_signal)\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if isinstance(data, core.RegularlySampledAnalogSignalArray):\n        if fs is None:\n            fs = data.fs\n        if fs is None:\n            raise ValueError(\n                \"The sampling rate fs cannot be inferred, and must be specified manually!\"\n            )\n        if data.n_signals &gt; 1:\n            raise NotImplementedError(\n                \"more than one signal is not yet supported for psdplot!\"\n            )\n        else:\n            data = data.data.squeeze()\n    else:\n        raise NotImplementedError(\n            \"datatype {} not yet supported by psdplot!\".format(str(type(data)))\n        )\n\n    kwargs = {\n        \"x\": data,\n        \"fs\": fs,\n        \"window\": window,\n        \"nfft\": nfft,\n        \"detrend\": detrend,\n        \"return_onesided\": return_onesided,\n        \"scaling\": scaling,\n    }\n\n    f, Pxx_den = signal.periodogram(**kwargs)\n\n    if scaling == \"density\":\n        ax.semilogy(f, np.sqrt(Pxx_den))\n        ax.set_ylabel(\"PSD [V**2/Hz]\")\n    elif scaling == \"spectrum\":\n        ax.semilogy(f, np.sqrt(Pxx_den))\n        ax.set_ylabel(\"Linear spectrum [V RMS]\")\n    ax.set_xlabel(\"frequency [Hz]\")\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.rastercountplot","title":"<code>rastercountplot(spiketrain, nbins=50, **kwargs)</code>","text":"<p>Plot a raster plot and spike count histogram for a SpikeTrainArray.</p> <p>Parameters:</p> Name Type Description Default <code>spiketrain</code> <code>SpikeTrainArray</code> <p>The spike train data to plot.</p> required <code>nbins</code> <code>int</code> <p>Number of bins for the histogram. Default is 50.</p> <code>50</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to rasterplot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax1</code> <code>Axes</code> <p>The axis with the histogram plot.</p> <code>ax2</code> <code>Axes</code> <p>The axis with the raster plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import SpikeTrainArray\n&gt;&gt;&gt; sta = SpikeTrainArray([[1, 2, 3], [2, 4, 6]], fs=10)\n&gt;&gt;&gt; rastercountplot(sta, nbins=20)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def rastercountplot(spiketrain, nbins=50, **kwargs):\n    \"\"\"\n    Plot a raster plot and spike count histogram for a SpikeTrainArray.\n\n    Parameters\n    ----------\n    spiketrain : nelpy.SpikeTrainArray\n        The spike train data to plot.\n    nbins : int, optional\n        Number of bins for the histogram. Default is 50.\n    **kwargs : dict\n        Additional keyword arguments passed to rasterplot.\n\n    Returns\n    -------\n    ax1 : matplotlib.axes.Axes\n        The axis with the histogram plot.\n    ax2 : matplotlib.axes.Axes\n        The axis with the raster plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import SpikeTrainArray\n    &gt;&gt;&gt; sta = SpikeTrainArray([[1, 2, 3], [2, 4, 6]], fs=10)\n    &gt;&gt;&gt; rastercountplot(sta, nbins=20)\n    \"\"\"\n    plt.figure(figsize=(14, 6))\n    gs = gridspec.GridSpec(2, 1, hspace=0.01, height_ratios=[0.2, 0.8])\n    ax1 = plt.subplot(gs[0])\n    ax2 = plt.subplot(gs[1])\n\n    color = kwargs.get(\"color\", None)\n    if color is None:\n        color = \"0.4\"\n\n    ds = (spiketrain.support.stop - spiketrain.support.start) / nbins\n    flattened = spiketrain.bin(ds=ds).flatten()\n    steps = np.squeeze(flattened.data)\n    stepsx = np.linspace(\n        spiketrain.support.start, spiketrain.support.stop, num=flattened.n_bins\n    )\n\n    #     ax1.plot(stepsx, steps, drawstyle='steps-mid', color='none');\n    ax1.set_ylim([-0.5, np.max(steps) + 1])\n    rasterplot(spiketrain, ax=ax2, **kwargs)\n\n    utils.clear_left_right(ax1)\n    utils.clear_top_bottom(ax1)\n    utils.clear_top(ax2)\n\n    ax1.fill_between(stepsx, steps, step=\"mid\", color=color)\n\n    utils.sync_xlims(ax1, ax2)\n\n    return ax1, ax2\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.rasterplot","title":"<code>rasterplot(data, *, cmap=None, color=None, ax=None, lw=None, lh=None, vertstack=None, labels=None, cmap_lo=0.25, cmap_hi=0.75, **kwargs)</code>","text":"<p>Make a raster plot from a SpikeTrainArray or EventArray object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>SpikeTrainArray or EventArray</code> <p>The spike/event data to plot.</p> required <code>cmap</code> <code>matplotlib colormap</code> <p>Colormap to use for the raster lines.</p> <code>None</code> <code>color</code> <code>matplotlib color</code> <p>Plot color; default is '0.25'.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Plot in given axis. If None, plots on current axes.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Linewidth, default is 1.5.</p> <code>None</code> <code>lh</code> <code>float</code> <p>Line height, default is 0.95.</p> <code>None</code> <code>vertstack</code> <code>bool</code> <p>If True, stack units in vertically adjacent positions. Default is False.</p> <code>None</code> <code>labels</code> <code>list</code> <p>Labels for input data units. If not specified, uses unit_labels from the input.</p> <code>None</code> <code>cmap_lo</code> <code>float</code> <p>Lower bound for colormap normalization. Default is 0.25.</p> <code>0.25</code> <code>cmap_hi</code> <code>float</code> <p>Upper bound for colormap normalization. Default is 0.75.</p> <code>0.75</code> <code>**kwargs</code> <code>dict</code> <p>Other keyword arguments are passed to main vlines() call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis object with plot data.</p> <p>Examples:</p> <p>Instantiate a SpikeTrainArray and create a raster plot:</p> <pre><code>&gt;&gt;&gt; stdata1 = [1, 2, 4, 5, 6, 10, 20]\n&gt;&gt;&gt; stdata2 = [3, 4, 4.5, 5, 5.5, 19]\n&gt;&gt;&gt; stdata3 = [5, 12, 14, 15, 16, 18, 22, 23, 24]\n&gt;&gt;&gt; stdata4 = [5, 12, 14, 15, 16, 18, 23, 25, 32]\n\n&gt;&gt;&gt; sta1 = nelpy.SpikeTrainArray([stdata1, stdata2, stdata3,\n                                  stdata4, stdata1+stdata4],\n                                  fs=5, unit_ids=[1,2,3,4,6])\n&gt;&gt;&gt; ax = rasterplot(sta1, color=\"cyan\", lw=2, lh=2)\n</code></pre> <p>Instantiate another SpikeTrain Array, stack units, and specify labels. Note that the user-specified labels in the call to raster() will be shown instead of the unit_labels associated with the input data:</p> <pre><code>&gt;&gt;&gt; sta3 = nelpy.SpikeTrainArray([stdata1, stdata4, stdata2+stdata3],\n                                 support=ep1, fs=5, unit_ids=[10,5,12],\n                                 unit_labels=['some', 'more', 'cells'])\n&gt;&gt;&gt; rasterplot(sta3, color=plt.cm.Blues, lw=2, lh=2, vertstack=True,\n           labels=['units', 'of', 'interest'])\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def rasterplot(\n    data,\n    *,\n    cmap=None,\n    color=None,\n    ax=None,\n    lw=None,\n    lh=None,\n    vertstack=None,\n    labels=None,\n    cmap_lo=0.25,\n    cmap_hi=0.75,\n    **kwargs,\n):\n    \"\"\"\n    Make a raster plot from a SpikeTrainArray or EventArray object.\n\n    Parameters\n    ----------\n    data : nelpy.SpikeTrainArray or nelpy.EventArray\n        The spike/event data to plot.\n    cmap : matplotlib colormap, optional\n        Colormap to use for the raster lines.\n    color : matplotlib color, optional\n        Plot color; default is '0.25'.\n    ax : matplotlib.axes.Axes, optional\n        Plot in given axis. If None, plots on current axes.\n    lw : float, optional\n        Linewidth, default is 1.5.\n    lh : float, optional\n        Line height, default is 0.95.\n    vertstack : bool, optional\n        If True, stack units in vertically adjacent positions. Default is False.\n    labels : list, optional\n        Labels for input data units. If not specified, uses unit_labels from the input.\n    cmap_lo : float, optional\n        Lower bound for colormap normalization. Default is 0.25.\n    cmap_hi : float, optional\n        Upper bound for colormap normalization. Default is 0.75.\n    **kwargs : dict\n        Other keyword arguments are passed to main vlines() call.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        Axis object with plot data.\n\n    Examples\n    --------\n    Instantiate a SpikeTrainArray and create a raster plot:\n\n        &gt;&gt;&gt; stdata1 = [1, 2, 4, 5, 6, 10, 20]\n        &gt;&gt;&gt; stdata2 = [3, 4, 4.5, 5, 5.5, 19]\n        &gt;&gt;&gt; stdata3 = [5, 12, 14, 15, 16, 18, 22, 23, 24]\n        &gt;&gt;&gt; stdata4 = [5, 12, 14, 15, 16, 18, 23, 25, 32]\n\n        &gt;&gt;&gt; sta1 = nelpy.SpikeTrainArray([stdata1, stdata2, stdata3,\n                                          stdata4, stdata1+stdata4],\n                                          fs=5, unit_ids=[1,2,3,4,6])\n        &gt;&gt;&gt; ax = rasterplot(sta1, color=\"cyan\", lw=2, lh=2)\n\n    Instantiate another SpikeTrain Array, stack units, and specify labels.\n    Note that the user-specified labels in the call to raster() will be\n    shown instead of the unit_labels associated with the input data:\n\n        &gt;&gt;&gt; sta3 = nelpy.SpikeTrainArray([stdata1, stdata4, stdata2+stdata3],\n                                         support=ep1, fs=5, unit_ids=[10,5,12],\n                                         unit_labels=['some', 'more', 'cells'])\n        &gt;&gt;&gt; rasterplot(sta3, color=plt.cm.Blues, lw=2, lh=2, vertstack=True,\n                   labels=['units', 'of', 'interest'])\n    \"\"\"\n\n    # Sort out default values for the parameters\n    if ax is None:\n        ax = plt.gca()\n    if cmap is None and color is None:\n        color = \"0.25\"\n    if lw is None:\n        lw = 1.5\n    if lh is None:\n        lh = 0.95\n    if vertstack is None:\n        vertstack = False\n\n    firstplot = False\n    if not ax.findobj(match=RasterLabelData):\n        firstplot = True\n        ax.add_artist(RasterLabelData())\n\n    # override labels\n    if labels is not None:\n        series_labels = labels\n    else:\n        series_labels = []\n\n    hh = lh / 2.0  # half the line height\n\n    # Handle different types of input data\n    if isinstance(data, core.EventArray):\n        label_data = ax.findobj(match=RasterLabelData)[0].label_data\n        serieslist = [-np.inf for element in data.series_ids]\n        # no override labels so use unit_labels from input\n        if not series_labels:\n            series_labels = data.series_labels\n\n        if firstplot:\n            if vertstack:\n                minunit = 1\n                maxunit = data.n_series\n                serieslist = range(1, data.n_series + 1)\n            else:\n                minunit = np.array(data.series_ids).min()\n                maxunit = np.array(data.series_ids).max()\n                serieslist = data.series_ids\n        # see if any of the series_ids has already been plotted. If so,\n        # then merge\n        else:\n            for idx, series_id in enumerate(data.series_ids):\n                if series_id in label_data.keys():\n                    position, _ = label_data[series_id]\n                    serieslist[idx] = position\n                else:  # unit not yet plotted\n                    if vertstack:\n                        serieslist[idx] = 1 + max(\n                            int(ax.get_yticks()[-1]), max(serieslist)\n                        )\n                    else:\n                        warnings.warn(\n                            \"Spike trains may be plotted in \"\n                            \"the same vertical position as \"\n                            \"another unit\"\n                        )\n                        serieslist[idx] = data.series_ids[idx]\n\n        if firstplot:\n            minunit = int(minunit)\n            maxunit = int(maxunit)\n        else:\n            (prev_ymin, prev_ymax) = ax.findobj(match=RasterLabelData)[0].yrange\n            minunit = int(np.min([np.ceil(prev_ymin), np.min(serieslist)]))\n            maxunit = int(np.max([np.floor(prev_ymax), np.max(serieslist)]))\n\n        yrange = (minunit - 0.5, maxunit + 0.5)\n\n        if cmap is not None:\n            color_range = range(data.n_series)\n            # TODO: if we go from 0 then most colormaps are invisible at one end of the spectrum\n            colors = cmap(np.linspace(cmap_lo, cmap_hi, data.n_series))\n            for series_ii, series, color_idx in zip(serieslist, data.data, color_range):\n                ax.vlines(\n                    series,\n                    series_ii - hh,\n                    series_ii + hh,\n                    colors=colors[color_idx],\n                    lw=lw,\n                    **kwargs,\n                )\n        else:  # use a constant color:\n            for series_ii, series in zip(serieslist, data.data):\n                ax.vlines(\n                    series,\n                    series_ii - hh,\n                    series_ii + hh,\n                    colors=color,\n                    lw=lw,\n                    **kwargs,\n                )\n\n        # get existing label data so we can set some attributes\n        rld = ax.findobj(match=RasterLabelData)[0]\n\n        ax.set_ylim(yrange)\n        rld.yrange = yrange\n\n        for series_id, loc, label in zip(data.series_ids, serieslist, series_labels):\n            rld.label_data[series_id] = (loc, label)\n        serieslocs = []\n        serieslabels = []\n        for loc, label in label_data.values():\n            serieslocs.append(loc)\n            serieslabels.append(label)\n        ax.set_yticks(serieslocs)\n        ax.set_yticklabels(serieslabels)\n\n    else:\n        raise NotImplementedError(\n            \"plotting {} not yet supported\".format(str(type(data)))\n        )\n    return ax\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.reset_defaults","title":"<code>reset_defaults()</code>","text":"<p>Restore all matplotlib RC params to default settings.</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def reset_defaults():\n    \"\"\"\n    Restore all matplotlib RC params to default settings.\n    \"\"\"\n    mpl.rcParams.update(mpl.rcParamsDefault)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.reset_orig","title":"<code>reset_orig()</code>","text":"<p>Restore all matplotlib RC params to original settings (respects custom rc).</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def reset_orig():\n    \"\"\"\n    Restore all matplotlib RC params to original settings (respects custom rc).\n    \"\"\"\n    mpl.rcParams.update(_orig_rc_params)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.savefig","title":"<code>savefig(name, fig=None, formats=None, dpi=None, verbose=True, overwrite=False)</code>","text":"<p>Save a figure in one or multiple formats.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename without an extension. If an extension is present, AND if formats is empty, then the filename extension will be used.</p> required <code>fig</code> <code>Figure</code> <p>Figure to save, default uses current figure.</p> <code>None</code> <code>formats</code> <code>list</code> <p>List of formats to export. Defaults to ['pdf', 'png']</p> <code>None</code> <code>dpi</code> <code>float</code> <p>Resolution of the figure in dots per inch (DPI).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print additional output to screen.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>If True, file will be overwritten.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; ax.plot([1, 2, 3], [4, 5, 6])\n&gt;&gt;&gt; savefig(\"myplot\", fig=fig, formats=[\"png\"], overwrite=True)\n</code></pre> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def savefig(name, fig=None, formats=None, dpi=None, verbose=True, overwrite=False):\n    \"\"\"\n    Save a figure in one or multiple formats.\n\n    Parameters\n    ----------\n    name : str\n        Filename without an extension. If an extension is present,\n        AND if formats is empty, then the filename extension will be used.\n    fig : matplotlib.figure.Figure, optional\n        Figure to save, default uses current figure.\n    formats : list, optional\n        List of formats to export. Defaults to ['pdf', 'png']\n    dpi : float, optional\n        Resolution of the figure in dots per inch (DPI).\n    verbose : bool, optional\n        If True, print additional output to screen.\n    overwrite : bool, optional\n        If True, file will be overwritten.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; ax.plot([1, 2, 3], [4, 5, 6])\n    &gt;&gt;&gt; savefig(\"myplot\", fig=fig, formats=[\"png\"], overwrite=True)\n    \"\"\"\n    # Check inputs\n    # if not 0 &lt;= prop &lt;= 1:\n    #     raise ValueError(\"prop must be between 0 and 1\")\n\n    if dpi is None:\n        dpi = 300\n\n    supportedFormats = [\n        \"eps\",\n        \"jpeg\",\n        \"jpg\",\n        \"pdf\",\n        \"pgf\",\n        \"png\",\n        \"ps\",\n        \"raw\",\n        \"rgba\",\n        \"svg\",\n        \"svgz\",\n        \"tif\",\n        \"tiff\",\n    ]\n\n    name, ext = get_extension_from_filename(name)\n\n    # if no list of formats is given, use defaults\n    if formats is None and ext is None:\n        formats = [\"pdf\", \"png\"]\n    # if the filename has an extension, AND a list of extensions is given, then use only the list\n    elif formats is not None and ext is not None:\n        if not isinstance(formats, list):\n            formats = [formats]\n        print(\"WARNING! Extension in filename ignored in favor of formats list.\")\n    # if no list of extensions is given, use the extension from the filename\n    elif formats is None and ext is not None:\n        formats = [ext]\n    else:\n        pass\n\n    if fig is None:\n        fig = plt.gcf()\n\n    for extension in formats:\n        if extension not in supportedFormats:\n            print(\"WARNING! Format '{}' not supported. Aborting...\".format(extension))\n        else:\n            my_file = \"figures/{}.{}\".format(name, extension)\n\n            if os.path.isfile(my_file):\n                # file exists\n                print(\"{} already exists!\".format(my_file))\n\n                if overwrite:\n                    fig.savefig(my_file, dpi=dpi, bbox_inches=\"tight\")\n\n                    if verbose:\n                        print(\n                            \"{} saved successfully... [using overwrite]\".format(\n                                extension\n                            )\n                        )\n            else:\n                fig.savefig(my_file, dpi=dpi, bbox_inches=\"tight\")\n\n                if verbose:\n                    print(\"{} saved successfully...\".format(extension))\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.set_context","title":"<code>set_context(context=None, font_scale=1, rc=None)</code>","text":"<p>Set the plotting context parameters.</p> <p>This affects things like the size of the labels, lines, and other elements of the plot, but not the overall style. The base context is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\", which are versions of the notebook parameters scaled by .8, 1.3, and 1.6, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict, None, or one of {paper, notebook, talk, poster}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>font_scale</code> <code>float</code> <p>Separate scaling factor to independently scale the size of the font elements.</p> <code>1</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn context dictionaries. This only updates parameters that are considered part of the context definition.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_context(\"paper\")\n&gt;&gt;&gt; set_context(\"talk\", font_scale=1.4)\n&gt;&gt;&gt; set_context(\"talk\", rc={\"lines.linewidth\": 2})\n</code></pre> See Also <p>plotting_context : return a dictionary of rc parameters, or use in                    a <code>with</code> statement to temporarily set the context. set_style : set the default parameters for figure style set_palette : set the default color palette for figures</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def set_context(context=None, font_scale=1, rc=None):\n    \"\"\"\n    Set the plotting context parameters.\n\n    This affects things like the size of the labels, lines, and other\n    elements of the plot, but not the overall style. The base context\n    is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\",\n    which are versions of the notebook parameters scaled by .8, 1.3, and 1.6,\n    respectively.\n\n    Parameters\n    ----------\n    context : dict, None, or one of {paper, notebook, talk, poster}\n        A dictionary of parameters or the name of a preconfigured set.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        context dictionaries. This only updates parameters that are\n        considered part of the context definition.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set_context(\"paper\")\n    &gt;&gt;&gt; set_context(\"talk\", font_scale=1.4)\n    &gt;&gt;&gt; set_context(\"talk\", rc={\"lines.linewidth\": 2})\n\n    See Also\n    --------\n    plotting_context : return a dictionary of rc parameters, or use in\n                       a ``with`` statement to temporarily set the context.\n    set_style : set the default parameters for figure style\n    set_palette : set the default color palette for figures\n    \"\"\"\n    context_object = plotting_context(context, font_scale, rc)\n    mpl.rcParams.update(context_object)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.set_palette","title":"<code>set_palette(palette, n_colors=None, desat=None)</code>","text":"<p>Set the matplotlib color cycle using a seaborn palette.</p> <p>Parameters:</p> Name Type Description Default <code>palette</code> <code>hls | husl | matplotlib colormap | seaborn color palette</code> <p>Palette definition. Should be something that :func:<code>color_palette</code> can process.</p> required <code>n_colors</code> <code>int</code> <p>Number of colors in the cycle. The default number of colors will depend on the format of <code>palette</code>, see the :func:<code>color_palette</code> documentation for more information.</p> <code>None</code> <code>desat</code> <code>float</code> <p>Proportion to desaturate each color by.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_palette(\"Reds\")\n&gt;&gt;&gt; set_palette(\"Set1\", 8, 0.75)\n</code></pre> See Also <p>color_palette : build a color palette or set the color cycle temporarily                 in a <code>with</code> statement. set_context : set parameters to scale plot elements set_style : set the default parameters for figure style</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def set_palette(palette, n_colors=None, desat=None):\n    \"\"\"\n    Set the matplotlib color cycle using a seaborn palette.\n\n    Parameters\n    ----------\n    palette : hls | husl | matplotlib colormap | seaborn color palette\n        Palette definition. Should be something that :func:`color_palette` can process.\n    n_colors : int, optional\n        Number of colors in the cycle. The default number of colors will depend\n        on the format of ``palette``, see the :func:`color_palette`\n        documentation for more information.\n    desat : float, optional\n        Proportion to desaturate each color by.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set_palette(\"Reds\")\n    &gt;&gt;&gt; set_palette(\"Set1\", 8, 0.75)\n\n    See Also\n    --------\n    color_palette : build a color palette or set the color cycle temporarily\n                    in a ``with`` statement.\n    set_context : set parameters to scale plot elements\n    set_style : set the default parameters for figure style\n    \"\"\"\n    colors = palettes.color_palette(palette, n_colors, desat)\n    if mpl_ge_150:\n        from cycler import cycler\n\n        cyl = cycler(\"color\", colors)\n        mpl.rcParams[\"axes.prop_cycle\"] = cyl\n    else:\n        mpl.rcParams[\"axes.color_cycle\"] = list(colors)\n    mpl.rcParams[\"patch.facecolor\"] = colors[0]\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.set_style","title":"<code>set_style(style=None, rc=None)</code>","text":"<p>Set the aesthetic style of the plots.</p> <p>This affects things like the color of the axes, whether a grid is enabled by default, and other aesthetic elements.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn style dictionaries. This only updates parameters that are considered part of the style definition.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_style(\"whitegrid\")\n&gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n</code></pre> See Also <p>axes_style : return a dict of parameters or use in a <code>with</code> statement              to temporarily set the style. set_context : set parameters to scale plot elements set_palette : set the default color palette for figures</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def set_style(style=None, rc=None):\n    \"\"\"\n    Set the aesthetic style of the plots.\n\n    This affects things like the color of the axes, whether a grid is\n    enabled by default, and other aesthetic elements.\n\n    Parameters\n    ----------\n    style : dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}\n        A dictionary of parameters or the name of a preconfigured set.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        style dictionaries. This only updates parameters that are\n        considered part of the style definition.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set_style(\"whitegrid\")\n    &gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n\n    See Also\n    --------\n    axes_style : return a dict of parameters or use in a ``with`` statement\n                 to temporarily set the style.\n    set_context : set parameters to scale plot elements\n    set_palette : set the default color palette for figures\n    \"\"\"\n    style_object = axes_style(style, rc)\n    mpl.rcParams.update(style_object)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.setup","title":"<code>setup(context='notebook', style='ticks', palette='sweet', font='sans-serif', font_scale=1, rc=None)</code>","text":"<p>Set aesthetic figure parameters for matplotlib plots.</p> <p>Each set of parameters can be set directly or temporarily. See the referenced functions below for more information.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str or dict</code> <p>Plotting context parameters, see :func:<code>plotting_context</code>.</p> <code>'notebook'</code> <code>style</code> <code>str or dict</code> <p>Axes style parameters, see :func:<code>axes_style</code>.</p> <code>'ticks'</code> <code>palette</code> <code>str or sequence</code> <p>Color palette, see :func:<code>color_palette</code>.</p> <code>'sweet'</code> <code>font</code> <code>str</code> <p>Font family, see matplotlib font manager.</p> <code>'sans-serif'</code> <code>font_scale</code> <code>float</code> <p>Separate scaling factor to independently scale the size of the font elements.</p> <code>1</code> <code>rc</code> <code>dict or None</code> <p>Dictionary of rc parameter mappings to override the above.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; setup(\n...     context=\"talk\",\n...     style=\"whitegrid\",\n...     palette=\"muted\",\n...     font=\"Arial\",\n...     font_scale=1.2,\n... )\n</code></pre> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def setup(\n    context=\"notebook\",\n    style=\"ticks\",\n    palette=\"sweet\",\n    font=\"sans-serif\",\n    font_scale=1,\n    rc=None,\n):\n    \"\"\"\n    Set aesthetic figure parameters for matplotlib plots.\n\n    Each set of parameters can be set directly or temporarily. See the\n    referenced functions below for more information.\n\n    Parameters\n    ----------\n    context : str or dict, optional\n        Plotting context parameters, see :func:`plotting_context`.\n    style : str or dict, optional\n        Axes style parameters, see :func:`axes_style`.\n    palette : str or sequence, optional\n        Color palette, see :func:`color_palette`.\n    font : str, optional\n        Font family, see matplotlib font manager.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict or None, optional\n        Dictionary of rc parameter mappings to override the above.\n\n    Examples\n    --------\n    &gt;&gt;&gt; setup(\n    ...     context=\"talk\",\n    ...     style=\"whitegrid\",\n    ...     palette=\"muted\",\n    ...     font=\"Arial\",\n    ...     font_scale=1.2,\n    ... )\n    \"\"\"\n    set_context(context, font_scale)\n    set_style(style, rc={\"font.family\": font})\n    set_palette(palette=palette)\n    if rc is not None:\n        mpl.rcParams.update(rc)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.stripplot","title":"<code>stripplot(*eps, voffset=None, lw=None, labels=None)</code>","text":"<p>Plot epochs as segments on a line.</p> <p>Parameters:</p> Name Type Description Default <code>*eps</code> <code>EpochArray</code> <p>One or more EpochArray objects to plot.</p> <code>()</code> <code>voffset</code> <code>float</code> <p>Vertical offset between lines.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width.</p> <code>None</code> <code>labels</code> <code>array-like of str</code> <p>Labels for each EpochArray.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the strip plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import EpochArray\n&gt;&gt;&gt; ep1 = EpochArray([[0, 1], [2, 3]])\n&gt;&gt;&gt; ep2 = EpochArray([[4, 5], [6, 7]])\n&gt;&gt;&gt; stripplot(ep1, ep2, labels=[\"A\", \"B\"])\n</code></pre> Source code in <code>nelpy/plotting/miscplot.py</code> <pre><code>def stripplot(*eps, voffset=None, lw=None, labels=None):\n    \"\"\"\n    Plot epochs as segments on a line.\n\n    Parameters\n    ----------\n    *eps : nelpy.EpochArray\n        One or more EpochArray objects to plot.\n    voffset : float, optional\n        Vertical offset between lines.\n    lw : float, optional\n        Line width.\n    labels : array-like of str, optional\n        Labels for each EpochArray.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the strip plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import EpochArray\n    &gt;&gt;&gt; ep1 = EpochArray([[0, 1], [2, 3]])\n    &gt;&gt;&gt; ep2 = EpochArray([[4, 5], [6, 7]])\n    &gt;&gt;&gt; stripplot(ep1, ep2, labels=[\"A\", \"B\"])\n    \"\"\"\n\n    # TODO: this plot is in alpha mode; i.e., needs lots of work...\n    # TODO: list unpacking if eps is a list of EpochArrays...\n\n    fig = plt.figure(figsize=(10, 2))\n    ax0 = fig.add_subplot(111)\n\n    prop_cycle = plt.rcParams[\"axes.prop_cycle\"]\n    colors = prop_cycle.by_key()[\"color\"]\n\n    epmin = np.inf\n    epmax = -np.inf\n\n    for ii, epa in enumerate(eps):\n        epmin = np.min((epa.start, epmin))\n        epmax = np.max((epa.stop, epmax))\n\n    # WARNING TODO: this does not yet wrap the color cycler, but it's easy to do with mod arith\n    y = 0.2\n    for ii, epa in enumerate(eps):\n        ax0.hlines(y, epmin, epmax, \"0.7\")\n        for ep in epa:\n            ax0.plot(\n                [ep.start, ep.stop],\n                [y, y],\n                lw=6,\n                color=colors[ii],\n                solid_capstyle=\"round\",\n            )\n        y += 0.2\n\n    utils.clear_top(ax0)\n    #     npl.utils.clear_bottom(ax0)\n\n    if labels is None:\n        # try to get labels from epoch arrays\n        labels = [\"\"]\n        labels.extend([epa.label for epa in eps])\n    else:\n        labels.insert(0, \"\")\n\n    ax0.set_yticklabels(labels)\n\n    ax0.set_xlim(epmin - 10, epmax + 10)\n    ax0.set_ylim(0, 0.2 * (ii + 2))\n\n    utils.no_yticks(ax0)\n    utils.clear_left(ax0)\n    utils.clear_right(ax0)\n\n    return ax0\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.suptitle","title":"<code>suptitle(t, gs=None, rect=(0, 0, 1, 0.95), **kwargs)</code>","text":"<p>Add a suptitle to a figure with an embedded gridspec.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>str</code> <p>The suptitle text.</p> required <code>gs</code> <code>GridSpec</code> <p>The gridspec to use. If None, uses fig.npl_gs.</p> <code>None</code> <code>rect</code> <code>tuple</code> <p>Rectangle in figure coordinates (x1, y1, x2, y2).</p> <code>(0, 0, 1, 0.95)</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to fig.suptitle().</p> <code>{}</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If no gridspec is found in the figure.</p> See Also <p>https://matplotlib.org/users/tight_layout_guide.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def suptitle(t, gs=None, rect=(0, 0, 1, 0.95), **kwargs):\n    \"\"\"\n    Add a suptitle to a figure with an embedded gridspec.\n\n    Parameters\n    ----------\n    t : str\n        The suptitle text.\n    gs : matplotlib.gridspec.GridSpec, optional\n        The gridspec to use. If None, uses fig.npl_gs.\n    rect : tuple, optional\n        Rectangle in figure coordinates (x1, y1, x2, y2).\n    **kwargs : dict\n        Additional keyword arguments passed to fig.suptitle().\n\n    Raises\n    ------\n    AttributeError\n        If no gridspec is found in the figure.\n\n    See Also\n    --------\n    https://matplotlib.org/users/tight_layout_guide.html\n    \"\"\"\n    fig = plt.gcf()\n    if gs is None:\n        try:\n            gs = fig.npl_gs\n        except AttributeError:\n            raise AttributeError(\n                \"nelpy suptitle requires an embedded gridspec! Use the nelpy FigureManager.\"\n            )\n\n    fig.suptitle(t, **kwargs)\n    gs.tight_layout(fig, rect=rect)\n</code></pre>"},{"location":"reference/plotting/#nelpy.plotting.veva_scatter","title":"<code>veva_scatter(data, *, cmap=None, color=None, ax=None, lw=None, lh=None, **kwargs)</code>","text":"<p>Scatter plot for ValueEventArray objects, colored by value.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ValueEventArray</code> <p>The value event data to plot.</p> required <code>cmap</code> <code>matplotlib colormap</code> <p>Colormap to use for the event values.</p> <code>None</code> <code>color</code> <code>matplotlib color</code> <p>Color for the events if cmap is not specified.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width for the event markers.</p> <code>None</code> <code>lh</code> <code>float</code> <p>Line height for the event markers.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to vlines.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the scatter plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.core import ValueEventArray\n&gt;&gt;&gt; vea = ValueEventArray(\n...     [[1, 2, 3], [4, 5, 6]], values=[[10, 20, 30], [40, 50, 60]]\n... )\n&gt;&gt;&gt; veva_scatter(vea)\n</code></pre> Source code in <code>nelpy/plotting/miscplot.py</code> <pre><code>def veva_scatter(data, *, cmap=None, color=None, ax=None, lw=None, lh=None, **kwargs):\n    \"\"\"\n    Scatter plot for ValueEventArray objects, colored by value.\n\n    Parameters\n    ----------\n    data : nelpy.ValueEventArray\n        The value event data to plot.\n    cmap : matplotlib colormap, optional\n        Colormap to use for the event values.\n    color : matplotlib color, optional\n        Color for the events if cmap is not specified.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    lw : float, optional\n        Line width for the event markers.\n    lh : float, optional\n        Line height for the event markers.\n    **kwargs : dict\n        Additional keyword arguments passed to vlines.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the scatter plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.core import ValueEventArray\n    &gt;&gt;&gt; vea = ValueEventArray(\n    ...     [[1, 2, 3], [4, 5, 6]], values=[[10, 20, 30], [40, 50, 60]]\n    ... )\n    &gt;&gt;&gt; veva_scatter(vea)\n    \"\"\"\n    # Sort out default values for the parameters\n    if ax is None:\n        ax = plt.gca()\n    if cmap is None and color is None:\n        color = \"0.25\"\n    if lw is None:\n        lw = 1.5\n    if lh is None:\n        lh = 0.95\n\n    hh = lh / 2.0  # half the line height\n\n    # Handle different types of input data\n    if isinstance(data, core.ValueEventArray):\n        vmin = (\n            np.min([np.min(x) for x in data.values]) - 1\n        )  # TODO: -1 because white is invisible... fix this properly\n        vmax = np.max([np.max(x) for x in data.values])\n        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n\n        for ii, (events, values) in enumerate(zip(data.events, data.values)):\n            if cmap is not None:\n                colors = cmap(norm(values))\n            else:\n                colors = color\n            ax.vlines(events, ii - hh, ii + hh, colors=colors, lw=lw, **kwargs)\n\n    else:\n        raise NotImplementedError(\n            \"plotting {} not yet supported\".format(str(type(data)))\n        )\n    return ax\n</code></pre>"},{"location":"reference/preprocessing/","title":"Preprocessing API Reference","text":"<p>Data preprocessing objects and functions.</p>"},{"location":"reference/preprocessing/#nelpy.preprocessing.DataWindow","title":"<code>DataWindow</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>DataWindow Data window description to describe stride and/or data aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>bins_before</code> <code>(int, optional(default=0))</code> <p>How many bins before the output to include in the window.</p> <code>0</code> <code>bins_after</code> <code>(int, optional(default=0))</code> <p>How many bins after the output to include in the window.</p> <code>0</code> <code>bins_current</code> <code>(int, optional(default=1))</code> <p>Whether (1) or not (0) to include the concurrent bin in the window.</p> <code>1</code> <code>bins_stride</code> <code>(int, optional(default=1))</code> <p>Number of bins to advance the window during each time step.</p> <code>1</code> <code>bin_width</code> <code>(float, optional(default=None))</code> <p>Width of single bin (default units are in seconds).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = DataWindow(1, 1, 1, 1)\nDataWindow(bins_before=1, bins_after=1, bins_current=1, bins_stride=1, bin_width=None)\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.DataWindow--implicit-bin-size-of-1-second-centered-window-of-duration-5-seconds-stride-of-2-seconds","title":"Implicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:","text":"<pre><code>&gt;&gt;&gt; w = DataWindow(2, 2, 1, 2)\nDataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2)\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.DataWindow--excplicit-bin-size-of-1-second-centered-window-of-duration-5-seconds-stride-of-2-seconds","title":"Excplicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:","text":"<pre><code>&gt;&gt;&gt; w = DataWindow(2, 2, 1, 2, 1)\nDataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2, bin_width=1)\n        Total bin width = 5 seconds\n</code></pre> Source code in <code>nelpy/preprocessing.py</code> <pre><code>class DataWindow(BaseEstimator):\n    \"\"\"\n    DataWindow\n    Data window description to describe stride and/or data aggregation.\n\n    Parameters\n    ----------\n    bins_before : int, optional (default=0)\n        How many bins before the output to include in the window.\n    bins_after : int, optional (default=0)\n        How many bins after the output to include in the window.\n    bins_current : int, optional (default=1)\n        Whether (1) or not (0) to include the concurrent bin in the window.\n    bins_stride : int, optional (default=1)\n        Number of bins to advance the window during each time step.\n    bin_width : float, optional (default=None)\n        Width of single bin (default units are in seconds).\n\n    Examples\n    --------\n    &gt;&gt;&gt; w = DataWindow(1, 1, 1, 1)\n    DataWindow(bins_before=1, bins_after=1, bins_current=1, bins_stride=1, bin_width=None)\n\n    # Implicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:\n    &gt;&gt;&gt; w = DataWindow(2, 2, 1, 2)\n    DataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2)\n\n    # Excplicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:\n    &gt;&gt;&gt; w = DataWindow(2, 2, 1, 2, 1)\n    DataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2, bin_width=1)\n            Total bin width = 5 seconds\n    \"\"\"\n\n    def __init__(\n        self,\n        bins_before=0,\n        bins_after=0,\n        bins_current=1,\n        bins_stride=1,\n        bin_width=None,\n        flatten=False,\n        sum=False,\n    ):\n        self.bins_before = bins_before\n        self.bins_after = bins_after\n        self.bins_current = bins_current\n        self.bins_stride = bins_stride\n        self.bin_width = bin_width\n        self._flatten = flatten\n        self._sum = sum\n\n    def __str__(self):\n        if self.bin_width is not None:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={}, bin_width={})\".format(\n                self._bins_before,\n                self._bins_after,\n                self._bins_current,\n                self._bins_stride,\n                self._bin_width,\n            )\n        else:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={})\".format(\n                self._bins_before,\n                self._bins_after,\n                self._bins_current,\n                self._bins_stride,\n            )\n        return repr_string\n\n    def __repr__(self):\n        if self.bin_width is not None:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={}, bin_width={})\".format(\n                self.bins_before,\n                self.bins_after,\n                self.bins_current,\n                self.bins_stride,\n                self.bin_width,\n            )\n            repr_string += \"\\n\\tTotal bin width = {}\".format(\n                PrettyDuration(\n                    (self.bins_before + self.bins_after + self.bins_current)\n                    * self.bin_width\n                )\n            )\n        else:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={})\".format(\n                self.bins_before, self.bins_after, self.bins_current, self.bins_stride\n            )\n        return repr_string\n\n    def fit(self, X, y=None, *, T=None, lengths=None, flatten=None):\n        \"\"\"Dummy fit function to support sklearn pipelines.\n        Parameters\n        ----------\n        X\n            Ignored\n        y\n            Ignored\n        flatten : bool, optional (default=False)\n            Whether or not to flatten the output data during transformation.\n        \"\"\"\n        if flatten is not None:\n            self._flatten = flatten\n\n        bins_before = self.bins_before\n        bins_after = self.bins_after\n        # bins_current = self.bins_current\n        stride = self.bins_stride\n\n        X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n        L = np.insert(np.cumsum(lengths), 0, 0)\n        idx = []\n        n_zamples_tot = 0\n        for kk, (ii, jj) in enumerate(self._iter_from_X_lengths(X=X, lengths=lengths)):\n            X_ = X[ii:jj]  # , T[ii:jj]\n            n_samples, n_features = X_.shape\n            n_zamples = int(np.ceil((n_samples - bins_before - bins_after) / stride))\n            n_zamples_tot += n_zamples\n            idx += list(\n                L[kk] + np.array(range(bins_before, n_samples - bins_after, stride))\n            )\n\n        self.n_samples = n_zamples_tot\n        self.idx = idx\n        self.T = T[idx]\n        return self\n\n    def transform(self, X, T=None, lengths=None, flatten=None, sum=None):\n        \"\"\"\n        Apply window specification to data in X.\n\n        NOTE: this function is epoch-aware.\n\n        WARNING: this function works in-core, and may use a lot of memory\n                 to represent the unwrapped (windowed) data. If you have\n                 a large dataset, using the streaming version may be better.\n\n        Parameters\n        ----------\n        X : numpy 2d array of shape (n_samples, n_features)\n                OR\n            array-like of shape (n_epochs, ), each element of which is\n            a numpy 2d array of shape (n_samples, n_features)\n                OR\n            nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n                The number of spikes in each time bin for each neuron/unit.\n        T : array-like of shape (n_samples,), optional (default=None)\n                Timestamps / sample numbers corresponding to data in X.\n        lengths : array-like, optional (default=None)\n                Only used / allowed when X is a 2d numpy array, in which case\n                sum(lengths) must equal n_samples.\n                Array of lengths (in number of bins) for each contiguous segment\n                in X.\n        flatten : int, optional (default=False)\n            Whether or not to flatten the output data.\n        sum : boolean, optional (default=False)\n            Whether or not to sum all the spikes in the window per time bin. If\n            sum==True, then the dimensions of Z will be (n_samples, n_features).\n\n        Returns\n        -------\n        Z : Windowed data of shape (n_samples, window_size, n_features).\n            Note that n_samples in the output may not be the same as n_samples\n            in the input, since window specifications can affect which and how\n            many samples to return.\n            When flatten is True, then Z has shape (n_samples, window_size*n_features).\n            When sum is True, then Z has shape (n_samples, n_features)\n        T : array-like of shape (n_samples,)\n            Timestamps associated with data contained in Z.\n        \"\"\"\n        if flatten is None:\n            flatten = self._flatten\n\n        if sum is None:\n            sum = self._sum\n\n        X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n        z = []\n        t = []\n        for ii, jj in self._iter_from_X_lengths(X=X, lengths=lengths):\n            x, tx = self._apply_contiguous(X[ii:jj], T[ii:jj], flatten=flatten, sum=sum)\n            if x is not None:\n                z.append(x)\n                t.extend(tx)\n\n        Z = np.vstack(z)\n        T = np.array(t)\n\n        return Z, T\n\n    def _apply_contiguous(self, X, T=None, flatten=None, sum=False):\n        \"\"\"\n        Apply window specification to data in X.\n\n        NOTE: this function works on a single epoch only (i.e. assumes data\n              is contiguous).\n\n        NOTE: instead of returning partial data (with NaNs filling the rest),\n              we only return those bins (windows) whose specifications are wholly\n              contained in the data, similar to how binning in nelpy only includes\n              those bins that fit wholly in the data support.\n\n        WARNING: this function works in-core, and may use a lot of memory\n                 to represent the unwrapped (windowed) data. If you have\n                 a large dataset, using the streaming version may be better.\n\n        Parameters\n        ----------\n        X : numpy 2d array of shape (n_samples, n_features)\n        T : array-like of shape (n_samples,), optional (default=None)\n                Timestamps / sample numbers corresponding to data in X.\n        flatten : int, optional (default=False)\n            Whether or not to flatten the output data.\n        sum : boolean, optional (default=False)\n            Whether or not to sum all the spikes in the window per time bin. If\n            sum==True, then the dimensions of Z will be (n_samples, n_features).\n\n        Returns\n        -------\n        Z : Windowed data of shape (n_samples, window_size, n_features).\n            Note that n_samples in the output may not be the same as n_samples\n            in the input, since window specifications can affect which and how\n            many samples to return.\n            When flatten is True, then Z has shape (n_samples, window_size*n_features).\n        T : array-like of shape (n_samples,)\n            Timestamps associated with data contained in Z.\n        \"\"\"\n        if flatten is None:\n            flatten = self._flatten\n\n        bins_before = self.bins_before\n        bins_after = self.bins_after\n        bins_current = self.bins_current\n        stride = self.bins_stride\n\n        n_samples, n_features = X.shape\n        n_zamples = int(np.ceil((n_samples - bins_before - bins_after) / stride))\n\n        if n_zamples &lt; 1:\n            Z = None\n            T = None\n            return Z, T\n\n        Z = np.empty([n_zamples, bins_before + bins_after + bins_current, n_features])\n        Z[:] = np.nan\n\n        frm_idx = 0\n        curr_idx = bins_before\n\n        for zz in range(n_zamples):\n            if bins_current == 1:\n                idx = np.arange(\n                    frm_idx, frm_idx + bins_before + bins_after + bins_current\n                )\n            else:\n                idx = list(range(frm_idx, frm_idx + bins_before))\n                idx.extend(\n                    list(\n                        range(\n                            frm_idx + bins_before + 1,\n                            frm_idx + bins_before + 1 + bins_after,\n                        )\n                    )\n                )\n\n            #     print('{}  @ {}'.format(idx, curr_idx))\n\n            Z[zz, :] = X[idx, :]\n            curr_idx += stride\n            frm_idx += stride\n\n        if sum:\n            Z = Z.sum(axis=1)\n        elif flatten:\n            Z = Z.reshape(Z.shape[0], (Z.shape[1] * Z.shape[2]))\n\n        if T is not None:\n            t_idx = list(range(bins_before, n_samples - bins_after, stride))\n            T = T[t_idx]\n\n        return Z, T\n\n    def stream(self, X, chunk_size=1, flatten=False):\n        \"\"\"Streaming window specification on data X.\n\n        Q. Should this return a generator? Should it BE a generator? I think we\n            should return an iterable?\n\n        Examples\n        --------\n        &gt;&gt;&gt; w = DataWindow()\n        &gt;&gt;&gt; ws = w.stream(X)\n        &gt;&gt;&gt; for x in ws:\n                print(x)\n\n        \"\"\"\n        X, T, lengths = self._tidy(X)\n        return StreamingDataWindow(self, X=X, flatten=flatten)\n\n    def _tidy(self, X, T=None, lengths=None):\n        \"\"\"Transform data into a tidy, standardized, minimalist form.\n\n        NOTE: No windowing is present in tidy data; windowing is APPLIED\n              to tidy data when using DataWindow.apply().\n\n        Parameters\n        ----------\n        X : numpy 2d array of shape (n_samples, n_features)\n                OR\n            array-like of shape (n_epochs, ), each element of which is\n            a numpy 2d array of shape (n_samples, n_features)\n                OR\n            nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n                The number of spikes in each time bin for each neuron/unit.\n        T : array-like of shape (n_samples,), optional (default=None)\n                Timestamps / sample numbers corresponding to data in X.\n        lengths : array-like, optional (default=None)\n                Only used / allowed when X is a 2d numpy array, in which case\n                sum(lengths) must equal n_samples.\n                Array of lengths (in number of bins) for each contiguous segment\n                in X.\n\n        Returns\n        -------\n        tidyX : numpy 2d array of shape (n_samples, n_features)\n            The number of spikes in each time bin for each neuron/unit.\n        tidyT : array-like of shape (n_samples,)\n            Timestamps / sample numbers corresponding to data in X.\n        lengths : array-like\n            Array of lengths (in number of bins) for each contiguous segment\n            in tidyX.\n\n        Examples\n        --------\n\n        X = np.zeros((20, 8))\n        X = [np.zeros((20,50)), np.zeros((30, 50)), np.zeros((80, 50))]\n        X = [np.zeros((20,50)), np.zeros((30, 50)), np.zeros((80, 30))]\n        w = DataWindow(bin_width=0.02)\n\n        X, T, lengths = w._tidy(X)\n        X, T, lengths = w._tidy(X, T=np.arange(50))\n        X, T, lengths = w._tidy(X, lengths=[20,5,10])\n        \"\"\"\n\n        # here we should transform BSTs, numpy arrays, check for dimensions, etc\n        if isinstance(X, core.BinnedEventArray):\n            if self._bin_width is not None:\n                if self._bin_width != X.ds:\n                    raise ValueError(\n                        \"The DataWindow has ``bin_width``={}, whereas ``X.ds``={}.\".format(\n                            self._bin_width, X.ds\n                        )\n                    )\n\n            if (T is not None) or (lengths is not None):\n                logging.warning(\n                    \"A {} was passed in, so 'T' and 'lengths' will be ignored...\".format(\n                        X.type_name\n                    )\n                )\n\n            T = X.bin_centers\n            lengths = X.lengths\n            X = X.data.T\n\n            return X, T, lengths\n\n        try:\n            x = X[0, 0]\n            if X.ndim != 2:\n                raise ValueError(\n                    \"X is expected to be array-like with shape (n_samples, n_features).\"\n                )\n            n_samples, n_features = X.shape\n            if lengths is not None:\n                tot_length = np.sum(lengths)\n                if tot_length != n_samples:\n                    raise ValueError(\n                        \"The sum of ``lengths`` should equal ``n_samples``. [sum(lengths)={}; n_samples={}]\".format(\n                            tot_length, n_samples\n                        )\n                    )\n        except (IndexError, TypeError):\n            try:\n                x = X[0]\n                if x.ndim != 2:\n                    raise ValueError(\n                        \"Each element of X is expected to be array-like with shape (n_samples, n_features).\"\n                    )\n                if lengths is not None:\n                    raise ValueError(\n                        \"``lengths`` should not be specified when the shape of X is (n_epochs,)\"\n                    )\n                n_samples, n_features = x.shape\n                lengths = []\n                for x in X:\n                    lengths.append(x.shape[0])\n                    if x.ndim != 2:\n                        raise ValueError(\n                            \"Each element of X is expected to be array-like with shape (n_samples, n_features).\"\n                        )\n                    if x.shape[1] != n_features:\n                        raise ValueError(\n                            \"Each element of X is expected to have the same number of features.\"\n                        )\n                X = np.vstack(X)\n            except (IndexError, TypeError):\n                raise TypeError(\n                    \"Windowing of type {} not supported!\".format(str(type(X)))\n                )\n        n_samples, n_features = X.shape\n        if T is not None:\n            assert len(T) == n_samples, (\n                \"T must have the same number of elements as n_samples.\"\n            )\n        else:\n            if self._bin_width is not None:\n                ds = self._bin_width\n            else:\n                ds = 1\n            T = np.arange(n_samples) * ds + ds / 2\n\n        return X, T, lengths\n\n    def _iter_from_X_lengths(self, X, lengths=None):\n        \"\"\"\n        Helper function to iterate over contiguous segments of data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n                Feature matrix of individual samples.\n                Typically the number of spikes in each time bin for each neuron.\n        lengths : array-like of integers, shape (n_epochs, ), optional\n                Lengths of the individual epochs in ``X``. The sum of\n                these should be ``n_samples``.\n                Array of lengths (in number of bins) for each contiguous segment\n                in X.\n\n        Returns\n        -------\n        start, end : indices of a contiguous segment in data, so that\n                     segment = data[start:end]\n        \"\"\"\n\n        if X.ndim != 2:\n            raise ValueError(\n                \"X is expected to be array-like with shape (n_samples, n_features).\"\n            )\n\n        n_samples = X.shape[0]\n\n        if lengths is None:\n            try:\n                yield 0, n_samples\n            except StopIteration:\n                return\n        else:\n            end = np.cumsum(lengths).astype(np.int)\n\n            if end[-1] != n_samples:\n                raise ValueError(\n                    \"The sum of ``lengths`` should equal ``n_samples``. [sum(lengths)={}; n_samples={}]\".format(\n                        end[-1], n_samples\n                    )\n                )\n\n            start = end - lengths\n\n            for i in range(len(lengths)):\n                try:\n                    yield start[i], end[i]\n                except StopIteration:\n                    return\n\n    @property\n    def bins_before(self):\n        return self._bins_before\n\n    @bins_before.setter\n    def bins_before(self, val):\n        assert float(val).is_integer(), (\n            \"``bins_before`` must be a non-negative integer!\"\n        )\n        assert val &gt;= 0, \"``bins_before`` must be a non-negative integer!\"\n        self._bins_before = int(val)\n\n    @property\n    def bins_after(self):\n        return self._bins_after\n\n    @bins_after.setter\n    def bins_after(self, val):\n        assert float(val).is_integer(), \"``bins_after`` must be a non-negative integer!\"\n        assert val &gt;= 0, \"``bins_after`` must be a non-negative integer!\"\n        self._bins_after = int(val)\n\n    @property\n    def bins_current(self):\n        return self._bins_current\n\n    @bins_current.setter\n    def bins_current(self, val):\n        assert float(val).is_integer(), \"``bins_current`` must be a either 1 or 0!\"\n        assert val in [0, 1], \"``bins_current`` must be a either 1 or 0!\"\n        self._bins_current = int(val)\n\n    @property\n    def bins_stride(self):\n        return self._bins_stride\n\n    @bins_stride.setter\n    def bins_stride(self, val):\n        assert float(val).is_integer(), (\n            \"``bins_stride`` must be a non-negative integer!\"\n        )\n        assert val &gt;= 0, \"``bins_stride`` must be a non-negative integer!\"\n        self._bins_stride = int(val)\n\n    @property\n    def bin_width(self):\n        return self._bin_width\n\n    @bin_width.setter\n    def bin_width(self, val):\n        if val is not None:\n            assert float(val) &gt; 0, (\n                \"``bin_width`` must be a non-negative number (float)!\"\n            )\n        self._bin_width = val\n\n    @property\n    def flatten(self):\n        return self._flatten\n\n    @flatten.setter\n    def flatten(self, val):\n        try:\n            if val:\n                val = True\n        except Exception:\n            val = False\n        self._flatten = val\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.DataWindow.fit","title":"<code>fit(X, y=None, *, T=None, lengths=None, flatten=None)</code>","text":"<p>Dummy fit function to support sklearn pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Ignored</p> required <code>y</code> <p>Ignored</p> <code>None</code> <code>flatten</code> <code>(bool, optional(default=False))</code> <p>Whether or not to flatten the output data during transformation.</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def fit(self, X, y=None, *, T=None, lengths=None, flatten=None):\n    \"\"\"Dummy fit function to support sklearn pipelines.\n    Parameters\n    ----------\n    X\n        Ignored\n    y\n        Ignored\n    flatten : bool, optional (default=False)\n        Whether or not to flatten the output data during transformation.\n    \"\"\"\n    if flatten is not None:\n        self._flatten = flatten\n\n    bins_before = self.bins_before\n    bins_after = self.bins_after\n    # bins_current = self.bins_current\n    stride = self.bins_stride\n\n    X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n    L = np.insert(np.cumsum(lengths), 0, 0)\n    idx = []\n    n_zamples_tot = 0\n    for kk, (ii, jj) in enumerate(self._iter_from_X_lengths(X=X, lengths=lengths)):\n        X_ = X[ii:jj]  # , T[ii:jj]\n        n_samples, n_features = X_.shape\n        n_zamples = int(np.ceil((n_samples - bins_before - bins_after) / stride))\n        n_zamples_tot += n_zamples\n        idx += list(\n            L[kk] + np.array(range(bins_before, n_samples - bins_after, stride))\n        )\n\n    self.n_samples = n_zamples_tot\n    self.idx = idx\n    self.T = T[idx]\n    return self\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.DataWindow.stream","title":"<code>stream(X, chunk_size=1, flatten=False)</code>","text":"<p>Streaming window specification on data X.</p> <p>Q. Should this return a generator? Should it BE a generator? I think we     should return an iterable?</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = DataWindow()\n&gt;&gt;&gt; ws = w.stream(X)\n&gt;&gt;&gt; for x in ws:\n        print(x)\n</code></pre> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def stream(self, X, chunk_size=1, flatten=False):\n    \"\"\"Streaming window specification on data X.\n\n    Q. Should this return a generator? Should it BE a generator? I think we\n        should return an iterable?\n\n    Examples\n    --------\n    &gt;&gt;&gt; w = DataWindow()\n    &gt;&gt;&gt; ws = w.stream(X)\n    &gt;&gt;&gt; for x in ws:\n            print(x)\n\n    \"\"\"\n    X, T, lengths = self._tidy(X)\n    return StreamingDataWindow(self, X=X, flatten=flatten)\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.DataWindow.transform","title":"<code>transform(X, T=None, lengths=None, flatten=None, sum=None)</code>","text":"<p>Apply window specification to data in X.</p> <p>NOTE: this function is epoch-aware.</p> <p>WARNING: this function works in-core, and may use a lot of memory          to represent the unwrapped (windowed) data. If you have          a large dataset, using the streaming version may be better.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy 2d array of shape (n_samples, n_features)</code> <pre><code>OR\n</code></pre> <p>array-like of shape (n_epochs, ), each element of which is a numpy 2d array of shape (n_samples, n_features)     OR nelpy.core.BinnedEventArray / BinnedSpikeTrainArray     The number of spikes in each time bin for each neuron/unit.</p> required <code>T</code> <code>array-like of shape (n_samples,), optional (default=None)</code> <pre><code>Timestamps / sample numbers corresponding to data in X.\n</code></pre> <code>None</code> <code>lengths</code> <code>(array - like, optional(default=None))</code> <pre><code>Only used / allowed when X is a 2d numpy array, in which case\nsum(lengths) must equal n_samples.\nArray of lengths (in number of bins) for each contiguous segment\nin X.\n</code></pre> <code>None</code> <code>flatten</code> <code>(int, optional(default=False))</code> <p>Whether or not to flatten the output data.</p> <code>None</code> <code>sum</code> <code>(boolean, optional(default=False))</code> <p>Whether or not to sum all the spikes in the window per time bin. If sum==True, then the dimensions of Z will be (n_samples, n_features).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Z</code> <code>Windowed data of shape (n_samples, window_size, n_features).</code> <p>Note that n_samples in the output may not be the same as n_samples in the input, since window specifications can affect which and how many samples to return. When flatten is True, then Z has shape (n_samples, window_size*n_features). When sum is True, then Z has shape (n_samples, n_features)</p> <code>T</code> <code>array-like of shape (n_samples,)</code> <p>Timestamps associated with data contained in Z.</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def transform(self, X, T=None, lengths=None, flatten=None, sum=None):\n    \"\"\"\n    Apply window specification to data in X.\n\n    NOTE: this function is epoch-aware.\n\n    WARNING: this function works in-core, and may use a lot of memory\n             to represent the unwrapped (windowed) data. If you have\n             a large dataset, using the streaming version may be better.\n\n    Parameters\n    ----------\n    X : numpy 2d array of shape (n_samples, n_features)\n            OR\n        array-like of shape (n_epochs, ), each element of which is\n        a numpy 2d array of shape (n_samples, n_features)\n            OR\n        nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n            The number of spikes in each time bin for each neuron/unit.\n    T : array-like of shape (n_samples,), optional (default=None)\n            Timestamps / sample numbers corresponding to data in X.\n    lengths : array-like, optional (default=None)\n            Only used / allowed when X is a 2d numpy array, in which case\n            sum(lengths) must equal n_samples.\n            Array of lengths (in number of bins) for each contiguous segment\n            in X.\n    flatten : int, optional (default=False)\n        Whether or not to flatten the output data.\n    sum : boolean, optional (default=False)\n        Whether or not to sum all the spikes in the window per time bin. If\n        sum==True, then the dimensions of Z will be (n_samples, n_features).\n\n    Returns\n    -------\n    Z : Windowed data of shape (n_samples, window_size, n_features).\n        Note that n_samples in the output may not be the same as n_samples\n        in the input, since window specifications can affect which and how\n        many samples to return.\n        When flatten is True, then Z has shape (n_samples, window_size*n_features).\n        When sum is True, then Z has shape (n_samples, n_features)\n    T : array-like of shape (n_samples,)\n        Timestamps associated with data contained in Z.\n    \"\"\"\n    if flatten is None:\n        flatten = self._flatten\n\n    if sum is None:\n        sum = self._sum\n\n    X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n    z = []\n    t = []\n    for ii, jj in self._iter_from_X_lengths(X=X, lengths=lengths):\n        x, tx = self._apply_contiguous(X[ii:jj], T[ii:jj], flatten=flatten, sum=sum)\n        if x is not None:\n            z.append(x)\n            t.extend(tx)\n\n    Z = np.vstack(z)\n    T = np.array(t)\n\n    return Z, T\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.StandardScaler","title":"<code>StandardScaler</code>","text":"<p>               Bases: <code>StandardScaler</code></p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>class StandardScaler(SklearnStandardScaler):\n    def __init__(self, copy=True, with_mean=True, with_std=True):\n        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n\n    def fit(self, X, y=None):\n        \"\"\"Compute the mean and std to be used for later scaling.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n        y\n            Ignored\n        \"\"\"\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            X = X.data.T\n\n        return super().fit(X, y)\n\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Online computation of mean and std on X for later scaling.\n        All of X is processed as a single batch. This is intended for cases\n        when `fit` is not feasible due to very large number of `n_samples`\n        or because X is read from a continuous stream.\n        The algorithm for incremental mean and std is given in Equation 1.5a,b\n        in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n        for computing the sample variance: Analysis and recommendations.\"\n        The American Statistician 37.3 (1983): 242-247:\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n        y\n            Ignored\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n        \"\"\"\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            X = X.data.T\n\n        return super().partial_fit(X, y, sample_weight)\n\n    def transform(self, X, copy=None):\n        \"\"\"Perform standardization by centering and scaling\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n\n        if copy is None:\n            copy = self.copy\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            if copy:\n                Xdata = copycopy(X.data.T)\n                X = X.copy()\n            else:\n                Xdata = X.data.T\n            Xdata = super().transform(Xdata, copy).T\n\n            X._data = Xdata\n        else:\n            X = super().transform(X, copy)\n        return X\n\n    def inverse_transform(self, X, copy=None):\n        \"\"\"Scale back the data to the original representation\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        Returns\n        -------\n        X_tr : array-like, shape [n_samples, n_features]\n            Transformed array.\n        \"\"\"\n\n        if copy is None:\n            copy = self.copy\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            if copy:\n                Xdata = copycopy(X.data.T)\n                X = X.copy()\n            else:\n                Xdata = X.data.T\n            Xdata = super().inverse_transform(Xdata, copy).T\n\n            X._data = Xdata\n        else:\n            X = super().inverse_transform(X, copy)\n\n        return X\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.StandardScaler.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the mean and std to be used for later scaling.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, sparse matrix</code> <p>The data used to compute the mean and standard deviation used for later scaling along the features axis.</p> <code>array-like</code> <code>y</code> <p>Ignored</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Compute the mean and std to be used for later scaling.\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        The data used to compute the mean and standard deviation\n        used for later scaling along the features axis.\n    y\n        Ignored\n    \"\"\"\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        X = X.data.T\n\n    return super().fit(X, y)\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.StandardScaler.inverse_transform","title":"<code>inverse_transform(X, copy=None)</code>","text":"<p>Scale back the data to the original representation</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape[n_samples, n_features])</code> <p>The data used to scale along the features axis.</p> required <code>copy</code> <code>bool, optional (default: None)</code> <p>Copy the input X or not.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_tr</code> <code>(array - like, shape[n_samples, n_features])</code> <p>Transformed array.</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def inverse_transform(self, X, copy=None):\n    \"\"\"Scale back the data to the original representation\n    Parameters\n    ----------\n    X : array-like, shape [n_samples, n_features]\n        The data used to scale along the features axis.\n    copy : bool, optional (default: None)\n        Copy the input X or not.\n    Returns\n    -------\n    X_tr : array-like, shape [n_samples, n_features]\n        Transformed array.\n    \"\"\"\n\n    if copy is None:\n        copy = self.copy\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        if copy:\n            Xdata = copycopy(X.data.T)\n            X = X.copy()\n        else:\n            Xdata = X.data.T\n        Xdata = super().inverse_transform(Xdata, copy).T\n\n        X._data = Xdata\n    else:\n        X = super().inverse_transform(X, copy)\n\n    return X\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.StandardScaler.partial_fit","title":"<code>partial_fit(X, y=None, sample_weight=None)</code>","text":"<p>Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when <code>fit</code> is not feasible due to very large number of <code>n_samples</code> or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247:</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, sparse matrix</code> <p>The data used to compute the mean and standard deviation used for later scaling along the features axis.</p> <code>array-like</code> <code>y</code> <p>Ignored</p> <code>None</code> <code>sample_weight</code> <code>array-like of shape (n_samples,)</code> <p>Individual weights for each sample.</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def partial_fit(self, X, y=None, sample_weight=None):\n    \"\"\"Online computation of mean and std on X for later scaling.\n    All of X is processed as a single batch. This is intended for cases\n    when `fit` is not feasible due to very large number of `n_samples`\n    or because X is read from a continuous stream.\n    The algorithm for incremental mean and std is given in Equation 1.5a,b\n    in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n    for computing the sample variance: Analysis and recommendations.\"\n    The American Statistician 37.3 (1983): 242-247:\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        The data used to compute the mean and standard deviation\n        used for later scaling along the features axis.\n    y\n        Ignored\n    sample_weight : array-like of shape (n_samples,), default=None\n        Individual weights for each sample.\n    \"\"\"\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        X = X.data.T\n\n    return super().partial_fit(X, y, sample_weight)\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.StandardScaler.transform","title":"<code>transform(X, copy=None)</code>","text":"<p>Perform standardization by centering and scaling</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape[n_samples, n_features])</code> <p>The data used to scale along the features axis.</p> required <code>copy</code> <code>bool, optional (default: None)</code> <p>Copy the input X or not.</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def transform(self, X, copy=None):\n    \"\"\"Perform standardization by centering and scaling\n    Parameters\n    ----------\n    X : array-like, shape [n_samples, n_features]\n        The data used to scale along the features axis.\n    copy : bool, optional (default: None)\n        Copy the input X or not.\n    \"\"\"\n\n    if copy is None:\n        copy = self.copy\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        if copy:\n            Xdata = copycopy(X.data.T)\n            X = X.copy()\n        else:\n            Xdata = X.data.T\n        Xdata = super().transform(Xdata, copy).T\n\n        X._data = Xdata\n    else:\n        X = super().transform(X, copy)\n    return X\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.StreamingDataWindow","title":"<code>StreamingDataWindow</code>","text":"<p>StreamingDataWindow</p> <p>StreamingDataWindow is an iterable with an associated data object.</p> <p>See https://hackmag.com/coding/lets-tame-data-streams-with-python/</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>class StreamingDataWindow:\n    \"\"\"\n    StreamingDataWindow\n\n    StreamingDataWindow is an iterable with an associated data object.\n\n    See https://hackmag.com/coding/lets-tame-data-streams-with-python/\n    \"\"\"\n\n    def __init__(self, w, X, flatten=False):\n        self._w = w\n        self.X = X\n        self._flatten = False\n\n    def flatten(self, inplace=False):\n        # what's the opposite of flatten?\n        pass\n\n    def __repr__(self):\n        return \"StreamingDataWindow(\\n\\tw={},\\n\\tX={},\\n\\tflatten={})\".format(\n            str(self.w), str(self.X), str(self._flatten)\n        )  # + str(self.w)\n\n    def __iter__(self):\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        # index = self._index\n        # if index &gt; self.n_intervals - 1:\n        #     raise StopIteration\n\n        self._index += 1\n\n    @property\n    def w(self):\n        return self._w\n\n    @w.setter\n    def w(self, val):\n        if not isinstance(val, DataWindow):\n            raise TypeError(\"w must be a nelpy.preprocessing.DataWindow type!\")\n        else:\n            self._w = val\n</code></pre>"},{"location":"reference/preprocessing/#nelpy.preprocessing.standardize_asa","title":"<code>standardize_asa(func=None, *, asa, lengths=None, timestamps=None, fs=None, n_signals=None)</code>","text":"<p>Standardize nelpy RegularlySampledAnalogSignalArray to numpy representation.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>string</code> <p>Argument name corresponding to 'asa' in decorated function.</p> required <code>lengths</code> <code>string</code> <p>Argument name corresponding to 'lengths' in decorated function.</p> <code>None</code> <code>timestamps</code> <code>string</code> <p>Argument name corresponding to 'timestamps' in decorated function.</p> <code>None</code> <code>fs</code> <code>string</code> <p>Argument name corresponding to 'fs' in decorated function.</p> <code>None</code> <code>n_signals</code> <code>int</code> <p>Number of signals required in asa.</p> <code>None</code> Notes <ul> <li>asa is replaced with a (n_samples, n_signals) numpy array</li> <li>lenghts is replaced with a (n_intervals, ) numpy array, each containing    the number of samples in the associated interval.</li> <li>timestmaps is replaced with an (n_samples, ) numpy array, containing the    timestamps or abscissa_vals of the RegularlySampledAnalogSignalArray.</li> <li>fs is replaced with the float corresponding to the sampling frequency.</li> </ul> <p>Examples:</p> <p>@standardize_asa(asa='X', lengths='lengths', n_signals=2) def myfunc(*args, X=None, lengths=None):     pass</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def standardize_asa(\n    func=None, *, asa, lengths=None, timestamps=None, fs=None, n_signals=None\n):\n    \"\"\"\n    Standardize nelpy RegularlySampledAnalogSignalArray to numpy representation.\n\n    Parameters\n    ----------\n    asa : string\n        Argument name corresponding to 'asa' in decorated function.\n    lengths : string, optional\n        Argument name corresponding to 'lengths' in decorated function.\n    timestamps : string, optional\n        Argument name corresponding to 'timestamps' in decorated function.\n    fs : string, optional\n        Argument name corresponding to 'fs' in decorated function.\n    n_signals : int, optional\n        Number of signals required in asa.\n\n    Notes\n    -----\n     - asa is replaced with a (n_samples, n_signals) numpy array\n     - lenghts is replaced with a (n_intervals, ) numpy array, each containing\n       the number of samples in the associated interval.\n     - timestmaps is replaced with an (n_samples, ) numpy array, containing the\n       timestamps or abscissa_vals of the RegularlySampledAnalogSignalArray.\n     - fs is replaced with the float corresponding to the sampling frequency.\n\n    Examples\n    --------\n    @standardize_asa(asa='X', lengths='lengths', n_signals=2)\n    def myfunc(*args, X=None, lengths=None):\n        pass\n\n    \"\"\"\n    if n_signals is not None:\n        try:\n            assert float(n_signals).is_integer(), (\n                \"'n_signals' must be a positive integer!\"\n            )\n            n_signals = int(n_signals)\n        except ValueError:\n            raise ValueError(\"'n_signals' must be a positive integer!\")\n        assert n_signals &gt; 0, \"'n_signals' must be a positive integer!\"\n\n    assert isinstance(asa, str), \"'asa' decorator argument must be a string!\"\n    if lengths is not None:\n        assert isinstance(lengths, str), (\n            \"'lengths' decorator argument must be a string!\"\n        )\n    if timestamps is not None:\n        assert isinstance(timestamps, str), (\n            \"'timestamps' decorator argument must be a string!\"\n        )\n    if fs is not None:\n        assert isinstance(fs, str), \"'fs' decorator argument must be a string!\"\n\n    def _decorate(function):\n        @wraps(function)\n        def wrapped_function(*args, **kwargs):\n            kw = True\n            # TODO: check that all decorator kwargs are strings\n            asa_ = kwargs.pop(asa, None)\n            lengths_ = kwargs.pop(lengths, None)\n            fs_ = kwargs.pop(fs, None)\n            timestamps_ = kwargs.pop(timestamps, None)\n\n            if asa_ is None:\n                try:\n                    asa_ = args[0]\n                    kw = False\n                except IndexError:\n                    raise TypeError(\n                        \"{}() missing 1 required positional argument: '{}'\".format(\n                            function.__name__, asa\n                        )\n                    )\n\n            # standardize asa_ here...\n            if isinstance(asa_, core.RegularlySampledAnalogSignalArray):\n                if n_signals is not None:\n                    if not asa_.n_signals == n_signals:\n                        raise ValueError(\n                            \"Input object '{}'.n_signals=={}, but {} was expected!\".format(\n                                asa, asa_.n_signals, n_signals\n                            )\n                        )\n                if lengths_ is not None:\n                    logging.warning(\n                        \"'{}' was passed in, but will be overwritten\"\n                        \" by '{}'s 'lengths' attribute\".format(lengths, asa)\n                    )\n                if timestamps_ is not None:\n                    logging.warning(\n                        \"'{}' was passed in, but will be overwritten\"\n                        \" by '{}'s 'abscissa_vals' attribute\".format(timestamps, asa)\n                    )\n                if fs_ is not None:\n                    logging.warning(\n                        \"'{}' was passed in, but will be overwritten\"\n                        \" by '{}'s 'fs' attribute\".format(fs, asa)\n                    )\n\n                fs_ = asa_.fs\n                lengths_ = asa_.lengths\n                timestamps_ = asa_.abscissa_vals\n                asa_ = asa_.data.squeeze().copy()\n\n            elif not isinstance(asa_, np.ndarray):\n                raise TypeError(\n                    \"'{}' was not a nelpy.RegularlySampledAnalogSignalArray\"\n                    \" so expected a numpy ndarray but got {}\".format(asa, type(asa_))\n                )\n\n            if kw:\n                kwargs[asa] = asa_\n            else:\n                args = tuple([arg if ii &gt; 0 else asa_ for (ii, arg) in enumerate(args)])\n\n            if lengths is not None:\n                if lengths_ is None:\n                    lengths_ = np.array([len(asa_)])\n                kwargs[lengths] = lengths_\n            if timestamps is not None:\n                if timestamps_ is None:\n                    raise TypeError(\n                        \"{}() missing 1 required keyword argument: '{}'\".format(\n                            function.__name__, timestamps\n                        )\n                    )\n                kwargs[timestamps] = timestamps_\n            if fs is not None:\n                if fs_ is None:\n                    raise TypeError(\n                        \"{}() missing 1 required keyword argument: '{}'\".format(\n                            function.__name__, fs\n                        )\n                    )\n                kwargs[fs] = fs_\n\n            return function(*args, **kwargs)\n\n        return wrapped_function\n\n    if func:\n        return _decorate(func)\n\n    return _decorate\n</code></pre>"},{"location":"reference/utils/","title":"Utilities API Reference","text":"<p>This module contains helper functions and utilities for nelpy.</p>"},{"location":"reference/utils/#nelpy.utils.PrettyBytes","title":"<code>PrettyBytes</code>","text":"<p>               Bases: <code>int</code></p> <p>Prints number of bytes in a more readable format</p> Source code in <code>nelpy/utils.py</code> <pre><code>class PrettyBytes(int):\n    \"\"\"Prints number of bytes in a more readable format\"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    def __str__(self):\n        if self.val &lt; 1024:\n            return \"{} bytes\".format(self.val)\n        elif self.val &lt; 1024**2:\n            return \"{:.3f} kilobytes\".format(self.val / 1024)\n        elif self.val &lt; 1024**3:\n            return \"{:.3f} megabytes\".format(self.val / 1024**2)\n        elif self.val &lt; 1024**4:\n            return \"{:.3f} gigabytes\".format(self.val / 1024**3)\n\n    def __repr__(self):\n        return self.__str__()\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.PrettyDuration","title":"<code>PrettyDuration</code>","text":"<p>               Bases: <code>float</code></p> <p>Time duration with pretty print.</p> <p>Behaves like a float, and can always be cast to a float.</p> Source code in <code>nelpy/utils.py</code> <pre><code>class PrettyDuration(float):\n    \"\"\"Time duration with pretty print.\n\n    Behaves like a float, and can always be cast to a float.\n    \"\"\"\n\n    def __init__(self, seconds):\n        self.duration = seconds\n\n    def __str__(self):\n        return self.time_string(self.duration)\n\n    def __repr__(self):\n        return self.time_string(self.duration)\n\n    @staticmethod\n    def to_dhms(seconds):\n        \"\"\"\n        Convert seconds into days, hours, minutes, seconds, and milliseconds.\n\n        Parameters\n        ----------\n        seconds : float\n            Time duration in seconds.\n\n        Returns\n        -------\n        namedtuple\n            Named tuple with fields: pos (bool), dd (days), hh (hours),\n            mm (minutes), ss (seconds), ms (milliseconds).\n\n        Examples\n        --------\n        &gt;&gt;&gt; PrettyDuration.to_dhms(3661.5)\n        Time(pos=True, dd=0, hh=1, mm=1, ss=1, ms=500.0)\n        \"\"\"\n        pos = seconds &gt;= 0\n        if not pos:\n            seconds = -seconds\n        ms = seconds % 1\n        ms = round(ms * 10000) / 10\n        seconds = floor(seconds)\n        m, s = divmod(seconds, 60)\n        h, m = divmod(m, 60)\n        d, h = divmod(h, 24)\n        Time = namedtuple(\"Time\", \"pos dd hh mm ss ms\")\n        time = Time(pos=pos, dd=d, hh=h, mm=m, ss=s, ms=ms)\n        return time\n\n    @staticmethod\n    def time_string(seconds):\n        \"\"\"\n        Return a formatted time string.\n\n        Parameters\n        ----------\n        seconds : float\n            Time duration in seconds.\n\n        Returns\n        -------\n        str\n            Formatted time string (e.g., \"1:01:01.5 hours\", \"30.5 seconds\").\n\n        Examples\n        --------\n        &gt;&gt;&gt; PrettyDuration.time_string(3661.5)\n        '1:01:01.5 hours'\n        &gt;&gt;&gt; PrettyDuration.time_string(30.5)\n        '30.5 seconds'\n        \"\"\"\n        if np.isinf(seconds):\n            return \"inf\"\n        pos, dd, hh, mm, ss, s = PrettyDuration.to_dhms(seconds)\n        if s &gt; 0:\n            if mm == 0:\n                # in this case, represent milliseconds in terms of\n                # seconds (i.e. a decimal)\n                sstr = str(s / 1000).lstrip(\"0\")\n                if s &gt;= 999.5:\n                    ss += 1\n                    s = 0\n                    sstr = \"\"\n                    # now propagate the carry:\n                    if ss == 60:\n                        mm += 1\n                        ss = 0\n                    if mm == 60:\n                        hh += 1\n                        mm = 0\n                    if hh == 24:\n                        dd += 1\n                        hh = 0\n            else:\n                # for all other cases, milliseconds will be represented\n                # as an integer\n                if s &gt;= 999.5:\n                    ss += 1\n                    s = 0\n                    sstr = \"\"\n                    # now propagate the carry:\n                    if ss == 60:\n                        mm += 1\n                        ss = 0\n                    if mm == 60:\n                        hh += 1\n                        mm = 0\n                    if hh == 24:\n                        dd += 1\n                        hh = 0\n                else:\n                    sstr = \":{:03d}\".format(int(s))\n        else:\n            sstr = \"\"\n        if dd &gt; 0:\n            daystr = \"{:01d} days \".format(dd)\n        else:\n            daystr = \"\"\n        if hh &gt; 0:\n            timestr = daystr + \"{:01d}:{:02d}:{:02d}{} hours\".format(hh, mm, ss, sstr)\n        elif mm &gt; 0:\n            timestr = daystr + \"{:01d}:{:02d}{} minutes\".format(mm, ss, sstr)\n        elif ss &gt; 0:\n            timestr = daystr + \"{:01d}{} seconds\".format(ss, sstr)\n        else:\n            timestr = daystr + \"{} milliseconds\".format(s)\n        if not pos:\n            timestr = \"-\" + timestr\n        return timestr\n\n    def __add__(self, other):\n        \"\"\"\n        Add another value to this duration.\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to add.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration + other)\n\n    def __radd__(self, other):\n        \"\"\"\n        Add this duration to another value (right addition).\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to add to.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return self.__add__(other)\n\n    def __sub__(self, other):\n        \"\"\"\n        Subtract another value from this duration.\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to subtract.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration - other)\n\n    def __rsub__(self, other):\n        \"\"\"\n        Subtract this duration from another value (right subtraction).\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to subtract from.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return other - self.duration\n\n    def __mul__(self, other):\n        \"\"\"\n        Multiply this duration by another value.\n\n        Parameters\n        ----------\n        other : float\n            Value to multiply by.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration * other)\n\n    def __rmul__(self, other):\n        \"\"\"\n        Multiply another value by this duration (right multiplication).\n\n        Parameters\n        ----------\n        other : float\n            Value to multiply.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return self.__mul__(other)\n\n    def __truediv__(self, other):\n        \"\"\"\n        Divide this duration by another value.\n\n        Parameters\n        ----------\n        other : float\n            Value to divide by.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration / other)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.PrettyDuration.time_string","title":"<code>time_string(seconds)</code>  <code>staticmethod</code>","text":"<p>Return a formatted time string.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Time duration in seconds.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted time string (e.g., \"1:01:01.5 hours\", \"30.5 seconds\").</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PrettyDuration.time_string(3661.5)\n'1:01:01.5 hours'\n&gt;&gt;&gt; PrettyDuration.time_string(30.5)\n'30.5 seconds'\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>@staticmethod\ndef time_string(seconds):\n    \"\"\"\n    Return a formatted time string.\n\n    Parameters\n    ----------\n    seconds : float\n        Time duration in seconds.\n\n    Returns\n    -------\n    str\n        Formatted time string (e.g., \"1:01:01.5 hours\", \"30.5 seconds\").\n\n    Examples\n    --------\n    &gt;&gt;&gt; PrettyDuration.time_string(3661.5)\n    '1:01:01.5 hours'\n    &gt;&gt;&gt; PrettyDuration.time_string(30.5)\n    '30.5 seconds'\n    \"\"\"\n    if np.isinf(seconds):\n        return \"inf\"\n    pos, dd, hh, mm, ss, s = PrettyDuration.to_dhms(seconds)\n    if s &gt; 0:\n        if mm == 0:\n            # in this case, represent milliseconds in terms of\n            # seconds (i.e. a decimal)\n            sstr = str(s / 1000).lstrip(\"0\")\n            if s &gt;= 999.5:\n                ss += 1\n                s = 0\n                sstr = \"\"\n                # now propagate the carry:\n                if ss == 60:\n                    mm += 1\n                    ss = 0\n                if mm == 60:\n                    hh += 1\n                    mm = 0\n                if hh == 24:\n                    dd += 1\n                    hh = 0\n        else:\n            # for all other cases, milliseconds will be represented\n            # as an integer\n            if s &gt;= 999.5:\n                ss += 1\n                s = 0\n                sstr = \"\"\n                # now propagate the carry:\n                if ss == 60:\n                    mm += 1\n                    ss = 0\n                if mm == 60:\n                    hh += 1\n                    mm = 0\n                if hh == 24:\n                    dd += 1\n                    hh = 0\n            else:\n                sstr = \":{:03d}\".format(int(s))\n    else:\n        sstr = \"\"\n    if dd &gt; 0:\n        daystr = \"{:01d} days \".format(dd)\n    else:\n        daystr = \"\"\n    if hh &gt; 0:\n        timestr = daystr + \"{:01d}:{:02d}:{:02d}{} hours\".format(hh, mm, ss, sstr)\n    elif mm &gt; 0:\n        timestr = daystr + \"{:01d}:{:02d}{} minutes\".format(mm, ss, sstr)\n    elif ss &gt; 0:\n        timestr = daystr + \"{:01d}{} seconds\".format(ss, sstr)\n    else:\n        timestr = daystr + \"{} milliseconds\".format(s)\n    if not pos:\n        timestr = \"-\" + timestr\n    return timestr\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.PrettyDuration.to_dhms","title":"<code>to_dhms(seconds)</code>  <code>staticmethod</code>","text":"<p>Convert seconds into days, hours, minutes, seconds, and milliseconds.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Time duration in seconds.</p> required <p>Returns:</p> Type Description <code>namedtuple</code> <p>Named tuple with fields: pos (bool), dd (days), hh (hours), mm (minutes), ss (seconds), ms (milliseconds).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PrettyDuration.to_dhms(3661.5)\nTime(pos=True, dd=0, hh=1, mm=1, ss=1, ms=500.0)\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>@staticmethod\ndef to_dhms(seconds):\n    \"\"\"\n    Convert seconds into days, hours, minutes, seconds, and milliseconds.\n\n    Parameters\n    ----------\n    seconds : float\n        Time duration in seconds.\n\n    Returns\n    -------\n    namedtuple\n        Named tuple with fields: pos (bool), dd (days), hh (hours),\n        mm (minutes), ss (seconds), ms (milliseconds).\n\n    Examples\n    --------\n    &gt;&gt;&gt; PrettyDuration.to_dhms(3661.5)\n    Time(pos=True, dd=0, hh=1, mm=1, ss=1, ms=500.0)\n    \"\"\"\n    pos = seconds &gt;= 0\n    if not pos:\n        seconds = -seconds\n    ms = seconds % 1\n    ms = round(ms * 10000) / 10\n    seconds = floor(seconds)\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    d, h = divmod(h, 24)\n    Time = namedtuple(\"Time\", \"pos dd hh mm ss ms\")\n    time = Time(pos=pos, dd=d, hh=h, mm=m, ss=s, ms=ms)\n    return time\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.PrettyInt","title":"<code>PrettyInt</code>","text":"<p>               Bases: <code>int</code></p> <p>Prints integers in a more readable format</p> Source code in <code>nelpy/utils.py</code> <pre><code>class PrettyInt(int):\n    \"\"\"Prints integers in a more readable format\"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    def __str__(self):\n        return \"{:,}\".format(self.val)\n\n    def __repr__(self):\n        return \"{:,}\".format(self.val)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.argsort","title":"<code>argsort(seq)</code>","text":"<p>Return indices that would sort a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>sequence</code> <p>Sequence to sort (list, tuple, array, etc.).</p> required <p>Returns:</p> Name Type Description <code>indices</code> <code>list</code> <p>List of indices that would sort the sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; argsort([3, 1, 4, 1, 5])\n[1, 3, 0, 2, 4]\n&gt;&gt;&gt; seq = [\"c\", \"a\", \"b\"]\n&gt;&gt;&gt; [seq[i] for i in argsort(seq)]\n['a', 'b', 'c']\n</code></pre> Notes <p>Based on http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python</p> Source code in <code>nelpy/utils.py</code> <pre><code>def argsort(seq):\n    \"\"\"\n    Return indices that would sort a sequence.\n\n    Parameters\n    ----------\n    seq : sequence\n        Sequence to sort (list, tuple, array, etc.).\n\n    Returns\n    -------\n    indices : list\n        List of indices that would sort the sequence.\n\n    Examples\n    --------\n    &gt;&gt;&gt; argsort([3, 1, 4, 1, 5])\n    [1, 3, 0, 2, 4]\n    &gt;&gt;&gt; seq = [\"c\", \"a\", \"b\"]\n    &gt;&gt;&gt; [seq[i] for i in argsort(seq)]\n    ['a', 'b', 'c']\n\n    Notes\n    -----\n    Based on http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python\n    \"\"\"\n    return sorted(range(len(seq)), key=seq.__getitem__)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.asa_indices_within_epochs","title":"<code>asa_indices_within_epochs(asa, intervalarray)</code>","text":"<p>Return indices of ASA within epochs.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray object.</p> required <code>intervalarray</code> <code>IntervalArray</code> <p>IntervalArray containing epochs of interest.</p> required <p>Returns:</p> Name Type Description <code>indices</code> <code>ndarray</code> <p>Array of shape (n_epochs, 2) containing [start, stop] indices for each epoch, so that data can be associated with asa._data[:,start:stop] for each epoch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epochs = nelpy.EpochArray([[0, 10], [20, 30]])\n&gt;&gt;&gt; indices = asa_indices_within_epochs(asa, epochs)\n&gt;&gt;&gt; data_in_epochs = [asa._data[:, start:stop] for start, stop in indices]\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def asa_indices_within_epochs(asa, intervalarray):\n    \"\"\"\n    Return indices of ASA within epochs.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray\n        AnalogSignalArray object.\n    intervalarray : IntervalArray\n        IntervalArray containing epochs of interest.\n\n    Returns\n    -------\n    indices : np.ndarray\n        Array of shape (n_epochs, 2) containing [start, stop] indices\n        for each epoch, so that data can be associated with\n        asa._data[:,start:stop] for each epoch.\n\n    Examples\n    --------\n    &gt;&gt;&gt; epochs = nelpy.EpochArray([[0, 10], [20, 30]])\n    &gt;&gt;&gt; indices = asa_indices_within_epochs(asa, epochs)\n    &gt;&gt;&gt; data_in_epochs = [asa._data[:, start:stop] for start, stop in indices]\n    \"\"\"\n    indices = []\n    intervalarray = intervalarray[asa.support]\n    for interval in intervalarray.merge().data:\n        a_start = interval[0]\n        a_stop = interval[1]\n        frm, to = np.searchsorted(asa._abscissa_vals, (a_start, a_stop))\n        indices.append((frm, to))\n    indices = np.array(indices, ndmin=2)\n\n    return indices\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.cartesian","title":"<code>cartesian(xcenters, ycenters)</code>","text":"<p>Find every combination of elements in two arrays (Cartesian product).</p> <p>Parameters:</p> Name Type Description Default <code>xcenters</code> <code>ndarray</code> <p>First array of values.</p> required <code>ycenters</code> <code>ndarray</code> <p>Second array of values.</p> required <p>Returns:</p> Name Type Description <code>cartesian</code> <code>ndarray</code> <p>Array of shape (n_samples, 2) containing all combinations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.array([1, 2])\n&gt;&gt;&gt; y = np.array([3, 4, 5])\n&gt;&gt;&gt; cartesian(x, y)\narray([[1, 3],\n       [1, 4],\n       [1, 5],\n       [2, 3],\n       [2, 4],\n       [2, 5]])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def cartesian(xcenters, ycenters):\n    \"\"\"\n    Find every combination of elements in two arrays (Cartesian product).\n\n    Parameters\n    ----------\n    xcenters : np.ndarray\n        First array of values.\n    ycenters : np.ndarray\n        Second array of values.\n\n    Returns\n    -------\n    cartesian : np.ndarray\n        Array of shape (n_samples, 2) containing all combinations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; x = np.array([1, 2])\n    &gt;&gt;&gt; y = np.array([3, 4, 5])\n    &gt;&gt;&gt; cartesian(x, y)\n    array([[1, 3],\n           [1, 4],\n           [1, 5],\n           [2, 3],\n           [2, 4],\n           [2, 5]])\n    \"\"\"\n    return np.transpose(\n        [np.tile(xcenters, len(ycenters)), np.repeat(ycenters, len(xcenters))]\n    )\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.collapse_time","title":"<code>collapse_time(obj, gap=0)</code>","text":"<p>Collapse all epochs in an object into a single, contiguous object.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>SpikeTrainArray or AnalogSignalArray</code> <p>Object with multiple epochs to collapse.</p> required <code>gap</code> <code>float</code> <p>Gap to insert between epochs in the collapsed object. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>obj</code> <p>New object of the same type with all epochs collapsed into a single contiguous epoch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Collapse a SpikeTrainArray with multiple epochs\n&gt;&gt;&gt; collapsed_st = collapse_time(spiketrain)\n&gt;&gt;&gt; # Collapse an AnalogSignalArray with gaps between epochs\n&gt;&gt;&gt; collapsed_asa = collapse_time(analogsignal, gap=0.1)\n</code></pre> Notes <p>For SpikeTrainArrays, gaps are not yet supported. The function adjusts spike times or signal times to create a continuous timeline while preserving the original epoch boundaries information.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def collapse_time(obj, gap=0):\n    \"\"\"\n    Collapse all epochs in an object into a single, contiguous object.\n\n    Parameters\n    ----------\n    obj : SpikeTrainArray or AnalogSignalArray\n        Object with multiple epochs to collapse.\n    gap : float, optional\n        Gap to insert between epochs in the collapsed object. Default is 0.\n\n    Returns\n    -------\n    obj\n        New object of the same type with all epochs collapsed into a single\n        contiguous epoch.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Collapse a SpikeTrainArray with multiple epochs\n    &gt;&gt;&gt; collapsed_st = collapse_time(spiketrain)\n    &gt;&gt;&gt; # Collapse an AnalogSignalArray with gaps between epochs\n    &gt;&gt;&gt; collapsed_asa = collapse_time(analogsignal, gap=0.1)\n\n    Notes\n    -----\n    For SpikeTrainArrays, gaps are not yet supported.\n    The function adjusts spike times or signal times to create a continuous\n    timeline while preserving the original epoch boundaries information.\n    \"\"\"\n\n    # TODO: redo SpikeTrainArray so as to keep the epochs separate!, and to support gaps!\n\n    # We'll have to ajust all the spikes per epoch... and we'll have to compute a new support. Also set a flag!\n\n    # If it's a SpikeTrainArray, then we left-shift the spike times. If it's an AnalogSignalArray, then we\n    # left-shift the time and tdata.\n\n    # Also set a new attribute, with the boundaries in seconds.\n\n    if isinstance(obj, core.RegularlySampledAnalogSignalArray):\n        new_obj = type(obj)(empty=True)\n        new_obj._data = obj._data\n\n        durations = obj.support.durations\n        starts = np.insert(np.cumsum(durations + gap), 0, 0)[:-1]\n        stops = starts + durations\n        newsupport = type(obj._abscissa.support)(np.vstack((starts, stops)).T)\n        new_obj._support = newsupport\n\n        new_time = obj.time.astype(float)  # fast copy\n        time_idx = np.insert(np.cumsum(obj.lengths), 0, 0)\n\n        new_offset = 0\n        for epidx in range(obj.n_epochs):\n            if epidx &gt; 0:\n                new_time[time_idx[epidx] : time_idx[epidx + 1]] = (\n                    new_time[time_idx[epidx] : time_idx[epidx + 1]]\n                    - obj.time[time_idx[epidx]]\n                    + new_offset\n                    + gap\n                )\n                new_offset += durations[epidx] + gap\n            else:\n                new_time[time_idx[epidx] : time_idx[epidx + 1]] = (\n                    new_time[time_idx[epidx] : time_idx[epidx + 1]]\n                    - obj.time[time_idx[epidx]]\n                    + new_offset\n                )\n                new_offset += durations[epidx]\n        new_obj._time = new_time\n\n        new_obj._fs = obj._fs\n\n    elif isinstance(obj, core.EventArray):\n        if gap &gt; 0:\n            raise ValueError(\"gaps not supported for SpikeTrainArrays yet!\")\n        new_obj = type(obj)(empty=True)\n        new_time = [[] for _ in range(obj.n_series)]\n        duration = 0\n        for st_ in obj:\n            le = st_.support.start\n            for unit_ in range(obj.n_series):\n                new_time[unit_].extend(st_._data[unit_] - le + duration)\n            duration += st_.support.duration\n        new_time = np.asanyarray(\n            [np.asanyarray(unittime) for unittime in new_time], dtype=object\n        )\n        new_obj._data = new_time\n        new_obj.support = type(obj._abscissa.support)([0, duration])\n        new_obj._series_ids = obj._series_ids\n        new_obj._series_labels = obj._series_labels\n        new_obj._series_tags = obj._series_tags\n    elif isinstance(obj, core.BinnedEventArray):\n        raise NotImplementedError(\n            \"BinnedEventArrays are not yet supported, but bst.data is essentially already collapsed!\"\n        )\n    else:\n        raise TypeError(\"unsupported type for collapse_time\")\n\n    return new_obj\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.ddt_asa","title":"<code>ddt_asa(asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None, norm=False)</code>","text":"<p>Numerical differentiation of a regularly sampled AnalogSignalArray.</p> <p>Optionally also smooths result with a Gaussian kernel.</p> <p>Smoothing is applied in time, and the same smoothing is applied to each signal in the AnalogSignalArray.</p> <p>Differentiation, (and if requested, smoothing) is applied within each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>RegularlySampledAnalogSignalArray</code> <p>Input object.</p> required <code>fs</code> <code>float</code> <p>Sampling rate (in Hz) of input RSASA. If not provided, it will be obtained from asa.fs.</p> <code>None</code> <code>smooth</code> <code>bool</code> <p>If true, result will be smoothed. Default is False</p> <code>False</code> <code>rectify</code> <code>bool</code> <p>If True, absolute value of derivative is computed. Default is True.</p> <code>True</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.05 (50 ms).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <code>norm</code> <p>If True, then apply the L2 norm to the result.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>A RegularlySampledAnalogSignalArray with derivative data (in units per second) is returned.</p> Notes <p>Central differences are used here.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef ddt_asa(\n    asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None, norm=False\n):\n    \"\"\"Numerical differentiation of a regularly sampled AnalogSignalArray.\n\n    Optionally also smooths result with a Gaussian kernel.\n\n    Smoothing is applied in time, and the same smoothing is applied to each\n    signal in the AnalogSignalArray.\n\n    Differentiation, (and if requested, smoothing) is applied within each epoch.\n\n    Parameters\n    ----------\n    asa : nelpy.RegularlySampledAnalogSignalArray\n        Input object.\n    fs : float, optional\n        Sampling rate (in Hz) of input RSASA. If not provided, it will be obtained\n        from asa.fs.\n    smooth : bool, optional\n        If true, result will be smoothed. Default is False\n    rectify : bool, optional\n        If True, absolute value of derivative is computed. Default is True.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.05\n        (50 ms).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n    norm: boolean, optional\n        If True, then apply the L2 norm to the result.\n    Returns\n    -------\n    out : nelpy.RegularlySampledAnalogSignalArray\n        A RegularlySampledAnalogSignalArray with derivative data (in units\n        per second) is returned.\n\n    Notes\n    -----\n    Central differences are used here.\n    \"\"\"\n\n    if not (\n        isinstance(asa, core.RegularlySampledAnalogSignalArray)\n        or isinstance(asa, core._analogsignalarray.PositionArray)\n    ):\n        raise TypeError(\"Input object must be a RegularlySampledAnalogSignalArray!\")\n    if fs is None:\n        fs = asa.fs\n    if sigma is None:\n        sigma = 0.05  # 50 ms default\n\n    out = asa.copy()\n    cum_lengths = np.insert(np.cumsum(asa.lengths), 0, 0)\n\n    # ensure that datatype is float\n    # TODO: this will break complex data\n    out._data = out.data.astype(float)\n\n    # now obtain the derivative for each epoch separately\n    for idx in range(asa.n_epochs):\n        # if 1D:\n        if asa.n_signals == 1:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = np.gradient(\n                    asa._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]], axis=1\n                )\n        else:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = np.gradient(\n                    asa._data[:, cum_lengths[idx] : cum_lengths[idx + 1]], axis=1\n                )\n\n    out._data = out._data * fs\n\n    if norm:\n        out._data = np.atleast_2d(np.linalg.norm(out._data, axis=0))\n\n    if rectify:\n        out._data = np.abs(out._data)\n\n    if smooth:\n        out = gaussian_filter(out, fs=fs, sigma=sigma, truncate=truncate)\n\n    return out\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.dxdt_AnalogSignalArray","title":"<code>dxdt_AnalogSignalArray(asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None)</code>","text":"<p>Numerical differentiation of a regularly sampled AnalogSignalArray.</p> <p>Optionally also smooths result with a Gaussian kernel.</p> <p>Smoothing is applied in time, and the same smoothing is applied to each signal in the AnalogSignalArray.</p> <p>Differentiation, (and if requested, smoothing) is applied within each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray</code> required <code>fs</code> <code>float</code> <p>Sampling rate (in Hz) of AnalogSignalArray. If not provided, it will be obtained from asa.fs</p> <code>None</code> <code>smooth</code> <code>bool</code> <p>If true, result will be smoothed. Default is False</p> <code>False</code> <code>rectify</code> <code>bool</code> <p>If True, absolute value of derivative is computed. Default is True.</p> <code>True</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.05 (50 ms).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>AnalogSignalArray</code> <p>An AnalogSignalArray with derivative data (in units per second) is returned.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef dxdt_AnalogSignalArray(\n    asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None\n):\n    \"\"\"Numerical differentiation of a regularly sampled AnalogSignalArray.\n\n    Optionally also smooths result with a Gaussian kernel.\n\n    Smoothing is applied in time, and the same smoothing is applied to each\n    signal in the AnalogSignalArray.\n\n    Differentiation, (and if requested, smoothing) is applied within each epoch.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray\n    fs : float, optional\n        Sampling rate (in Hz) of AnalogSignalArray. If not provided, it will\n        be obtained from asa.fs\n    smooth : bool, optional\n        If true, result will be smoothed. Default is False\n    rectify : bool, optional\n        If True, absolute value of derivative is computed. Default is True.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.05\n        (50 ms).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n\n    Returns\n    -------\n    out : AnalogSignalArray\n        An AnalogSignalArray with derivative data (in units per second) is returned.\n    \"\"\"\n\n    raise DeprecationWarning(\"use ddt_asa instead!\")\n\n    if fs is None:\n        fs = asa.fs\n    if fs is None:\n        raise ValueError(\n            \"fs must either be specified, or must be contained in the AnalogSignalArray!\"\n        )\n    if sigma is None:\n        sigma = 0.05  # 50 ms default\n\n    out = copy.deepcopy(asa)\n    cum_lengths = np.insert(np.cumsum(asa.lengths), 0, 0)\n\n    # ensure that datatype is float\n    out._data = out.data.astype(float)\n\n    if asa.n_signals == 2:\n        out._data = out._data[[0], :]\n\n    # now obtain the derivative for each epoch separately\n    for idx in range(asa.n_epochs):\n        # if 1D:\n        if asa.n_signals == 1:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = np.gradient(\n                    asa._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]], axis=1\n                )\n        elif asa.n_signals == 2:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                    np.linalg.norm(\n                        np.gradient(\n                            asa._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                            axis=1,\n                        ),\n                        axis=0,\n                    )\n                )\n        else:\n            raise TypeError(\"more than 2D not currently supported!\")\n\n    out._data = out._data * fs\n\n    if rectify:\n        out._data = np.abs(out._data)\n\n    if smooth:\n        out = gaussian_filter(out, fs=fs, sigma=sigma, truncate=truncate)\n\n    return out\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.find_nearest_idx","title":"<code>find_nearest_idx(array, val)</code>","text":"<p>Find the index of the nearest value in an array.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array to search in.</p> required <code>val</code> <code>float</code> <p>Value to find the nearest index for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index into array that is closest to val.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n&gt;&gt;&gt; find_nearest_idx(array, 4)\n1\n&gt;&gt;&gt; find_nearest_idx(array, 6)\n2\n</code></pre> Notes <p>TODO: A more efficient version using searchsorted should be incorporated: Based on answer here: http://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array</p> Source code in <code>nelpy/utils.py</code> <pre><code>def find_nearest_idx(array, val):\n    \"\"\"\n    Find the index of the nearest value in an array.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Input array to search in.\n    val : float\n        Value to find the nearest index for.\n\n    Returns\n    -------\n    int\n        Index into array that is closest to val.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n    &gt;&gt;&gt; find_nearest_idx(array, 4)\n    1\n    &gt;&gt;&gt; find_nearest_idx(array, 6)\n    2\n\n    Notes\n    -----\n    TODO: A more efficient version using searchsorted should be incorporated:\n    Based on answer here: http://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array\n    \"\"\"\n    return (np.abs(array - val)).argmin()\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.find_nearest_indices","title":"<code>find_nearest_indices(array, vals)</code>","text":"<p>Find indices of nearest values in an array for multiple values.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array to search in.</p> required <code>vals</code> <code>ndarray</code> <p>Array of values to find nearest indices for.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of indices into array that are closest to vals.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n&gt;&gt;&gt; vals = np.array([2, 4, 6, 8])\n&gt;&gt;&gt; find_nearest_indices(array, vals)\narray([0, 1, 2, 3])\n</code></pre> Notes <p>Wrapper around find_nearest_idx().</p> Source code in <code>nelpy/utils.py</code> <pre><code>def find_nearest_indices(array, vals):\n    \"\"\"\n    Find indices of nearest values in an array for multiple values.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Input array to search in.\n    vals : np.ndarray\n        Array of values to find nearest indices for.\n\n    Returns\n    -------\n    np.ndarray\n        Array of indices into array that are closest to vals.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n    &gt;&gt;&gt; vals = np.array([2, 4, 6, 8])\n    &gt;&gt;&gt; find_nearest_indices(array, vals)\n    array([0, 1, 2, 3])\n\n    Notes\n    -----\n    Wrapper around find_nearest_idx().\n    \"\"\"\n    return np.array([find_nearest_idx(array, val) for val in vals], dtype=int)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.find_threshold_crossing_events","title":"<code>find_threshold_crossing_events(x, threshold, *, mode='above')</code>","text":"<p>Find threshold crossing events. INCLUSIVE</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>numpy array</code> <p>Input data</p> required <code>threshold</code> <code>float</code> <p>The value whose crossing triggers an event</p> required <code>mode</code> <code>string, optional in ['above', 'below']; default 'above'</code> <p>event triggering above, or below threshold</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>eventlist</code> <code>list</code> <p>List containing the indices corresponding to threshold crossings</p> <code>eventmax</code> <code>list</code> <p>List containing the maximum value of each event</p> Source code in <code>nelpy/utils.py</code> <pre><code>def find_threshold_crossing_events(x, threshold, *, mode=\"above\"):\n    \"\"\"Find threshold crossing events. INCLUSIVE\n\n    Parameters\n    ----------\n    x : numpy array\n        Input data\n    threshold : float\n        The value whose crossing triggers an event\n    mode : string, optional in ['above', 'below']; default 'above'\n        event triggering above, or below threshold\n\n    Returns\n    -------\n    eventlist : list\n        List containing the indices corresponding to threshold crossings\n    eventmax : list\n        List containing the maximum value of each event\n    \"\"\"\n    from itertools import groupby\n    from operator import itemgetter\n\n    if mode == \"below\":\n        cross_threshold = np.where(x &lt;= threshold, 1, 0)\n    elif mode == \"above\":\n        cross_threshold = np.where(x &gt;= threshold, 1, 0)\n    else:\n        raise NotImplementedError(\n            \"mode {} not understood for find_threshold_crossing_events\".format(\n                str(mode)\n            )\n        )\n    eventlist = []\n    eventmax = []\n    for k, v in groupby(enumerate(cross_threshold), key=itemgetter(1)):\n        if k:\n            v = list(v)\n            eventlist.append([v[0][0], v[-1][0]])\n            try:\n                eventmax.append(x[v[0][0] : (v[-1][0] + 1)].max())\n            except Exception:\n                print(v, x[v[0][0] : v[-1][0]])\n    eventmax = np.asarray(eventmax)\n    eventlist = np.asarray(eventlist)\n    return eventlist, eventmax\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.frange","title":"<code>frange(start, stop, step)</code>","text":"<p>Generate a range of floating point values with a given step size.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>Start value.</p> required <code>stop</code> <code>float</code> <p>Stop value (exclusive).</p> required <code>step</code> <code>float</code> <p>Step size.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Array of values from start to stop (exclusive) with given step.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; frange(0, 1, 0.2)\narray([0. , 0.2, 0.4, 0.6, 0.8])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def frange(start, stop, step):\n    \"\"\"\n    Generate a range of floating point values with a given step size.\n\n    Parameters\n    ----------\n    start : float\n        Start value.\n    stop : float\n        Stop value (exclusive).\n    step : float\n        Step size.\n\n    Returns\n    -------\n    out : np.ndarray\n        Array of values from start to stop (exclusive) with given step.\n\n    Examples\n    --------\n    &gt;&gt;&gt; frange(0, 1, 0.2)\n    array([0. , 0.2, 0.4, 0.6, 0.8])\n    \"\"\"\n    num_steps = int(np.floor((stop - start) / step))\n    return np.linspace(start, stop, num=num_steps, endpoint=False)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.gaussian_filter","title":"<code>gaussian_filter(obj, *, fs=None, sigma=None, truncate=None, inplace=False, mode=None, cval=None, within_intervals=False)</code>","text":"<p>Smooths with a Gaussian kernel.</p> <p>Smoothing is applied along the abscissa, and the same smoothing is applied to each signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.</p> <p>Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.</code> required <code>fs</code> <code>float</code> <p>Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will be inferred.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05 (50 ms if base_unit=seconds).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <code>mode</code> <code>(reflect, constant, nearest, mirror, wrap)</code> <p>The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to 'constant'. Default is 'reflect'.</p> <code>'reflect'</code> <code>cval</code> <code>scalar</code> <p>Value to fill past edges of input if mode is 'constant'. Default is 0.0.</p> <code>None</code> <code>within_intervals</code> <code>boolean</code> <p>If True, then smooth within each epoch. Otherwise smooth across epochs. Default is False. Note that when mode = 'wrap', then smoothing within epochs aren't affected by wrapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as obj</code> <p>An object with smoothed data is returned.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef gaussian_filter(\n    obj,\n    *,\n    fs=None,\n    sigma=None,\n    truncate=None,\n    inplace=False,\n    mode=None,\n    cval=None,\n    within_intervals=False,\n):\n    \"\"\"Smooths with a Gaussian kernel.\n\n    Smoothing is applied along the abscissa, and the same smoothing is applied to each\n    signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n    Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n    Parameters\n    ----------\n    obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n    fs : float, optional\n        Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n        be inferred.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n        (50 ms if base_unit=seconds).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0.\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        'reflect'.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n    within_intervals : boolean, optional\n        If True, then smooth within each epoch. Otherwise smooth across epochs.\n        Default is False.\n        Note that when mode = 'wrap', then smoothing within epochs aren't affected\n        by wrapping.\n\n    Returns\n    -------\n    out : same type as obj\n        An object with smoothed data is returned.\n\n    \"\"\"\n    if sigma is None:\n        sigma = 0.05\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n\n    if not inplace:\n        out = copy.deepcopy(obj)\n    else:\n        out = obj\n\n    if isinstance(out, core.RegularlySampledAnalogSignalArray) or isinstance(\n        out, core._analogsignalarray.PositionArray\n    ):\n        if fs is None:\n            fs = out.fs\n        if fs is None:\n            raise ValueError(\n                \"fs must either be specified, or must be contained in the {}!\".format(\n                    out.type_name\n                )\n            )\n    elif isinstance(out, core.BinnedEventArray) or isinstance(\n        out, core._valeventarray.BinnedValueEventArray\n    ):\n        bst = out\n        if fs is None:\n            fs = 1 / bst.ds\n        if fs is None:\n            raise ValueError(\n                \"fs must either be specified, or must be contained in the {}!\".format(\n                    out.type_name\n                )\n            )\n    else:\n        raise NotImplementedError(\n            \"gaussian_filter for {} is not yet supported!\".format(str(type(out)))\n        )\n\n    sigma = sigma * fs\n\n    if not within_intervals:\n        # see https://stackoverflow.com/questions/18697532/gaussian-filtering-a-image-with-nan-in-python\n        # (1) if smoothing across intervals, we work on a merged support\n        # (2) build abscissa_vals, including existing ones, and out-of-support ones\n        # (3) to smooth U, build auxiliary arrays V and W, with (V=U).nan=0, and (W=1).nan=0\n        # (4) Z = smooth(V)/smooth(W)\n        # (5) only keep original support, and original abscissa_vals\n\n        if isinstance(\n            out,\n            (\n                core.RegularlySampledAnalogSignalArray,\n                core.BinnedEventArray,\n                core._analogsignalarray.PositionArray,\n                core._valeventarray.BinnedValueEventArray,\n            ),\n        ):\n            support = out._abscissa.support.merge()\n            if not support.domain.is_finite:\n                support.domain = (\n                    support.start,\n                    support.stop,\n                )  # TODO: #FIXME might come from abscissa definition, and not from support\n\n            missing_abscissa_vals = []\n            for interval in ~support:\n                missing_vals = frange(interval.start, interval.stop, 1 / fs)\n                missing_abscissa_vals.extend(missing_vals)\n\n            if isinstance(\n                out,\n                (\n                    core.RegularlySampledAnalogSignalArray,\n                    core._analogsignalarray.PositionArray,\n                ),\n            ):\n                n_signals = out.n_signals\n                n_samples = out.n_samples\n                V = np.zeros((n_signals, n_samples + len(missing_abscissa_vals)))\n                W = np.ones(V.shape)\n                all_abscissa_vals = np.sort(\n                    np.append(out._abscissa_vals, missing_abscissa_vals)\n                )\n                data_idx = np.searchsorted(all_abscissa_vals, out._abscissa_vals)\n                missing_idx = np.searchsorted(all_abscissa_vals, missing_abscissa_vals)\n                V[:, data_idx] = out.data\n                W[:, missing_idx] = 0\n\n                VV = scipy_gaussian_filter(\n                    V, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                )\n                WW = scipy_gaussian_filter(\n                    W, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                )\n\n                Z = VV[:, data_idx] / WW[:, data_idx]\n                out._data = Z\n            elif isinstance(\n                out, (core.BinnedEventArray, core._valeventarray.BinnedValueEventArray)\n            ):\n                n_signals = out.n_series\n                n_samples = out.n_bins\n                if out.data.ndim == 2:\n                    # (n_series, n_bins) - legacy BinnedEventArray\n                    V = np.zeros((n_signals, n_samples + len(missing_abscissa_vals)))\n                    W = np.ones(V.shape)\n                    all_abscissa_vals = np.sort(\n                        np.append(out._abscissa_vals, missing_abscissa_vals)\n                    )\n                    data_idx = np.searchsorted(all_abscissa_vals, out._abscissa_vals)\n                    missing_idx = np.searchsorted(\n                        all_abscissa_vals, missing_abscissa_vals\n                    )\n                    V[:, data_idx] = out.data\n                    W[:, missing_idx] = 0\n\n                    VV = scipy_gaussian_filter(\n                        V, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                    )\n                    WW = scipy_gaussian_filter(\n                        W, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                    )\n\n                    Z = VV[:, data_idx] / WW[:, data_idx]\n                    out._data = Z\n                elif out.data.ndim == 3:\n                    # (n_series, n_bins, n_values) - BinnedValueEventArray\n                    n_values = out.data.shape[2]\n                    V = np.zeros(\n                        (n_signals, n_samples + len(missing_abscissa_vals), n_values)\n                    )\n                    W = np.ones(V.shape)\n                    all_abscissa_vals = np.sort(\n                        np.append(out._abscissa_vals, missing_abscissa_vals)\n                    )\n                    data_idx = np.searchsorted(all_abscissa_vals, out._abscissa_vals)\n                    missing_idx = np.searchsorted(\n                        all_abscissa_vals, missing_abscissa_vals\n                    )\n                    V[:, data_idx, :] = out.data\n                    W[:, missing_idx, :] = 0\n                    VV = np.empty_like(V)\n                    WW = np.empty_like(W)\n                    for v in range(n_values):\n                        VV[..., v] = scipy_gaussian_filter(\n                            V[..., v],\n                            sigma=(0, sigma),\n                            truncate=truncate,\n                            mode=mode,\n                            cval=cval,\n                        )\n                        WW[..., v] = scipy_gaussian_filter(\n                            W[..., v],\n                            sigma=(0, sigma),\n                            truncate=truncate,\n                            mode=mode,\n                            cval=cval,\n                        )\n                    Z = VV[:, data_idx, :] / WW[:, data_idx, :]\n                    out._data = Z\n                else:\n                    raise ValueError(\n                        \"Unsupported data shape for BinnedValueEventArray: {}\".format(\n                            out.data.shape\n                        )\n                    )\n        else:\n            raise NotImplementedError(\n                \"gaussian_filter across intervals for {} is not yet supported!\".format(\n                    str(type(out))\n                )\n            )\n    else:  # within intervals:\n        cum_lengths = np.insert(np.cumsum(out.lengths), 0, 0)\n        out._data = out._data.astype(float)\n\n        if isinstance(out, core.RegularlySampledAnalogSignalArray):\n            # now smooth each interval separately\n            for idx in range(out.n_intervals):\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                    scipy_gaussian_filter(\n                        out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                        sigma=(0, sigma),\n                        truncate=truncate,\n                    )\n                )\n        elif isinstance(out, core.BinnedSpikeTrainArray):\n            # now smooth each interval separately\n            for idx in range(out.n_epochs):\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                    scipy_gaussian_filter(\n                        out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                        sigma=(0, sigma),\n                        truncate=truncate,\n                    )\n                )\n                # out._data[:,cum_lengths[idx]:cum_lengths[idx+1]] = self._smooth_array(out._data[:,cum_lengths[idx]:cum_lengths[idx+1]], w=w)\n        elif isinstance(out, core._valeventarray.BinnedValueEventArray):\n            # now smooth each interval separately for each value type\n            if out.data.ndim == 3:\n                for idx in range(out.n_intervals):\n                    for v in range(out.data.shape[2]):\n                        out._data[:, cum_lengths[idx] : cum_lengths[idx + 1], v] = (\n                            scipy_gaussian_filter(\n                                out._data[\n                                    :, cum_lengths[idx] : cum_lengths[idx + 1], v\n                                ],\n                                sigma=(0, sigma),\n                                truncate=truncate,\n                            )\n                        )\n            else:\n                # fallback to 2D smoothing (shouldn't happen for BinnedValueEventArray, but for safety)\n                for idx in range(out.n_intervals):\n                    out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                        scipy_gaussian_filter(\n                            out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                            sigma=(0, sigma),\n                            truncate=truncate,\n                        )\n                    )\n\n    return out\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_PBEs","title":"<code>get_PBEs(data, fs=None, ds=None, sigma=None, truncate=None, unsorted_id=0, min_active=None, minLength=None, maxLength=None, PrimaryThreshold=None, minThresholdLength=None, SecondaryThreshold=None)</code>","text":"<p>Determine PBEs from multiunit activity or spike trains.</p> Definitions <p>MUA : multiunit activity PBE : population burst event</p> Summary <p>This function can be used to identify PBE epochs from spike trains, binned spike trains, or multiunit activity (in the form of an AnalogSignalArray).</p> <p>It is recommended to either pass in a SpikeTrainArray or a BinnedSpikeTrainArray, so that a <code>min_active</code> number of sorted units can be set.</p> <p>It is also recommended that the unsorted units (but not noise artifacts!) should be included in the spike train that is used to estimate the PBEs. By default, unit_id=0 is assumed to be unsorted, but this can be changed, or if no unsorted units are present, you can set unsorted_id=None. Equivalently, if min_active=0, then no restriction will apply, and the unsorted_id will have no effect on the final PBE epochs.</p> <p>Examples:</p> <p>PBE_epochs = get_PBEs(mua_asa) PBE_epochs = get_PBEs(spiketrain, min_active=5) PBE_epochs = get_PBEs(binnedspiketrain, min_active=5)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz]. -- OR --</p> required <code>data</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray with multiple units, including unsorted unit(s), but excluding any noise artifects. -- OR --</p> required <code>data</code> <code>BinnedSpikeTrainArray</code> <p>BinnedSpikeTrainArray containing multiunit activity.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of mua, in Hz. If not specified, it will be inferred from data.</p> <code>None</code> <code>ds</code> <code>float</code> <p>Time step in which to bin spikes. Default is 1 ms.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation (in seconds) of Gaussian smoothing kernel. Default is 10 ms. If sigma==0 then no smoothing is applied.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth of the Gaussian filter. Default is 6.</p> <code>None</code> <code>unsorted_id</code> <code>int</code> <p>unit_id of the unsorted unit. Default is 0. If no unsorted unit is present, then set unsorted_id = None</p> <code>0</code> <code>min_active</code> <code>int</code> <p>Minimum number of active units per event, excluding unsorted unit. Default is 5.</p> <code>None</code> <code>minLength</code> <code>float</code> <p>Minimum event duration in seconds. Default is 50 ms.</p> <code>None</code> <code>maxLength</code> <code>float</code> <p>Maximum event duration in seconds. Default is 750 ms.</p> <code>None</code> <code>PrimaryThreshold</code> <code>float</code> <p>Primary threshold to exceed. Default is mean() + 3*std()</p> <code>None</code> <code>SecondaryThreshold</code> <code>float</code> <p>Secondary threshold to fall back to. Default is mean().</p> <code>None</code> <code>minThresholdLength</code> <code>float</code> <p>Minimum duration to stay above PrimaryThreshold. Default is 0 ms.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>PBE_epochs</code> <code>EpochArray</code> <p>EpochArray containing all the PBEs.</p> Future improvements <p>As of now, it is possible, but not easy to specify the Primary and Secondary thresholds for event detection. A slight change in API might be needed to make this specification more flexible.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef get_PBEs(\n    data,\n    fs=None,\n    ds=None,\n    sigma=None,\n    truncate=None,\n    unsorted_id=0,\n    min_active=None,\n    minLength=None,\n    maxLength=None,\n    PrimaryThreshold=None,\n    minThresholdLength=None,\n    SecondaryThreshold=None,\n):\n    \"\"\"Determine PBEs from multiunit activity or spike trains.\n\n    Definitions\n    -----------\n    MUA : multiunit activity\n    PBE : population burst event\n\n    Summary\n    -------\n    This function can be used to identify PBE epochs from spike trains, binned\n    spike trains, or multiunit activity (in the form of an AnalogSignalArray).\n\n    It is recommended to either pass in a SpikeTrainArray or a\n    BinnedSpikeTrainArray, so that a `min_active` number of sorted units can be\n    set.\n\n    It is also recommended that the unsorted units (but not noise artifacts!)\n    should be included in the spike train that is used to estimate the PBEs. By\n    default, unit_id=0 is assumed to be unsorted, but this can be changed, or if\n    no unsorted units are present, you can set unsorted_id=None. Equivalently,\n    if min_active=0, then no restriction will apply, and the unsorted_id will\n    have no effect on the final PBE epochs.\n\n    Examples\n    --------\n    PBE_epochs = get_PBEs(mua_asa)\n    PBE_epochs = get_PBEs(spiketrain, min_active=5)\n    PBE_epochs = get_PBEs(binnedspiketrain, min_active=5)\n\n    Parameters\n    ----------\n    data : AnalogSignalArray\n        AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz].\n     -- OR --\n    data : SpikeTrainArray\n        SpikeTrainArray with multiple units, including unsorted unit(s), but\n        excluding any noise artifects.\n     -- OR --\n    data : BinnedSpikeTrainArray\n        BinnedSpikeTrainArray containing multiunit activity.\n    fs : float, optional\n        Sampling frequency of mua, in Hz. If not specified, it will be inferred\n        from data.\n    ds : float, optional\n        Time step in which to bin spikes. Default is 1 ms.\n    sigma : float, optional\n        Standard deviation (in seconds) of Gaussian smoothing kernel.\n        Default is 10 ms. If sigma==0 then no smoothing is applied.\n    truncate : float, optional\n        Bandwidth of the Gaussian filter. Default is 6.\n    unsorted_id : int, optional\n        unit_id of the unsorted unit. Default is 0. If no unsorted unit is\n        present, then set unsorted_id = None\n    min_active : int, optional\n        Minimum number of active units per event, excluding unsorted unit.\n        Default is 5.\n    minLength : float, optional\n        Minimum event duration in seconds. Default is 50 ms.\n    maxLength : float, optional\n        Maximum event duration in seconds. Default is 750 ms.\n    PrimaryThreshold : float, optional\n        Primary threshold to exceed. Default is mean() + 3*std()\n    SecondaryThreshold : float, optional\n        Secondary threshold to fall back to. Default is mean().\n    minThresholdLength : float, optional\n        Minimum duration to stay above PrimaryThreshold. Default is 0 ms.\n\n    Returns\n    -------\n    PBE_epochs : EpochArray\n        EpochArray containing all the PBEs.\n\n    Future improvements\n    -------------------\n    As of now, it is possible, but not easy to specify the Primary and Secondary\n    thresholds for event detection. A slight change in API might be needed to\n    make this specification more flexible.\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.01  # 10 ms standard deviation\n    if truncate is None:\n        truncate = 6\n\n    if isinstance(data, core.AnalogSignalArray):\n        # if we have only mua, then we cannot set (ds, unsorted_id, min_active)\n        if ds is not None:\n            raise ValueError(\n                \"if data is an AnalogSignalArray then ds cannot be specified!\"\n            )\n        if unsorted_id:\n            raise ValueError(\n                \"if data is an AnalogSignalArray then unsorted_id cannot be specified!\"\n            )\n        if min_active is not None:\n            raise ValueError(\n                \"if data is an AnalogSignalArray then min_active cannot be specified!\"\n            )\n        mua = data\n        mua._data = mua._data.astype(float)\n        if (sigma != 0) and (truncate &gt; 0):\n            mua = gaussian_filter(mua, sigma=sigma, truncate=truncate)\n\n    elif isinstance(data, (core.EventArray, core.BinnedEventArray)):\n        # set default parameter values:\n        if ds is None:\n            ds = 0.001  # default 1 ms\n        if min_active is None:\n            min_active = 5\n        mua = get_mua(data, ds=ds, sigma=sigma, truncate=truncate, _fast=True)\n    else:\n        raise TypeError(\n            \"data has to be one of (AnalogSignalArray, SpikeTrainArray, BinnedSpikeTrainArray)\"\n        )\n\n    # set default parameter values:\n    if fs is None:\n        fs = mua.fs\n    if minLength is None:\n        minLength = 0.050  # 50 ms minimum event duration\n    if maxLength is None:\n        maxLength = 0.750  # 750 ms maximum event duration\n    if minThresholdLength is None:\n        minThresholdLength = 0.0\n    # if PrimaryThreshold is None:\n    #         PrimaryThreshold =\n    # if SecondaryThreshold is None:\n    #     SecondaryThreshold =\n    PBE_epochs = get_mua_events(\n        mua=mua,\n        fs=fs,\n        minLength=minLength,\n        maxLength=maxLength,\n        PrimaryThreshold=PrimaryThreshold,\n        minThresholdLength=minThresholdLength,\n        SecondaryThreshold=SecondaryThreshold,\n    )\n\n    # now require min_active number of sorted cells\n    if isinstance(data, (core.EventArray, core.BinnedEventArray)):\n        if min_active &gt; 0:\n            if unsorted_id is not None:\n                # remove unsorted unit, if present:\n                unit_ids = copy.deepcopy(data.unit_ids)\n                try:\n                    unit_ids.remove(unsorted_id)\n                except ValueError:\n                    pass\n                # data_ = data._unit_subset(unit_ids)\n                data_ = data.loc[:, unit_ids]\n            else:\n                data_ = data\n            # determine number of active units per epoch:\n            n_active = np.array([snippet.n_active for snippet in data_[PBE_epochs]])\n            active_epochs_idx = np.argwhere(n_active &gt; min_active).squeeze()\n            # only keep those epochs where sufficiently many units are active:\n            PBE_epochs = PBE_epochs[active_epochs_idx]\n    return PBE_epochs\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_contiguous_segments","title":"<code>get_contiguous_segments(data, *, step=None, assume_sorted=None, in_core=True, index=False, inclusive=False, fs=None, sort=None, in_memory=None)</code>","text":"<p>Compute contiguous segments (seperated by step) in a list.</p> <p>Note! This function requires that a sorted list is passed. It first checks if the list is sorted O(n), and only sorts O(n log(n)) if necessary. But if you know that the list is already sorted, you can pass assume_sorted=True, in which case it will skip the O(n) check.</p> <p>Returns an array of size (n_segments, 2), with each row being of the form ([start, stop]) [inclusive, exclusive].</p> <p>NOTE: when possible, use assume_sorted=True, and step=1 as explicit       arguments to function call.</p> <p>WARNING! Step is robustly computed in-core (i.e., when in_core is     True), but is assumed to be 1 when out-of-core.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [1, 2, 3, 4, 10, 11, 12]\n&gt;&gt;&gt; get_contiguous_segments(data)\n([1,5], [10,13])\n&gt;&gt;&gt; get_contiguous_segments(data, index=True)\n([0,4], [4,7])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>1D array of sequential data, typically assumed to be integral (sample numbers).</p> required <code>step</code> <code>float</code> <p>Expected step size for neighboring samples. Default uses numpy to find the median, but it is much faster and memory efficient to explicitly pass in step=1.</p> <code>None</code> <code>assume_sorted</code> <code>bool</code> <p>If assume_sorted == True, then data is not inspected or re-ordered. This can be significantly faster, especially for out-of-core computation, but it should only be used when you are confident that the data is indeed sorted, otherwise the results from get_contiguous_segments will not be reliable.</p> <code>None</code> <code>in_core</code> <code>bool</code> <p>If True, then we use np.diff which requires all the data to fit into memory simultaneously, otherwise we use groupby, which uses a generator to process potentially much larger chunks of data, but also much slower.</p> <code>True</code> <code>index</code> <code>bool</code> <p>If True, the indices of segment boundaries will be returned. Otherwise, the segment boundaries will be returned in terms of the data itself. Default is False.</p> <code>False</code> <code>inclusive</code> <code>bool</code> <p>If True, the boundaries are returned as [(inclusive idx, inclusive idx)] Default is False, and can only be used when index==True.</p> <code>False</code> Source code in <code>nelpy/utils.py</code> <pre><code>def get_contiguous_segments(\n    data,\n    *,\n    step=None,\n    assume_sorted=None,\n    in_core=True,\n    index=False,\n    inclusive=False,\n    fs=None,\n    sort=None,\n    in_memory=None,\n):\n    \"\"\"Compute contiguous segments (seperated by step) in a list.\n\n    Note! This function requires that a sorted list is passed.\n    It first checks if the list is sorted O(n), and only sorts O(n log(n))\n    if necessary. But if you know that the list is already sorted,\n    you can pass assume_sorted=True, in which case it will skip\n    the O(n) check.\n\n    Returns an array of size (n_segments, 2), with each row\n    being of the form ([start, stop]) [inclusive, exclusive].\n\n    NOTE: when possible, use assume_sorted=True, and step=1 as explicit\n          arguments to function call.\n\n    WARNING! Step is robustly computed in-core (i.e., when in_core is\n        True), but is assumed to be 1 when out-of-core.\n\n    Examples\n    -------\n    &gt;&gt;&gt; data = [1, 2, 3, 4, 10, 11, 12]\n    &gt;&gt;&gt; get_contiguous_segments(data)\n    ([1,5], [10,13])\n    &gt;&gt;&gt; get_contiguous_segments(data, index=True)\n    ([0,4], [4,7])\n\n    Parameters\n    ----------\n    data : array-like\n        1D array of sequential data, typically assumed to be integral (sample\n        numbers).\n    step : float, optional\n        Expected step size for neighboring samples. Default uses numpy to find\n        the median, but it is much faster and memory efficient to explicitly\n        pass in step=1.\n    assume_sorted : bool, optional\n        If assume_sorted == True, then data is not inspected or re-ordered. This\n        can be significantly faster, especially for out-of-core computation, but\n        it should only be used when you are confident that the data is indeed\n        sorted, otherwise the results from get_contiguous_segments will not be\n        reliable.\n    in_core : bool, optional\n        If True, then we use np.diff which requires all the data to fit\n        into memory simultaneously, otherwise we use groupby, which uses\n        a generator to process potentially much larger chunks of data,\n        but also much slower.\n    index : bool, optional\n        If True, the indices of segment boundaries will be returned. Otherwise,\n        the segment boundaries will be returned in terms of the data itself.\n        Default is False.\n    inclusive : bool, optional\n        If True, the boundaries are returned as [(inclusive idx, inclusive idx)]\n        Default is False, and can only be used when index==True.\n\n    Deprecated\n    ----------\n    in_memory : bool, optional\n        This is equivalent to the new 'in-core'.\n    sort : bool, optional\n        This is equivalent to the new 'assume_sorted'\n    fs : sampling rate (Hz) used to extend half-open interval support by 1/fs\n    \"\"\"\n\n    # handle deprecated API calls:\n    if in_memory:\n        in_core = in_memory\n        logging.warning(\"'in_memory' has been deprecated; use 'in_core' instead\")\n    if sort:\n        assume_sorted = sort\n        logging.warning(\"'sort' has been deprecated; use 'assume_sorted' instead\")\n    if fs:\n        step = 1 / fs\n        logging.warning(\"'fs' has been deprecated; use 'step' instead\")\n\n    if inclusive:\n        assert index, \"option 'inclusive' can only be used with 'index=True'\"\n    if in_core:\n        data = np.asarray(data)\n\n        if not assume_sorted:\n            if not is_sorted(data):\n                data = np.sort(data)  # algorithm assumes sorted list\n\n        if step is None:\n            step = np.median(np.diff(data))\n\n        # assuming that data(t1) is sampled somewhere on [t, t+1/fs) we have a 'continuous' signal as long as\n        # data(t2 = t1+1/fs) is sampled somewhere on [t+1/fs, t+2/fs). In the most extreme case, it could happen\n        # that t1 = t and t2 = t + 2/fs, i.e. a difference of 2 steps.\n\n        if np.any(np.diff(data) &lt; step):\n            logging.warning(\n                \"some steps in the data are smaller than the requested step size.\"\n            )\n\n        breaks = np.argwhere(np.diff(data) &gt;= 2 * step)\n        starts = np.insert(breaks + 1, 0, 0)\n        stops = np.append(breaks, len(data) - 1)\n        bdries = np.vstack((data[starts], data[stops] + step)).T\n        if index:\n            if inclusive:\n                indices = np.vstack((starts, stops)).T\n            else:\n                indices = np.vstack((starts, stops + 1)).T\n            return indices\n    else:\n        from itertools import groupby\n        from operator import itemgetter\n\n        if not assume_sorted:\n            if not is_sorted(data):\n                # data = np.sort(data)  # algorithm assumes sorted list\n                raise NotImplementedError(\n                    \"out-of-core sorting has not been implemented yet...\"\n                )\n\n        if step is None:\n            step = 1\n\n        bdries = []\n\n        if not index:\n            for k, g in groupby(enumerate(data), lambda ix: (ix[0] - ix[1])):\n                f = itemgetter(1)\n                gen = (f(x) for x in g)\n                start = next(gen)\n                stop = start\n                for stop in gen:\n                    pass\n                bdries.append([start, stop + step])\n        else:\n            counter = 0\n            for k, g in groupby(enumerate(data), lambda ix: (ix[0] - ix[1])):\n                f = itemgetter(1)\n                gen = (f(x) for x in g)\n                _ = next(gen)\n                start = counter\n                stop = start\n                for _ in gen:\n                    stop += 1\n                if inclusive:\n                    bdries.append([start, stop])\n                else:\n                    bdries.append([start, stop + 1])\n                counter = stop + 1\n\n    return np.asarray(bdries)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_direction","title":"<code>get_direction(asa, *, sigma=None)</code>","text":"<p>Return epochs during which an animal was running left to right, or right to left.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray 1D</code> <p>AnalogSignalArray containing the 1D position data.</p> required <code>sigma</code> <code>float</code> <p>Smoothing to apply to position (x) before computing gradient estimate. Default is 0.</p> <code>None</code> <p>Returns:</p> Type Description <code>l2r, r2l : EpochArrays</code> <p>EpochArrays corresponding to left-to-right and right-to-left movement.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_direction(asa, *, sigma=None):\n    \"\"\"Return epochs during which an animal was running left to right, or right\n    to left.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray 1D\n        AnalogSignalArray containing the 1D position data.\n    sigma : float, optional\n        Smoothing to apply to position (x) before computing gradient estimate.\n        Default is 0.\n\n    Returns\n    -------\n    l2r, r2l : EpochArrays\n        EpochArrays corresponding to left-to-right and right-to-left movement.\n    \"\"\"\n    if sigma is None:\n        sigma = 0\n    if not isinstance(asa, core.AnalogSignalArray):\n        raise TypeError(\"AnalogSignalArray expected!\")\n    assert asa.n_signals == 1, \"1D AnalogSignalArray expected!\"\n\n    direction = dxdt_AnalogSignalArray(asa.smooth(sigma=sigma), rectify=False).data\n    direction[direction &gt;= 0] = 1\n    direction[direction &lt; 0] = -1\n    direction = direction.squeeze()\n\n    l2r = get_contiguous_segments(np.argwhere(direction &gt; 0).squeeze(), step=1)\n    l2r[:, 1] -= (\n        1  # change bounds from [inclusive, exclusive] to [inclusive, inclusive]\n    )\n    l2r = core.EpochArray(asa.abscissa_vals[l2r])\n\n    r2l = get_contiguous_segments(np.argwhere(direction &lt; 0).squeeze(), step=1)\n    r2l[:, 1] -= (\n        1  # change bounds from [inclusive, exclusive] to [inclusive, inclusive]\n    )\n    r2l = core.EpochArray(asa.abscissa_vals[r2l])\n\n    return l2r, r2l\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_events_boundaries","title":"<code>get_events_boundaries(x, *, PrimaryThreshold=None, SecondaryThreshold=None, minThresholdLength=None, minLength=None, maxLength=None, ds=None, mode='above')</code>","text":"<p>get event boundaries such that event.max &gt;= PrimaryThreshold and the event extent is defined by SecondaryThreshold.</p> <p>Note that when PrimaryThreshold==SecondaryThreshold, then this is a simple threshold crossing algorithm.</p> <p>NB. minLength and maxLength are applied to the SecondaryThreshold     events, whereas minThresholdLength is applied to the     PrimaryThreshold events.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>numpy array</code> <p>Input data</p> required <code>mode</code> <code>string, optional in ['above', 'below']; default 'above'</code> <p>event triggering above, or below threshold</p> <code>'above'</code> <code>PrimaryThreshold</code> <code>float</code> <p>If mode=='above', requires that event.max &gt;= PrimaryThreshold If mode=='below', requires that event.min &lt;= PrimaryThreshold</p> <code>None</code> <code>SecondaryThreshold</code> <code>float</code> <p>The value that defines the event extent</p> <code>None</code> <code>minThresholdLength</code> <code>float</code> <p>Minimum duration for which the PrimaryThreshold is crossed</p> <code>None</code> <code>minLength</code> <code>float</code> <p>Minimum duration for which the SecondaryThreshold is crossed</p> <code>None</code> <code>maxLength</code> <code>float</code> <p>Maximum duration for which the SecondaryThreshold is crossed</p> <code>None</code> <code>ds</code> <code>float</code> <p>Time step of the input data x</p> <code>None</code> <p>Returns:</p> Type Description <code>returns bounds, maxes, events</code> <p>where bounds &lt;==&gt; SecondaryThreshold to SecondaryThreshold, inclusive       maxes  &lt;==&gt; maximum value during each event       events &lt;==&gt; PrimaryThreshold to PrimaryThreshold, inclusive</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_events_boundaries(\n    x,\n    *,\n    PrimaryThreshold=None,\n    SecondaryThreshold=None,\n    minThresholdLength=None,\n    minLength=None,\n    maxLength=None,\n    ds=None,\n    mode=\"above\",\n):\n    \"\"\"get event boundaries such that event.max &gt;= PrimaryThreshold\n    and the event extent is defined by SecondaryThreshold.\n\n    Note that when PrimaryThreshold==SecondaryThreshold, then this is a\n    simple threshold crossing algorithm.\n\n    NB. minLength and maxLength are applied to the SecondaryThreshold\n        events, whereas minThresholdLength is applied to the\n        PrimaryThreshold events.\n\n    Parameters\n    ----------\n    x : numpy array\n        Input data\n    mode : string, optional in ['above', 'below']; default 'above'\n        event triggering above, or below threshold\n    PrimaryThreshold : float, optional\n        If mode=='above', requires that event.max &gt;= PrimaryThreshold\n        If mode=='below', requires that event.min &lt;= PrimaryThreshold\n    SecondaryThreshold : float, optional\n        The value that defines the event extent\n    minThresholdLength : float, optional\n        Minimum duration for which the PrimaryThreshold is crossed\n    minLength : float, optional\n        Minimum duration for which the SecondaryThreshold is crossed\n    maxLength : float, optional\n        Maximum duration for which the SecondaryThreshold is crossed\n    ds : float, optional\n        Time step of the input data x\n\n    Returns\n    -------\n    returns bounds, maxes, events\n        where bounds &lt;==&gt; SecondaryThreshold to SecondaryThreshold, inclusive\n              maxes  &lt;==&gt; maximum value during each event\n              events &lt;==&gt; PrimaryThreshold to PrimaryThreshold, inclusive\n    \"\"\"\n\n    # TODO: x must be a numpy array\n    # TODO: ds is often used, but we have no default, and no check for when\n    #       it is left as None.\n    # TODO: the Docstring should equally be improved.\n\n    x = x.squeeze()\n    if x.ndim &gt; 1:\n        raise TypeError(\"multidimensional arrays not supported!\")\n\n    if PrimaryThreshold is None:  # by default, threshold is 3 SDs above mean of x\n        PrimaryThreshold = np.mean(x) + 3 * np.std(x)\n\n    if SecondaryThreshold is None:  # by default, revert back to mean of x\n        SecondaryThreshold = np.mean(x)  # + 0*np.std(x)\n\n    events, primary_maxes = find_threshold_crossing_events(\n        x=x, threshold=PrimaryThreshold, mode=mode\n    )\n\n    # apply minThresholdLength criterion:\n    if minThresholdLength is not None and len(events) &gt; 0:\n        durations = (events[:, 1] - events[:, 0] + 1) * ds\n        events = events[durations &gt;= minThresholdLength]\n\n    if len(events) == 0:\n        bounds, maxes, events = [], [], []\n        logging.warning(\"no events satisfied criteria\")\n        return bounds, maxes, events\n\n    # Find periods where value is &gt; SecondaryThreshold; note that the previous periods should be within these!\n    if mode == \"above\":\n        assert SecondaryThreshold &lt;= PrimaryThreshold, (\n            \"Secondary Threshold by definition should include more data than Primary Threshold\"\n        )\n    elif mode == \"below\":\n        assert SecondaryThreshold &gt;= PrimaryThreshold, (\n            \"Secondary Threshold by definition should include more data than Primary Threshold\"\n        )\n    else:\n        raise NotImplementedError(\n            \"mode {} not understood for find_threshold_crossing_events\".format(\n                str(mode)\n            )\n        )\n\n    bounds, broader_maxes = find_threshold_crossing_events(\n        x=x, threshold=SecondaryThreshold, mode=mode\n    )\n\n    # Find corresponding big windows for potential events\n    #  Specifically, look for closest left edge that is just smaller\n    outer_boundary_indices = np.searchsorted(bounds[:, 0], events[:, 0], side=\"right\")\n    #  searchsorted finds the index after, so subtract one to get index before\n    outer_boundary_indices = outer_boundary_indices - 1\n\n    # Find extended boundaries for events by pairing to larger windows\n    #   (Note that there may be repeats if the larger window contains multiple &gt; 3SD sections)\n    bounds = bounds[outer_boundary_indices, :]\n    maxes = broader_maxes[outer_boundary_indices]\n\n    if minLength is not None and len(events) &gt; 0:\n        durations = (bounds[:, 1] - bounds[:, 0] + 1) * ds\n        # TODO: refactor [durations &lt;= maxLength] but be careful about edge cases\n        bounds = bounds[durations &gt;= minLength]\n        maxes = maxes[durations &gt;= minLength]\n        events = events[durations &gt;= minLength]\n\n    if maxLength is not None and len(events) &gt; 0:\n        durations = (bounds[:, 1] - bounds[:, 0] + 1) * ds\n        # TODO: refactor [durations &lt;= maxLength] but be careful about edge cases\n        bounds = bounds[durations &lt;= maxLength]\n        maxes = maxes[durations &lt;= maxLength]\n        events = events[durations &lt;= maxLength]\n\n    if len(events) == 0:\n        bounds, maxes, events = [], [], []\n        logging.warning(\"no events satisfied criteria\")\n        return bounds, maxes, events\n\n    # Now, since all that we care about are the larger windows, so we should get rid of repeats\n    _, unique_idx = np.unique(bounds[:, 0], return_index=True)\n    bounds = bounds[unique_idx, :]  # SecondaryThreshold to SecondaryThreshold\n    maxes = maxes[unique_idx]  # maximum value during event\n    events = events[unique_idx, :]  # PrimaryThreshold to PrimaryThreshold\n\n    return bounds, maxes, events\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_inactive_epochs","title":"<code>get_inactive_epochs(speed, v1=5, v2=7)</code>","text":"<p>Return epochs where animal is running no faster than specified by v1 and v2.</p> <p>Parameters:</p> Name Type Description Default <code>speed</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray containing single channel speed, in units/sec</p> required <code>v1</code> <code>float</code> <p>Minimum speed (in same units as speed) that has to be reached / exceeded during an event. Default is 10 [units/sec]</p> <code>5</code> <code>v2</code> <code>float</code> <p>Speed that defines the event boundaries. Default is 8 [units/sec]</p> <code>7</code> <p>Returns:</p> Name Type Description <code>inactive_epochs</code> <code>EpochArray</code> <p>EpochArray with all the epochs where speed satisfied the criteria.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_inactive_epochs(speed, v1=5, v2=7):\n    \"\"\"Return epochs where animal is running no faster than specified by\n    v1 and v2.\n\n    Parameters\n    ----------\n    speed : AnalogSignalArray\n        AnalogSignalArray containing single channel speed, in units/sec\n    v1 : float, optional\n        Minimum speed (in same units as speed) that has to be reached /\n        exceeded during an event. Default is 10 [units/sec]\n    v2 : float, optional\n        Speed that defines the event boundaries. Default is 8 [units/sec]\n    Returns\n    -------\n    inactive_epochs : EpochArray\n        EpochArray with all the epochs where speed satisfied the criteria.\n    \"\"\"\n    inactive_epochs = get_threshold_crossing_epochs(\n        asa=speed, t1=v1, t2=v2, mode=\"below\"\n    )\n    return inactive_epochs\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_mua","title":"<code>get_mua(st, ds=None, sigma=None, truncate=None, _fast=True)</code>","text":"<p>Compute the multiunit activity (MUA) from a spike train.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray containing one or more units. -- OR --</p> required <code>st</code> <code>BinnedSpikeTrainArray</code> <p>BinnedSpikeTrainArray containing multiunit activity.</p> required <code>ds</code> <code>float</code> <p>Time step in which to bin spikes. Default is 1 ms.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation (in seconds) of Gaussian smoothing kernel. Default is 10 ms. If sigma==0 then no smoothing is applied.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth of the Gaussian filter. Default is 6.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mua</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray with MUA.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef get_mua(st, ds=None, sigma=None, truncate=None, _fast=True):\n    \"\"\"Compute the multiunit activity (MUA) from a spike train.\n\n    Parameters\n    ----------\n    st : SpikeTrainArray\n        SpikeTrainArray containing one or more units.\n     -- OR --\n    st : BinnedSpikeTrainArray\n        BinnedSpikeTrainArray containing multiunit activity.\n    ds : float, optional\n        Time step in which to bin spikes. Default is 1 ms.\n    sigma : float, optional\n        Standard deviation (in seconds) of Gaussian smoothing kernel.\n        Default is 10 ms. If sigma==0 then no smoothing is applied.\n    truncate : float, optional\n        Bandwidth of the Gaussian filter. Default is 6.\n\n    Returns\n    -------\n    mua : AnalogSignalArray\n        AnalogSignalArray with MUA.\n    \"\"\"\n\n    if ds is None:\n        ds = 0.001  # 1 ms bin size\n    if sigma is None:\n        sigma = 0.01  # 10 ms standard deviation\n    if truncate is None:\n        truncate = 6\n\n    if isinstance(st, core.EventArray):\n        # bin spikes, so that we can count the spikes\n        mua_binned = st.bin(ds=ds).flatten()\n    elif isinstance(st, core.BinnedEventArray):\n        mua_binned = st.flatten()\n        ds = mua_binned.ds\n    else:\n        raise TypeError(\"st has to be one of (SpikeTrainArray, BinnedSpikeTrainArray)\")\n\n    # make sure data type is float, so that smoothing works, and convert to rate\n    mua_binned._data = mua_binned._data.astype(float) / ds\n\n    # TODO: now that we can simply cast from BST to ASA and back, the following logic could be simplified:\n    # put mua rate inside an AnalogSignalArray\n    if _fast:\n        mua = core.AnalogSignalArray([], empty=True)\n        mua._data = mua_binned.data\n        mua._abscissa_vals = mua_binned.bin_centers\n        mua._abscissa.support = mua_binned.support\n    else:\n        mua = core.AnalogSignalArray(\n            mua_binned.data, timestamps=mua_binned.bin_centers, fs=1 / ds\n        )\n\n    mua._fs = 1 / ds\n\n    if (sigma != 0) and (truncate &gt; 0):\n        mua = gaussian_filter(mua, sigma=sigma, truncate=truncate)\n\n    return mua\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_mua_events","title":"<code>get_mua_events(mua, fs=None, minLength=None, maxLength=None, PrimaryThreshold=None, minThresholdLength=None, SecondaryThreshold=None)</code>","text":"<p>Determine MUA/PBEs from multiunit activity.</p> <p>MUA : multiunit activity PBE : population burst event</p> <p>Parameters:</p> Name Type Description Default <code>mua</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz].</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of mua, in Hz. If not specified, it will be inferred from mua.fs</p> <code>None</code> <code>minLength</code> <code>float</code> <code>None</code> <code>maxLength</code> <code>float</code> <code>None</code> <code>PrimaryThreshold</code> <code>float</code> <code>None</code> <code>SecondaryThreshold</code> <code>float</code> <code>None</code> <code>minThresholdLength</code> <code>float</code> <code>None</code> <p>Returns:</p> Name Type Description <code>mua_epochs</code> <code>EpochArray</code> <p>EpochArray containing all the MUA events / PBEs.</p> <p>Examples:</p> <p>mua = get_mua(spiketrain) mua_epochs = get_mua_events(mua) PBEs = get_PBEs(spiketrain, min_active=5)      = get_PBEs(get_mua_events(get_mua(*)), spiketrain, min_active=5)</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_mua_events(\n    mua,\n    fs=None,\n    minLength=None,\n    maxLength=None,\n    PrimaryThreshold=None,\n    minThresholdLength=None,\n    SecondaryThreshold=None,\n):\n    \"\"\"Determine MUA/PBEs from multiunit activity.\n\n    MUA : multiunit activity\n    PBE : population burst event\n\n    Parameters\n    ----------\n    mua : AnalogSignalArray\n        AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz].\n    fs : float, optional\n        Sampling frequency of mua, in Hz. If not specified, it will be inferred from\n        mua.fs\n    minLength : float, optional\n    maxLength : float, optional\n    PrimaryThreshold : float, optional\n    SecondaryThreshold : float, optional\n    minThresholdLength : float, optional\n\n    Returns\n    -------\n    mua_epochs : EpochArray\n        EpochArray containing all the MUA events / PBEs.\n\n    Examples\n    --------\n    mua = get_mua(spiketrain)\n    mua_epochs = get_mua_events(mua)\n    PBEs = get_PBEs(spiketrain, min_active=5)\n         = get_PBEs(get_mua_events(get_mua(*)), spiketrain, min_active=5)\n    \"\"\"\n\n    if fs is None:\n        fs = mua.fs\n    if fs is None:\n        raise ValueError(\"fs must either be specified, or must be contained in mua!\")\n\n    if PrimaryThreshold is None:\n        PrimaryThreshold = mua.mean() + 3 * mua.std()\n    if SecondaryThreshold is None:\n        SecondaryThreshold = mua.mean()\n    if minLength is None:\n        minLength = 0.050  # 50 ms minimum event duration\n    if maxLength is None:\n        maxLength = 0.750  # 750 ms maximum event duration\n    if minThresholdLength is None:\n        minThresholdLength = 0.0\n\n    # determine MUA event bounds:\n    mua_bounds_idx, maxes, _ = get_events_boundaries(\n        x=mua.data,\n        PrimaryThreshold=PrimaryThreshold,\n        SecondaryThreshold=SecondaryThreshold,\n        minThresholdLength=minThresholdLength,\n        minLength=minLength,\n        maxLength=maxLength,\n        ds=1 / fs,\n    )\n\n    if len(mua_bounds_idx) == 0:\n        logging.warning(\"no mua events detected\")\n        return core.EpochArray(empty=True)\n\n    # store MUA bounds in an EpochArray\n    mua_epochs = core.EpochArray(mua.time[mua_bounds_idx])\n\n    return mua_epochs\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_run_epochs","title":"<code>get_run_epochs(speed, v1=10, v2=8)</code>","text":"<p>Return epochs where animal is running at least as fast as specified by v1 and v2.</p> <p>Parameters:</p> Name Type Description Default <code>speed</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray containing single channel speed, in units/sec</p> required <code>v1</code> <code>float</code> <p>Minimum speed (in same units as speed) that has to be reached / exceeded during an event. Default is 10 [units/sec]</p> <code>10</code> <code>v2</code> <code>float</code> <p>Speed that defines the event boundaries. Default is 8 [units/sec]</p> <code>8</code> <p>Returns:</p> Name Type Description <code>run_epochs</code> <code>EpochArray</code> <p>EpochArray with all the epochs where speed satisfied the criteria.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_run_epochs(speed, v1=10, v2=8):\n    \"\"\"Return epochs where animal is running at least as fast as\n    specified by v1 and v2.\n\n    Parameters\n    ----------\n    speed : AnalogSignalArray\n        AnalogSignalArray containing single channel speed, in units/sec\n    v1 : float, optional\n        Minimum speed (in same units as speed) that has to be reached /\n        exceeded during an event. Default is 10 [units/sec]\n    v2 : float, optional\n        Speed that defines the event boundaries. Default is 8 [units/sec]\n\n    Returns\n    -------\n    run_epochs : EpochArray\n        EpochArray with all the epochs where speed satisfied the criteria.\n    \"\"\"\n\n    run_epochs = get_threshold_crossing_epochs(asa=speed, t1=v1, t2=v2, mode=\"above\")\n\n    return run_epochs\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_sort_idx","title":"<code>get_sort_idx(tuning_curves)</code>","text":"<p>Find indices to sort neurons by maximum firing rate location in tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>list of lists</code> <p>List where each inner list contains the tuning curve for an individual neuron.</p> required <p>Returns:</p> Name Type Description <code>sorted_idx</code> <code>list</code> <p>List of integers that correspond to neurons sorted by the location of their maximum firing rate in the tuning curve.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tuning_curves = [[1, 5, 2], [3, 1, 4], [2, 3, 6]]\n&gt;&gt;&gt; get_sort_idx(tuning_curves)\n[1, 0, 2]  # sorted by position of max firing rate\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def get_sort_idx(tuning_curves):\n    \"\"\"\n    Find indices to sort neurons by maximum firing rate location in tuning curve.\n\n    Parameters\n    ----------\n    tuning_curves : list of lists\n        List where each inner list contains the tuning curve for an individual\n        neuron.\n\n    Returns\n    -------\n    sorted_idx : list\n        List of integers that correspond to neurons sorted by the location\n        of their maximum firing rate in the tuning curve.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tuning_curves = [[1, 5, 2], [3, 1, 4], [2, 3, 6]]\n    &gt;&gt;&gt; get_sort_idx(tuning_curves)\n    [1, 0, 2]  # sorted by position of max firing rate\n    \"\"\"\n    tc_max_loc = []\n    for i, neuron_tc in enumerate(tuning_curves):\n        tc_max_loc.append((i, np.where(neuron_tc == np.max(neuron_tc))[0][0]))\n    sorted_by_tc = sorted(tc_max_loc, key=lambda x: x[1])\n\n    sorted_idx = []\n    for idx in sorted_by_tc:\n        sorted_idx.append(idx[0])\n\n    return sorted_idx\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.get_threshold_crossing_epochs","title":"<code>get_threshold_crossing_epochs(asa, t1=None, t2=None, mode='above')</code>","text":"<p>Return epochs where a signal crosses a compound threshold specified by t1 and t2.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray containing a single channel</p> required <code>t1</code> <code>float</code> <p>Primary threshold. Minimum signal value that has to be reached / exceeded during an event. Default is 3 standard deviations above signal mean.</p> <code>None</code> <code>t2</code> <code>float</code> <p>Secondary threshold. Signal value that defines the event boundaries. Default is signal mean.</p> <code>None</code> <code>mode</code> <code>string</code> <p>Mode of operation. One of ['above', 'below']. If 'above', then return epochs where the signal exceeds the compound threshold, and if 'below', then return epochs where the signal falls below the compound threshold. Default is 'above'.</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>epochs</code> <code>EpochArray</code> <p>EpochArray with all the epochs where the signal satisfied the criteria.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_threshold_crossing_epochs(asa, t1=None, t2=None, mode=\"above\"):\n    \"\"\"Return epochs where a signal crosses a compound threshold specified by t1\n    and t2.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray\n        AnalogSignalArray containing a single channel\n    t1 : float, optional\n        Primary threshold. Minimum signal value that has to be reached /\n        exceeded during an event. Default is 3 standard deviations above signal\n        mean.\n    t2 : float, optional\n        Secondary threshold. Signal value that defines the event boundaries.\n        Default is signal mean.\n    mode : string, optional\n        Mode of operation. One of ['above', 'below']. If 'above', then return\n        epochs where the signal exceeds the compound threshold, and if 'below',\n        then return epochs where the signal falls below the compound threshold.\n        Default is 'above'.\n\n    Returns\n    -------\n    epochs : EpochArray\n        EpochArray with all the epochs where the signal satisfied the criteria.\n    \"\"\"\n\n    if asa.n_signals &gt; 1:\n        raise TypeError(\"multidimensional AnalogSignalArrays not supported!\")\n    x = asa.data.squeeze()\n\n    if t1 is None:  # by default, threshold is 3 SDs above mean of x\n        t1 = np.mean(x) + 3 * np.std(x)\n\n    if t2 is None:  # by default, revert back to mean of x\n        t2 = np.mean(x)\n\n    # compute periods where signal exceeds compound threshold\n    epoch_bounds, _, _ = get_events_boundaries(\n        x=x, PrimaryThreshold=t1, SecondaryThreshold=t2, mode=mode\n    )\n    # convert bounds to time in seconds\n    epoch_bounds = asa.time[epoch_bounds]\n    if len(epoch_bounds) == 0:\n        return type(asa._abscissa.support)(empty=True)\n    # add 1/fs to stops for open interval\n    epoch_bounds[:, 1] += 1 / asa.fs\n    # create EpochArray with threshould exceeding bounds\n    epochs = type(asa._abscissa.support)(epoch_bounds)\n    return epochs\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.information_rate","title":"<code>information_rate(ratemap, Pi=1)</code>","text":"<p>This computes the spatial information rate of cell spikes given variable x in bits/second.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>ndarray</code> <p>A firing rate map, any number of dimensions.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution over the bins of the rate map. If not provided, it is assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>ir</code> <code>ndarray</code> <p>The information rate of the cell, in bits/second.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def information_rate(ratemap, Pi=1):\n    \"\"\"\n    This computes the spatial information rate of cell spikes given variable x in\n    bits/second.\n\n    Parameters\n    ----------\n    ratemap : numpy.ndarray\n        A firing rate map, any number of dimensions.\n    Pi : numpy.ndarray\n        A probability distribution over the bins of the rate map. If not\n        provided, it is assumed to be uniform.\n\n    Returns\n    -------\n    ir : numpy.ndarray\n        The information rate of the cell, in bits/second.\n\n    \"\"\"\n    # convert Pi to probability distribution\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)\n    return spatial_information(ratemap, Pi) * R\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.is_odd","title":"<code>is_odd(n)</code>","text":"<p>Check if a number is odd.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Integer to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if n is odd, False if n is even.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_odd(3)\nTrue\n&gt;&gt;&gt; is_odd(4)\nFalse\n&gt;&gt;&gt; is_odd(-1)\nTrue\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def is_odd(n):\n    \"\"\"\n    Check if a number is odd.\n\n    Parameters\n    ----------\n    n : int\n        Integer to check.\n\n    Returns\n    -------\n    bool\n        True if n is odd, False if n is even.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_odd(3)\n    True\n    &gt;&gt;&gt; is_odd(4)\n    False\n    &gt;&gt;&gt; is_odd(-1)\n    True\n    \"\"\"\n    return bool(n &amp; 1)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.is_sorted","title":"<code>is_sorted(x, chunk_size=None)</code>","text":"<p>Check if a 1D array, list, or tuple is monotonically increasing (sorted).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>1D array, list, or tuple to check.</p> required <code>chunk_size</code> <code>int</code> <p>Size of chunks to check at a time (for large arrays).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>sorted</code> <code>bool</code> <p>True if x is sorted, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_sorted([1, 2, 3])\nTrue\n&gt;&gt;&gt; is_sorted([1, 3, 2])\nFalse\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def is_sorted(x, chunk_size=None):\n    \"\"\"\n    Check if a 1D array, list, or tuple is monotonically increasing (sorted).\n\n    Parameters\n    ----------\n    x : array-like\n        1D array, list, or tuple to check.\n    chunk_size : int, optional\n        Size of chunks to check at a time (for large arrays).\n\n    Returns\n    -------\n    sorted : bool\n        True if x is sorted, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_sorted([1, 2, 3])\n    True\n    &gt;&gt;&gt; is_sorted([1, 3, 2])\n    False\n    \"\"\"\n\n    if not isinstance(x, (tuple, list, np.ndarray)):\n        raise TypeError(\"Unsupported type {}\".format(type(x)))\n\n    x = np.atleast_1d(np.array(x).squeeze())\n    if x.ndim &gt; 1:\n        raise ValueError(\"Input x must be 1-dimensional\")\n\n    if chunk_size is None:\n        chunk_size = 500000\n    stop = x.size\n    for chunk_start in range(0, stop, chunk_size):\n        chunk_stop = int(min(stop, chunk_start + chunk_size + 1))\n        chunk = x[chunk_start:chunk_stop]\n        if not np.all(chunk[:-1] &lt;= chunk[1:]):\n            return False\n    return True\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.is_sorted_general","title":"<code>is_sorted_general(iterable, key=lambda a, b: a &lt;= b)</code>","text":"<p>Check if an iterable is sorted according to a custom key function.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>iterable</code> <p>Sequence to check.</p> required <code>key</code> <code>callable</code> <p>Function that takes two elements and returns True if they are in order. Default is lambda a, b: a &lt;= b (monotonic increasing).</p> <code>lambda a, b: a &lt;= b</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if iterable is sorted according to key function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_sorted_general([1, 2, 3, 4])\nTrue\n&gt;&gt;&gt; is_sorted_general([4, 3, 2, 1])\nFalse\n&gt;&gt;&gt; is_sorted_general([4, 3, 2, 1], key=lambda a, b: a &gt;= b)  # decreasing\nTrue\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def is_sorted_general(iterable, key=lambda a, b: a &lt;= b):\n    \"\"\"\n    Check if an iterable is sorted according to a custom key function.\n\n    Parameters\n    ----------\n    iterable : iterable\n        Sequence to check.\n    key : callable, optional\n        Function that takes two elements and returns True if they are in order.\n        Default is lambda a, b: a &lt;= b (monotonic increasing).\n\n    Returns\n    -------\n    bool\n        True if iterable is sorted according to key function.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_sorted_general([1, 2, 3, 4])\n    True\n    &gt;&gt;&gt; is_sorted_general([4, 3, 2, 1])\n    False\n    &gt;&gt;&gt; is_sorted_general([4, 3, 2, 1], key=lambda a, b: a &gt;= b)  # decreasing\n    True\n    \"\"\"\n    return all(key(a, b) for a, b in pairwise(iterable))\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.linear_merge","title":"<code>linear_merge(list1, list2)</code>","text":"<p>Merge two sorted lists in linear time.</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <code>list or ndarray</code> <p>First sorted list.</p> required <code>list2</code> <code>list or ndarray</code> <p>Second sorted list.</p> required <p>Returns:</p> Name Type Description <code>merged</code> <code>generator</code> <p>Generator yielding merged, sorted elements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(linear_merge([1, 3, 5], [2, 4, 6]))\n[1, 2, 3, 4, 5, 6]\n&gt;&gt;&gt; list(linear_merge([1, 2, 2, 3], [2, 2, 4, 4]))\n[1, 2, 2, 2, 2, 3, 4, 4]\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def linear_merge(list1, list2):\n    \"\"\"\n    Merge two sorted lists in linear time.\n\n    Parameters\n    ----------\n    list1 : list or np.ndarray\n        First sorted list.\n    list2 : list or np.ndarray\n        Second sorted list.\n\n    Returns\n    -------\n    merged : generator\n        Generator yielding merged, sorted elements.\n\n    Examples\n    --------\n    &gt;&gt;&gt; list(linear_merge([1, 3, 5], [2, 4, 6]))\n    [1, 2, 3, 4, 5, 6]\n    &gt;&gt;&gt; list(linear_merge([1, 2, 2, 3], [2, 2, 4, 4]))\n    [1, 2, 2, 2, 2, 3, 4, 4]\n    \"\"\"\n\n    # if any of the lists are empty, return the other (possibly also\n    # empty) list: (this is necessary because having either list1 or\n    # list2 be empty makes this quite a bit more complicated...)\n    if isinstance(list1, (list, np.ndarray)):\n        if len(list1) == 0:\n            list2 = iter(list2)\n            while True:\n                try:\n                    yield next(list2)\n                except StopIteration:\n                    return\n    if isinstance(list2, (list, np.ndarray)):\n        if len(list2) == 0:\n            list1 = iter(list1)\n            while True:\n                try:\n                    yield next(list1)\n                except StopIteration:\n                    return\n\n    list1 = iter(list1)\n    list2 = iter(list2)\n\n    value1 = next(list1)\n    value2 = next(list2)\n\n    # We'll normally exit this loop from a next() call raising\n    # StopIteration, which is how a generator function exits anyway.\n    while True:\n        if value1 &lt;= value2:\n            # Yield the lower value.\n            try:\n                yield value1\n            except StopIteration:\n                return\n            try:\n                # Grab the next value from list1.\n                value1 = next(list1)\n            except StopIteration:\n                # list1 is empty.  Yield the last value we received from list2, then\n                # yield the rest of list2.\n                try:\n                    yield value2\n                except StopIteration:\n                    return\n                while True:\n                    try:\n                        yield next(list2)\n                    except StopIteration:\n                        return\n        else:\n            try:\n                yield value2\n            except StopIteration:\n                return\n            try:\n                value2 = next(list2)\n\n            except StopIteration:\n                # list2 is empty.\n                try:\n                    yield value1\n                except StopIteration:\n                    return\n                while True:\n                    try:\n                        yield next(list1)\n                    except StopIteration:\n                        return\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.nextfastpower","title":"<code>nextfastpower(n)</code>","text":"<p>Return the next integral power of small factors greater than the given number.  Specifically, return m such that     m &gt;= n     m == 2x * 3y * 5**z where x, y, and z are integers. This is useful for ensuring fast FFT sizes.</p> <p>From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)</p> <p>See also http://scipy.github.io/devdocs/generated/scipy.fftpack.next_fast_len.html</p> Source code in <code>nelpy/utils.py</code> <pre><code>def nextfastpower(n):\n    \"\"\"Return the next integral power of small factors greater than the given\n    number.  Specifically, return m such that\n        m &gt;= n\n        m == 2**x * 3**y * 5**z\n    where x, y, and z are integers.\n    This is useful for ensuring fast FFT sizes.\n\n    From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)\n\n    See also http://scipy.github.io/devdocs/generated/scipy.fftpack.next_fast_len.html\n    \"\"\"\n    if n &lt; 7:\n        return max(n, 1)\n\n    # x, y, and z are all bounded from above by the formula of nextpower.\n    # Compute all possible combinations for powers of 3 and 5.\n    # (Not too many for reasonable FFT sizes.)\n    def power_series(x, base):\n        nmax = int(ceil(log(x) / log(base)))\n        return np.logspace(0.0, nmax, num=nmax + 1, base=base)\n\n    n35 = np.outer(power_series(n, 3.0), power_series(n, 5.0))\n    n35 = n35[n35 &lt;= n]\n    # Lump the powers of 3 and 5 together and solve for the powers of 2.\n    n2 = nextpower(n / n35)\n    return int(min(n2 * n35))\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.nextpower","title":"<code>nextpower(n, base=2.0)</code>","text":"<p>Return the next integral power of two greater than the given number. Specifically, return m such that     m &gt;= n     m == 2**x where x is an integer. Use base argument to specify a base other than 2. This is useful for ensuring fast FFT sizes.</p> <p>From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)</p> Source code in <code>nelpy/utils.py</code> <pre><code>def nextpower(n, base=2.0):\n    \"\"\"Return the next integral power of two greater than the given number.\n    Specifically, return m such that\n        m &gt;= n\n        m == 2**x\n    where x is an integer. Use base argument to specify a base other than 2.\n    This is useful for ensuring fast FFT sizes.\n\n    From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)\n    \"\"\"\n    x = base ** ceil(log(n) / log(base))\n    if isinstance(n, np.ndarray):\n        return np.asarray(x, dtype=int)\n    else:\n        return int(x)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.pairwise","title":"<code>pairwise(iterable)</code>","text":"<p>Return a zip of all neighboring pairs in an iterable.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>iterable</code> <p>Input iterable.</p> required <p>Returns:</p> Name Type Description <code>pairs</code> <code>zip</code> <p>Iterator of pairs (a, b).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(pairwise([2, 3, 6, 8, 7]))\n[(2, 3), (3, 6), (6, 8), (8, 7)]\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def pairwise(iterable):\n    \"\"\"\n    Return a zip of all neighboring pairs in an iterable.\n\n    Parameters\n    ----------\n    iterable : iterable\n        Input iterable.\n\n    Returns\n    -------\n    pairs : zip\n        Iterator of pairs (a, b).\n\n    Examples\n    --------\n    &gt;&gt;&gt; list(pairwise([2, 3, 6, 8, 7]))\n    [(2, 3), (3, 6), (6, 8), (8, 7)]\n    \"\"\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.ragged_array","title":"<code>ragged_array(arr)</code>","text":"<p>Convert a list of arrays into a ragged array (object dtype).</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>list of np.ndarray</code> <p>List of arrays to combine into a ragged array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Ragged array of dtype object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ragged_array([np.arange(3), np.arange(5)])\narray([array([0, 1, 2]), array([0, 1, 2, 3, 4])], dtype=object)\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def ragged_array(arr):\n    \"\"\"\n    Convert a list of arrays into a ragged array (object dtype).\n\n    Parameters\n    ----------\n    arr : list of np.ndarray\n        List of arrays to combine into a ragged array.\n\n    Returns\n    -------\n    out : np.ndarray\n        Ragged array of dtype object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; ragged_array([np.arange(3), np.arange(5)])\n    array([array([0, 1, 2]), array([0, 1, 2, 3, 4])], dtype=object)\n    \"\"\"\n    n_elem = len(arr)\n    out = np.array(n_elem * [None])\n    for ii in range(out.shape[0]):\n        out[ii] = arr[ii]\n    return out\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.shrinkMatColsTo","title":"<code>shrinkMatColsTo(mat, numCols)</code>","text":"<p>Shrink a matrix by reducing the number of columns using interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>mat</code> <code>ndarray</code> <p>Input matrix of shape (N, M1).</p> required <code>numCols</code> <code>int</code> <p>Target number of columns M2, where M2 &lt;= M1.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Resized matrix of shape (N, numCols).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n&gt;&gt;&gt; shrinkMatColsTo(mat, 2)\narray([[1.5, 3.5],\n       [5.5, 7.5]])\n</code></pre> Notes <p>Uses scipy.ndimage.zoom with order=1 (linear interpolation).</p> Source code in <code>nelpy/utils.py</code> <pre><code>def shrinkMatColsTo(mat, numCols):\n    \"\"\"\n    Shrink a matrix by reducing the number of columns using interpolation.\n\n    Parameters\n    ----------\n    mat : np.ndarray\n        Input matrix of shape (N, M1).\n    numCols : int\n        Target number of columns M2, where M2 &lt;= M1.\n\n    Returns\n    -------\n    np.ndarray\n        Resized matrix of shape (N, numCols).\n\n    Examples\n    --------\n    &gt;&gt;&gt; mat = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n    &gt;&gt;&gt; shrinkMatColsTo(mat, 2)\n    array([[1.5, 3.5],\n           [5.5, 7.5]])\n\n    Notes\n    -----\n    Uses scipy.ndimage.zoom with order=1 (linear interpolation).\n    \"\"\"\n    from scipy.ndimage import zoom\n\n    numCells = mat.shape[0]\n    numColsMat = mat.shape[1]\n    a = np.zeros((numCells, numCols))\n    for row in np.arange(numCells):\n        niurou = zoom(input=mat[row, :], zoom=(numCols / numColsMat), order=1)\n        a[row, :] = niurou\n    return a\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.signal_envelope1D","title":"<code>signal_envelope1D(data, *, sigma=None, fs=None)</code>","text":"<p>Find the signal envelope using the Hilbert transform (deprecated).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>numpy array, list, or RegularlySampledAnalogSignalArray</code> <p>Input data.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian smoothing kernel in seconds.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate of the signal.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as input</code> <p>Signal envelope.</p> Notes <p>This function is deprecated. Use <code>signal_envelope_1d</code> instead.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def signal_envelope1D(data, *, sigma=None, fs=None):\n    \"\"\"\n    Find the signal envelope using the Hilbert transform (deprecated).\n\n    Parameters\n    ----------\n    data : numpy array, list, or RegularlySampledAnalogSignalArray\n        Input data.\n    sigma : float, optional\n        Standard deviation of Gaussian smoothing kernel in seconds.\n    fs : float, optional\n        Sampling rate of the signal.\n\n    Returns\n    -------\n    out : same type as input\n        Signal envelope.\n\n    Notes\n    -----\n    This function is deprecated. Use `signal_envelope_1d` instead.\n    \"\"\"\n    logging.warnings(\n        \"'signal_envelope1D' is deprecated; use 'signal_envelope_1d' instead!\"\n    )\n    return signal_envelope_1d(data, sigma=sigma, fs=fs)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.signal_envelope_1d","title":"<code>signal_envelope_1d(data, *, sigma=None, fs=None)</code>","text":"<p>Finds the signal envelope by taking the absolute value of the Hilbert transform</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>numpy array, list, or RegularlySampledAnalogSignalArray</code> <p>Input data If data is a numpy array, it is expected to have shape (n_signals, n_samples) If data is a list, it is expected to have length n_signals, where each sublist has length n_samples, i.e. data is not jagged</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the Gaussian kernel used to smooth the envelope after applying the Hilbert transform. Units of seconds. Default is 4 ms</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate of the signal</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as the input object</code> <p>An object containing the signal envelope</p> <code>TODO</code> <code>this is not yet epoch-aware!</code> <code>UPDATE</code> <code>this is actually epoch-aware by now!</code> Source code in <code>nelpy/utils.py</code> <pre><code>def signal_envelope_1d(data, *, sigma=None, fs=None):\n    \"\"\"Finds the signal envelope by taking the absolute value\n    of the Hilbert transform\n\n    Parameters\n    ----------\n    data : numpy array, list, or RegularlySampledAnalogSignalArray\n        Input data\n        If data is a numpy array, it is expected to have shape\n        (n_signals, n_samples)\n        If data is a list, it is expected to have length n_signals,\n        where each sublist has length n_samples, i.e. data is not\n        jagged\n    sigma : float, optional\n        Standard deviation of the Gaussian kernel used to\n        smooth the envelope after applying the Hilbert transform.\n        Units of seconds. Default is 4 ms\n    fs : float, optional\n        Sampling rate of the signal\n\n    Returns\n    -------\n    out : same type as the input object\n        An object containing the signal envelope\n\n    TODO: this is not yet epoch-aware!\n    UPDATE: this is actually epoch-aware by now!\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.004  # 4 ms standard deviation\n    if fs is None:\n        if isinstance(data, (np.ndarray, list)):\n            raise ValueError(\"sampling frequency must be specified!\")\n        elif isinstance(data, core.RegularlySampledAnalogSignalArray):\n            fs = data.fs\n\n    if isinstance(data, (np.ndarray, list)):\n        data_array = np.array(data)\n        n_dims = np.array(data).ndim\n        assert n_dims &lt;= 2, \"Only 1D signals supported!\"\n        if n_dims == 1:\n            input_data = data_array.reshape((1, data_array.size))\n        else:\n            input_data = data_array\n        n_signals, n_samples = input_data.shape\n        # Compute number of samples to compute fast FFTs\n        padlen = next_fast_len(n_samples) - n_samples\n        # Pad data\n        paddeddata = np.hstack((input_data, np.zeros((n_signals, padlen))))\n        # Use hilbert transform to get an envelope\n        envelope = np.absolute(hilbert(paddeddata, axis=-1))\n        # free up memory\n        del paddeddata\n        # Truncate results back to original length\n        envelope = envelope[..., :n_samples]\n        if sigma:\n            # Smooth envelope with a gaussian (sigma = 4 ms default)\n            EnvelopeSmoothingSD = sigma * fs\n            smoothed_envelope = gaussian_filter1d(\n                envelope, EnvelopeSmoothingSD, mode=\"constant\", axis=-1\n            )\n            envelope = smoothed_envelope\n        if isinstance(data, list):\n            envelope = envelope.tolist()\n        return envelope\n    elif isinstance(data, core.RegularlySampledAnalogSignalArray):\n        # Only ASA data of shape (n_signals, n_timepoints) -&gt; 2D currently supported\n        assert data.data.ndim == 2\n        cum_lengths = np.insert(np.cumsum(data.lengths), 0, 0)\n\n        newasa = data.copy()\n        # for segment in data:\n        for idx in range(data.n_epochs):\n            # print('hilberting epoch {}/{}'.format(idx+1, data.n_epochs))\n            segment_data = data._data[:, cum_lengths[idx] : cum_lengths[idx + 1]]\n            n_signals, n_samples = segment_data.shape\n            # Compute number of samples to compute fast FFTs:\n            padlen = next_fast_len(n_samples) - n_samples\n            # Pad data\n            paddeddata = np.hstack((segment_data, np.zeros((n_signals, padlen))))\n            # Use hilbert transform to get an envelope\n            envelope = np.absolute(hilbert(paddeddata, axis=-1))\n            # free up memory\n            del paddeddata\n            # Truncate results back to original length\n            envelope = envelope[..., :n_samples]\n            if sigma:\n                # Smooth envelope with a gaussian (sigma = 4 ms default)\n                EnvelopeSmoothingSD = sigma * fs\n                smoothed_envelope = gaussian_filter1d(\n                    envelope, EnvelopeSmoothingSD, mode=\"constant\", axis=-1\n                )\n                envelope = smoothed_envelope\n            newasa._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = np.atleast_2d(\n                envelope\n            )\n        return newasa\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.spatial_information","title":"<code>spatial_information(ratemap, Pi=1)</code>","text":"<p>Compute the spatial information and firing sparsity...</p> <p>The specificity index examines the amount of information (in bits) that a single spike conveys about the animal's location (i.e., how well cell firing predicts the animal's location).The spatial information content of cell discharge was calculated using the formula:     information content = \\Sum P_i(R_i/R)log_2(R_i/R) where i is the bin number, P_i, is the probability for occupancy of bin i, R_i, is the mean firing rate for bin i, and R is the overall mean firing rate.</p> <p>In order to account for the effects of low firing rates (with fewer spikes there is a tendency toward higher information content) or random bursts of firing, the spike firing time-series was randomly offset in time from the rat location time-series, and the information content was calculated. A distribution of the information content based on 100 such random shifts was obtained and was used to compute a standardized score (Zscore) of information content for that cell. While the distribution is not composed of independent samples, it was nominally normally distributed, and a Z value of 2.29 was chosen as a cut-off for significance (the equivalent of a one-tailed t-test with P = 0.01 under a normal distribution).</p> Reference(s) <p>Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,     and Skaggs, W. E. (1994). \"Spatial information content and     reliability of hippocampal CA1 neurons: effects of visual     input\", Hippocampus, 4(4), 410-421.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>array of shape (n_units, n_bins)</code> <p>Rate map in Hz.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution over the bins of the rate map. If not provided, it is assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>si</code> <code>array of shape (n_units,)</code> <p>spatial information (in bits per spike)</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spatial_information(ratemap, Pi=1):\n    \"\"\"Compute the spatial information and firing sparsity...\n\n    The specificity index examines the amount of information\n    (in bits) that a single spike conveys about the animal's\n    location (i.e., how well cell firing predicts the animal's\n    location).The spatial information content of cell discharge was\n    calculated using the formula:\n        information content = \\\\Sum P_i(R_i/R)log_2(R_i/R)\n    where i is the bin number, P_i, is the probability for occupancy\n    of bin i, R_i, is the mean firing rate for bin i, and R is the\n    overall mean firing rate.\n\n    In order to account for the effects of low firing rates (with\n    fewer spikes there is a tendency toward higher information\n    content) or random bursts of firing, the spike firing\n    time-series was randomly offset in time from the rat location\n    time-series, and the information content was calculated. A\n    distribution of the information content based on 100 such random\n    shifts was obtained and was used to compute a standardized score\n    (Zscore) of information content for that cell. While the\n    distribution is not composed of independent samples, it was\n    nominally normally distributed, and a Z value of 2.29 was chosen\n    as a cut-off for significance (the equivalent of a one-tailed\n    t-test with P = 0.01 under a normal distribution).\n\n    Reference(s)\n    ------------\n    Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n        and Skaggs, W. E. (1994). \"Spatial information content and\n        reliability of hippocampal CA1 neurons: effects of visual\n        input\", Hippocampus, 4(4), 410-421.\n\n    Parameters\n    ----------\n    ratemap : array of shape (n_units, n_bins)\n        Rate map in Hz.\n\n    Pi : numpy.ndarray\n        A probability distribution over the bins of the rate map. If not\n        provided, it is assumed to be uniform.\n    Returns\n    -------\n    si : array of shape (n_units,)\n        spatial information (in bits per spike)\n    \"\"\"\n    # convert Pi to probability distribution\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    ratemap = copy.deepcopy(ratemap)\n    # ensure that the ratemap always has nonzero firing rates,\n    # otherwise the spatial information might return NaNs:\n    ratemap[np.isnan(ratemap) | (ratemap == 0)] = np.finfo(float).eps\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)  # mean firing rate for each unit\n\n    # Compute spatial information for each unit\n    # We need to compute sum over spatial bins of: Pi * (Ri/R) * log2(Ri/R)\n    # where Ri is the firing rate in each spatial bin\n\n    si = np.zeros(ratemap.shape[0])  # one value per unit\n\n    for unit_idx in range(ratemap.shape[0]):\n        unit_ratemap = ratemap[unit_idx]  # spatial dimensions only\n        unit_mean_rate = R[unit_idx]\n\n        # Compute (Ri / R) * log2(Ri / R) for each spatial bin\n        rate_ratio = unit_ratemap / unit_mean_rate\n        log_term = rate_ratio * np.log2(rate_ratio)\n\n        # Weight by occupancy probability and sum over all spatial dimensions\n        si[unit_idx] = np.sum(Pi * log_term)\n\n    return si\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.spatial_selectivity","title":"<code>spatial_selectivity(ratemap, Pi=1)</code>","text":"<p>The selectivity measure max(rate)/mean(rate)  of the cell. The more tightly concentrated the cell's activity, the higher the selectivity. A cell with no spatial tuning at all will have a selectivity of 1.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>A firing rate map, any number of dimensions.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution of the occupancy of each bin in the rate map. If not provided, the occupancy will be assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>selectivity</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spatial_selectivity(ratemap, Pi=1):\n    \"\"\"\n    The selectivity measure max(rate)/mean(rate)  of the cell. The more\n    tightly concentrated the cell's activity, the higher the selectivity.\n    A cell with no spatial tuning at all will have a selectivity of 1.\n\n    Parameters\n    ----------\n    rate_map : numpy.ndarray\n        A firing rate map, any number of dimensions.\n    Pi : numpy.ndarray\n        A probability distribution of the occupancy of each bin in the\n        rate map. If not provided, the occupancy will be assumed to be uniform.\n\n    Returns\n    -------\n    out : float\n        selectivity\n    \"\"\"\n    # convert Pi to probability distribution\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)\n\n    # Get maximum rate over all spatial dimensions for each unit\n    max_rate = np.max(ratemap, axis=spatial_axes)\n    return max_rate / R\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.spatial_sparsity","title":"<code>spatial_sparsity(ratemap, Pi=1)</code>","text":"<p>Compute the firing sparsity... Compute sparsity of a rate map, The sparsity  measure is an adaptation to space. The adaptation measures the fraction of the environment  in which a cell is  active. A sparsity of, 0.1 means that the place field of the cell occupies 1/10 of the area the subject traverses</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>A firing rate map, any number of dimensions.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution over the bins of the rate map. If not provided, it is assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>sparsity</p> References <p>.. [2] Skaggs, W. E., McNaughton, B. L., Wilson, M., &amp; Barnes, C. (1996). Theta phase precession in hippocampal neuronal populations and the compression of temporal sequences. Hippocampus, 6, 149-172.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>array of shape (n_units, n_bins)</code> <p>Rate map in Hz.</p> required <code>Pi</code> <code>array of shape (n_bins,)</code> <p>Occupancy of the animal.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>sparsity</code> <code>array of shape (n_units,)</code> <p>sparsity (in percent) for each unit</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spatial_sparsity(ratemap, Pi=1):\n    \"\"\"Compute the firing sparsity...\n    Compute sparsity of a rate map, The sparsity  measure is an adaptation\n    to space. The adaptation measures the fraction of the environment  in which\n    a cell is  active. A sparsity of, 0.1 means that the place field of the\n    cell occupies 1/10 of the area the subject traverses\n\n    Parameters\n    ----------\n    rate_map : numpy.ndarray\n        A firing rate map, any number of dimensions.\n    Pi : numpy.ndarray\n        A probability distribution over the bins of the rate map. If not\n        provided, it is assumed to be uniform.\n    Returns\n    -------\n    out : float\n        sparsity\n\n    References\n    ----------\n    .. [2] Skaggs, W. E., McNaughton, B. L., Wilson, M., &amp; Barnes, C. (1996).\n    Theta phase precession in hippocampal neuronal populations and the\n    compression of temporal sequences. Hippocampus, 6, 149-172.\n\n    Parameters\n    ----------\n\n    ratemap : array of shape (n_units, n_bins)\n        Rate map in Hz.\n    Pi : array of shape (n_bins,)\n        Occupancy of the animal.\n    Returns\n    -------\n    sparsity: array of shape (n_units,)\n        sparsity (in percent) for each unit\n    \"\"\"\n\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    ratemap = copy.deepcopy(ratemap)\n    # ensure that the ratemap always has nonzero firing rates,\n    # otherwise the spatial information might return NaNs:\n    ratemap[np.isnan(ratemap) | (ratemap == 0)] = np.finfo(float).eps\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)\n\n    # Compute average squared rate over all spatial dimensions\n    avg_sqr_rate = np.sum(ratemap**2 * Pi, axis=spatial_axes)\n\n    return R**2 / avg_sqr_rate\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.spiketrain_union","title":"<code>spiketrain_union(st1, st2)</code>","text":"<p>Join two spiketrains together.</p> <p>Parameters:</p> Name Type Description Default <code>st1</code> <code>SpikeTrainArray</code> <p>First spiketrain.</p> required <code>st2</code> <code>SpikeTrainArray</code> <p>Second spiketrain.</p> required <p>Returns:</p> Type Description <code>SpikeTrainArray</code> <p>Combined spiketrain with joined support.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combined_st = spiketrain_union(st1, st2)\n</code></pre> Notes <p>WARNING! This function should be improved a lot! Currently assumes both spiketrains have the same number of units.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spiketrain_union(st1, st2):\n    \"\"\"\n    Join two spiketrains together.\n\n    Parameters\n    ----------\n    st1 : SpikeTrainArray\n        First spiketrain.\n    st2 : SpikeTrainArray\n        Second spiketrain.\n\n    Returns\n    -------\n    SpikeTrainArray\n        Combined spiketrain with joined support.\n\n    Examples\n    --------\n    &gt;&gt;&gt; combined_st = spiketrain_union(st1, st2)\n\n    Notes\n    -----\n    WARNING! This function should be improved a lot!\n    Currently assumes both spiketrains have the same number of units.\n    \"\"\"\n    assert st1.n_units == st2.n_units\n    support = st1.support.join(st2.support)\n\n    newdata = []\n    for unit in range(st1.n_units):\n        newdata.append(np.append(st1.time[unit], st2.time[unit]))\n\n    fs = None\n    if st1.fs == st2.fs:\n        fs = st1.fs\n\n    return core.SpikeTrainArray(newdata, support=support, fs=fs)\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.swap_cols","title":"<code>swap_cols(arr, frm, to)</code>","text":"<p>Swap columns of a 2D numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>2D array to modify.</p> required <code>frm</code> <code>int</code> <p>Index of first column to swap.</p> required <code>to</code> <code>int</code> <p>Index of second column to swap.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies array in-place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; swap_cols(arr, 0, 2)\n&gt;&gt;&gt; arr\narray([[3, 2, 1],\n       [6, 5, 4]])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def swap_cols(arr, frm, to):\n    \"\"\"\n    Swap columns of a 2D numpy array.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        2D array to modify.\n    frm : int\n        Index of first column to swap.\n    to : int\n        Index of second column to swap.\n\n    Returns\n    -------\n    None\n        Modifies array in-place.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6]])\n    &gt;&gt;&gt; swap_cols(arr, 0, 2)\n    &gt;&gt;&gt; arr\n    array([[3, 2, 1],\n           [6, 5, 4]])\n    \"\"\"\n    if arr.ndim &gt; 1:\n        arr[:, [frm, to]] = arr[:, [to, frm]]\n    else:\n        arr[frm], arr[to] = arr[to], arr[frm]\n</code></pre>"},{"location":"reference/utils/#nelpy.utils.swap_rows","title":"<code>swap_rows(arr, frm, to)</code>","text":"<p>Swap rows of a 2D numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>2D array to modify.</p> required <code>frm</code> <code>int</code> <p>Index of first row to swap.</p> required <code>to</code> <code>int</code> <p>Index of second row to swap.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies array in-place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; swap_rows(arr, 0, 2)\n&gt;&gt;&gt; arr\narray([[7, 8, 9],\n       [4, 5, 6],\n       [1, 2, 3]])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def swap_rows(arr, frm, to):\n    \"\"\"\n    Swap rows of a 2D numpy array.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        2D array to modify.\n    frm : int\n        Index of first row to swap.\n    to : int\n        Index of second row to swap.\n\n    Returns\n    -------\n    None\n        Modifies array in-place.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    &gt;&gt;&gt; swap_rows(arr, 0, 2)\n    &gt;&gt;&gt; arr\n    array([[7, 8, 9],\n           [4, 5, 6],\n           [1, 2, 3]])\n    \"\"\"\n    if arr.ndim &gt; 1:\n        arr[[frm, to], :] = arr[[to, frm], :]\n    else:\n        arr[frm], arr[to] = arr[to], arr[frm]\n</code></pre>"},{"location":"reference/core/accessors/","title":"Accessors","text":"<p>This file contains generic accessors to handle getting data from core objects</p>"},{"location":"reference/core/accessors/#nelpy.core._accessors.ItemGetterIloc","title":"<code>ItemGetterIloc</code>","text":"<p>               Bases: <code>object</code></p> <p>Accessor for integer-based indexing, similar to pandas' <code>.iloc</code>.</p> <p>Allows selection of intervals and series by integer position (from 0 to length-1). Raises IndexError if an integer index is out of bounds (except for slices, which allow out-of-bounds indices).</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The parent object supporting integer-based indexing.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; obj = ...  # some core object\n&gt;&gt;&gt; iloc = ItemGetterIloc(obj)\n&gt;&gt;&gt; subset = iloc[0]\n&gt;&gt;&gt; subset = iloc[[0, 2, 4]]\n&gt;&gt;&gt; subset = iloc[1:5]\n</code></pre> Notes <ul> <li>This accessor is typically available as the <code>.iloc</code> property on core objects.</li> <li>Follows Python/NumPy slice semantics for out-of-bounds indices.</li> </ul> Source code in <code>nelpy/core/_accessors.py</code> <pre><code>class ItemGetterIloc(object):\n    \"\"\"\n    Accessor for integer-based indexing, similar to pandas' `.iloc`.\n\n    Allows selection of intervals and series by integer position (from 0 to length-1).\n    Raises IndexError if an integer index is out of bounds (except for slices, which allow out-of-bounds indices).\n\n    Parameters\n    ----------\n    obj : object\n        The parent object supporting integer-based indexing.\n\n    Examples\n    --------\n    &gt;&gt;&gt; obj = ...  # some core object\n    &gt;&gt;&gt; iloc = ItemGetterIloc(obj)\n    &gt;&gt;&gt; subset = iloc[0]\n    &gt;&gt;&gt; subset = iloc[[0, 2, 4]]\n    &gt;&gt;&gt; subset = iloc[1:5]\n\n    Notes\n    -----\n    - This accessor is typically available as the `.iloc` property on core objects.\n    - Follows Python/NumPy slice semantics for out-of-bounds indices.\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n        self.slice_extractor = SliceExtractor()\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = self.slice_extractor.extract(idx)\n\n        if isinstance(seriesslice, int):\n            seriesslice = [seriesslice]\n\n        out = self.obj.copy()\n        # It is now the object's responsibility to do the\n        # restriction and set its attributes properly\n        out._restrict(intervalslice, seriesslice)\n        out.__renew__()\n\n        return out\n</code></pre>"},{"location":"reference/core/accessors/#nelpy.core._accessors.ItemGetterLoc","title":"<code>ItemGetterLoc</code>","text":"<p>               Bases: <code>object</code></p> <p>Accessor for label-based indexing, similar to pandas' <code>.loc</code>.</p> <p>Allows selection of intervals and series by label (e.g., series_id) rather than integer position. Raises KeyError if a label is not found.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The parent object supporting label-based indexing (must have <code>_series_ids</code> and <code>series_ids</code>).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; obj = ...  # some core object with series_ids\n&gt;&gt;&gt; loc = ItemGetterLoc(obj)\n&gt;&gt;&gt; subset = loc[\"seriesA\"]\n&gt;&gt;&gt; subset = loc[[\"seriesA\", \"seriesB\"]]\n&gt;&gt;&gt; subset = loc[\"seriesA\":\"seriesC\"]\n</code></pre> Notes <ul> <li>Slices with labels include both the start and stop labels (unlike standard Python slices).</li> <li>This accessor is typically available as the <code>.loc</code> property on core objects.</li> </ul> Source code in <code>nelpy/core/_accessors.py</code> <pre><code>class ItemGetterLoc(object):\n    \"\"\"\n    Accessor for label-based indexing, similar to pandas' `.loc`.\n\n    Allows selection of intervals and series by label (e.g., series_id) rather than integer position.\n    Raises KeyError if a label is not found.\n\n    Parameters\n    ----------\n    obj : object\n        The parent object supporting label-based indexing (must have `_series_ids` and `series_ids`).\n\n    Examples\n    --------\n    &gt;&gt;&gt; obj = ...  # some core object with series_ids\n    &gt;&gt;&gt; loc = ItemGetterLoc(obj)\n    &gt;&gt;&gt; subset = loc[\"seriesA\"]\n    &gt;&gt;&gt; subset = loc[[\"seriesA\", \"seriesB\"]]\n    &gt;&gt;&gt; subset = loc[\"seriesA\":\"seriesC\"]\n\n    Notes\n    -----\n    - Slices with labels include both the start and stop labels (unlike standard Python slices).\n    - This accessor is typically available as the `.loc` property on core objects.\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n        self.slice_extractor = SliceExtractor()\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = self.slice_extractor.extract(idx)\n\n        # first convert series slice into list\n        if isinstance(seriesslice, slice):\n            start = seriesslice.start\n            stop = seriesslice.stop\n            istep = seriesslice.step\n            try:\n                if start is None:\n                    istart = 0\n                else:\n                    istart = self.obj._series_ids.index(start)\n            except ValueError:\n                raise KeyError(\"series_id {} could not be found!\".format(start))\n            try:\n                if stop is None:\n                    istop = self.obj.n_series\n                else:\n                    istop = self.obj._series_ids.index(stop) + 1\n            except ValueError:\n                raise KeyError(\"series_id {} could not be found!\".format(stop))\n            if istep is None:\n                istep = 1\n            if istep &lt; 0:\n                istop -= 1\n                istart -= 1\n                istart, istop = istop, istart\n            series_idx_list = list(range(istart, istop, istep))\n        else:\n            series_idx_list = []\n            seriesslice = np.atleast_1d(seriesslice)\n            for series in seriesslice:\n                try:\n                    uidx = self.obj.series_ids.index(series)\n                except ValueError:\n                    raise KeyError(\"series_id {} could not be found!\".format(series))\n                else:\n                    series_idx_list.append(uidx)\n\n        if not isinstance(series_idx_list, list):\n            series_idx_list = list(series_idx_list)\n\n        # this is mainly to make code easier to read since the _restrict\n        # function prototypes say they accept intervalslice and\n        # seriesslies\n        seriesslice = series_idx_list\n\n        out = self.obj.copy()\n        # It is now the object's responsibility to do the\n        # restriction and set its attributes properly\n        out._restrict(intervalslice, seriesslice)\n        out.__renew__()\n\n        return out\n</code></pre>"},{"location":"reference/core/accessors/#nelpy.core._accessors.SliceExtractor","title":"<code>SliceExtractor</code>","text":"<p>               Bases: <code>object</code></p> <p>Extracts and validates slice indices for interval, series, and event dimensions.</p> <p>This class is used internally by accessor classes to parse and verify slicing/indexing arguments for core objects that support multi-dimensional indexing (e.g., [interval, series, event]).</p> <p>Methods:</p> Name Description <code>extract</code> <p>Parses the input index and returns validated slices for interval and series dimensions.</p> <code>verify_interval_slice</code> <p>Checks if the interval slice is of a supported type.</p> <code>verify_series_slice</code> <p>Checks if the series slice is of a supported type.</p> <code>verify_event_slice</code> <p>Checks if the event slice is of a supported type.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; extractor = SliceExtractor()\n&gt;&gt;&gt; intervalslice, seriesslice = extractor.extract((slice(0, 2), [0, 1]))\n&gt;&gt;&gt; intervalslice\nslice(0, 2, None)\n&gt;&gt;&gt; seriesslice\n[0, 1]\n</code></pre> Notes <ul> <li>Only [interval, series, events] indexing is supported.</li> <li>Event slice extraction is not yet implemented in the return value.</li> </ul> Source code in <code>nelpy/core/_accessors.py</code> <pre><code>class SliceExtractor(object):\n    \"\"\"\n    Extracts and validates slice indices for interval, series, and event dimensions.\n\n    This class is used internally by accessor classes to parse and verify slicing/indexing\n    arguments for core objects that support multi-dimensional indexing (e.g., [interval, series, event]).\n\n    Methods\n    -------\n    extract(idx)\n        Parses the input index and returns validated slices for interval and series dimensions.\n    verify_interval_slice(testslice)\n        Checks if the interval slice is of a supported type.\n    verify_series_slice(testslice)\n        Checks if the series slice is of a supported type.\n    verify_event_slice(testslice)\n        Checks if the event slice is of a supported type.\n\n    Examples\n    --------\n    &gt;&gt;&gt; extractor = SliceExtractor()\n    &gt;&gt;&gt; intervalslice, seriesslice = extractor.extract((slice(0, 2), [0, 1]))\n    &gt;&gt;&gt; intervalslice\n    slice(0, 2, None)\n    &gt;&gt;&gt; seriesslice\n    [0, 1]\n\n    Notes\n    -----\n    - Only [interval, series, events] indexing is supported.\n    - Event slice extraction is not yet implemented in the return value.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def extract(self, idx):\n        # By default, keep all slices\n        intervalslice = slice(None, None, None)\n        seriesslice = slice(None, None, None)\n        eventslice = slice(None, None, None)\n\n        # The one case this breaks is when the idx is a tuple\n        # and no other slices were requested. Otherwise,\n        # something like obj[tuple, [7, 8, 9], 4] works\n\n        # Handle special case where only one slice is provided\n        if isinstance(\n            idx, (core.EpochArray, core.IntervalArray, int, list, slice, np.ndarray)\n        ):\n            intervalslice = idx\n        elif not isinstance(idx, tuple):\n            raise TypeError(\"A slice of type {} is not supported\".format(idx))\n        # Multidimensional cases\n        elif len(idx) == 2:\n            intervalslice = idx[0]\n            seriesslice = idx[1]\n        elif len(idx) == 3:\n            intervalslice = idx[0]\n            seriesslice = idx[1]\n            eventslice = idx[2]\n        elif len(idx) &gt; 3:\n            raise ValueError(\"Only [interval, series, events] indexing is supported\")\n        else:\n            raise ValueError(\n                \"Some other error occurred that we didn't handle.\"\n                \" Please contact a developer\"\n            )\n\n        self.verify_interval_slice(intervalslice)\n        self.verify_series_slice(seriesslice)\n        self.verify_event_slice(eventslice)\n\n        # not returning eventslice because haven't implemented yet, but\n        # it's in the works\n        return intervalslice, seriesslice\n\n    def verify_interval_slice(self, testslice):\n        if not isinstance(\n            testslice, (int, list, tuple, slice, np.ndarray, core.IntervalArray)\n        ):\n            raise TypeError(\n                \"An interval slice of type {} is not supported\".format(type(testslice))\n            )\n\n    def verify_series_slice(self, testslice):\n        if not isinstance(testslice, (int, list, tuple, slice, np.ndarray)):\n            raise TypeError(\n                \"A series slice of type {} is not supported\".format(type(testslice))\n            )\n\n    def verify_event_slice(self, testslice):\n        if not isinstance(testslice, (int, list, tuple, slice, np.ndarray)):\n            raise TypeError(\n                \"An event indexing slice of type {} is not supported\".format(\n                    type(testslice)\n                )\n            )\n\n        if isinstance(testslice, slice):\n            # Case 1: slice(None, val, stride)\n            # Case 2: slice(val, None, stride)\n            # Case 3: slice(val1, val2, stride)\n            # Only need to check bounds for case 3 but check stride\n            # for all cases\n\n            is_start_val = testslice.start is not None\n            is_stop_val = testslice.stop is not None\n            is_step_val = testslice.step is not None\n\n            if is_start_val and is_stop_val:\n                if testslice.stop &lt; testslice.start:\n                    raise ValueError(\n                        \"The stop index {} for event indexing\"\n                        \" must be greater than or equal to\"\n                        \" the start index {}\".format(testslice.stop, testslice.start)\n                    )\n\n            if is_step_val:\n                if testslice.step &lt;= 0:\n                    raise ValueError(\"The stride for event indexing must be positive\")\n</code></pre>"},{"location":"reference/core/analogsignalarray/","title":"AnalogSignalArray","text":"RegularlySampledAnalogSignalArray <p>Core object definition and implementation for RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.AbscissaSlicer","title":"<code>AbscissaSlicer</code>","text":"<p>               Bases: <code>object</code></p> <p>Slicer for extracting abscissa values from analog signal arrays by interval.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>object</code> <p>The parent object to slice.</p> required Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class AbscissaSlicer(object):\n    \"\"\"\n    Slicer for extracting abscissa values from analog signal arrays by interval.\n\n    Parameters\n    ----------\n    parent : object\n        The parent object to slice.\n    \"\"\"\n\n    def __init__(self, parent):\n        \"\"\"\n        Initialize the AbscissaSlicer.\n\n        Parameters\n        ----------\n        parent : object\n            The parent object to slice.\n        \"\"\"\n        self._parent = parent\n\n    def _abscissa_vals_generator(self, interval_indices):\n        \"\"\"\n        Generator for abscissa values by interval.\n\n        Parameters\n        ----------\n        interval_indices : list or array-like\n            Indices of intervals.\n\n        Yields\n        ------\n        abscissa_vals : np.ndarray\n            Abscissa values for each interval.\n        \"\"\"\n        for start, stop in interval_indices:\n            try:\n                yield self._parent._abscissa_vals[start:stop]\n            except StopIteration:\n                return\n\n    def __getitem__(self, idx):\n        intervalslice, signalslice = self._parent._intervalsignalslicer[idx]\n\n        interval_indices = self._parent._data_interval_indices()\n        interval_indices = np.atleast_2d(interval_indices[intervalslice])\n\n        if len(interval_indices) &lt; 2:\n            start, stop = interval_indices[0]\n            return self._parent._abscissa_vals[start:stop]\n        else:\n            return self._abscissa_vals_generator(interval_indices)\n\n    def plot_generator(self):\n        interval_indices = self._parent._data_interval_indices()\n        for start, stop in interval_indices:\n            try:\n                yield self._parent._abscissa_vals[start:stop]\n            except StopIteration:\n                return\n\n    def __iter__(self):\n        self._index = 0\n        return self\n\n    def __next__(self):\n        index = self._index\n\n        if index &gt; self._parent.n_intervals - 1:\n            raise StopIteration\n\n        interval_indices = self._parent._data_interval_indices()\n        interval_indices = interval_indices[index]\n        start, stop = interval_indices\n\n        self._index += 1\n\n        return self._parent._abscissa_vals[start:stop]\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.AnalogSignalArray","title":"<code>AnalogSignalArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>Array of continuous analog signals with regular sampling rates.</p> <p>This class extends RegularlySampledAnalogSignalArray with additional aliases and legacy support for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of signal data with shape (n_signals, n_samples). Default is empty array.</p> required <code>abscissa_vals</code> <code>ndarray</code> <p>Time values corresponding to samples, with shape (n_samples,). Default is None.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Default is None.</p> required <code>step</code> <code>float</code> <p>Sampling interval in seconds. Default is None.</p> required <code>merge_sample_gap</code> <code>float</code> <p>Maximum gap between samples to merge intervals (seconds). Default is 0.</p> required <code>support</code> <code>IntervalArray</code> <p>Time intervals where signal is defined. Default is None.</p> required <code>in_core</code> <code>bool</code> <p>Whether to keep data in core memory. Default is True.</p> required <code>labels</code> <code>array - like</code> <p>Labels for each signal. Default is None.</p> required <code>empty</code> <code>bool</code> <p>If True, creates empty array. Default is False.</p> required <code>abscissa</code> <code>AnalogSignalArrayAbscissa</code> <p>Abscissa object. Default is created from support.</p> required <code>ordinate</code> <code>AnalogSignalArrayOrdinate</code> <p>Ordinate object. Default is empty.</p> required Aliases <p>time : abscissa_vals     Alias for time values.</p> <p>n_epochs : n_intervals     Alias for number of intervals. ydata : data     Legacy alias for data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy import AnalogSignalArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a simple sine wave signal\n&gt;&gt;&gt; time = np.linspace(0, 1, 1000)  # 1 second of data\n&gt;&gt;&gt; signal = np.sin(2 * np.pi * 5 * time)  # 5 Hz sine wave\n</code></pre> <pre><code>&gt;&gt;&gt; # Create AnalogSignalArray with default parameters\n&gt;&gt;&gt; asa = AnalogSignalArray(\n...     data=signal[np.newaxis, :], abscissa_vals=time, fs=1000\n... )  # 1 kHz sampling\n</code></pre> <pre><code>&gt;&gt;&gt; # Access data using different aliases\n&gt;&gt;&gt; print(asa.data.shape)  # (1, 1000)\n&gt;&gt;&gt; print(asa.ydata.shape)  # same as data (legacy alias)\n&gt;&gt;&gt; print(asa.time.shape)  # (1000,) alias for abscissa_vals\n</code></pre> <pre><code>&gt;&gt;&gt; # Plot the signal (requires matplotlib)\n&gt;&gt;&gt; # asa.plot()\n</code></pre> <pre><code>&gt;&gt;&gt; # Create multi-channel signal with labels\n&gt;&gt;&gt; signals = np.vstack([signal, np.cos(2 * np.pi * 5 * time)])  # add cosine wave\n&gt;&gt;&gt; asa2 = AnalogSignalArray(\n...     data=signals, abscissa_vals=time, fs=1000, labels=[\"sine\", \"cosine\"]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Access individual channels\n&gt;&gt;&gt; sine_channel = asa2[:, 0]\n&gt;&gt;&gt; cosine_channel = asa2[:, 1]\n</code></pre> Notes <ul> <li>Inherits all attributes and methods from RegularlySampledAnalogSignalArray</li> <li>Provides backward compatibility with legacy parameter names</li> <li>Automatically handles abscissa and ordinate objects if not provided</li> </ul> See Also <p>RegularlySampledAnalogSignalArray : Parent class with core functionality</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class AnalogSignalArray(RegularlySampledAnalogSignalArray):\n    \"\"\"Array of continuous analog signals with regular sampling rates.\n\n    This class extends RegularlySampledAnalogSignalArray with additional aliases\n    and legacy support for backward compatibility.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Array of signal data with shape (n_signals, n_samples).\n        Default is empty array.\n    abscissa_vals : np.ndarray, optional\n        Time values corresponding to samples, with shape (n_samples,).\n        Default is None.\n    fs : float, optional\n        Sampling frequency in Hz. Default is None.\n    step : float, optional\n        Sampling interval in seconds. Default is None.\n    merge_sample_gap : float, optional\n        Maximum gap between samples to merge intervals (seconds).\n        Default is 0.\n    support : nelpy.IntervalArray, optional\n        Time intervals where signal is defined. Default is None.\n    in_core : bool, optional\n        Whether to keep data in core memory. Default is True.\n    labels : array-like, optional\n        Labels for each signal. Default is None.\n    empty : bool, optional\n        If True, creates empty array. Default is False.\n    abscissa : nelpy.core.AnalogSignalArrayAbscissa, optional\n        Abscissa object. Default is created from support.\n    ordinate : nelpy.core.AnalogSignalArrayOrdinate, optional\n        Ordinate object. Default is empty.\n\n    Aliases\n    -------\n    time : abscissa_vals\n        Alias for time values.\n\n    n_epochs : n_intervals\n        Alias for number of intervals.\n    ydata : data\n        Legacy alias for data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy import AnalogSignalArray\n\n    &gt;&gt;&gt; # Create a simple sine wave signal\n    &gt;&gt;&gt; time = np.linspace(0, 1, 1000)  # 1 second of data\n    &gt;&gt;&gt; signal = np.sin(2 * np.pi * 5 * time)  # 5 Hz sine wave\n\n    &gt;&gt;&gt; # Create AnalogSignalArray with default parameters\n    &gt;&gt;&gt; asa = AnalogSignalArray(\n    ...     data=signal[np.newaxis, :], abscissa_vals=time, fs=1000\n    ... )  # 1 kHz sampling\n\n    &gt;&gt;&gt; # Access data using different aliases\n    &gt;&gt;&gt; print(asa.data.shape)  # (1, 1000)\n    &gt;&gt;&gt; print(asa.ydata.shape)  # same as data (legacy alias)\n    &gt;&gt;&gt; print(asa.time.shape)  # (1000,) alias for abscissa_vals\n\n    &gt;&gt;&gt; # Plot the signal (requires matplotlib)\n    &gt;&gt;&gt; # asa.plot()\n\n    &gt;&gt;&gt; # Create multi-channel signal with labels\n    &gt;&gt;&gt; signals = np.vstack([signal, np.cos(2 * np.pi * 5 * time)])  # add cosine wave\n    &gt;&gt;&gt; asa2 = AnalogSignalArray(\n    ...     data=signals, abscissa_vals=time, fs=1000, labels=[\"sine\", \"cosine\"]\n    ... )\n\n    &gt;&gt;&gt; # Access individual channels\n    &gt;&gt;&gt; sine_channel = asa2[:, 0]\n    &gt;&gt;&gt; cosine_channel = asa2[:, 1]\n\n    Notes\n    -----\n    - Inherits all attributes and methods from RegularlySampledAnalogSignalArray\n    - Provides backward compatibility with legacy parameter names\n    - Automatically handles abscissa and ordinate objects if not provided\n\n    See Also\n    --------\n    RegularlySampledAnalogSignalArray : Parent class with core functionality\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"abscissa_vals\",\n        \"_time\": \"_abscissa_vals\",\n        \"n_epochs\": \"n_intervals\",\n        \"ydata\": \"data\",  # legacy support\n        \"_ydata\": \"_data\",  # legacy support\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        # legacy ASA constructor support for backward compatibility\n        kwargs = legacyASAkwargs(**kwargs)\n\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        abscissa = kwargs.get(\n            \"abscissa\", core.AnalogSignalArrayAbscissa(support=support)\n        )\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.DataSlicer","title":"<code>DataSlicer</code>","text":"<p>               Bases: <code>object</code></p> <p>Slicer for extracting data from analog signal arrays by interval and signal.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>object</code> <p>The parent object to slice.</p> required Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class DataSlicer(object):\n    \"\"\"\n    Slicer for extracting data from analog signal arrays by interval and signal.\n\n    Parameters\n    ----------\n    parent : object\n        The parent object to slice.\n    \"\"\"\n\n    def __init__(self, parent):\n        \"\"\"\n        Initialize the DataSlicer.\n\n        Parameters\n        ----------\n        parent : object\n            The parent object to slice.\n        \"\"\"\n        self._parent = parent\n\n    def _data_generator(self, interval_indices, signalslice):\n        \"\"\"\n        Generator for data slices by interval and signal.\n\n        Parameters\n        ----------\n        interval_indices : list or array-like\n            Indices of intervals.\n        signalslice : int or slice\n            Signal slice.\n\n        Yields\n        ------\n        data : np.ndarray\n            Data for each interval and signal.\n        \"\"\"\n        for start, stop in interval_indices:\n            try:\n                yield self._parent._data[signalslice, start:stop]\n            except StopIteration:\n                return\n\n    def __getitem__(self, idx):\n        intervalslice, signalslice = self._parent._intervalsignalslicer[idx]\n\n        interval_indices = self._parent._data_interval_indices()\n        interval_indices = np.atleast_2d(interval_indices[intervalslice])\n\n        if len(interval_indices) &lt; 2:\n            start, stop = interval_indices[0]\n            return self._parent._data[signalslice, start:stop]\n        else:\n            return self._data_generator(interval_indices, signalslice)\n\n    def plot_generator(self):\n        interval_indices = self._parent._data_interval_indices()\n        for start, stop in interval_indices:\n            try:\n                yield self._parent._data[:, start:stop]\n            except StopIteration:\n                return\n\n    def __iter__(self):\n        self._index = 0\n        return self\n\n    def __next__(self):\n        index = self._index\n\n        if index &gt; self._parent.n_intervals - 1:\n            raise StopIteration\n\n        interval_indices = self._parent._data_interval_indices()\n        interval_indices = interval_indices[index]\n        start, stop = interval_indices\n\n        self._index += 1\n\n        return self._parent._data[:, start:stop]\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.IMUSensorArray","title":"<code>IMUSensorArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>Array for storing IMU (Inertial Measurement Unit) sensor data with regular sampling rates.</p> <p>This class extends RegularlySampledAnalogSignalArray for IMU-specific data, such as accelerometer, gyroscope, and magnetometer signals.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>__aliases__</code> <code>dict</code> <p>Dictionary of class-specific aliases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; imu = IMUSensorArray(data=[[0, 1, 2], [3, 4, 5]], fs=100)\n&gt;&gt;&gt; imu.data\narray([[0, 1, 2],\n       [3, 4, 5]])\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class IMUSensorArray(RegularlySampledAnalogSignalArray):\n    \"\"\"\n    Array for storing IMU (Inertial Measurement Unit) sensor data with regular sampling rates.\n\n    This class extends RegularlySampledAnalogSignalArray for IMU-specific data, such as accelerometer, gyroscope, and magnetometer signals.\n\n    Parameters\n    ----------\n    *args :\n        Positional arguments passed to the parent class.\n    **kwargs :\n        Keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    __aliases__ : dict\n        Dictionary of class-specific aliases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; imu = IMUSensorArray(data=[[0, 1, 2], [3, 4, 5]], fs=100)\n    &gt;&gt;&gt; imu.data\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.IntervalSignalSlicer","title":"<code>IntervalSignalSlicer</code>","text":"<p>               Bases: <code>object</code></p> <p>Slicer for extracting intervals and signals from analog signal arrays.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The parent object to slice.</p> required Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class IntervalSignalSlicer(object):\n    \"\"\"\n    Slicer for extracting intervals and signals from analog signal arrays.\n\n    Parameters\n    ----------\n    obj : object\n        The parent object to slice.\n    \"\"\"\n\n    def __init__(self, obj):\n        \"\"\"\n        Initialize the IntervalSignalSlicer.\n\n        Parameters\n        ----------\n        obj : object\n            The parent object to slice.\n        \"\"\"\n        self.obj = obj\n\n    def __getitem__(self, *args):\n        \"\"\"\n        Extract intervals and signals based on the provided arguments.\n\n        Parameters\n        ----------\n        *args : int, slice, or IntervalArray\n            Indices or slices for intervals and signals.\n\n        Returns\n        -------\n        intervalslice : int, slice, or IntervalArray\n            The interval slice.\n        signalslice : int or slice\n            The signal slice.\n        \"\"\"\n        # by default, keep all signals\n        signalslice = slice(None, None, None)\n        if isinstance(*args, int):\n            intervalslice = args[0]\n        elif isinstance(*args, core.IntervalArray):\n            intervalslice = args[0]\n        else:\n            try:\n                slices = np.s_[args]\n                slices = slices[0]\n                if len(slices) &gt; 2:\n                    raise IndexError(\n                        \"only [intervals, signal] slicing is supported at this time!\"\n                    )\n                elif len(slices) == 2:\n                    intervalslice, signalslice = slices\n                else:\n                    intervalslice = slices[0]\n            except TypeError:\n                # only interval to slice:\n                intervalslice = slices\n\n        return intervalslice, signalslice\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.MinimalExampleArray","title":"<code>MinimalExampleArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>MinimalExampleArray is a custom example subclass of RegularlySampledAnalogSignalArray.</p> <p>This class demonstrates how to extend RegularlySampledAnalogSignalArray with custom aliases and methods.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>__aliases__</code> <code>dict</code> <p>Dictionary of class-specific aliases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n&gt;&gt;&gt; arr.custom_func()\nWoot! We have some special skillz!\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class MinimalExampleArray(RegularlySampledAnalogSignalArray):\n    \"\"\"\n    MinimalExampleArray is a custom example subclass of RegularlySampledAnalogSignalArray.\n\n    This class demonstrates how to extend RegularlySampledAnalogSignalArray with custom aliases and methods.\n\n    Parameters\n    ----------\n    *args :\n        Positional arguments passed to the parent class.\n    **kwargs :\n        Keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    __aliases__ : dict\n        Dictionary of class-specific aliases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n    &gt;&gt;&gt; arr.custom_func()\n    Woot! We have some special skillz!\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n    def custom_func(self):\n        \"\"\"\n        Print a custom message demonstrating a special method.\n\n        Examples\n        --------\n        &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n        &gt;&gt;&gt; arr.custom_func()\n        Woot! We have some special skillz!\n        \"\"\"\n        print(\"Woot! We have some special skillz!\")\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.MinimalExampleArray.custom_func","title":"<code>custom_func()</code>","text":"<p>Print a custom message demonstrating a special method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n&gt;&gt;&gt; arr.custom_func()\nWoot! We have some special skillz!\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def custom_func(self):\n    \"\"\"\n    Print a custom message demonstrating a special method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n    &gt;&gt;&gt; arr.custom_func()\n    Woot! We have some special skillz!\n    \"\"\"\n    print(\"Woot! We have some special skillz!\")\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.PositionArray","title":"<code>PositionArray</code>","text":"<p>               Bases: <code>AnalogSignalArray</code></p> <p>An array for storing position data in 1D or 2D space.</p> <p>PositionArray is a specialized subclass of AnalogSignalArray designed to handle position tracking data. It provides convenient access to x and y coordinates, supports both 1D and 2D positional data, and includes spatial boundary information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like or None</code> <p>Position data with shape (n_signals, n_samples). For 1D position data, n_signals should be 1. For 2D position data, n_signals should be 2, where the first row contains x-coordinates and the second row contains y-coordinates. Can also be specified using the alias 'posdata'.</p> required <code>timestamps</code> <code>array_like or None</code> <p>Time stamps corresponding to each sample in data. If None, timestamps are automatically generated based on fs and start time.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Used to generate timestamps if not provided.</p> required <code>support</code> <code>EpochArray or None</code> <p>EpochArray defining the time intervals over which the position data is valid.</p> required <code>label</code> <code>str</code> <p>Descriptive label for the position array.</p> required <code>xlim</code> <code>tuple or None</code> <p>Spatial boundaries for x-coordinate as (min_x, max_x).</p> required <code>ylim</code> <code>tuple or None</code> <p>Spatial boundaries for y-coordinate as (min_y, max_y).</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>X-coordinates as a 1D numpy array.</p> <code>y</code> <code>ndarray</code> <p>Y-coordinates as a 1D numpy array (only available for 2D data).</p> <code>is_1d</code> <code>bool</code> <p>True if position data is 1-dimensional.</p> <code>is_2d</code> <code>bool</code> <p>True if position data is 2-dimensional.</p> <code>xlim</code> <code>tuple or None</code> <p>Spatial boundaries for x-coordinate (only for 2D data).</p> <code>ylim</code> <code>tuple or None</code> <p>Spatial boundaries for y-coordinate (only for 2D data).</p> <p>Examples:</p> <p>Create a 1D position array:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; # 1D position data (e.g., position on a linear track)\n&gt;&gt;&gt; x_pos = np.linspace(0, 100, 1000)  # 100 cm track\n&gt;&gt;&gt; timestamps = np.linspace(0, 10, 1000)  # 10 seconds\n&gt;&gt;&gt; pos_1d = nel.PositionArray(\n...     data=x_pos[np.newaxis, :],\n...     timestamps=timestamps,\n...     label=\"Linear track position\",\n... )\n&gt;&gt;&gt; print(f\"1D position: {pos_1d.is_1d}\")\n&gt;&gt;&gt; print(f\"X range: {pos_1d.x.min():.1f} to {pos_1d.x.max():.1f} cm\")\n</code></pre> <p>Create a 2D position array:</p> <pre><code>&gt;&gt;&gt; # 2D position data (e.g., open field behavior)\n&gt;&gt;&gt; t = np.linspace(0, 2 * np.pi, 1000)\n&gt;&gt;&gt; x_pos = 50 + 30 * np.cos(t)  # circular trajectory\n&gt;&gt;&gt; y_pos = 50 + 30 * np.sin(t)\n&gt;&gt;&gt; pos_data = np.vstack([x_pos, y_pos])\n&gt;&gt;&gt; pos_2d = nel.PositionArray(\n...     posdata=pos_data,\n...     fs=100,  # 100 Hz sampling\n...     xlim=(0, 100),\n...     ylim=(0, 100),\n...     label=\"Open field position\",\n... )\n&gt;&gt;&gt; print(f\"2D position: {pos_2d.is_2d}\")\n&gt;&gt;&gt; print(f\"X position shape: {pos_2d.x.shape}\")\n&gt;&gt;&gt; print(f\"Y position shape: {pos_2d.y.shape}\")\n&gt;&gt;&gt; print(f\"Spatial bounds: x={pos_2d.xlim}, y={pos_2d.ylim}\")\n</code></pre> <p>Access position data:</p> <pre><code>&gt;&gt;&gt; # Get position at specific time\n&gt;&gt;&gt; time_idx = 500\n&gt;&gt;&gt; if pos_2d.is_2d:\n...     x_at_time = pos_2d.x[time_idx]\n...     y_at_time = pos_2d.y[time_idx]\n...     print(f\"Position at sample {time_idx}: ({x_at_time:.1f}, {y_at_time:.1f})\")\n</code></pre> Notes <ul> <li>For 2D position data, the first row of data should contain x-coordinates   and the second row should contain y-coordinates.</li> <li>The xlim and ylim parameters are only meaningful for 2D position data.</li> <li>Attempting to access y-coordinates or spatial limits on 1D data will   raise a ValueError.</li> <li>The 'posdata' alias can be used interchangeably with 'data' parameter.</li> </ul> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class PositionArray(AnalogSignalArray):\n    \"\"\"An array for storing position data in 1D or 2D space.\n\n    PositionArray is a specialized subclass of AnalogSignalArray designed to\n    handle position tracking data. It provides convenient access to x and y\n    coordinates, supports both 1D and 2D positional data, and includes\n    spatial boundary information.\n\n    Parameters\n    ----------\n    data : array_like or None, optional\n        Position data with shape (n_signals, n_samples). For 1D position data,\n        n_signals should be 1. For 2D position data, n_signals should be 2,\n        where the first row contains x-coordinates and the second row contains\n        y-coordinates. Can also be specified using the alias 'posdata'.\n    timestamps : array_like or None, optional\n        Time stamps corresponding to each sample in data. If None, timestamps\n        are automatically generated based on fs and start time.\n    fs : float, optional\n        Sampling frequency in Hz. Used to generate timestamps if not provided.\n    support : EpochArray or None, optional\n        EpochArray defining the time intervals over which the position data\n        is valid.\n    label : str, optional\n        Descriptive label for the position array.\n    xlim : tuple or None, optional\n        Spatial boundaries for x-coordinate as (min_x, max_x).\n    ylim : tuple or None, optional\n        Spatial boundaries for y-coordinate as (min_y, max_y).\n\n    Attributes\n    ----------\n    x : ndarray\n        X-coordinates as a 1D numpy array.\n    y : ndarray\n        Y-coordinates as a 1D numpy array (only available for 2D data).\n    is_1d : bool\n        True if position data is 1-dimensional.\n    is_2d : bool\n        True if position data is 2-dimensional.\n    xlim : tuple or None\n        Spatial boundaries for x-coordinate (only for 2D data).\n    ylim : tuple or None\n        Spatial boundaries for y-coordinate (only for 2D data).\n\n    Examples\n    --------\n    Create a 1D position array:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; # 1D position data (e.g., position on a linear track)\n    &gt;&gt;&gt; x_pos = np.linspace(0, 100, 1000)  # 100 cm track\n    &gt;&gt;&gt; timestamps = np.linspace(0, 10, 1000)  # 10 seconds\n    &gt;&gt;&gt; pos_1d = nel.PositionArray(\n    ...     data=x_pos[np.newaxis, :],\n    ...     timestamps=timestamps,\n    ...     label=\"Linear track position\",\n    ... )\n    &gt;&gt;&gt; print(f\"1D position: {pos_1d.is_1d}\")\n    &gt;&gt;&gt; print(f\"X range: {pos_1d.x.min():.1f} to {pos_1d.x.max():.1f} cm\")\n\n    Create a 2D position array:\n\n    &gt;&gt;&gt; # 2D position data (e.g., open field behavior)\n    &gt;&gt;&gt; t = np.linspace(0, 2 * np.pi, 1000)\n    &gt;&gt;&gt; x_pos = 50 + 30 * np.cos(t)  # circular trajectory\n    &gt;&gt;&gt; y_pos = 50 + 30 * np.sin(t)\n    &gt;&gt;&gt; pos_data = np.vstack([x_pos, y_pos])\n    &gt;&gt;&gt; pos_2d = nel.PositionArray(\n    ...     posdata=pos_data,\n    ...     fs=100,  # 100 Hz sampling\n    ...     xlim=(0, 100),\n    ...     ylim=(0, 100),\n    ...     label=\"Open field position\",\n    ... )\n    &gt;&gt;&gt; print(f\"2D position: {pos_2d.is_2d}\")\n    &gt;&gt;&gt; print(f\"X position shape: {pos_2d.x.shape}\")\n    &gt;&gt;&gt; print(f\"Y position shape: {pos_2d.y.shape}\")\n    &gt;&gt;&gt; print(f\"Spatial bounds: x={pos_2d.xlim}, y={pos_2d.ylim}\")\n\n    Access position data:\n\n    &gt;&gt;&gt; # Get position at specific time\n    &gt;&gt;&gt; time_idx = 500\n    &gt;&gt;&gt; if pos_2d.is_2d:\n    ...     x_at_time = pos_2d.x[time_idx]\n    ...     y_at_time = pos_2d.y[time_idx]\n    ...     print(f\"Position at sample {time_idx}: ({x_at_time:.1f}, {y_at_time:.1f})\")\n\n    Notes\n    -----\n    - For 2D position data, the first row of data should contain x-coordinates\n      and the second row should contain y-coordinates.\n    - The xlim and ylim parameters are only meaningful for 2D position data.\n    - Attempting to access y-coordinates or spatial limits on 1D data will\n      raise a ValueError.\n    - The 'posdata' alias can be used interchangeably with 'data' parameter.\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\"posdata\": \"data\"}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        xlim = kwargs.pop(\"xlim\", None)\n        ylim = kwargs.pop(\"ylim\", None)\n        super().__init__(*args, **kwargs)\n        self._xlim = xlim\n        self._ylim = ylim\n\n    @property\n    def is_2d(self):\n        try:\n            return self.n_signals == 2\n        except IndexError:\n            return False\n\n    @property\n    def is_1d(self):\n        try:\n            return self.n_signals == 1\n        except IndexError:\n            return False\n\n    @property\n    def x(self):\n        \"\"\"return x-values, as numpy array.\"\"\"\n        return self.data[0, :]\n\n    @property\n    def y(self):\n        \"\"\"return y-values, as numpy array.\"\"\"\n        if self.is_2d:\n            return self.data[1, :]\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so y-values are undefined!\"\n        )\n\n    @property\n    def xlim(self):\n        if self.is_2d:\n            return self._xlim\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so xlim is not undefined!\"\n        )\n\n    @xlim.setter\n    def xlim(self, val):\n        if self.is_2d:\n            self._xlim = xlim  # noqa: F821\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so xlim cannot be defined!\"\n        )\n\n    @property\n    def ylim(self):\n        if self.is_2d:\n            return self._ylim\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so ylim is not undefined!\"\n        )\n\n    @ylim.setter\n    def ylim(self, val):\n        if self.is_2d:\n            self._ylim = ylim  # noqa: F821\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so ylim cannot be defined!\"\n        )\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.PositionArray.x","title":"<code>x</code>  <code>property</code>","text":"<p>return x-values, as numpy array.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.PositionArray.y","title":"<code>y</code>  <code>property</code>","text":"<p>return y-values, as numpy array.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray","title":"<code>RegularlySampledAnalogSignalArray</code>","text":"<p>Continuous analog signal(s) with regular sampling rates (irregular sampling rates can be corrected with operations on the support) and same support. NOTE: data that is not equal dimensionality will NOT work and error/warning messages may/may not be sent out. Assumes abscissa_vals are identical for all signals passed through and are therefore expected to be 1-dimensional.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray, with shape (n_signals, n_samples).</code> <p>Data samples.</p> <code>[]</code> <code>abscissa_vals</code> <code>np.ndarray, with shape (n_samples, ).</code> <p>The abscissa coordinate values. Currently we assume that (1) these values are timestamps, and (2) the timestamps are sampled regularly (we rely on these assumptions to generate intervals). Irregular sampling rates can be corrected with operations on the support.</p> <code>None</code> <code>fs</code> <code>float</code> <p>The sampling rate. abscissa_vals are still expected to be in units of time and fs is expected to be in the corresponding sampling rate (e.g. abscissa_vals in seconds, fs in Hz). Default is 1 Hz.</p> <code>None</code> <code>step</code> <code>float</code> <p>The sampling interval of the data, in seconds. Default is None. specifies step size of samples passed as tdata if fs is given, default is None. If not passed it is inferred by the minimum difference in between samples of tdata passed in (based on if FS is passed). e.g. decimated data would have sample numbers every ten samples so step=10</p> <code>None</code> <code>merge_sample_gap</code> <code>float</code> <p>Optional merging of gaps between support intervals. If intervals are within a certain amount of time, gap, they will be merged as one interval. Example use case is when there is a dropped sample</p> <code>0</code> <code>support</code> <code>IntervalArray</code> <p>Where the data are defined. Default is [0, last abscissa value] inclusive.</p> <code>None</code> <code>in_core</code> <code>bool</code> <p>Whether the abscissa values should be treated as residing in core memory. During RSASA construction, np.diff() is called, so for large data, passing in in_core=True might help. In that case, a slower but much smaller memory footprint function is used.</p> <code>True</code> <code>labels</code> <code>np.array, dtype=np.str</code> <p>Labels for each of the signals. If fewer labels than signals are passed in, labels are padded with None's to match the number of signals. If more labels than signals are passed in, labels are truncated to match the number of signals. Default is None.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Return an empty RegularlySampledAnalogSignalArray if true else false. Default is false.</p> <code>False</code> <code>abscissa</code> <code>optional</code> <p>The object handling the abscissa values. It is recommended to leave this parameter alone and let nelpy take care of this. Default is a nelpy.core.Abscissa object.</p> <code>None</code> <code>ordinate</code> <code>optional</code> <p>The object handling the ordinate values. It is recommended to leave this parameter alone and let nelpy take care of this. Default is a nelpy.core.Ordinate object.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>np.ndarray, with shape (n_signals, n_samples)</code> <p>The underlying data.</p> <code>abscissa_vals</code> <code>np.ndarray, with shape (n_samples, )</code> <p>The values of the abscissa coordinate.</p> <code>is1d</code> <code>bool</code> <p>Whether there is only 1 signal in the RSASA</p> <code>iswrapped</code> <code>bool</code> <p>Whether the RSASA's data is wrapping.</p> <code>base_unit</code> <code>string</code> <p>Base unit of the abscissa.</p> <code>signals</code> <code>list</code> <p>A list of RegularlySampledAnalogSignalArrays, each RSASA containing a single signal (channel). WARNING: this method creates a copy of each signal, so is not particularly efficient at this time.</p> <code>isreal</code> <code>bool</code> <p>Whether ALL of the values in the RSASA's data are real.</p> <code>iscomplex</code> <code>bool</code> <p>Whether ANY values in the data are complex.</p> <code>abs</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is the absolute value of the original original RSASA's (potentially complex) data.</p> <code>phase</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the phase angle (in radians) of the original RSASA's data.</p> <code>real</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the real part of the original RSASA's data.</p> <code>imag</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the imaginary part of the original RSASA's data.</p> <code>lengths</code> <code>list</code> <p>The number of samples in each interval.</p> <code>labels</code> <code>list</code> <p>The labels corresponding to each signal.</p> <code>n_signals</code> <code>int</code> <p>The number of signals in the RSASA.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the RSASA.</p> <code>domain</code> <code>IntervalArray</code> <p>The domain of the RSASA.</p> <code>range</code> <code>IntervalArray</code> <p>The range of the RSASA's data.</p> <code>step</code> <code>float</code> <p>The sampling interval of the RSASA. Currently the units are in seconds.</p> <code>fs</code> <code>float</code> <p>The sampling frequency of the RSASA. Currently the units are in Hz.</p> <code>isempty</code> <code>bool</code> <p>Whether the underlying data has zero length, i.e. 0 samples</p> <code>n_bytes</code> <code>int</code> <p>Approximate number of bytes taken up by the RSASA.</p> <code>n_intervals</code> <code>int</code> <p>The number of underlying intervals in the RSASA.</p> <code>n_samples</code> <code>int</code> <p>The number of abscissa values in the RSASA.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class RegularlySampledAnalogSignalArray:\n    \"\"\"Continuous analog signal(s) with regular sampling rates (irregular\n    sampling rates can be corrected with operations on the support) and same\n    support. NOTE: data that is not equal dimensionality will NOT work\n    and error/warning messages may/may not be sent out. Assumes abscissa_vals\n    are identical for all signals passed through and are therefore expected\n    to be 1-dimensional.\n\n    Parameters\n    ----------\n    data : np.ndarray, with shape (n_signals, n_samples).\n        Data samples.\n    abscissa_vals : np.ndarray, with shape (n_samples, ).\n        The abscissa coordinate values. Currently we assume that (1) these values\n        are timestamps, and (2) the timestamps are sampled regularly (we rely on\n        these assumptions to generate intervals). Irregular sampling rates can be\n        corrected with operations on the support.\n    fs : float, optional\n        The sampling rate. abscissa_vals are still expected to be in units of\n        time and fs is expected to be in the corresponding sampling rate (e.g.\n        abscissa_vals in seconds, fs in Hz).\n        Default is 1 Hz.\n    step : float, optional\n        The sampling interval of the data, in seconds.\n        Default is None.\n        specifies step size of samples passed as tdata if fs is given,\n        default is None. If not passed it is inferred by the minimum\n        difference in between samples of tdata passed in (based on if FS\n        is passed). e.g. decimated data would have sample numbers every\n        ten samples so step=10\n    merge_sample_gap : float, optional\n        Optional merging of gaps between support intervals. If intervals are within\n        a certain amount of time, gap, they will be merged as one interval. Example\n        use case is when there is a dropped sample\n    support : nelpy.IntervalArray, optional\n        Where the data are defined. Default is [0, last abscissa value] inclusive.\n    in_core : bool, optional\n        Whether the abscissa values should be treated as residing in core memory.\n        During RSASA construction, np.diff() is called, so for large data, passing\n        in in_core=True might help. In that case, a slower but much smaller memory\n        footprint function is used.\n    labels : np.array, dtype=np.str\n        Labels for each of the signals. If fewer labels than signals are passed in,\n        labels are padded with None's to match the number of signals. If more labels\n        than signals are passed in, labels are truncated to match the number of\n        signals.\n        Default is None.\n    empty : bool, optional\n        Return an empty RegularlySampledAnalogSignalArray if true else false.\n        Default is false.\n    abscissa : optional\n        The object handling the abscissa values. It is recommended to leave\n        this parameter alone and let nelpy take care of this.\n        Default is a nelpy.core.Abscissa object.\n    ordinate : optional\n        The object handling the ordinate values. It is recommended to leave\n        this parameter alone and let nelpy take care of this.\n        Default is a nelpy.core.Ordinate object.\n\n    Attributes\n    ----------\n    data : np.ndarray, with shape (n_signals, n_samples)\n        The underlying data.\n    abscissa_vals : np.ndarray, with shape (n_samples, )\n        The values of the abscissa coordinate.\n    is1d : bool\n        Whether there is only 1 signal in the RSASA\n    iswrapped : bool\n        Whether the RSASA's data is wrapping.\n    base_unit : string\n        Base unit of the abscissa.\n    signals : list\n        A list of RegularlySampledAnalogSignalArrays, each RSASA containing\n        a single signal (channel).\n        WARNING: this method creates a copy of each signal, so is not\n        particularly efficient at this time.\n    isreal : bool\n        Whether ALL of the values in the RSASA's data are real.\n    iscomplex : bool\n        Whether ANY values in the data are complex.\n    abs : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is the absolute value of the original\n        original RSASA's (potentially complex) data.\n    phase : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the phase angle (in radians) of\n        the original RSASA's data.\n    real : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the real part of the original\n        RSASA's data.\n    imag : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the imaginary part of the\n        original RSASA's data.\n    lengths : list\n        The number of samples in each interval.\n    labels : list\n        The labels corresponding to each signal.\n    n_signals : int\n        The number of signals in the RSASA.\n    support : nelpy.IntervalArray\n        The support of the RSASA.\n    domain : nelpy.IntervalArray\n        The domain of the RSASA.\n    range : nelpy.IntervalArray\n        The range of the RSASA's data.\n    step : float\n        The sampling interval of the RSASA. Currently the units are\n        in seconds.\n    fs : float\n        The sampling frequency of the RSASA. Currently the units are\n        in Hz.\n    isempty : bool\n        Whether the underlying data has zero length, i.e. 0 samples\n    n_bytes : int\n        Approximate number of bytes taken up by the RSASA.\n    n_intervals : int\n        The number of underlying intervals in the RSASA.\n    n_samples : int\n        The number of abscissa values in the RSASA.\n    \"\"\"\n\n    __aliases__ = {}\n\n    __attributes__ = [\n        \"_data\",\n        \"_abscissa_vals\",\n        \"_fs\",\n        \"_support\",\n        \"_interp\",\n        \"_step\",\n        \"_labels\",\n    ]\n\n    @rsasa_init_wrapper\n    def __init__(\n        self,\n        data=[],\n        *,\n        abscissa_vals=None,\n        fs=None,\n        step=None,\n        merge_sample_gap=0,\n        support=None,\n        in_core=True,\n        labels=None,\n        empty=False,\n        abscissa=None,\n        ordinate=None,\n    ):\n        self._intervalsignalslicer = IntervalSignalSlicer(self)\n        self._intervaldata = DataSlicer(self)\n        self._intervaltime = AbscissaSlicer(self)\n\n        self.type_name = self.__class__.__name__\n        if abscissa is None:\n            abscissa = core.Abscissa()  # TODO: integrate into constructor?\n        if ordinate is None:\n            ordinate = core.Ordinate()  # TODO: integrate into constructor?\n\n        self._abscissa = abscissa\n        self._ordinate = ordinate\n\n        # TODO: #FIXME abscissa and ordinate domain, range, and supports should be integrated and/or coerced with support\n\n        self.__version__ = version.__version__\n\n        # cast derivatives of RegularlySampledAnalogSignalArray back into RegularlySampledAnalogSignalArray:\n        # if isinstance(data, auxiliary.PositionArray):\n        if isinstance(data, RegularlySampledAnalogSignalArray):\n            self.__dict__ = copy.deepcopy(data.__dict__)\n            # if self._has_changed:\n            # self.__renew__()\n            self.__renew__()\n            return\n\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self._data = np.array([])\n            self._abscissa_vals = np.array([])\n            self.__bake__()\n            return\n\n        self._step = step\n        self._fs = fs\n\n        # Note; if we have an empty array of data with no dimension,\n        # then calling len(data) will return a TypeError\n        try:\n            # if no data are given return empty RegularlySampledAnalogSignalArray\n            if data.size == 0:\n                self.__init__(empty=True)\n                return\n        except TypeError:\n            logging.warning(\n                \"unsupported type; creating empty RegularlySampledAnalogSignalArray\"\n            )\n            self.__init__(empty=True)\n            return\n\n        # Note: if both abscissa_vals and data are given and dimensionality does not\n        # match, then TypeError!\n\n        abscissa_vals = np.squeeze(abscissa_vals).astype(float)\n        if abscissa_vals.shape[0] != data.shape[1]:\n            # self.__init__([],empty=True)\n            raise TypeError(\n                \"abscissa_vals and data size mismatch! Note: data \"\n                \"is expected to have rows containing signals\"\n            )\n        # data is not sorted and user wants it to be\n        # TODO: use faster is_sort from jagular\n        if not utils.is_sorted(abscissa_vals):\n            logging.warning(\"Data is _not_ sorted! Data will be sorted automatically.\")\n            ind = np.argsort(abscissa_vals)\n            abscissa_vals = abscissa_vals[ind]\n            data = np.take(a=data, indices=ind, axis=-1)\n\n        self._data = data\n        self._abscissa_vals = abscissa_vals\n\n        # handle labels\n        if labels is not None:\n            labels = np.asarray(labels, dtype=str)\n            # label size doesn't match\n            if labels.shape[0] &gt; data.shape[0]:\n                logging.warning(\n                    \"More labels than data! Labels are truncated to size of data\"\n                )\n                labels = labels[0 : data.shape[0]]\n            elif labels.shape[0] &lt; data.shape[0]:\n                logging.warning(\n                    \"Fewer labels than abscissa_vals! Labels are filled with \"\n                    \"None to match data shape\"\n                )\n                for i in range(labels.shape[0], data.shape[0]):\n                    labels.append(None)\n        self._labels = labels\n\n        # Alright, let's handle all the possible parameter cases!\n        if support is not None:\n            self._restrict_to_interval_array_fast(intervalarray=support)\n        else:\n            logging.warning(\n                \"creating support from abscissa_vals and sampling rate, fs!\"\n            )\n            self._abscissa.support = type(self._abscissa.support)(\n                utils.get_contiguous_segments(\n                    self._abscissa_vals, step=self._step, fs=fs, in_core=in_core\n                )\n            )\n            if merge_sample_gap &gt; 0:\n                self._abscissa.support = self._abscissa.support.merge(\n                    gap=merge_sample_gap\n                )\n\n        if np.abs((self.fs - self._estimate_fs()) / self.fs) &gt; 0.01:\n            logging.warning(\"estimated fs and provided fs differ by more than 1%\")\n\n    def __bake__(self):\n        \"\"\"Fix object as-is, and bake a new hash.\n\n        For example, if a label has changed, or if an interp has been attached,\n        then the object's hash will change, and it needs to be baked\n        again for efficiency / consistency.\n        \"\"\"\n        self._stored_hash_ = self.__hash__()\n\n    # def _has_changed_data(self):\n    #     \"\"\"Compute hash on abscissa_vals and data and compare to cached hash.\"\"\"\n    #     return self.data.__hash__ elf._data_hash_\n\n    def _has_changed(self):\n        \"\"\"Compute hash on current object, and compare to previously stored hash\"\"\"\n        return self.__hash__() == self._stored_hash_\n\n    def __renew__(self):\n        \"\"\"Re-attach data slicers.\"\"\"\n        self._intervalsignalslicer = IntervalSignalSlicer(self)\n        self._intervaldata = DataSlicer(self)\n        self._intervaltime = AbscissaSlicer(self)\n        self._interp = None\n        self.__bake__()\n\n    def __call__(self, x):\n        \"\"\"RegularlySampledAnalogSignalArray callable method. Returns\n        interpolated data at requested points. Note that points falling\n        outside the support will not be interpolated.\n\n        Parameters\n        ----------\n        x : np.ndarray, list, or tuple, with length n_requested_samples\n            Points at which to interpolate the RSASA's data\n\n        Returns\n        -------\n        A np.ndarray with shape (n_signals, n_samples). If all the requested\n        points lie in the support, then n_samples = n_requested_samples.\n        Otherwise n_samples &lt; n_requested_samples.\n        \"\"\"\n\n        return self.asarray(at=x).yvals\n\n    def center(self, inplace=False):\n        \"\"\"\n        Center the data to have zero mean along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The centered signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; centered = asa.center()\n        &gt;&gt;&gt; centered.mean()\n        0.0\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        return out\n\n    def normalize(self, inplace=False):\n        \"\"\"\n        Normalize the data to have unit standard deviation along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The normalized signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; normalized = asa.normalize()\n        &gt;&gt;&gt; normalized.std()\n        1.0\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        std = np.atleast_1d(out.std())\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    def standardize(self, inplace=False):\n        \"\"\"\n        Standardize the data to zero mean and unit standard deviation along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The standardized signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; standardized = asa.standardize()\n        &gt;&gt;&gt; standardized.mean(), standardized.std()\n        (0.0, 1.0)\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        std = np.atleast_1d(out.std())\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    @property\n    def is_1d(self):\n        try:\n            return self.n_signals == 1\n        except IndexError:\n            return False\n\n    @property\n    def is_wrapped(self):\n        if np.any(self.max() &gt; self._ordinate.range.stop) | np.any(\n            self.min() &lt; self._ordinate.range.min\n        ):\n            self._ordinate._is_wrapped = False\n        else:\n            self._ordinate._is_wrapped = True\n\n        # if self._ordinate._is_wrapped is None:\n        #     if np.any(self.max() &gt; self._ordinate.range.stop) | np.any(self.min() &lt; self._ordinate.range.min):\n        #         self._ordinate._is_wrapped = False\n        #     else:\n        #         self._ordinate._is_wrapped = True\n        return self._ordinate._is_wrapped\n\n    def _wrap(self, arr, vmin, vmax):\n        \"\"\"Wrap array within finite range.\"\"\"\n        if np.isinf(vmax - vmin):\n            raise ValueError(\"range has to be finite!\")\n        return ((arr - vmin) % (vmax - vmin)) + vmin\n\n    def wrap(self, inplace=False):\n        \"\"\"\n        Wrap the ordinate values within the finite range defined by the ordinate's range.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The wrapped signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; wrapped = asa.wrap()\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        out.data = np.atleast_2d(\n            out._wrap(out.data, out._ordinate.range.min, out._ordinate.range.max)\n        )\n        # out._is_wrapped = True\n        return out\n\n    def _unwrap(self, arr, vmin, vmax):\n        \"\"\"Unwrap 2D array (with one signal per row) by minimizing total displacement.\"\"\"\n        d = vmax - vmin\n        dh = d / 2\n\n        lin = copy.deepcopy(arr) - vmin\n        n_signals, n_samples = arr.shape\n        for ii in range(1, n_samples):\n            h1 = lin[:, ii] - lin[:, ii - 1] &gt;= dh\n            lin[h1, ii:] = lin[h1, ii:] - d\n            h2 = lin[:, ii] - lin[:, ii - 1] &lt; -dh\n            lin[h2, ii:] = lin[h2, ii:] + d\n        return np.atleast_2d(lin + vmin)\n\n    def unwrap(self, inplace=False):\n        \"\"\"\n        Unwrap the ordinate values by minimizing total displacement, useful for phase data.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The unwrapped signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; unwrapped = asa.unwrap()\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        out.data = np.atleast_2d(\n            out._unwrap(out._data, out._ordinate.range.min, out._ordinate.range.max)\n        )\n        # out._is_wrapped = False\n        return out\n\n    def _crossvals(self):\n        \"\"\"Return all abscissa values where the orinate crosses.\n\n        Note that this can return multiple values close in succession\n        if the signal oscillates around the maximum or minimum range.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def base_unit(self):\n        \"\"\"Base unit of the abscissa.\"\"\"\n        return self._abscissa.base_unit\n\n    def _data_interval_indices(self):\n        \"\"\"\n        Get the start and stop indices for each interval in the analog signal array.\n\n        Returns\n        -------\n        indices : np.ndarray\n            Array of shape (n_intervals, 2), where each row contains the start and stop indices for an interval.\n        \"\"\"\n        tmp = np.insert(np.cumsum(self.lengths), 0, 0)\n        indices = np.vstack((tmp[:-1], tmp[1:])).T\n        return indices\n\n    def ddt(self, rectify=False):\n        \"\"\"Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n        asa.data = f(t)\n        asa.ddt = d/dt (asa.data)\n\n        Parameters\n        ----------\n        rectify : boolean, optional\n            If True, the absolute value of the derivative will be returned.\n            Default is False.\n\n        Returns\n        -------\n        ddt : RegularlySampledAnalogSignalArray\n            Time derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n        Note\n        ----\n        Second order central differences are used here, and it is assumed that\n        the signals are sampled uniformly. If the signals are not uniformly\n        sampled, it is recommended to resample the signal before computing the\n        derivative.\n        \"\"\"\n        ddt = utils.ddt_asa(self, rectify=rectify)\n        return ddt\n\n    @property\n    def signals(self):\n        \"\"\"Returns a list of RegularlySampledAnalogSignalArrays, each array containing\n        a single signal (channel).\n\n        WARNING: this method creates a copy of each signal, so is not\n        particularly efficient at this time.\n\n        Examples\n        --------\n        &gt;&gt;&gt; for channel in lfp.signals:\n            print(channel)\n        \"\"\"\n        signals = []\n        for ii in range(self.n_signals):\n            signals.append(self[:, ii])\n        return signals\n        # return np.asanyarray(signals).squeeze()\n\n    @property\n    def isreal(self):\n        \"\"\"Returns True if entire signal is real.\"\"\"\n        return np.all(np.isreal(self.data))\n        # return np.isrealobj(self._data)\n\n    @property\n    def iscomplex(self):\n        \"\"\"Returns True if any part of the signal is complex.\"\"\"\n        return np.any(np.iscomplex(self.data))\n        # return np.iscomplexobj(self._data)\n\n    @property\n    def abs(self):\n        \"\"\"RegularlySampledAnalogSignalArray with absolute value of (potentially complex) data.\"\"\"\n        out = self.copy()\n        out._data = np.abs(self.data)\n        return out\n\n    @property\n    def angle(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only phase angle (in radians) of data.\"\"\"\n        out = self.copy()\n        out._data = np.angle(self.data)\n        return out\n\n    @property\n    def imag(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only imaginary part of data.\"\"\"\n        out = self.copy()\n        out._data = self.data.imag\n        return out\n\n    @property\n    def real(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only real part of data.\"\"\"\n        out = self.copy()\n        out._data = self.data.real\n        return out\n\n    def __mul__(self, other):\n        \"\"\"overloaded * operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data * other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T * other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to multiply.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data * other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for *: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __add__(self, other):\n        \"\"\"overloaded + operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data + other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T + other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to add.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data + other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __sub__(self, other):\n        \"\"\"overloaded - operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data - other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T - other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to subtract.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data - other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for -: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def zscore(self):\n        \"\"\"\n        Normalize each signal in the array using z-scores (zero mean, unit variance).\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            New object with z-scored data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; zscored = asa.zscore()\n        \"\"\"\n        out = self.copy()\n        out._data = zscore(out._data, axis=1)\n        return out\n\n    def __truediv__(self, other):\n        \"\"\"overloaded / operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data / other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T / other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to divide.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data / other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for /: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __lshift__(self, val):\n        \"\"\"shift abscissa and support to left (&lt;&lt;)\"\"\"\n        if isinstance(val, numbers.Number):\n            new = self.copy()\n            new._abscissa_vals -= val\n            new._abscissa.support = new._abscissa.support &lt;&lt; val\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &lt;&lt;: {} and {}\".format(\n                    str(type(self)), str(type(val))\n                )\n            )\n\n    def __rshift__(self, val):\n        \"\"\"shift abscissa and support to right (&gt;&gt;)\"\"\"\n        if isinstance(val, numbers.Number):\n            new = self.copy()\n            new._abscissa_vals += val\n            new._abscissa.support = new._abscissa.support &gt;&gt; val\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &gt;&gt;: {} and {}\".format(\n                    str(type(self)), str(type(val))\n                )\n            )\n\n    def __len__(self):\n        return self.n_intervals\n\n    def _drop_empty_intervals(self):\n        \"\"\"Drops empty intervals from support. In-place.\"\"\"\n        keep_interval_ids = np.argwhere(self.lengths).squeeze().tolist()\n        self._abscissa.support = self._abscissa.support[keep_interval_ids]\n        return self\n\n    def _estimate_fs(self, abscissa_vals=None):\n        \"\"\"Estimate the sampling rate of the data.\"\"\"\n        if abscissa_vals is None:\n            abscissa_vals = self._abscissa_vals\n        return 1.0 / np.median(np.diff(abscissa_vals))\n\n    def downsample(self, *, fs_out, aafilter=True, inplace=False, **kwargs):\n        \"\"\"Downsamples the RegularlySampledAnalogSignalArray\n\n        Parameters\n        ----------\n        fs_out : float, optional\n            Desired output sampling rate in Hz\n        aafilter : boolean, optional\n            Whether to apply an anti-aliasing filter before performing the actual\n            downsampling. Default is True\n        inplace : boolean, optional\n            If True, the output ASA will replace the input ASA. Default is False\n        kwargs :\n            Other keyword arguments are passed to sosfiltfilt() in the `filtering`\n            module\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The downsampled RegularlySampledAnalogSignalArray\n        \"\"\"\n\n        if not fs_out &lt; self._fs:\n            raise ValueError(\"fs_out must be less than current sampling rate!\")\n\n        if aafilter:\n            fh = fs_out / 2.0\n            out = filtering.sosfiltfilt(self, fl=None, fh=fh, inplace=inplace, **kwargs)\n\n        downsampled = out.simplify(ds=1 / fs_out)\n        out._data = downsampled._data\n        out._abscissa_vals = downsampled._abscissa_vals\n        out._fs = fs_out\n\n        out.__renew__()\n        return out\n\n    def add_signal(self, signal, label=None):\n        \"\"\"Docstring goes here.\n        Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?\n        \"\"\"\n        # TODO: add functionality to check that supports are the same, etc.\n        if isinstance(signal, RegularlySampledAnalogSignalArray):\n            signal = signal.data\n\n        signal = np.squeeze(signal)\n        if signal.ndim &gt; 1:\n            raise TypeError(\"Can only add one signal at a time!\")\n        if self.data.ndim == 1:\n            self._data = np.vstack(\n                [np.array(self.data, ndmin=2), np.array(signal, ndmin=2)]\n            )\n        else:\n            self._data = np.vstack([self.data, np.array(signal, ndmin=2)])\n        if label is None:\n            logging.warning(\"None label appended\")\n        self._labels = np.append(self._labels, label)\n        return self\n\n    def _restrict_to_interval_array_fast(self, *, intervalarray=None, update=True):\n        \"\"\"Restrict self._abscissa_vals and self._data to an IntervalArray. If no\n        IntervalArray is specified, self._abscissa.support is used.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray, optional\n                IntervalArray on which to restrict AnalogSignal. Default is\n                self._abscissa.support\n        update : bool, optional\n                Overwrite self._abscissa.support with intervalarray if True (default).\n        \"\"\"\n        if intervalarray is None:\n            intervalarray = self._abscissa.support\n            update = False  # support did not change; no need to update\n\n        try:\n            if intervalarray.isempty:\n                logging.warning(\"Support specified is empty\")\n                # self.__init__([],empty=True)\n                exclude = [\"_support\", \"_data\", \"_fs\", \"_step\"]\n                attrs = (x for x in self.__attributes__ if x not in exclude)\n                logging.disable(logging.CRITICAL)\n                for attr in attrs:\n                    exec(\"self.\" + attr + \" = None\")\n                logging.disable(0)\n                self._data = np.zeros([0, self.data.shape[0]])\n                self._data[:] = np.nan\n                self._abscissa.support = intervalarray\n                return\n        except AttributeError:\n            raise AttributeError(\"IntervalArray expected\")\n\n        indices = []\n        for interval in intervalarray.merge().data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            frm, to = np.searchsorted(self._abscissa_vals, (a_start, a_stop + 1e-10))\n            indices.append((frm, to))\n        indices = np.array(indices, ndmin=2)\n        if np.diff(indices).sum() &lt; len(self._abscissa_vals):\n            logging.warning(\"ignoring signal outside of support\")\n        # check if only one interval and interval is already bounds of data\n        # if so, we don't need to do anything\n        if len(indices) == 1:\n            if indices[0, 0] == 0 and indices[0, 1] == len(self._abscissa_vals):\n                if update:\n                    self._abscissa.support = intervalarray\n                    return\n        try:\n            data_list = []\n            for start, stop in indices:\n                data_list.append(self._data[:, start:stop])\n            self._data = np.hstack(data_list)\n        except IndexError:\n            self._data = np.zeros([0, self.data.shape[0]])\n            self._data[:] = np.nan\n        time_list = []\n        for start, stop in indices:\n            time_list.extend(self._abscissa_vals[start:stop])\n        self._abscissa_vals = np.array(time_list)\n        if update:\n            self._abscissa.support = intervalarray\n\n    def _restrict_to_interval_array(self, *, intervalarray=None, update=True):\n        \"\"\"Restrict self._abscissa_vals and self._data to an IntervalArray. If no\n        IntervalArray is specified, self._abscissa.support is used.\n\n        This function is quite slow, as it checks each sample for inclusion.\n        It does this in a vectorized form, which is fast for small or moderately\n        sized objects, but the memory penalty can be large, and it becomes very\n        slow for large objects. Consequently, _restrict_to_interval_array_fast\n        should be used when possible.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray, optional\n                IntervalArray on which to restrict AnalogSignal. Default is\n                self._abscissa.support\n        update : bool, optional\n                Overwrite self._abscissa.support with intervalarray if True (default).\n        \"\"\"\n        if intervalarray is None:\n            intervalarray = self._abscissa.support\n            update = False  # support did not change; no need to update\n\n        try:\n            if intervalarray.isempty:\n                logging.warning(\"Support specified is empty\")\n                # self.__init__([],empty=True)\n                exclude = [\"_support\", \"_data\", \"_fs\", \"_step\"]\n                attrs = (x for x in self.__attributes__ if x not in exclude)\n                logging.disable(logging.CRITICAL)\n                for attr in attrs:\n                    exec(\"self.\" + attr + \" = None\")\n                logging.disable(0)\n                self._data = np.zeros([0, self.data.shape[0]])\n                self._data[:] = np.nan\n                self._abscissa.support = intervalarray\n                return\n        except AttributeError:\n            raise AttributeError(\"IntervalArray expected\")\n\n        indices = []\n        for interval in intervalarray.merge().data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            indices.append(\n                (self._abscissa_vals &gt;= a_start) &amp; (self._abscissa_vals &lt; a_stop)\n            )\n        indices = np.any(np.column_stack(indices), axis=1)\n        if np.count_nonzero(indices) &lt; len(self._abscissa_vals):\n            logging.warning(\"ignoring signal outside of support\")\n        try:\n            self._data = self.data[:, indices]\n        except IndexError:\n            self._data = np.zeros([0, self.data.shape[0]])\n            self._data[:] = np.nan\n        self._abscissa_vals = self._abscissa_vals[indices]\n        if update:\n            self._abscissa.support = intervalarray\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(\n        self,\n        *,\n        fs=None,\n        sigma=None,\n        truncate=None,\n        inplace=False,\n        mode=None,\n        cval=None,\n        within_intervals=False,\n    ):\n        \"\"\"Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.\n\n        Smoothing is applied along the abscissa, and the same smoothing is applied to each\n        signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n        Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n        Parameters\n        ----------\n        obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n        fs : float, optional\n            Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n            be inferred.\n        sigma : float, optional\n            Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n            (50 ms if base_unit=seconds).\n        truncate : float, optional\n            Bandwidth outside of which the filter value will be zero. Default is 4.0.\n        inplace : bool\n            If True the data will be replaced with the smoothed data.\n            Default is False.\n        mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to 'constant'. Default is\n            'reflect'.\n        cval : scalar, optional\n            Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n        within_intervals : boolean, optional\n            If True, then smooth within each epoch. Otherwise smooth across epochs.\n            Default is False.\n            Note that when mode = 'wrap', then smoothing within epochs aren't affected\n            by wrapping.\n\n        Returns\n        -------\n        out : same type as obj\n            An object with smoothed data is returned.\n\n        \"\"\"\n\n        if sigma is None:\n            sigma = 0.05\n        if truncate is None:\n            truncate = 4\n\n        kwargs = {\n            \"inplace\": inplace,\n            \"fs\": fs,\n            \"sigma\": sigma,\n            \"truncate\": truncate,\n            \"mode\": mode,\n            \"cval\": cval,\n            \"within_intervals\": within_intervals,\n        }\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        if self._ordinate.is_wrapping:\n            ord_is_wrapped = self.is_wrapped\n\n            if ord_is_wrapped:\n                out = out.unwrap()\n\n        # case 1: abs.wrapping=False, ord.linking=False, ord.wrapping=False\n        if (\n            not self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            pass\n\n        # case 2: abs.wrapping=False, ord.linking=False, ord.wrapping=True\n        elif (\n            not self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            pass\n\n        # case 3: abs.wrapping=False, ord.linking=True, ord.wrapping=False\n        elif (\n            not self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 4: abs.wrapping=False, ord.linking=True, ord.wrapping=True\n        elif (\n            not self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 5: abs.wrapping=True, ord.linking=False, ord.wrapping=False\n        elif (\n            self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            if mode is None:\n                kwargs[\"mode\"] = \"wrap\"\n\n        # case 6: abs.wrapping=True, ord.linking=False, ord.wrapping=True\n        elif (\n            self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            # (1) unwrap ordinate (abscissa wrap=False)\n            # (2) smooth unwrapped ordinate (absissa wrap=False)\n            # (3) repeat unwrapped signal based on conditions from (2):\n            # if smoothed wrapped ordinate samples\n            # HH ==&gt; SSS (this must be done on a per-signal basis!!!) H = high; L = low; S = same\n            # LL ==&gt; SSS (the vertical offset must be such that neighbors have smallest displacement)\n            # LH ==&gt; LSH\n            # HL ==&gt; HSL\n            # (4) smooth expanded and unwrapped ordinate (abscissa wrap=False)\n            # (5) cut out orignal signal\n\n            # (1)\n            kwargs[\"mode\"] = \"reflect\"\n            L = out._ordinate.range.max - out._ordinate.range.min\n            D = out.domain.length\n\n            tmp = utils.gaussian_filter(out.unwrap(), **kwargs)\n            # (2) (3)\n            n_reps = int(np.ceil((sigma * truncate) / float(D)))\n\n            smooth_data = []\n            for ss, signal in enumerate(tmp.signals):\n                # signal = signal.wrap()\n                offset = (\n                    float((signal._data[:, -1] - signal._data[:, 0]) // (L / 2)) * L\n                )\n                # print(offset)\n                # left_high = signal._data[:,0] &gt;= out._ordinate.range.min + L/2\n                # right_high = signal._data[:,-1] &gt;= out._ordinate.range.min + L/2\n                # signal = signal.unwrap()\n\n                expanded = signal.copy()\n                for nn in range(n_reps):\n                    expanded = expanded.join((signal &lt;&lt; D * (nn + 1)) - offset).join(\n                        (signal &gt;&gt; D * (nn + 1)) + offset\n                    )\n                    # print(expanded)\n                    # if left_high == right_high:\n                    #     print('extending flat! signal {}'.format(ss))\n                    #     expanded = expanded.join(signal &lt;&lt; D*(nn+1)).join(signal &gt;&gt; D*(nn+1))\n                    # elif left_high &lt; right_high:\n                    #     print('extending LSH! signal {}'.format(ss))\n                    #     # LSH\n                    #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))-L).join((signal &gt;&gt; D*(nn+1))+L)\n                    # else:\n                    #     # HSL\n                    #     print('extending HSL! signal {}'.format(ss))\n                    #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))+L).join((signal &gt;&gt; D*(nn+1))-L)\n                # (4)\n                smooth_signal = utils.gaussian_filter(expanded, **kwargs)\n                smooth_data.append(\n                    smooth_signal._data[\n                        :, n_reps * tmp.n_samples : (n_reps + 1) * (tmp.n_samples)\n                    ].squeeze()\n                )\n            # (5)\n            out._data = np.array(smooth_data)\n            out.__renew__()\n\n            if self._ordinate.is_wrapping:\n                if ord_is_wrapped:\n                    out = out.wrap()\n\n            return out\n\n        # case 7: abs.wrapping=True, ord.linking=True, ord.wrapping=False\n        elif (\n            self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 8: abs.wrapping=True, ord.linking=True, ord.wrapping=True\n        elif (\n            self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        out = utils.gaussian_filter(out, **kwargs)\n        out.__renew__()\n\n        if self._ordinate.is_wrapping:\n            if ord_is_wrapped:\n                out = out.wrap()\n\n        return out\n\n    @property\n    def lengths(self):\n        \"\"\"(list) The number of samples in each interval.\"\"\"\n        indices = []\n        for interval in self.support.data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            frm, to = np.searchsorted(self._abscissa_vals, (a_start, a_stop))\n            indices.append((frm, to))\n        indices = np.array(indices, ndmin=2)\n        lengths = np.atleast_1d(np.diff(indices).squeeze())\n        return lengths\n\n    @property\n    def labels(self):\n        \"\"\"(list) The labels corresponding to each signal.\"\"\"\n        # TODO: make this faster and better!\n        return self._labels\n\n    @property\n    def n_signals(self):\n        \"\"\"(int) The number of signals.\"\"\"\n        try:\n            return utils.PrettyInt(self.data.shape[0])\n        except AttributeError:\n            return 0\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self.n_intervals)\n        else:\n            epstr = \"\"\n        try:\n            if self.n_signals &gt; 0:\n                nstr = \" %s signals%s\" % (self.n_signals, epstr)\n        except IndexError:\n            nstr = \" 1 signal%s\" % epstr\n        dstr = \" for a total of {}\".format(\n            self._abscissa.formatter(self.support.length)\n        )\n        return \"&lt;%s%s:%s&gt;%s\" % (self.type_name, address_str, nstr, dstr)\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns an RegularlySampledAnalogSignalArray whose support has been\n        partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_samples : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            RegularlySampledAnalogSignalArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_samples or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        out._abscissa.support = out.support.partition(ds=ds, n_intervals=n_intervals)\n        return out\n\n    # @property\n    # def ydata(self):\n    #     \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n    #     # LEGACY\n    #     return self.data\n\n    @property\n    def data(self):\n        \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, val):\n        \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n        self._data = val\n        # print('data was modified, so clearing interp, etc.')\n        self.__renew__()\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval_array_fast(intervalarray=self._abscissa.support)\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval_array_fast(intervalarray=self._abscissa.support)\n\n    @property\n    def range(self):\n        \"\"\"(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._ordinate.range\n\n    @range.setter\n    def range(self, val):\n        \"\"\"(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify range\n        self._ordinate.range = val\n\n    @property\n    def step(self):\n        \"\"\"steps per sample\n        Example 1: sample_numbers = np.array([1,2,3,4,5,6]) #aka time\n        Steps per sample in the above case would be 1\n\n        Example 2: sample_numbers = np.array([1,3,5,7,9]) #aka time\n        Steps per sample in Example 2 would be 2\n        \"\"\"\n        return self._step\n\n    @property\n    def abscissa_vals(self):\n        \"\"\"(np.array 1D) Time in seconds.\"\"\"\n        return self._abscissa_vals\n\n    @abscissa_vals.setter\n    def abscissa_vals(self, vals):\n        \"\"\"(np.array 1D) Time in seconds.\"\"\"\n        self._abscissa_vals = vals\n\n    @property\n    def fs(self):\n        \"\"\"(float) Sampling frequency.\"\"\"\n        if self._fs is None:\n            logging.warning(\"No sampling frequency has been specified!\")\n        return self._fs\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) checks length of data input\"\"\"\n        try:\n            return self.data.shape[1] == 0\n        except IndexError:  # IndexError should happen if _data = []\n            return True\n\n    @property\n    def n_bytes(self):\n        \"\"\"Approximate number of bytes taken up by object.\"\"\"\n        return utils.PrettyBytes(self.data.nbytes + self._abscissa_vals.nbytes)\n\n    @property\n    def n_intervals(self):\n        \"\"\"(int) number of intervals in RegularlySampledAnalogSignalArray\"\"\"\n        return self._abscissa.support.n_intervals\n\n    @property\n    def n_samples(self):\n        \"\"\"(int) number of abscissa samples where signal is defined.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self._abscissa_vals))\n\n    def __iter__(self):\n        \"\"\"AnalogSignal iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"AnalogSignal iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_intervals - 1:\n            raise StopIteration\n        logging.disable(logging.CRITICAL)\n        intervalarray = type(self.support)(empty=True)\n        exclude = [\"_abscissa_vals\"]\n        attrs = (x for x in self._abscissa.support.__attributes__ if x not in exclude)\n\n        for attr in attrs:\n            exec(\"intervalarray.\" + attr + \" = self._abscissa.support.\" + attr)\n        try:\n            intervalarray._data = self._abscissa.support.data[\n                tuple([index]), :\n            ]  # use np integer indexing! Cool!\n        except IndexError:\n            # index is out of bounds, so return an empty IntervalArray\n            pass\n        logging.disable(0)\n\n        self._index += 1\n\n        asa = type(self)([], empty=True)\n        exclude = [\"_interp\", \"_support\"]\n        attrs = (x for x in self.__attributes__ if x not in exclude)\n        logging.disable(logging.CRITICAL)\n        for attr in attrs:\n            exec(\"asa.\" + attr + \" = self.\" + attr)\n        logging.disable(0)\n        asa._restrict_to_interval_array_fast(intervalarray=intervalarray)\n        if asa.support.isempty:\n            logging.warning(\n                \"Support is empty. Empty RegularlySampledAnalogSignalArray returned\"\n            )\n            asa = type(self)([], empty=True)\n\n        asa.__renew__()\n        return asa\n\n    def empty(self, inplace=True):\n        \"\"\"Remove data (but not metadata) from RegularlySampledAnalogSignalArray.\n\n        Attributes 'data', 'abscissa_vals', and 'support' are all emptied.\n\n        Note: n_signals is preserved.\n        \"\"\"\n        n_signals = self.n_signals\n        if not inplace:\n            out = self._copy_without_data()\n        else:\n            out = self\n            out._data = np.zeros((n_signals, 0))\n        out._abscissa.support = type(self.support)(empty=True)\n        out._abscissa_vals = []\n        out.__renew__()\n        return out\n\n    def __getitem__(self, idx):\n        \"\"\"RegularlySampledAnalogSignalArray index access.\n\n        Parameters\n        ----------\n        idx : IntervalArray, int, slice\n            intersect passed intervalarray with support,\n            index particular a singular interval or multiple intervals with slice\n        \"\"\"\n        intervalslice, signalslice = self._intervalsignalslicer[idx]\n\n        asa = self._subset(signalslice)\n\n        if asa.isempty:\n            asa.__renew__()\n            return asa\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                asa.__renew__()\n                return asa\n\n        newintervals = self._abscissa.support[intervalslice]\n        # TODO: this needs to change so that n_signals etc. are preserved\n        ################################################################\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=False)\n        ################################################################\n\n        asa._restrict_to_interval_array_fast(intervalarray=newintervals)\n        asa.__renew__()\n        return asa\n\n    def _subset(self, idx):\n        asa = self.copy()\n        try:\n            asa._data = np.atleast_2d(self.data[idx, :])\n        except IndexError:\n            raise IndexError(\n                \"index {} is out of bounds for n_signals with size {}\".format(\n                    idx, self.n_signals\n                )\n            )\n        asa.__renew__()\n        return asa\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without data and abscissa_vals.\n\n        Note: the support is left unchanged.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._abscissa_vals = None\n        out._data = np.zeros((self.n_signals, 0))\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Return a copy of the current object.\"\"\"\n        out = copy.deepcopy(self)\n        out.__renew__()\n        return out\n\n    def median(self, *, axis=1):\n        \"\"\"Returns the median of each signal in RegularlySampledAnalogSignalArray.\"\"\"\n        try:\n            medians = np.nanmedian(self.data, axis=axis).squeeze()\n            if medians.size == 1:\n                return medians.item()\n            return medians\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate median\"\n            )\n\n    def mean(self, *, axis=1):\n        \"\"\"\n        Compute the mean of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the mean (default is 1, i.e., across samples).\n\n        Returns\n        -------\n        mean : np.ndarray\n            Mean values along the specified axis.\n        \"\"\"\n        try:\n            means = np.nanmean(self.data, axis=axis).squeeze()\n            if means.size == 1:\n                return means.item()\n            return means\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate mean\"\n            )\n\n    def std(self, *, axis=1):\n        \"\"\"\n        Compute the standard deviation of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the standard deviation (default is 1).\n\n        Returns\n        -------\n        std : np.ndarray\n            Standard deviation values along the specified axis.\n        \"\"\"\n        try:\n            stds = np.nanstd(self.data, axis=axis).squeeze()\n            if stds.size == 1:\n                return stds.item()\n            return stds\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate standard deviation\"\n            )\n\n    def max(self, *, axis=1):\n        \"\"\"\n        Compute the maximum value of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the maximum (default is 1).\n\n        Returns\n        -------\n        max : np.ndarray\n            Maximum values along the specified axis.\n        \"\"\"\n        try:\n            maxes = np.amax(self.data, axis=axis).squeeze()\n            if maxes.size == 1:\n                return maxes.item()\n            return maxes\n        except ValueError:\n            raise ValueError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate maximum\"\n            )\n\n    def min(self, *, axis=1):\n        \"\"\"\n        Compute the minimum value of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the minimum (default is 1).\n\n        Returns\n        -------\n        min : np.ndarray\n            Minimum values along the specified axis.\n        \"\"\"\n        try:\n            mins = np.amin(self.data, axis=axis).squeeze()\n            if mins.size == 1:\n                return mins.item()\n            return mins\n        except ValueError:\n            raise ValueError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate minimum\"\n            )\n\n    def clip(self, min, max):\n        \"\"\"\n        Clip (limit) the values in the data to the interval [min, max].\n\n        Parameters\n        ----------\n        min : float\n            Minimum value.\n        max : float\n            Maximum value.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            New object with clipped data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; clipped = asa.clip(-1, 1)\n        \"\"\"\n        out = self.copy()\n        out._data = np.clip(self.data, min, max)\n        return out\n\n    def trim(self, start, stop=None, *, fs=None):\n        \"\"\"\n        Trim the signal to the specified start and stop times.\n\n        Parameters\n        ----------\n        start : float\n            Start time.\n        stop : float, optional\n            Stop time. If None, trims to the end.\n        fs : float, optional\n            Sampling frequency. If None, uses self.fs.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Trimmed signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; trimmed = asa.trim(0, 10)\n        \"\"\"\n        logging.warning(\"RegularlySampledAnalogSignalArray: Trim may not work!\")\n        # TODO: do comprehensive input validation\n        if stop is not None:\n            try:\n                start = np.array(start, ndmin=1)\n                if len(start) != 1:\n                    raise TypeError(\"start must be a scalar float\")\n            except TypeError:\n                raise TypeError(\"start must be a scalar float\")\n            try:\n                stop = np.array(stop, ndmin=1)\n                if len(stop) != 1:\n                    raise TypeError(\"stop must be a scalar float\")\n            except TypeError:\n                raise TypeError(\"stop must be a scalar float\")\n        else:  # start must have two elements\n            try:\n                if len(np.array(start, ndmin=1)) &gt; 2:\n                    raise TypeError(\n                        \"unsupported input to RegularlySampledAnalogSignalArray.trim()\"\n                    )\n                stop = np.array(start[1], ndmin=1)\n                start = np.array(start[0], ndmin=1)\n                if len(start) != 1 or len(stop) != 1:\n                    raise TypeError(\"start and stop must be scalar floats\")\n            except TypeError:\n                raise TypeError(\"start and stop must be scalar floats\")\n\n        logging.disable(logging.CRITICAL)\n        interval = self._abscissa.support.intersect(\n            type(self.support)([start, stop], fs=fs)\n        )\n        if not interval.isempty:\n            analogsignalarray = self[interval]\n        else:\n            analogsignalarray = type(self)([], empty=True)\n        logging.disable(0)\n        analogsignalarray.__renew__()\n        return analogsignalarray\n\n    @property\n    def _ydata_rowsig(self):\n        \"\"\"returns wide-format data s.t. each row is a signal.\"\"\"\n        # LEGACY\n        return self.data\n\n    @property\n    def _ydata_colsig(self):\n        # LEGACY\n        \"\"\"returns skinny-format data s.t. each column is a signal.\"\"\"\n        return self.data.T\n\n    @property\n    def _data_rowsig(self):\n        \"\"\"returns wide-format data s.t. each row is a signal.\"\"\"\n        return self.data\n\n    @property\n    def _data_colsig(self):\n        \"\"\"returns skinny-format data s.t. each column is a signal.\"\"\"\n        return self.data.T\n\n    def _get_interp1d(\n        self,\n        *,\n        kind=\"linear\",\n        copy=True,\n        bounds_error=False,\n        fill_value=np.nan,\n        assume_sorted=None,\n    ):\n        \"\"\"returns a scipy interp1d object, extended to have values at all interval\n        boundaries!\n        \"\"\"\n\n        if assume_sorted is None:\n            assume_sorted = utils.is_sorted(self._abscissa_vals)\n\n        if self.n_signals &gt; 1:\n            axis = 1\n        else:\n            axis = -1\n\n        abscissa_vals = self._abscissa_vals\n\n        if self._ordinate.is_wrapping:\n            yvals = self._unwrap(\n                self._data_rowsig, self._ordinate.range.min, self._ordinate.range.max\n            )  # always interpolate on the unwrapped data!\n        else:\n            yvals = self._data_rowsig\n\n        lengths = self.lengths\n        empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n        first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n        first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n        last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        first_abscissavals_per_interval = self._abscissa_vals[\n            first_abscissavals_per_interval_idx\n        ]\n        last_abscissavals_per_interval = self._abscissa_vals[\n            last_abscissavals_per_interval_idx\n        ]\n\n        boundary_abscissa_vals = []\n        boundary_vals = []\n        for ii, (start, stop) in enumerate(self.support.data):\n            if lengths[ii] == 0:\n                continue\n            if first_abscissavals_per_interval[ii] &gt; start:\n                boundary_abscissa_vals.append(start)\n                boundary_vals.append(yvals[:, first_abscissavals_per_interval_idx[ii]])\n                # print('adding {} at abscissa_vals {}'.format(yvals[:,first_abscissavals_per_interval_idx[ii]], start))\n            if last_abscissavals_per_interval[ii] &lt; stop:\n                boundary_abscissa_vals.append(stop)\n                boundary_vals.append(yvals[:, last_abscissavals_per_interval_idx[ii]])\n\n        if boundary_abscissa_vals:\n            insert_locs = np.searchsorted(abscissa_vals, boundary_abscissa_vals)\n            abscissa_vals = np.insert(\n                abscissa_vals, insert_locs, boundary_abscissa_vals\n            )\n            yvals = np.insert(yvals, insert_locs, np.array(boundary_vals).T, axis=1)\n\n            abscissa_vals, unique_idx = np.unique(abscissa_vals, return_index=True)\n            yvals = yvals[:, unique_idx]\n\n        f = interpolate.interp1d(\n            x=abscissa_vals,\n            y=yvals,\n            kind=kind,\n            axis=axis,\n            copy=copy,\n            bounds_error=bounds_error,\n            fill_value=fill_value,\n            assume_sorted=assume_sorted,\n        )\n        return f\n\n    def asarray(\n        self,\n        *,\n        where=None,\n        at=None,\n        kind=\"linear\",\n        copy=True,\n        bounds_error=False,\n        fill_value=np.nan,\n        assume_sorted=None,\n        recalculate=False,\n        store_interp=True,\n        n_samples=None,\n        split_by_interval=False,\n    ):\n        \"\"\"\n        Return a data-like array at requested points, with optional interpolation.\n\n        Parameters\n        ----------\n        where : array_like or tuple, optional\n            Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).\n        at : array_like, optional\n            Array of points to evaluate array at. If None, uses self._abscissa_vals.\n        n_samples : int, optional\n            Number of points to interpolate at, distributed uniformly from support start to stop.\n        split_by_interval : bool, optional\n            If True, separate arrays by intervals and return in a list.\n        kind : str, optional\n            Interpolation method. Default is 'linear'.\n        copy : bool, optional\n            If True, returns a copy. Default is True.\n        bounds_error : bool, optional\n            If True, raises an error for out-of-bounds interpolation. Default is False.\n        fill_value : float, optional\n            Value to use for out-of-bounds points. Default is np.nan.\n        assume_sorted : bool, optional\n            If True, assumes input is sorted. Default is None.\n        recalculate : bool, optional\n            If True, recalculates the interpolation. Default is False.\n        store_interp : bool, optional\n            If True, stores the interpolation object. Default is True.\n\n        Returns\n        -------\n        out : namedtuple (xvals, yvals)\n            xvals: array of abscissa values for which data are returned.\n            yvals: array of shape (n_signals, n_samples) with interpolated data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n        \"\"\"\n\n        # TODO: implement splitting by interval\n\n        if split_by_interval:\n            raise NotImplementedError(\"split_by_interval not yet implemented...\")\n\n        XYArray = namedtuple(\"XYArray\", [\"xvals\", \"yvals\"])\n\n        if (\n            at is None\n            and where is None\n            and split_by_interval is False\n            and n_samples is None\n        ):\n            xyarray = XYArray(self._abscissa_vals, self._data_rowsig.squeeze())\n            return xyarray\n\n        if where is not None:\n            assert at is None and n_samples is None, (\n                \"'where', 'at', and 'n_samples' cannot be used at the same time\"\n            )\n            if isinstance(where, tuple):\n                y = np.array(where[1]).squeeze()\n                x = where[0]\n                assert len(x) == len(y), (\n                    \"'where' condition and array must have same number of elements\"\n                )\n                at = y[x]\n            else:\n                x = np.asanyarray(where).squeeze()\n                assert len(x) == len(self._abscissa_vals), (\n                    \"'where' condition must have same number of elements as self._abscissa_vals\"\n                )\n                at = self._abscissa_vals[x]\n        elif at is not None:\n            assert n_samples is None, (\n                \"'at' and 'n_samples' cannot be used at the same time\"\n            )\n        else:\n            at = np.linspace(self.support.start, self.support.stop, n_samples)\n\n        at = np.atleast_1d(at)\n        if at.ndim &gt; 1:\n            raise ValueError(\"Requested points must be one-dimensional!\")\n        if at.shape[0] == 0:\n            raise ValueError(\"No points were requested to interpolate\")\n\n        # if we made it this far, either at or where has been specified, and at is now well defined.\n\n        kwargs = {\n            \"kind\": kind,\n            \"copy\": copy,\n            \"bounds_error\": bounds_error,\n            \"fill_value\": fill_value,\n            \"assume_sorted\": assume_sorted,\n        }\n\n        # retrieve an existing, or construct a new interpolation object\n        if recalculate:\n            interpobj = self._get_interp1d(**kwargs)\n        else:\n            try:\n                interpobj = self._interp\n                if interpobj is None:\n                    interpobj = self._get_interp1d(**kwargs)\n            except AttributeError:  # does not exist yet\n                interpobj = self._get_interp1d(**kwargs)\n\n        # store interpolation object, if desired\n        if store_interp:\n            self._interp = interpobj\n\n        # do not interpolate points that lie outside the support\n        interval_data = self.support.data[:, :, None]\n        # use broadcasting to check in a vectorized manner if\n        # each sample falls within the support, haha aren't we clever?\n        # (n_intervals, n_requested_samples)\n        valid = np.logical_and(\n            at &gt;= interval_data[:, 0, :], at &lt;= interval_data[:, 1, :]\n        )\n        valid_mask = np.any(valid, axis=0)\n        n_invalid = at.size - np.sum(valid_mask)\n        if n_invalid &gt; 0:\n            logging.warning(\n                \"{} values outside the support were removed\".format(n_invalid)\n            )\n        at = at[valid_mask]\n\n        # do the actual interpolation\n        if self._ordinate.is_wrapping:\n            try:\n                if self.is_wrapped:\n                    out = self._wrap(\n                        interpobj(at),\n                        self._ordinate.range.min,\n                        self._ordinate.range.max,\n                    )\n                else:\n                    out = interpobj(at)\n            except SystemError:\n                interpobj = self._get_interp1d(**kwargs)\n                if store_interp:\n                    self._interp = interpobj\n                if self.is_wrapped:\n                    out = self._wrap(\n                        interpobj(at),\n                        self._ordinate.range.min,\n                        self._ordinate.range.max,\n                    )\n                else:\n                    out = interpobj(at)\n        else:\n            try:\n                out = interpobj(at)\n            except SystemError:\n                interpobj = self._get_interp1d(**kwargs)\n                if store_interp:\n                    self._interp = interpobj\n                out = interpobj(at)\n\n        xyarray = XYArray(xvals=np.asanyarray(at), yvals=np.asanyarray(out))\n        return xyarray\n\n    def subsample(self, *, fs):\n        \"\"\"Subsamples a RegularlySampledAnalogSignalArray\n\n        WARNING! Aliasing can occur! It is better to use downsample when\n        lowering the sampling rate substantially.\n\n        Parameters\n        ----------\n        fs : float, optional\n            Desired output sampling rate, in Hz\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n            new subset of points.\n        \"\"\"\n\n        return self.simplify(ds=1 / fs)\n\n    def simplify(self, *, ds=None, n_samples=None, **kwargs):\n        \"\"\"Returns an RegularlySampledAnalogSignalArray where the data has been\n        simplified / subsampled.\n\n        This function is primarily intended to be used for plotting and\n        saving vector graphics without having too large file sizes as\n        a result of too many points.\n\n        Irrespective of whether 'ds' or 'n_samples' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_samples or ds to be violated.\n\n        WARNING! Simplify can create nan samples, when requesting a timestamp\n        within an interval, but outside of the (first, last) abscissa_vals within that\n        interval, since we don't extrapolate, but only interpolate. # TODO: fix\n\n        Parameters\n        ----------\n        ds : float, optional\n            Time (in seconds), in which to step points.\n        n_samples : int, optional\n            Number of points at which to intepolate data. If ds is None\n            and n_samples is None, then default is to use n_samples=5,000\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n            new subset of points.\n        \"\"\"\n\n        if self.isempty:\n            return self\n\n        # legacy kwarg support:\n        n_points = kwargs.pop(\"n_points\", False)\n        if n_points:\n            n_samples = n_points\n\n        if ds is not None and n_samples is not None:\n            raise ValueError(\"ds and n_samples cannot be used together\")\n\n        if n_samples is not None:\n            assert float(n_samples).is_integer(), (\n                \"n_samples must be a positive integer!\"\n            )\n            assert n_samples &gt; 1, \"n_samples must be a positive integer &gt; 1\"\n            # determine ds from number of desired points:\n            ds = self.support.length / (n_samples - 1)\n\n        if ds is None:\n            # neither n_samples nor ds was specified, so assume defaults:\n            n_samples = np.min((5000, 250 + self.n_samples // 2, self.n_samples))\n            ds = self.support.length / (n_samples - 1)\n\n        # build list of points at which to evaluate the RegularlySampledAnalogSignalArray\n\n        # we exclude all empty intervals:\n        at = []\n        lengths = self.lengths\n        empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n        first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n        first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n        last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        first_abscissavals_per_interval = self._abscissa_vals[\n            first_abscissavals_per_interval_idx\n        ]\n        last_abscissavals_per_interval = self._abscissa_vals[\n            last_abscissavals_per_interval_idx\n        ]\n\n        for ii, (start, stop) in enumerate(self.support.data):\n            if lengths[ii] == 0:\n                continue\n            newxvals = utils.frange(\n                first_abscissavals_per_interval[ii],\n                last_abscissavals_per_interval[ii],\n                step=ds,\n            ).tolist()\n            at.extend(newxvals)\n            try:\n                if newxvals[-1] &lt; last_abscissavals_per_interval[ii]:\n                    at.append(last_abscissavals_per_interval[ii])\n            except IndexError:\n                at.append(first_abscissavals_per_interval[ii])\n                at.append(last_abscissavals_per_interval[ii])\n\n        _, yvals = self.asarray(at=at, recalculate=True, store_interp=False)\n        yvals = np.array(yvals, ndmin=2)\n\n        asa = self.copy()\n        asa._abscissa_vals = np.asanyarray(at)\n        asa._data = yvals\n        asa._fs = 1 / ds\n\n        return asa\n\n    def join(self, other, *, mode=None, inplace=False):\n        \"\"\"Join another RegularlySampledAnalogSignalArray to this one.\n\n        WARNING! Numerical precision might cause some epochs to be considered\n        non-disjoint even when they really are, so a better check than ep1[ep2].isempty\n        is to check for samples contained in the intersection of ep1 and ep2.\n\n        Parameters\n        ----------\n        other : RegularlySampledAnalogSignalArray\n            RegularlySampledAnalogSignalArray (or derived type) to join to the current\n            RegularlySampledAnalogSignalArray. Other must have the same number of signals as\n            the current RegularlySampledAnalogSignalArray.\n        mode : string, optional\n            One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the\n            signals are merged inside overlapping intervals. Default is 'left'.\n        inplace : boolean, optional\n            If True, then current RegularlySampledAnalogSignalArray is modified. If False, then\n            a copy with the joined result is returned. Default is False.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been\n            joined to the current RegularlySampledAnalogSignalArray.\n        \"\"\"\n\n        if mode is None:\n            mode = \"left\"\n\n        asa = self.copy()  # copy without data since we change data at the end?\n\n        times = np.zeros((1, 0))\n        data = np.zeros((asa.n_signals, 0))\n\n        # if ASAs are disjoint:\n        if not self.support[other.support].length &gt; 50 * float_info.epsilon:\n            # do a simple-as-butter join (concat) and sort\n            times = np.append(times, self._abscissa_vals)\n            data = np.hstack((data, self.data))\n            times = np.append(times, other._abscissa_vals)\n            data = np.hstack((data, other.data))\n        else:  # not disjoint\n            both_eps = self.support[other.support]\n            self_eps = self.support - both_eps - other.support\n            other_eps = other.support - both_eps - self.support\n\n            if mode == \"left\":\n                self_eps += both_eps\n                # print(self_eps)\n\n                tmp = self[self_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n\n                if not other_eps.isempty:\n                    tmp = other[other_eps]\n                    times = np.append(times, tmp._abscissa_vals)\n                    data = np.hstack((data, tmp.data))\n            elif mode == \"right\":\n                other_eps += both_eps\n\n                tmp = other[other_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n\n                if not self_eps.isempty:\n                    tmp = self[self_eps]\n                    times = np.append(times, tmp._abscissa_vals)\n                    data = np.hstack((data, tmp.data))\n            else:\n                raise NotImplementedError(\n                    \"asa.join() has not yet been implemented for mode '{}'!\".format(\n                        mode\n                    )\n                )\n\n        sample_order = np.argsort(times)\n        times = times[sample_order]\n        data = data[:, sample_order]\n\n        asa._data = data\n        asa._abscissa_vals = times\n        dom1 = self.domain\n        dom2 = other.domain\n        asa._abscissa.support = (self.support + other.support).merge()\n        asa._abscissa.support.domain = (dom1 + dom2).merge()\n        return asa\n\n    def _pdf(self, bins=None, n_samples=None):\n        \"\"\"Return the probability distribution function for each signal.\"\"\"\n        from scipy import integrate\n\n        if bins is None:\n            bins = 100\n\n        if n_samples is None:\n            n_samples = 100\n\n        if self.n_signals &gt; 1:\n            raise NotImplementedError(\"multiple signals not supported yet!\")\n\n        # fx, bins = np.histogram(self.data.squeeze(), bins=bins, normed=True)\n        fx, bins = np.histogram(self.data.squeeze(), bins=bins)\n        bin_centers = (bins + (bins[1] - bins[0]) / 2)[:-1]\n\n        Ifx = integrate.simps(fx, bin_centers)\n\n        pdf = type(self)(\n            abscissa_vals=bin_centers,\n            data=fx / Ifx,\n            fs=1 / (bin_centers[1] - bin_centers[0]),\n            support=type(self.support)(self.data.min(), self.data.max()),\n        ).simplify(n_samples=n_samples)\n\n        return pdf\n\n        # data = []\n        # for signal in self.data:\n        #     fx, bins = np.histogram(signal, bins=bins)\n        #     bin_centers = (bins + (bins[1]-bins[0])/2)[:-1]\n\n    def _cdf(self, n_samples=None):\n        \"\"\"Return the probability distribution function for each signal.\"\"\"\n\n        if n_samples is None:\n            n_samples = 100\n\n        if self.n_signals &gt; 1:\n            raise NotImplementedError(\"multiple signals not supported yet!\")\n\n        X = np.sort(self.data.squeeze())\n        F = np.array(range(self.n_samples)) / float(self.n_samples)\n\n        logging.disable(logging.CRITICAL)\n        cdf = type(self)(\n            abscissa_vals=X,\n            data=F,\n            support=type(self.support)(self.data.min(), self.data.max()),\n        ).simplify(n_samples=n_samples)\n        logging.disable(0)\n\n        return cdf\n\n    def _eegplot(self, ax=None, normalize=False, pad=None, fill=True, color=None):\n        \"\"\"custom_func docstring goes here.\"\"\"\n\n        import matplotlib.pyplot as plt\n\n        from ..plotting import utils as plotutils\n\n        if ax is None:\n            ax = plt.gca()\n\n        xmin = self.support.min\n        xmax = self.support.max\n        xvals = self._abscissa_vals\n\n        if pad is None:\n            pad = np.mean(self.data) / 2\n\n        data = self.data.copy()\n\n        if normalize:\n            peak_vals = self.max()\n            data = (data.T / peak_vals).T\n\n        n_traces = self.n_signals\n\n        for tt, trace in enumerate(data):\n            if color is None:\n                line = ax.plot(\n                    xvals, tt * pad + trace, zorder=int(10 + 2 * n_traces - 2 * tt)\n                )\n            else:\n                line = ax.plot(\n                    xvals,\n                    tt * pad + trace,\n                    zorder=int(10 + 2 * n_traces - 2 * tt),\n                    color=color,\n                )\n            if fill:\n                # Get the color from the current curve\n                fillcolor = line[0].get_color()\n                ax.fill_between(\n                    xvals,\n                    tt * pad,\n                    tt * pad + trace,\n                    alpha=0.3,\n                    color=fillcolor,\n                    zorder=int(10 + 2 * n_traces - 2 * tt - 1),\n                )\n\n        ax.set_xlim(xmin, xmax)\n        if pad != 0:\n            # yticks = np.arange(n_traces)*pad + 0.5*pad\n            yticks = []\n            ax.set_yticks(yticks)\n            ax.set_xlabel(self._abscissa.label)\n            ax.set_ylabel(self._ordinate.label)\n            plotutils.no_yticks(ax)\n            plotutils.clear_left(ax)\n\n        plotutils.clear_top(ax)\n        plotutils.clear_right(ax)\n\n        return ax\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.abs","title":"<code>abs</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with absolute value of (potentially complex) data.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.abscissa_vals","title":"<code>abscissa_vals</code>  <code>property</code> <code>writable</code>","text":"<p>(np.array 1D) Time in seconds.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.angle","title":"<code>angle</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only phase angle (in radians) of data.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.base_unit","title":"<code>base_unit</code>  <code>property</code>","text":"<p>Base unit of the abscissa.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>(np.array N-Dimensional) data with shape (n_signals, n_samples).</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.fs","title":"<code>fs</code>  <code>property</code>","text":"<p>(float) Sampling frequency.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.imag","title":"<code>imag</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only imaginary part of data.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.iscomplex","title":"<code>iscomplex</code>  <code>property</code>","text":"<p>Returns True if any part of the signal is complex.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) checks length of data input</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.isreal","title":"<code>isreal</code>  <code>property</code>","text":"<p>Returns True if entire signal is real.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.labels","title":"<code>labels</code>  <code>property</code>","text":"<p>(list) The labels corresponding to each signal.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>(list) The number of samples in each interval.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.n_bytes","title":"<code>n_bytes</code>  <code>property</code>","text":"<p>Approximate number of bytes taken up by object.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.n_intervals","title":"<code>n_intervals</code>  <code>property</code>","text":"<p>(int) number of intervals in RegularlySampledAnalogSignalArray</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.n_samples","title":"<code>n_samples</code>  <code>property</code>","text":"<p>(int) number of abscissa samples where signal is defined.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.n_signals","title":"<code>n_signals</code>  <code>property</code>","text":"<p>(int) The number of signals.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.range","title":"<code>range</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.real","title":"<code>real</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only real part of data.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.signals","title":"<code>signals</code>  <code>property</code>","text":"<p>Returns a list of RegularlySampledAnalogSignalArrays, each array containing a single signal (channel).</p> <p>WARNING: this method creates a copy of each signal, so is not particularly efficient at this time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for channel in lfp.signals:\n    print(channel)\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.step","title":"<code>step</code>  <code>property</code>","text":"<p>steps per sample Example 1: sample_numbers = np.array([1,2,3,4,5,6]) #aka time Steps per sample in the above case would be 1</p> <p>Example 2: sample_numbers = np.array([1,3,5,7,9]) #aka time Steps per sample in Example 2 would be 2</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.add_signal","title":"<code>add_signal(signal, label=None)</code>","text":"<p>Docstring goes here. Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def add_signal(self, signal, label=None):\n    \"\"\"Docstring goes here.\n    Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?\n    \"\"\"\n    # TODO: add functionality to check that supports are the same, etc.\n    if isinstance(signal, RegularlySampledAnalogSignalArray):\n        signal = signal.data\n\n    signal = np.squeeze(signal)\n    if signal.ndim &gt; 1:\n        raise TypeError(\"Can only add one signal at a time!\")\n    if self.data.ndim == 1:\n        self._data = np.vstack(\n            [np.array(self.data, ndmin=2), np.array(signal, ndmin=2)]\n        )\n    else:\n        self._data = np.vstack([self.data, np.array(signal, ndmin=2)])\n    if label is None:\n        logging.warning(\"None label appended\")\n    self._labels = np.append(self._labels, label)\n    return self\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.asarray","title":"<code>asarray(*, where=None, at=None, kind='linear', copy=True, bounds_error=False, fill_value=np.nan, assume_sorted=None, recalculate=False, store_interp=True, n_samples=None, split_by_interval=False)</code>","text":"<p>Return a data-like array at requested points, with optional interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>array_like or tuple</code> <p>Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).</p> <code>None</code> <code>at</code> <code>array_like</code> <p>Array of points to evaluate array at. If None, uses self._abscissa_vals.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of points to interpolate at, distributed uniformly from support start to stop.</p> <code>None</code> <code>split_by_interval</code> <code>bool</code> <p>If True, separate arrays by intervals and return in a list.</p> <code>False</code> <code>kind</code> <code>str</code> <p>Interpolation method. Default is 'linear'.</p> <code>'linear'</code> <code>copy</code> <code>bool</code> <p>If True, returns a copy. Default is True.</p> <code>True</code> <code>bounds_error</code> <code>bool</code> <p>If True, raises an error for out-of-bounds interpolation. Default is False.</p> <code>False</code> <code>fill_value</code> <code>float</code> <p>Value to use for out-of-bounds points. Default is np.nan.</p> <code>nan</code> <code>assume_sorted</code> <code>bool</code> <p>If True, assumes input is sorted. Default is None.</p> <code>None</code> <code>recalculate</code> <code>bool</code> <p>If True, recalculates the interpolation. Default is False.</p> <code>False</code> <code>store_interp</code> <code>bool</code> <p>If True, stores the interpolation object. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>out</code> <code>namedtuple(xvals, yvals)</code> <p>xvals: array of abscissa values for which data are returned. yvals: array of shape (n_signals, n_samples) with interpolated data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def asarray(\n    self,\n    *,\n    where=None,\n    at=None,\n    kind=\"linear\",\n    copy=True,\n    bounds_error=False,\n    fill_value=np.nan,\n    assume_sorted=None,\n    recalculate=False,\n    store_interp=True,\n    n_samples=None,\n    split_by_interval=False,\n):\n    \"\"\"\n    Return a data-like array at requested points, with optional interpolation.\n\n    Parameters\n    ----------\n    where : array_like or tuple, optional\n        Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).\n    at : array_like, optional\n        Array of points to evaluate array at. If None, uses self._abscissa_vals.\n    n_samples : int, optional\n        Number of points to interpolate at, distributed uniformly from support start to stop.\n    split_by_interval : bool, optional\n        If True, separate arrays by intervals and return in a list.\n    kind : str, optional\n        Interpolation method. Default is 'linear'.\n    copy : bool, optional\n        If True, returns a copy. Default is True.\n    bounds_error : bool, optional\n        If True, raises an error for out-of-bounds interpolation. Default is False.\n    fill_value : float, optional\n        Value to use for out-of-bounds points. Default is np.nan.\n    assume_sorted : bool, optional\n        If True, assumes input is sorted. Default is None.\n    recalculate : bool, optional\n        If True, recalculates the interpolation. Default is False.\n    store_interp : bool, optional\n        If True, stores the interpolation object. Default is True.\n\n    Returns\n    -------\n    out : namedtuple (xvals, yvals)\n        xvals: array of abscissa values for which data are returned.\n        yvals: array of shape (n_signals, n_samples) with interpolated data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n    \"\"\"\n\n    # TODO: implement splitting by interval\n\n    if split_by_interval:\n        raise NotImplementedError(\"split_by_interval not yet implemented...\")\n\n    XYArray = namedtuple(\"XYArray\", [\"xvals\", \"yvals\"])\n\n    if (\n        at is None\n        and where is None\n        and split_by_interval is False\n        and n_samples is None\n    ):\n        xyarray = XYArray(self._abscissa_vals, self._data_rowsig.squeeze())\n        return xyarray\n\n    if where is not None:\n        assert at is None and n_samples is None, (\n            \"'where', 'at', and 'n_samples' cannot be used at the same time\"\n        )\n        if isinstance(where, tuple):\n            y = np.array(where[1]).squeeze()\n            x = where[0]\n            assert len(x) == len(y), (\n                \"'where' condition and array must have same number of elements\"\n            )\n            at = y[x]\n        else:\n            x = np.asanyarray(where).squeeze()\n            assert len(x) == len(self._abscissa_vals), (\n                \"'where' condition must have same number of elements as self._abscissa_vals\"\n            )\n            at = self._abscissa_vals[x]\n    elif at is not None:\n        assert n_samples is None, (\n            \"'at' and 'n_samples' cannot be used at the same time\"\n        )\n    else:\n        at = np.linspace(self.support.start, self.support.stop, n_samples)\n\n    at = np.atleast_1d(at)\n    if at.ndim &gt; 1:\n        raise ValueError(\"Requested points must be one-dimensional!\")\n    if at.shape[0] == 0:\n        raise ValueError(\"No points were requested to interpolate\")\n\n    # if we made it this far, either at or where has been specified, and at is now well defined.\n\n    kwargs = {\n        \"kind\": kind,\n        \"copy\": copy,\n        \"bounds_error\": bounds_error,\n        \"fill_value\": fill_value,\n        \"assume_sorted\": assume_sorted,\n    }\n\n    # retrieve an existing, or construct a new interpolation object\n    if recalculate:\n        interpobj = self._get_interp1d(**kwargs)\n    else:\n        try:\n            interpobj = self._interp\n            if interpobj is None:\n                interpobj = self._get_interp1d(**kwargs)\n        except AttributeError:  # does not exist yet\n            interpobj = self._get_interp1d(**kwargs)\n\n    # store interpolation object, if desired\n    if store_interp:\n        self._interp = interpobj\n\n    # do not interpolate points that lie outside the support\n    interval_data = self.support.data[:, :, None]\n    # use broadcasting to check in a vectorized manner if\n    # each sample falls within the support, haha aren't we clever?\n    # (n_intervals, n_requested_samples)\n    valid = np.logical_and(\n        at &gt;= interval_data[:, 0, :], at &lt;= interval_data[:, 1, :]\n    )\n    valid_mask = np.any(valid, axis=0)\n    n_invalid = at.size - np.sum(valid_mask)\n    if n_invalid &gt; 0:\n        logging.warning(\n            \"{} values outside the support were removed\".format(n_invalid)\n        )\n    at = at[valid_mask]\n\n    # do the actual interpolation\n    if self._ordinate.is_wrapping:\n        try:\n            if self.is_wrapped:\n                out = self._wrap(\n                    interpobj(at),\n                    self._ordinate.range.min,\n                    self._ordinate.range.max,\n                )\n            else:\n                out = interpobj(at)\n        except SystemError:\n            interpobj = self._get_interp1d(**kwargs)\n            if store_interp:\n                self._interp = interpobj\n            if self.is_wrapped:\n                out = self._wrap(\n                    interpobj(at),\n                    self._ordinate.range.min,\n                    self._ordinate.range.max,\n                )\n            else:\n                out = interpobj(at)\n    else:\n        try:\n            out = interpobj(at)\n        except SystemError:\n            interpobj = self._get_interp1d(**kwargs)\n            if store_interp:\n                self._interp = interpobj\n            out = interpobj(at)\n\n    xyarray = XYArray(xvals=np.asanyarray(at), yvals=np.asanyarray(out))\n    return xyarray\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.center","title":"<code>center(inplace=False)</code>","text":"<p>Center the data to have zero mean along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The centered signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; centered = asa.center()\n&gt;&gt;&gt; centered.mean()\n0.0\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def center(self, inplace=False):\n    \"\"\"\n    Center the data to have zero mean along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The centered signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; centered = asa.center()\n    &gt;&gt;&gt; centered.mean()\n    0.0\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.clip","title":"<code>clip(min, max)</code>","text":"<p>Clip (limit) the values in the data to the interval [min, max].</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float</code> <p>Minimum value.</p> required <code>max</code> <code>float</code> <p>Maximum value.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>New object with clipped data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clipped = asa.clip(-1, 1)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def clip(self, min, max):\n    \"\"\"\n    Clip (limit) the values in the data to the interval [min, max].\n\n    Parameters\n    ----------\n    min : float\n        Minimum value.\n    max : float\n        Maximum value.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        New object with clipped data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; clipped = asa.clip(-1, 1)\n    \"\"\"\n    out = self.copy()\n    out._data = np.clip(self.data, min, max)\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the current object.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def copy(self):\n    \"\"\"Return a copy of the current object.\"\"\"\n    out = copy.deepcopy(self)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.ddt","title":"<code>ddt(rectify=False)</code>","text":"<p>Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.</p> <p>asa.data = f(t) asa.ddt = d/dt (asa.data)</p> <p>Parameters:</p> Name Type Description Default <code>rectify</code> <code>boolean</code> <p>If True, the absolute value of the derivative will be returned. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ddt</code> <code>RegularlySampledAnalogSignalArray</code> <p>Time derivative of each signal in the RegularlySampledAnalogSignalArray.</p> Note <p>Second order central differences are used here, and it is assumed that the signals are sampled uniformly. If the signals are not uniformly sampled, it is recommended to resample the signal before computing the derivative.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def ddt(self, rectify=False):\n    \"\"\"Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n    asa.data = f(t)\n    asa.ddt = d/dt (asa.data)\n\n    Parameters\n    ----------\n    rectify : boolean, optional\n        If True, the absolute value of the derivative will be returned.\n        Default is False.\n\n    Returns\n    -------\n    ddt : RegularlySampledAnalogSignalArray\n        Time derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n    Note\n    ----\n    Second order central differences are used here, and it is assumed that\n    the signals are sampled uniformly. If the signals are not uniformly\n    sampled, it is recommended to resample the signal before computing the\n    derivative.\n    \"\"\"\n    ddt = utils.ddt_asa(self, rectify=rectify)\n    return ddt\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.downsample","title":"<code>downsample(*, fs_out, aafilter=True, inplace=False, **kwargs)</code>","text":"<p>Downsamples the RegularlySampledAnalogSignalArray</p> <p>Parameters:</p> Name Type Description Default <code>fs_out</code> <code>float</code> <p>Desired output sampling rate in Hz</p> required <code>aafilter</code> <code>boolean</code> <p>Whether to apply an anti-aliasing filter before performing the actual downsampling. Default is True</p> <code>True</code> <code>inplace</code> <code>boolean</code> <p>If True, the output ASA will replace the input ASA. Default is False</p> <code>False</code> <code>kwargs</code> <p>Other keyword arguments are passed to sosfiltfilt() in the <code>filtering</code> module</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The downsampled RegularlySampledAnalogSignalArray</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def downsample(self, *, fs_out, aafilter=True, inplace=False, **kwargs):\n    \"\"\"Downsamples the RegularlySampledAnalogSignalArray\n\n    Parameters\n    ----------\n    fs_out : float, optional\n        Desired output sampling rate in Hz\n    aafilter : boolean, optional\n        Whether to apply an anti-aliasing filter before performing the actual\n        downsampling. Default is True\n    inplace : boolean, optional\n        If True, the output ASA will replace the input ASA. Default is False\n    kwargs :\n        Other keyword arguments are passed to sosfiltfilt() in the `filtering`\n        module\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The downsampled RegularlySampledAnalogSignalArray\n    \"\"\"\n\n    if not fs_out &lt; self._fs:\n        raise ValueError(\"fs_out must be less than current sampling rate!\")\n\n    if aafilter:\n        fh = fs_out / 2.0\n        out = filtering.sosfiltfilt(self, fl=None, fh=fh, inplace=inplace, **kwargs)\n\n    downsampled = out.simplify(ds=1 / fs_out)\n    out._data = downsampled._data\n    out._abscissa_vals = downsampled._abscissa_vals\n    out._fs = fs_out\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.empty","title":"<code>empty(inplace=True)</code>","text":"<p>Remove data (but not metadata) from RegularlySampledAnalogSignalArray.</p> <p>Attributes 'data', 'abscissa_vals', and 'support' are all emptied.</p> <p>Note: n_signals is preserved.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def empty(self, inplace=True):\n    \"\"\"Remove data (but not metadata) from RegularlySampledAnalogSignalArray.\n\n    Attributes 'data', 'abscissa_vals', and 'support' are all emptied.\n\n    Note: n_signals is preserved.\n    \"\"\"\n    n_signals = self.n_signals\n    if not inplace:\n        out = self._copy_without_data()\n    else:\n        out = self\n        out._data = np.zeros((n_signals, 0))\n    out._abscissa.support = type(self.support)(empty=True)\n    out._abscissa_vals = []\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.join","title":"<code>join(other, *, mode=None, inplace=False)</code>","text":"<p>Join another RegularlySampledAnalogSignalArray to this one.</p> <p>WARNING! Numerical precision might cause some epochs to be considered non-disjoint even when they really are, so a better check than ep1[ep2].isempty is to check for samples contained in the intersection of ep1 and ep2.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RegularlySampledAnalogSignalArray</code> <p>RegularlySampledAnalogSignalArray (or derived type) to join to the current RegularlySampledAnalogSignalArray. Other must have the same number of signals as the current RegularlySampledAnalogSignalArray.</p> required <code>mode</code> <code>string</code> <p>One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the signals are merged inside overlapping intervals. Default is 'left'.</p> <code>None</code> <code>inplace</code> <code>boolean</code> <p>If True, then current RegularlySampledAnalogSignalArray is modified. If False, then a copy with the joined result is returned. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been joined to the current RegularlySampledAnalogSignalArray.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def join(self, other, *, mode=None, inplace=False):\n    \"\"\"Join another RegularlySampledAnalogSignalArray to this one.\n\n    WARNING! Numerical precision might cause some epochs to be considered\n    non-disjoint even when they really are, so a better check than ep1[ep2].isempty\n    is to check for samples contained in the intersection of ep1 and ep2.\n\n    Parameters\n    ----------\n    other : RegularlySampledAnalogSignalArray\n        RegularlySampledAnalogSignalArray (or derived type) to join to the current\n        RegularlySampledAnalogSignalArray. Other must have the same number of signals as\n        the current RegularlySampledAnalogSignalArray.\n    mode : string, optional\n        One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the\n        signals are merged inside overlapping intervals. Default is 'left'.\n    inplace : boolean, optional\n        If True, then current RegularlySampledAnalogSignalArray is modified. If False, then\n        a copy with the joined result is returned. Default is False.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been\n        joined to the current RegularlySampledAnalogSignalArray.\n    \"\"\"\n\n    if mode is None:\n        mode = \"left\"\n\n    asa = self.copy()  # copy without data since we change data at the end?\n\n    times = np.zeros((1, 0))\n    data = np.zeros((asa.n_signals, 0))\n\n    # if ASAs are disjoint:\n    if not self.support[other.support].length &gt; 50 * float_info.epsilon:\n        # do a simple-as-butter join (concat) and sort\n        times = np.append(times, self._abscissa_vals)\n        data = np.hstack((data, self.data))\n        times = np.append(times, other._abscissa_vals)\n        data = np.hstack((data, other.data))\n    else:  # not disjoint\n        both_eps = self.support[other.support]\n        self_eps = self.support - both_eps - other.support\n        other_eps = other.support - both_eps - self.support\n\n        if mode == \"left\":\n            self_eps += both_eps\n            # print(self_eps)\n\n            tmp = self[self_eps]\n            times = np.append(times, tmp._abscissa_vals)\n            data = np.hstack((data, tmp.data))\n\n            if not other_eps.isempty:\n                tmp = other[other_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n        elif mode == \"right\":\n            other_eps += both_eps\n\n            tmp = other[other_eps]\n            times = np.append(times, tmp._abscissa_vals)\n            data = np.hstack((data, tmp.data))\n\n            if not self_eps.isempty:\n                tmp = self[self_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n        else:\n            raise NotImplementedError(\n                \"asa.join() has not yet been implemented for mode '{}'!\".format(\n                    mode\n                )\n            )\n\n    sample_order = np.argsort(times)\n    times = times[sample_order]\n    data = data[:, sample_order]\n\n    asa._data = data\n    asa._abscissa_vals = times\n    dom1 = self.domain\n    dom2 = other.domain\n    asa._abscissa.support = (self.support + other.support).merge()\n    asa._abscissa.support.domain = (dom1 + dom2).merge()\n    return asa\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.max","title":"<code>max(*, axis=1)</code>","text":"<p>Compute the maximum value of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the maximum (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>max</code> <code>ndarray</code> <p>Maximum values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def max(self, *, axis=1):\n    \"\"\"\n    Compute the maximum value of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the maximum (default is 1).\n\n    Returns\n    -------\n    max : np.ndarray\n        Maximum values along the specified axis.\n    \"\"\"\n    try:\n        maxes = np.amax(self.data, axis=axis).squeeze()\n        if maxes.size == 1:\n            return maxes.item()\n        return maxes\n    except ValueError:\n        raise ValueError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate maximum\"\n        )\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.mean","title":"<code>mean(*, axis=1)</code>","text":"<p>Compute the mean of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the mean (default is 1, i.e., across samples).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>mean</code> <code>ndarray</code> <p>Mean values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def mean(self, *, axis=1):\n    \"\"\"\n    Compute the mean of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the mean (default is 1, i.e., across samples).\n\n    Returns\n    -------\n    mean : np.ndarray\n        Mean values along the specified axis.\n    \"\"\"\n    try:\n        means = np.nanmean(self.data, axis=axis).squeeze()\n        if means.size == 1:\n            return means.item()\n        return means\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate mean\"\n        )\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.median","title":"<code>median(*, axis=1)</code>","text":"<p>Returns the median of each signal in RegularlySampledAnalogSignalArray.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def median(self, *, axis=1):\n    \"\"\"Returns the median of each signal in RegularlySampledAnalogSignalArray.\"\"\"\n    try:\n        medians = np.nanmedian(self.data, axis=axis).squeeze()\n        if medians.size == 1:\n            return medians.item()\n        return medians\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate median\"\n        )\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.min","title":"<code>min(*, axis=1)</code>","text":"<p>Compute the minimum value of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the minimum (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>min</code> <code>ndarray</code> <p>Minimum values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def min(self, *, axis=1):\n    \"\"\"\n    Compute the minimum value of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the minimum (default is 1).\n\n    Returns\n    -------\n    min : np.ndarray\n        Minimum values along the specified axis.\n    \"\"\"\n    try:\n        mins = np.amin(self.data, axis=axis).squeeze()\n        if mins.size == 1:\n            return mins.item()\n        return mins\n    except ValueError:\n        raise ValueError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate minimum\"\n        )\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.normalize","title":"<code>normalize(inplace=False)</code>","text":"<p>Normalize the data to have unit standard deviation along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The normalized signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalized = asa.normalize()\n&gt;&gt;&gt; normalized.std()\n1.0\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def normalize(self, inplace=False):\n    \"\"\"\n    Normalize the data to have unit standard deviation along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The normalized signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; normalized = asa.normalize()\n    &gt;&gt;&gt; normalized.std()\n    1.0\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    std = np.atleast_1d(out.std())\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns an RegularlySampledAnalogSignalArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>RegularlySampledAnalogSignalArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_samples or ds to be violated.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns an RegularlySampledAnalogSignalArray whose support has been\n    partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_samples : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        RegularlySampledAnalogSignalArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_samples or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    out._abscissa.support = out.support.partition(ds=ds, n_intervals=n_intervals)\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.simplify","title":"<code>simplify(*, ds=None, n_samples=None, **kwargs)</code>","text":"<p>Returns an RegularlySampledAnalogSignalArray where the data has been simplified / subsampled.</p> <p>This function is primarily intended to be used for plotting and saving vector graphics without having too large file sizes as a result of too many points.</p> <p>Irrespective of whether 'ds' or 'n_samples' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_samples or ds to be violated.</p> <p>WARNING! Simplify can create nan samples, when requesting a timestamp within an interval, but outside of the (first, last) abscissa_vals within that interval, since we don't extrapolate, but only interpolate. # TODO: fix</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Time (in seconds), in which to step points.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of points at which to intepolate data. If ds is None and n_samples is None, then default is to use n_samples=5,000</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where data is only stored at the new subset of points.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def simplify(self, *, ds=None, n_samples=None, **kwargs):\n    \"\"\"Returns an RegularlySampledAnalogSignalArray where the data has been\n    simplified / subsampled.\n\n    This function is primarily intended to be used for plotting and\n    saving vector graphics without having too large file sizes as\n    a result of too many points.\n\n    Irrespective of whether 'ds' or 'n_samples' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_samples or ds to be violated.\n\n    WARNING! Simplify can create nan samples, when requesting a timestamp\n    within an interval, but outside of the (first, last) abscissa_vals within that\n    interval, since we don't extrapolate, but only interpolate. # TODO: fix\n\n    Parameters\n    ----------\n    ds : float, optional\n        Time (in seconds), in which to step points.\n    n_samples : int, optional\n        Number of points at which to intepolate data. If ds is None\n        and n_samples is None, then default is to use n_samples=5,000\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n        new subset of points.\n    \"\"\"\n\n    if self.isempty:\n        return self\n\n    # legacy kwarg support:\n    n_points = kwargs.pop(\"n_points\", False)\n    if n_points:\n        n_samples = n_points\n\n    if ds is not None and n_samples is not None:\n        raise ValueError(\"ds and n_samples cannot be used together\")\n\n    if n_samples is not None:\n        assert float(n_samples).is_integer(), (\n            \"n_samples must be a positive integer!\"\n        )\n        assert n_samples &gt; 1, \"n_samples must be a positive integer &gt; 1\"\n        # determine ds from number of desired points:\n        ds = self.support.length / (n_samples - 1)\n\n    if ds is None:\n        # neither n_samples nor ds was specified, so assume defaults:\n        n_samples = np.min((5000, 250 + self.n_samples // 2, self.n_samples))\n        ds = self.support.length / (n_samples - 1)\n\n    # build list of points at which to evaluate the RegularlySampledAnalogSignalArray\n\n    # we exclude all empty intervals:\n    at = []\n    lengths = self.lengths\n    empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n    first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n    first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n    last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n    last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n    first_abscissavals_per_interval = self._abscissa_vals[\n        first_abscissavals_per_interval_idx\n    ]\n    last_abscissavals_per_interval = self._abscissa_vals[\n        last_abscissavals_per_interval_idx\n    ]\n\n    for ii, (start, stop) in enumerate(self.support.data):\n        if lengths[ii] == 0:\n            continue\n        newxvals = utils.frange(\n            first_abscissavals_per_interval[ii],\n            last_abscissavals_per_interval[ii],\n            step=ds,\n        ).tolist()\n        at.extend(newxvals)\n        try:\n            if newxvals[-1] &lt; last_abscissavals_per_interval[ii]:\n                at.append(last_abscissavals_per_interval[ii])\n        except IndexError:\n            at.append(first_abscissavals_per_interval[ii])\n            at.append(last_abscissavals_per_interval[ii])\n\n    _, yvals = self.asarray(at=at, recalculate=True, store_interp=False)\n    yvals = np.array(yvals, ndmin=2)\n\n    asa = self.copy()\n    asa._abscissa_vals = np.asanyarray(at)\n    asa._data = yvals\n    asa._fs = 1 / ds\n\n    return asa\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.smooth","title":"<code>smooth(*, fs=None, sigma=None, truncate=None, inplace=False, mode=None, cval=None, within_intervals=False)</code>","text":"<p>Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.</p> <p>Smoothing is applied along the abscissa, and the same smoothing is applied to each signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.</p> <p>Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.</code> required <code>fs</code> <code>float</code> <p>Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will be inferred.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05 (50 ms if base_unit=seconds).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <code>mode</code> <code>(reflect, constant, nearest, mirror, wrap)</code> <p>The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to 'constant'. Default is 'reflect'.</p> <code>'reflect'</code> <code>cval</code> <code>scalar</code> <p>Value to fill past edges of input if mode is 'constant'. Default is 0.0.</p> <code>None</code> <code>within_intervals</code> <code>boolean</code> <p>If True, then smooth within each epoch. Otherwise smooth across epochs. Default is False. Note that when mode = 'wrap', then smoothing within epochs aren't affected by wrapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as obj</code> <p>An object with smoothed data is returned.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(\n    self,\n    *,\n    fs=None,\n    sigma=None,\n    truncate=None,\n    inplace=False,\n    mode=None,\n    cval=None,\n    within_intervals=False,\n):\n    \"\"\"Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.\n\n    Smoothing is applied along the abscissa, and the same smoothing is applied to each\n    signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n    Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n    Parameters\n    ----------\n    obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n    fs : float, optional\n        Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n        be inferred.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n        (50 ms if base_unit=seconds).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0.\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        'reflect'.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n    within_intervals : boolean, optional\n        If True, then smooth within each epoch. Otherwise smooth across epochs.\n        Default is False.\n        Note that when mode = 'wrap', then smoothing within epochs aren't affected\n        by wrapping.\n\n    Returns\n    -------\n    out : same type as obj\n        An object with smoothed data is returned.\n\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.05\n    if truncate is None:\n        truncate = 4\n\n    kwargs = {\n        \"inplace\": inplace,\n        \"fs\": fs,\n        \"sigma\": sigma,\n        \"truncate\": truncate,\n        \"mode\": mode,\n        \"cval\": cval,\n        \"within_intervals\": within_intervals,\n    }\n\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    if self._ordinate.is_wrapping:\n        ord_is_wrapped = self.is_wrapped\n\n        if ord_is_wrapped:\n            out = out.unwrap()\n\n    # case 1: abs.wrapping=False, ord.linking=False, ord.wrapping=False\n    if (\n        not self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        pass\n\n    # case 2: abs.wrapping=False, ord.linking=False, ord.wrapping=True\n    elif (\n        not self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        pass\n\n    # case 3: abs.wrapping=False, ord.linking=True, ord.wrapping=False\n    elif (\n        not self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 4: abs.wrapping=False, ord.linking=True, ord.wrapping=True\n    elif (\n        not self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 5: abs.wrapping=True, ord.linking=False, ord.wrapping=False\n    elif (\n        self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        if mode is None:\n            kwargs[\"mode\"] = \"wrap\"\n\n    # case 6: abs.wrapping=True, ord.linking=False, ord.wrapping=True\n    elif (\n        self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        # (1) unwrap ordinate (abscissa wrap=False)\n        # (2) smooth unwrapped ordinate (absissa wrap=False)\n        # (3) repeat unwrapped signal based on conditions from (2):\n        # if smoothed wrapped ordinate samples\n        # HH ==&gt; SSS (this must be done on a per-signal basis!!!) H = high; L = low; S = same\n        # LL ==&gt; SSS (the vertical offset must be such that neighbors have smallest displacement)\n        # LH ==&gt; LSH\n        # HL ==&gt; HSL\n        # (4) smooth expanded and unwrapped ordinate (abscissa wrap=False)\n        # (5) cut out orignal signal\n\n        # (1)\n        kwargs[\"mode\"] = \"reflect\"\n        L = out._ordinate.range.max - out._ordinate.range.min\n        D = out.domain.length\n\n        tmp = utils.gaussian_filter(out.unwrap(), **kwargs)\n        # (2) (3)\n        n_reps = int(np.ceil((sigma * truncate) / float(D)))\n\n        smooth_data = []\n        for ss, signal in enumerate(tmp.signals):\n            # signal = signal.wrap()\n            offset = (\n                float((signal._data[:, -1] - signal._data[:, 0]) // (L / 2)) * L\n            )\n            # print(offset)\n            # left_high = signal._data[:,0] &gt;= out._ordinate.range.min + L/2\n            # right_high = signal._data[:,-1] &gt;= out._ordinate.range.min + L/2\n            # signal = signal.unwrap()\n\n            expanded = signal.copy()\n            for nn in range(n_reps):\n                expanded = expanded.join((signal &lt;&lt; D * (nn + 1)) - offset).join(\n                    (signal &gt;&gt; D * (nn + 1)) + offset\n                )\n                # print(expanded)\n                # if left_high == right_high:\n                #     print('extending flat! signal {}'.format(ss))\n                #     expanded = expanded.join(signal &lt;&lt; D*(nn+1)).join(signal &gt;&gt; D*(nn+1))\n                # elif left_high &lt; right_high:\n                #     print('extending LSH! signal {}'.format(ss))\n                #     # LSH\n                #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))-L).join((signal &gt;&gt; D*(nn+1))+L)\n                # else:\n                #     # HSL\n                #     print('extending HSL! signal {}'.format(ss))\n                #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))+L).join((signal &gt;&gt; D*(nn+1))-L)\n            # (4)\n            smooth_signal = utils.gaussian_filter(expanded, **kwargs)\n            smooth_data.append(\n                smooth_signal._data[\n                    :, n_reps * tmp.n_samples : (n_reps + 1) * (tmp.n_samples)\n                ].squeeze()\n            )\n        # (5)\n        out._data = np.array(smooth_data)\n        out.__renew__()\n\n        if self._ordinate.is_wrapping:\n            if ord_is_wrapped:\n                out = out.wrap()\n\n        return out\n\n    # case 7: abs.wrapping=True, ord.linking=True, ord.wrapping=False\n    elif (\n        self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 8: abs.wrapping=True, ord.linking=True, ord.wrapping=True\n    elif (\n        self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    out = utils.gaussian_filter(out, **kwargs)\n    out.__renew__()\n\n    if self._ordinate.is_wrapping:\n        if ord_is_wrapped:\n            out = out.wrap()\n\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.standardize","title":"<code>standardize(inplace=False)</code>","text":"<p>Standardize the data to zero mean and unit standard deviation along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The standardized signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; standardized = asa.standardize()\n&gt;&gt;&gt; standardized.mean(), standardized.std()\n(0.0, 1.0)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def standardize(self, inplace=False):\n    \"\"\"\n    Standardize the data to zero mean and unit standard deviation along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The standardized signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; standardized = asa.standardize()\n    &gt;&gt;&gt; standardized.mean(), standardized.std()\n    (0.0, 1.0)\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    std = np.atleast_1d(out.std())\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.std","title":"<code>std(*, axis=1)</code>","text":"<p>Compute the standard deviation of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the standard deviation (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>std</code> <code>ndarray</code> <p>Standard deviation values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def std(self, *, axis=1):\n    \"\"\"\n    Compute the standard deviation of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the standard deviation (default is 1).\n\n    Returns\n    -------\n    std : np.ndarray\n        Standard deviation values along the specified axis.\n    \"\"\"\n    try:\n        stds = np.nanstd(self.data, axis=axis).squeeze()\n        if stds.size == 1:\n            return stds.item()\n        return stds\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate standard deviation\"\n        )\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.subsample","title":"<code>subsample(*, fs)</code>","text":"<p>Subsamples a RegularlySampledAnalogSignalArray</p> <p>WARNING! Aliasing can occur! It is better to use downsample when lowering the sampling rate substantially.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>Desired output sampling rate, in Hz</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where data is only stored at the new subset of points.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def subsample(self, *, fs):\n    \"\"\"Subsamples a RegularlySampledAnalogSignalArray\n\n    WARNING! Aliasing can occur! It is better to use downsample when\n    lowering the sampling rate substantially.\n\n    Parameters\n    ----------\n    fs : float, optional\n        Desired output sampling rate, in Hz\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n        new subset of points.\n    \"\"\"\n\n    return self.simplify(ds=1 / fs)\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.trim","title":"<code>trim(start, stop=None, *, fs=None)</code>","text":"<p>Trim the signal to the specified start and stop times.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>Start time.</p> required <code>stop</code> <code>float</code> <p>Stop time. If None, trims to the end.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling frequency. If None, uses self.fs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Trimmed signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trimmed = asa.trim(0, 10)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def trim(self, start, stop=None, *, fs=None):\n    \"\"\"\n    Trim the signal to the specified start and stop times.\n\n    Parameters\n    ----------\n    start : float\n        Start time.\n    stop : float, optional\n        Stop time. If None, trims to the end.\n    fs : float, optional\n        Sampling frequency. If None, uses self.fs.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Trimmed signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; trimmed = asa.trim(0, 10)\n    \"\"\"\n    logging.warning(\"RegularlySampledAnalogSignalArray: Trim may not work!\")\n    # TODO: do comprehensive input validation\n    if stop is not None:\n        try:\n            start = np.array(start, ndmin=1)\n            if len(start) != 1:\n                raise TypeError(\"start must be a scalar float\")\n        except TypeError:\n            raise TypeError(\"start must be a scalar float\")\n        try:\n            stop = np.array(stop, ndmin=1)\n            if len(stop) != 1:\n                raise TypeError(\"stop must be a scalar float\")\n        except TypeError:\n            raise TypeError(\"stop must be a scalar float\")\n    else:  # start must have two elements\n        try:\n            if len(np.array(start, ndmin=1)) &gt; 2:\n                raise TypeError(\n                    \"unsupported input to RegularlySampledAnalogSignalArray.trim()\"\n                )\n            stop = np.array(start[1], ndmin=1)\n            start = np.array(start[0], ndmin=1)\n            if len(start) != 1 or len(stop) != 1:\n                raise TypeError(\"start and stop must be scalar floats\")\n        except TypeError:\n            raise TypeError(\"start and stop must be scalar floats\")\n\n    logging.disable(logging.CRITICAL)\n    interval = self._abscissa.support.intersect(\n        type(self.support)([start, stop], fs=fs)\n    )\n    if not interval.isempty:\n        analogsignalarray = self[interval]\n    else:\n        analogsignalarray = type(self)([], empty=True)\n    logging.disable(0)\n    analogsignalarray.__renew__()\n    return analogsignalarray\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.unwrap","title":"<code>unwrap(inplace=False)</code>","text":"<p>Unwrap the ordinate values by minimizing total displacement, useful for phase data.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The unwrapped signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unwrapped = asa.unwrap()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def unwrap(self, inplace=False):\n    \"\"\"\n    Unwrap the ordinate values by minimizing total displacement, useful for phase data.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The unwrapped signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; unwrapped = asa.unwrap()\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    out.data = np.atleast_2d(\n        out._unwrap(out._data, out._ordinate.range.min, out._ordinate.range.max)\n    )\n    # out._is_wrapped = False\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.wrap","title":"<code>wrap(inplace=False)</code>","text":"<p>Wrap the ordinate values within the finite range defined by the ordinate's range.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The wrapped signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; wrapped = asa.wrap()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def wrap(self, inplace=False):\n    \"\"\"\n    Wrap the ordinate values within the finite range defined by the ordinate's range.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The wrapped signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; wrapped = asa.wrap()\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    out.data = np.atleast_2d(\n        out._wrap(out.data, out._ordinate.range.min, out._ordinate.range.max)\n    )\n    # out._is_wrapped = True\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.RegularlySampledAnalogSignalArray.zscore","title":"<code>zscore()</code>","text":"<p>Normalize each signal in the array using z-scores (zero mean, unit variance).</p> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>New object with z-scored data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zscored = asa.zscore()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def zscore(self):\n    \"\"\"\n    Normalize each signal in the array using z-scores (zero mean, unit variance).\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        New object with z-scored data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; zscored = asa.zscore()\n    \"\"\"\n    out = self.copy()\n    out._data = zscore(out._data, axis=1)\n    return out\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.legacyASAkwargs","title":"<code>legacyASAkwargs(**kwargs)</code>","text":"<p>Provide support for legacy AnalogSignalArray kwargs.</p> <p>kwarg: time &lt;==&gt; timestamps &lt;==&gt; abscissa_vals kwarg: data &lt;==&gt; ydata</p> <p>Examples:</p> <p>asa = nel.AnalogSignalArray(time=..., data=...) asa = nel.AnalogSignalArray(timestamps=..., data=...) asa = nel.AnalogSignalArray(time=..., ydata=...) asa = nel.AnalogSignalArray(ydata=...) asa = nel.AnalogSignalArray(abscissa_vals=..., data=...)</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def legacyASAkwargs(**kwargs):\n    \"\"\"Provide support for legacy AnalogSignalArray kwargs.\n\n    kwarg: time &lt;==&gt; timestamps &lt;==&gt; abscissa_vals\n    kwarg: data &lt;==&gt; ydata\n\n    Examples\n    --------\n    asa = nel.AnalogSignalArray(time=..., data=...)\n    asa = nel.AnalogSignalArray(timestamps=..., data=...)\n    asa = nel.AnalogSignalArray(time=..., ydata=...)\n    asa = nel.AnalogSignalArray(ydata=...)\n    asa = nel.AnalogSignalArray(abscissa_vals=..., data=...)\n    \"\"\"\n\n    def only_one_of(*args):\n        num_non_null_args = 0\n        out = None\n        for arg in args:\n            if arg is not None:\n                num_non_null_args += 1\n                out = arg\n        if num_non_null_args &gt; 1:\n            raise ValueError(\"multiple conflicting arguments received\")\n        return out\n\n    # legacy ASA constructor support for backward compatibility\n    abscissa_vals = kwargs.pop(\"abscissa_vals\", None)\n    timestamps = kwargs.pop(\"timestamps\", None)\n    time = kwargs.pop(\"time\", None)\n    # only one of the above, else raise exception\n    abscissa_vals = only_one_of(abscissa_vals, timestamps, time)\n    if abscissa_vals is not None:\n        kwargs[\"abscissa_vals\"] = abscissa_vals\n\n    data = kwargs.pop(\"data\", None)\n    ydata = kwargs.pop(\"ydata\", None)\n    # only one of the above, else raise exception\n    data = only_one_of(data, ydata)\n    if data is not None:\n        kwargs[\"data\"] = data\n\n    return kwargs\n</code></pre>"},{"location":"reference/core/analogsignalarray/#nelpy.core._analogsignalarray.rsasa_init_wrapper","title":"<code>rsasa_init_wrapper(func)</code>","text":"<p>Decorator that helps figure out abscissa_vals, fs, and sample numbers</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def rsasa_init_wrapper(func):\n    \"\"\"Decorator that helps figure out abscissa_vals, fs, and sample numbers\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if kwargs.get(\"empty\", False):\n            func(*args, **kwargs)\n            return\n\n        if len(args) &gt; 2:\n            raise TypeError(\n                \"__init__() takes 1 positional arguments but {} positional arguments (and {} keyword-only arguments) were given\".format(\n                    len(args) - 1, len(kwargs.items())\n                )\n            )\n\n        data = kwargs.get(\"data\", [])\n        if len(data) == 0:\n            data = args[1]\n\n        if len(data) == 0:\n            logging.warning(\n                \"No ordinate data! Returning empty RegularlySampledAnalogSignalArray.\"\n            )\n            func(*args, **kwargs)\n            return\n\n        # handle casting other nelpy objects to RegularlySampledAnalogSignalArrays:\n        if isinstance(data, core.BinnedEventArray):\n            abscissa_vals = data.bin_centers\n            kwargs[\"abscissa_vals\"] = abscissa_vals\n            # support = data.support\n            # kwargs['support'] = support\n            abscissa = data._abscissa\n            kwargs[\"abscissa\"] = abscissa\n            fs = 1 / data.ds\n            kwargs[\"fs\"] = fs\n            if list(data.series_labels):\n                labels = data.series_labels\n            else:\n                labels = data.series_ids\n            kwargs[\"labels\"] = labels\n            data = data.data.astype(float)\n        # elif isinstance(data, auxiliary.PositionArray):\n        elif isinstance(data, RegularlySampledAnalogSignalArray):\n            kwargs[\"data\"] = data\n            func(args[0], **kwargs)\n            return\n\n        # check if single AnalogSignal or multiple AnalogSignals in array\n        # and standardize data to 2D\n        if not isinstance(data, np.memmap):  # memmap is a special case\n            if not np.any(np.iscomplex(data)):\n                data = np.squeeze(data)\n        try:\n            if data.shape[0] == data.size:\n                data = np.expand_dims(data, axis=0)\n        except ValueError:\n            raise TypeError(\"Unsupported data type!\")\n\n        re_estimate_fs = False\n        no_fs = True\n        fs = kwargs.get(\"fs\", None)\n        if fs is not None:\n            no_fs = False\n            try:\n                if fs &lt;= 0:\n                    raise ValueError(\"fs must be positive\")\n            except TypeError:\n                raise TypeError(\"fs must be a scalar!\")\n        else:\n            fs = 1\n            re_estimate_fs = True\n\n        tdata = kwargs.get(\"tdata\", None)\n        if tdata is not None:\n            logging.warning(\n                \"'tdata' has been deprecated! Use 'abscissa_vals' instead. 'tdata' will be interpreted as 'abscissa_vals' in seconds.\"\n            )\n            abscissa_vals = tdata\n        else:\n            abscissa_vals = kwargs.get(\"abscissa_vals\", None)\n        if abscissa_vals is None:\n            abscissa_vals = np.linspace(0, data.shape[1] / fs, data.shape[1] + 1)\n            abscissa_vals = abscissa_vals[:-1]\n        else:\n            if re_estimate_fs:\n                logging.warning(\n                    \"fs was not specified, so we try to estimate it from the data...\"\n                )\n                fs = 1.0 / np.median(np.diff(abscissa_vals))\n                logging.warning(\"fs was estimated to be {} Hz\".format(fs))\n            else:\n                if no_fs:\n                    logging.warning(\n                        \"fs was not specified, so we will assume default of 1 Hz...\"\n                    )\n                    fs = 1\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"data\"] = data\n        kwargs[\"abscissa_vals\"] = np.squeeze(abscissa_vals)\n\n        func(args[0], **kwargs)\n        return\n\n    return wrapper\n</code></pre>"},{"location":"reference/core/coordinates/","title":"Coordinates","text":"<p>This module contains abscissa and ordinate objects for core nelpy objects.</p>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.Abscissa","title":"<code>Abscissa</code>","text":"<p>An abscissa (x-axis) object for core nelpy data containers.</p> <p>Parameters:</p> Name Type Description Default <code>support</code> <code>IntervalArray</code> <p>The support associated with the abscissa. Default is an empty IntervalArray.</p> <code>None</code> <code>is_wrapping</code> <code>bool</code> <p>Whether or not the abscissa is wrapping (continuous). Default is False.</p> <code>False</code> <code>labelstring</code> <code>str</code> <p>String template for the abscissa label. Default is '{}'.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>support</code> <code>IntervalArray</code> <p>The support associated with the abscissa.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the abscissa, inherited from support.</p> <code>is_wrapping</code> <code>bool</code> <p>Whether the abscissa is wrapping.</p> <code>label</code> <code>str</code> <p>The formatted label for the abscissa.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class Abscissa:\n    \"\"\"\n    An abscissa (x-axis) object for core nelpy data containers.\n\n    Parameters\n    ----------\n    support : nelpy.IntervalArray, optional\n        The support associated with the abscissa. Default is an empty IntervalArray.\n    is_wrapping : bool, optional\n        Whether or not the abscissa is wrapping (continuous). Default is False.\n    labelstring : str, optional\n        String template for the abscissa label. Default is '{}'.\n\n    Attributes\n    ----------\n    support : nelpy.IntervalArray\n        The support associated with the abscissa.\n    base_unit : str\n        The base unit of the abscissa, inherited from support.\n    is_wrapping : bool\n        Whether the abscissa is wrapping.\n    label : str\n        The formatted label for the abscissa.\n    \"\"\"\n\n    def __init__(self, support=None, is_wrapping=False, labelstring=None):\n        # TODO: add label support\n        if support is None:\n            support = core.IntervalArray(empty=True)\n        if labelstring is None:\n            labelstring = \"{}\"\n\n        self.formatter = formatters.ArbitraryFormatter\n        self.support = support\n        self.base_unit = self.support.base_unit\n        self._labelstring = labelstring\n        self.is_wrapping = is_wrapping\n\n    @property\n    def label(self):\n        \"\"\"\n        Get the abscissa label.\n\n        Returns\n        -------\n        label : str\n            The formatted abscissa label.\n        \"\"\"\n        return self._labelstring.format(self.base_unit)\n\n    @label.setter\n    def label(self, val):\n        \"\"\"\n        Set the abscissa label string template.\n\n        Parameters\n        ----------\n        val : str\n            String template for the abscissa label.\n        \"\"\"\n        if val is None:\n            val = \"{}\"\n        try:  # cast to str:\n            labelstring = str(val)\n        except TypeError:\n            raise TypeError(\"cannot convert label to string\")\n        else:\n            labelstring = val\n        self._labelstring = labelstring\n\n    def __repr__(self):\n        return \"Abscissa(base_unit={}, is_wrapping={}) on domain [{}, {})\".format(\n            self.base_unit, self.is_wrapping, self.domain.start, self.domain.stop\n        )\n\n    @property\n    def domain(self):\n        \"\"\"Domain (in base units) on which abscissa is defined.\"\"\"\n        return self.support.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"Domain (in base units) on which abscissa is defined.\"\"\"\n        # val can be an IntervalArray type, or (start, stop)\n        self.support.domain = val\n        self.support = self.support[self.support.domain]\n</code></pre>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.Abscissa.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>Domain (in base units) on which abscissa is defined.</p>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.Abscissa.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Get the abscissa label.</p> <p>Returns:</p> Name Type Description <code>label</code> <code>str</code> <p>The formatted abscissa label.</p>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.AnalogSignalArrayAbscissa","title":"<code>AnalogSignalArrayAbscissa</code>","text":"<p>               Bases: <code>Abscissa</code></p> <p>Abscissa for AnalogSignalArray.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class AnalogSignalArrayAbscissa(Abscissa):\n    \"\"\"Abscissa for AnalogSignalArray.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        labelstring = kwargs.get(\n            \"labelstring\", \"time ({})\"\n        )  # TODO FIXME after unit inheritance; inherit from formatter?\n\n        kwargs[\"support\"] = support\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n\n        self.formatter = self.support.formatter\n</code></pre>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.AnalogSignalArrayOrdinate","title":"<code>AnalogSignalArrayOrdinate</code>","text":"<p>               Bases: <code>Ordinate</code></p> <p>Ordinate for AnalogSignalArray.</p> <p>Examples:</p> <p>nel.AnalogSignalArrayOrdinate(base_unit='uV')</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class AnalogSignalArrayOrdinate(Ordinate):\n    \"\"\"Ordinate for AnalogSignalArray.\n\n    Examples\n    -------\n    nel.AnalogSignalArrayOrdinate(base_unit='uV')\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        base_unit = kwargs.get(\"base_unit\", \"V\")\n        labelstring = kwargs.get(\"labelstring\", \"voltage ({})\")\n\n        kwargs[\"base_unit\"] = base_unit\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.Ordinate","title":"<code>Ordinate</code>","text":"<p>An ordinate (y-axis) object for core nelpy data containers.</p> <p>Parameters:</p> Name Type Description Default <code>base_unit</code> <code>str</code> <p>The base unit for the ordinate. Default is ''.</p> <code>None</code> <code>is_linking</code> <code>bool</code> <p>Whether the ordinate is linking. Default is False.</p> <code>False</code> <code>is_wrapping</code> <code>bool</code> <p>Whether the ordinate is wrapping. Default is False.</p> <code>False</code> <code>labelstring</code> <code>str</code> <p>String template for the ordinate label. Default is '{}'.</p> <code>None</code> <code>_range</code> <code>IntervalArray</code> <p>The range of the ordinate. Default is [-inf, inf].</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>base_unit</code> <code>str</code> <p>The base unit for the ordinate.</p> <code>is_linking</code> <code>bool</code> <p>Whether the ordinate is linking.</p> <code>is_wrapping</code> <code>bool</code> <p>Whether the ordinate is wrapping.</p> <code>label</code> <code>str</code> <p>The formatted label for the ordinate.</p> <code>range</code> <code>IntervalArray</code> <p>The range of the ordinate.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class Ordinate:\n    \"\"\"\n    An ordinate (y-axis) object for core nelpy data containers.\n\n    Parameters\n    ----------\n    base_unit : str, optional\n        The base unit for the ordinate. Default is ''.\n    is_linking : bool, optional\n        Whether the ordinate is linking. Default is False.\n    is_wrapping : bool, optional\n        Whether the ordinate is wrapping. Default is False.\n    labelstring : str, optional\n        String template for the ordinate label. Default is '{}'.\n    _range : nelpy.IntervalArray, optional\n        The range of the ordinate. Default is [-inf, inf].\n\n    Attributes\n    ----------\n    base_unit : str\n        The base unit for the ordinate.\n    is_linking : bool\n        Whether the ordinate is linking.\n    is_wrapping : bool\n        Whether the ordinate is wrapping.\n    label : str\n        The formatted label for the ordinate.\n    range : nelpy.IntervalArray\n        The range of the ordinate.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_unit=None,\n        is_linking=False,\n        is_wrapping=False,\n        labelstring=None,\n        _range=None,\n    ):\n        # TODO: add label support\n\n        if base_unit is None:\n            base_unit = \"\"\n        if labelstring is None:\n            labelstring = \"{}\"\n\n        if _range is None:\n            _range = core.IntervalArray([-inf, inf])\n\n        self.base_unit = base_unit\n        self._labelstring = labelstring\n        self.is_linking = is_linking\n        self.is_wrapping = is_wrapping\n        self._is_wrapped = None  # intialize to unknown (None) state\n        self._range = _range\n\n    @property\n    def label(self):\n        \"\"\"\n        Get the ordinate label.\n\n        Returns\n        -------\n        label : str\n            The formatted ordinate label.\n        \"\"\"\n        return self._labelstring.format(self.base_unit)\n\n    @label.setter\n    def label(self, val):\n        \"\"\"\n        Set the ordinate label string template.\n\n        Parameters\n        ----------\n        val : str\n            String template for the ordinate label.\n        \"\"\"\n        if val is None:\n            val = \"{}\"\n        try:  # cast to str:\n            labelstring = str(val)\n        except TypeError:\n            raise TypeError(\"cannot convert label to string\")\n        else:\n            labelstring = val\n        self._labelstring = labelstring\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the Ordinate object.\n\n        Returns\n        -------\n        repr_str : str\n            String representation of the Ordinate.\n        \"\"\"\n        return \"Ordinate(base_unit={}, is_linking={}, is_wrapping={})\".format(\n            self.base_unit, self.is_linking, self.is_wrapping\n        )\n\n    @property\n    def range(self):\n        \"\"\"\n        Get the range (in ordinate base units) on which ordinate is defined.\n\n        Returns\n        -------\n        range : nelpy.IntervalArray\n            The range of the ordinate.\n        \"\"\"\n        return self._range\n\n    @range.setter\n    def range(self, val):\n        \"\"\"Range (in ordinate base units) on which ordinate is defined.\"\"\"\n        # val can be an IntervalArray type, or (start, stop)\n        if isinstance(val, type(self.range)):\n            self._range = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self.range.domain\n            self._range = type(self.range)([val[0], val[1]])\n            self._range.domain = prev_domain\n        else:\n            raise TypeError(\"range must be of type {}\".format(str(type(self.range))))\n\n        self._range = self.range[self.range.domain]\n</code></pre>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.Ordinate.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Get the ordinate label.</p> <p>Returns:</p> Name Type Description <code>label</code> <code>str</code> <p>The formatted ordinate label.</p>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.Ordinate.range","title":"<code>range</code>  <code>property</code> <code>writable</code>","text":"<p>Get the range (in ordinate base units) on which ordinate is defined.</p> <p>Returns:</p> Name Type Description <code>range</code> <code>IntervalArray</code> <p>The range of the ordinate.</p>"},{"location":"reference/core/coordinates/#nelpy.core._coordinates.TemporalAbscissa","title":"<code>TemporalAbscissa</code>","text":"<p>               Bases: <code>Abscissa</code></p> <p>Abscissa for time series data.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class TemporalAbscissa(Abscissa):\n    \"\"\"Abscissa for time series data.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        labelstring = kwargs.get(\n            \"labelstring\", \"time ({})\"\n        )  # TODO FIXME after unit inheritance; inherit from formatter?\n\n        if support is None:\n            support = core.EpochArray(empty=True)\n\n        kwargs[\"support\"] = support\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n\n        self.formatter = self.support.formatter\n</code></pre>"},{"location":"reference/core/eventarray/","title":"EventArray","text":"<p>idea is to have abscissa and ordinate, and to use aliasing to have n_series, _series_subset, series_ids, (or trial_ids), and so on.</p> <p>What is the genericized class? EventArray? eventseries, eventcollection</p> <p>when event &lt;==&gt; spike, abscissa &lt;==&gt; data, eventseries &lt;==&gt; eventarray      eventseries_id &lt;==&gt; series_id, eventseries_type &lt;==&gt; series, then we have a      EventArray. (n_events, n_spikes)</p> <p>series ==&gt; series (series, trial, DIO, ...)</p> <p>event rate (smooth; ds, sigma) series_id eventarray shift ISI PSTH</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray","title":"<code>BaseEventArray</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for EventArray and BinnedEventArray.</p> <p>NOTE: This class can't really be instantiated, almost like a pseudo abstract class. In particular, during initialization it might fail because it checks the n_series of its derived classes to validate input to series_ids and series_labels. If NoneTypes are used, then you may actually succeed in creating an instance of this class, but it will be pretty useless.</p> <p>This docstring only applies to this base class, as subclasses may have different behavior. Therefore read the particular subclass' docstring for the most accurate information.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <p>Sampling rate / frequency (Hz).</p> <code>None</code> <code>series_ids</code> <code>list of int</code> <p>Unit IDs</p> <code>None</code> <code>series_labels</code> <code>list of str</code> <p>Labels corresponding to series. Default casts series_ids to str.</p> <code>None</code> <code>series_tags</code> <code>optional</code> <p>Tags correponding to series. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> <code>None</code> <code>label</code> <code>str</code> <p>Information pertaining to the source of the event series. Default is None.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Whether to create an empty class instance (no data). Default is False</p> <code>False</code> <code>abscissa</code> <code>optional</code> <p>Object for the abscissa (x-axis) coordinate</p> <code>None</code> <code>ordinate</code> <code>optional</code> <p>Object for the ordinate (y-axis) coordinate</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Other keyword arguments. NOTE: Currently we do not do anything with these.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>n_series</code> <code>int</code> <p>Number of series in event series.</p> <code>issempty</code> <code>bool</code> <p>Whether the class instance is empty (no data)</p> <code>series_ids</code> <code>list of int</code> <p>Unit IDs</p> <code>series_labels</code> <code>list of str</code> <p>Labels corresponding to series. Default casts series_ids to str.</p> <code>series_tags</code> <p>Tags corresponding to series. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> <code>n_intervals</code> <code>int</code> <p>The number of underlying intervals.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the EventArray.</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz).</p> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the event series.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class BaseEventArray(ABC):\n    \"\"\"Base class for EventArray and BinnedEventArray.\n\n    NOTE: This class can't really be instantiated, almost like a pseudo\n    abstract class. In particular, during initialization it might fail\n    because it checks the n_series of its derived classes to validate\n    input to series_ids and series_labels. If NoneTypes are used, then you\n    may actually succeed in creating an instance of this class, but it\n    will be pretty useless.\n\n    This docstring only applies to this base class, as subclasses may have\n    different behavior. Therefore read the particular subclass' docstring\n    for the most accurate information.\n\n    Parameters\n    ----------\n    fs: float, optional\n        Sampling rate / frequency (Hz).\n    series_ids : list of int, optional\n        Unit IDs\n    series_labels : list of str, optional\n        Labels corresponding to series. Default casts series_ids to str.\n    series_tags : optional\n        Tags correponding to series.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    label : str, optional\n        Information pertaining to the source of the event series.\n        Default is None.\n    empty : bool, optional\n        Whether to create an empty class instance (no data).\n        Default is False\n    abscissa : optional\n        Object for the abscissa (x-axis) coordinate\n    ordinate : optional\n        Object for the ordinate (y-axis) coordinate\n    kwargs : optional\n        Other keyword arguments. NOTE: Currently we do not do anything\n        with these.\n\n    Attributes\n    ----------\n    n_series : int\n        Number of series in event series.\n    issempty : bool\n        Whether the class instance is empty (no data)\n    series_ids : list of int\n        Unit IDs\n    series_labels : list of str\n        Labels corresponding to series. Default casts series_ids to str.\n    series_tags :\n        Tags corresponding to series.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    n_intervals : int\n        The number of underlying intervals.\n    support : nelpy.IntervalArray\n        The support of the EventArray.\n    fs: float\n        Sampling frequency (Hz).\n    label : str or None\n        Information pertaining to the source of the event series.\n    \"\"\"\n\n    __aliases__ = {}\n\n    __attributes__ = [\"_fs\", \"_series_ids\", \"_series_labels\", \"_series_tags\", \"_label\"]\n\n    def __init__(\n        self,\n        *,\n        fs=None,\n        series_ids=None,\n        series_labels=None,\n        series_tags=None,\n        label=None,\n        empty=False,\n        abscissa=None,\n        ordinate=None,\n        **kwargs,\n    ):\n        self.__version__ = version.__version__\n        self.type_name = self.__class__.__name__\n        if abscissa is None:\n            abscissa = core.Abscissa()  # TODO: integrate into constructor?\n        if ordinate is None:\n            ordinate = core.Ordinate()  # TODO: integrate into constructor?\n        self._abscissa = abscissa\n        self._ordinate = ordinate\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"series\"\n        self._series_label = series_label\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self.loc = _accessors.ItemGetterLoc(self)\n            self.iloc = _accessors.ItemGetterIloc(self)\n            return\n\n        # set initial fs to None\n        self._fs = None\n        # then attempt to update the fs; this does input validation:\n        self.fs = fs\n\n        # WARNING! we need to ensure that self.n_series can work BEFORE\n        # we can set self.series_ids or self.series_labels, since those\n        # setters check that the lengths of the inputs are consistent\n        # with self.n_series.\n\n        # inherit series IDs if available, otherwise initialize to default\n        if series_ids is None:\n            series_ids = list(range(1, self.n_series + 1))\n\n        series_ids = np.array(series_ids, ndmin=1)  # standardize series_ids\n\n        # if series_labels is empty, default to series_ids\n        if series_labels is None:\n            series_labels = series_ids\n\n        series_labels = np.array(series_labels, ndmin=1)  # standardize\n\n        self.series_ids = series_ids\n        self.series_labels = series_labels\n        self._series_tags = series_tags  # no input validation yet\n        self.label = label\n\n        self.loc = _accessors.ItemGetterLoc(self)\n        self.iloc = _accessors.ItemGetterIloc(self)\n\n    def __renew__(self):\n        \"\"\"Re-attach slicers and indexers.\"\"\"\n        self.loc = _accessors.ItemGetterLoc(self)\n        self.iloc = _accessors.ItemGetterIloc(self)\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        return \"&lt;BaseEventArray\" + address_str + \"&gt;\"\n\n    @abstractmethod\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"\n        Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_intervals : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n        return\n\n    @abstractmethod\n    def isempty(self):\n        \"\"\"(bool) Empty BaseEventArray.\"\"\"\n        return\n\n    @abstractmethod\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        return\n\n    @property\n    def n_intervals(self):\n        if self.isempty:\n            return 0\n        \"\"\"(int) The number of underlying intervals.\"\"\"\n        return self._abscissa.support.n_intervals\n\n    @property\n    def series_ids(self):\n        \"\"\"Unit IDs contained in the BaseEventArray.\"\"\"\n        return self._series_ids\n\n    @series_ids.setter\n    def series_ids(self, val):\n        if len(val) != self.n_series:\n            raise TypeError(\"series_ids must be of length n_series\")\n        elif len(set(val)) &lt; len(val):\n            raise TypeError(\"duplicate series_ids are not allowed\")\n        else:\n            try:\n                # cast to int:\n                series_ids = [int(id) for id in val]\n            except TypeError:\n                raise TypeError(\"series_ids must be int-like\")\n        self._series_ids = series_ids\n\n    @property\n    def series_labels(self):\n        \"\"\"Labels corresponding to series contained in the BaseEventArray.\"\"\"\n        if self._series_labels is None:\n            logging.warning(\"series labels have not yet been specified\")\n            return self.series_ids\n        return self._series_labels\n\n    @series_labels.setter\n    def series_labels(self, val):\n        if len(val) != self.n_series:\n            raise TypeError(\"labels must be of length n_series\")\n        else:\n            try:\n                # cast to str:\n                labels = [str(label) for label in val]\n            except TypeError:\n                raise TypeError(\"labels must be string-like\")\n        self._series_labels = labels\n\n    @property\n    def series_tags(self):\n        \"\"\"Tags corresponding to series contained in the BaseEventArray\"\"\"\n        if self._series_tags is None:\n            logging.warning(\"series tags have not yet been specified\")\n        return self._series_tags\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the EventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the EventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval(self._abscissa.support)\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the EventArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the EventArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval(self._abscissa.support)\n\n    @property\n    def fs(self):\n        \"\"\"(float) Sampling rate.\"\"\"\n        return self._fs\n\n    @fs.setter\n    def fs(self, val):\n        \"\"\"(float) Sampling rate.\"\"\"\n        if self._fs == val:\n            return\n        try:\n            if val &lt;= 0:\n                raise ValueError(\"sampling rate must be positive\")\n        except TypeError:\n            raise TypeError(\"sampling rate must be a scalar\")\n        self._fs = val\n\n    @property\n    def label(self):\n        \"\"\"Label pertaining to the source of the event series.\"\"\"\n        if self._label is None:\n            logging.warning(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n\n    def _series_subset(self, series_list):\n        \"\"\"Return a BaseEventArray restricted to a subset of series.\n\n        Parameters\n        ----------\n        series_list : array-like\n            Array or list of series_ids.\n        \"\"\"\n        return self.loc[:, series_list]\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the EventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.fs","title":"<code>fs</code>  <code>property</code> <code>writable</code>","text":"<p>(float) Sampling rate.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label pertaining to the source of the event series.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.series_ids","title":"<code>series_ids</code>  <code>property</code> <code>writable</code>","text":"<p>Unit IDs contained in the BaseEventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.series_labels","title":"<code>series_labels</code>  <code>property</code> <code>writable</code>","text":"<p>Labels corresponding to series contained in the BaseEventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.series_tags","title":"<code>series_tags</code>  <code>property</code>","text":"<p>Tags corresponding to series contained in the BaseEventArray</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the EventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.isempty","title":"<code>isempty()</code>  <code>abstractmethod</code>","text":"<p>(bool) Empty BaseEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@abstractmethod\ndef isempty(self):\n    \"\"\"(bool) Empty BaseEventArray.\"\"\"\n    return\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.n_series","title":"<code>n_series()</code>  <code>abstractmethod</code>","text":"<p>(int) The number of series.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@abstractmethod\ndef n_series(self):\n    \"\"\"(int) The number of series.\"\"\"\n    return\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BaseEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>  <code>abstractmethod</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_intervals</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@abstractmethod\n@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"\n    Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_intervals : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n    return\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray","title":"<code>BinnedEventArray</code>","text":"<p>               Bases: <code>BaseEventArray</code></p> <p>BinnedEventArray.</p> <p>Parameters:</p> Name Type Description Default <code>eventarray</code> <code>EventArray or RegularlySampledAnalogSignalArray</code> <p>Input data.</p> <code>None</code> <code>ds</code> <code>float</code> <p>The bin width, in seconds. Default is 0.0625 (62.5 ms)</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Whether an empty BinnedEventArray should be constructed (no data).</p> <code>False</code> <code>fs</code> <code>float</code> <p>Sampling rate in Hz. If fs is passed as a parameter, then data is assumed to be in sample numbers instead of actual data.</p> required <code>kwargs</code> <code>optional</code> <p>Additional keyword arguments to forward along to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray superclass for additional</code> <code>attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the BinnedEventArray is empty (no data).</p> <code>n_series</code> <code>int</code> <p>The number of series.</p> <code>bin_centers</code> <code>ndarray</code> <p>The bin centers, in seconds.</p> <code>event_centers</code> <code>ndarray</code> <p>The centers of each event, in seconds.</p> <code>data</code> <code>np.array, with shape (n_series, n_bins)</code> <p>Event counts in all bins.</p> <code>bins</code> <code>ndarray</code> <p>The bin edges, in seconds.</p> <code>binned_support</code> <code>np.ndarray, with shape (n_intervals, 2)</code> <p>The binned support of the BinnedEventArray (in bin IDs).</p> <code>lengths</code> <code>ndarray</code> <p>Lengths of contiguous segments, in number of bins.</p> <code>eventarray</code> <code>EventArray</code> <p>The original eventarray associated with the binned data.</p> <code>n_bins</code> <code>int</code> <p>The number of bins.</p> <code>ds</code> <code>float</code> <p>Bin width, in seconds.</p> <code>n_active</code> <code>int</code> <p>The number of active series. A series is considered active if it fired at least one event.</p> <code>n_active_per_bin</code> <code>np.ndarray, with shape (n_bins, )</code> <p>Number of active series per data bin.</p> <code>n_events</code> <code>ndarray</code> <p>The number of events in each series.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class BinnedEventArray(BaseEventArray):\n    \"\"\"BinnedEventArray.\n\n    Parameters\n    ----------\n    eventarray : nelpy.EventArray or nelpy.RegularlySampledAnalogSignalArray\n        Input data.\n    ds : float\n        The bin width, in seconds.\n        Default is 0.0625 (62.5 ms)\n    empty : bool, optional\n        Whether an empty BinnedEventArray should be constructed (no data).\n    fs : float, optional\n        Sampling rate in Hz. If fs is passed as a parameter, then data\n        is assumed to be in sample numbers instead of actual data.\n    kwargs : optional\n        Additional keyword arguments to forward along to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray superclass for additional\n    attributes that are defined there.\n    isempty : bool\n        Whether the BinnedEventArray is empty (no data).\n    n_series : int\n        The number of series.\n    bin_centers : np.ndarray\n        The bin centers, in seconds.\n    event_centers : np.ndarray\n        The centers of each event, in seconds.\n    data : np.array, with shape (n_series, n_bins)\n        Event counts in all bins.\n    bins : np.ndarray\n        The bin edges, in seconds.\n    binned_support : np.ndarray, with shape (n_intervals, 2)\n        The binned support of the BinnedEventArray (in\n        bin IDs).\n    lengths : np.ndarray\n        Lengths of contiguous segments, in number of bins.\n    eventarray : nelpy.EventArray\n        The original eventarray associated with the binned data.\n    n_bins : int\n        The number of bins.\n    ds : float\n        Bin width, in seconds.\n    n_active : int\n        The number of active series. A series is considered active if\n        it fired at least one event.\n    n_active_per_bin : np.ndarray, with shape (n_bins, )\n        Number of active series per data bin.\n    n_events : np.ndarray\n        The number of events in each series.\n    support : nelpy.IntervalArray\n        The support of the BinnedEventArray.\n    \"\"\"\n\n    __attributes__ = [\n        \"_ds\",\n        \"_bins\",\n        \"_data\",\n        \"_bin_centers\",\n        \"_binned_support\",\n        \"_eventarray\",\n    ]\n    __attributes__.extend(BaseEventArray.__attributes__)\n\n    def __init__(self, eventarray=None, *, ds=None, empty=False, **kwargs):\n        super().__init__(empty=True)\n\n        # if an empty object is requested, return it:\n        if empty:\n            # super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self._event_centers = None\n            return\n\n        # handle casting other nelpy objects to BinnedEventArray:\n        if isinstance(eventarray, core.RegularlySampledAnalogSignalArray):\n            if eventarray.isempty:\n                for attr in self.__attributes__:\n                    exec(\"self.\" + attr + \" = None\")\n                self._abscissa.support = type(eventarray._abscissa.support)(empty=True)\n                self._event_centers = None\n                return\n            eventarray = eventarray.copy()  # Note: this is a deep copy\n            n_empty_epochs = np.sum(eventarray.support.lengths == 0)\n            if n_empty_epochs &gt; 0:\n                logging.warning(\n                    \"Detected {} empty epochs. Removing these in the cast object\".format(\n                        n_empty_epochs\n                    )\n                )\n                eventarray.support = eventarray.support._drop_empty_intervals()\n            if not eventarray.support.ismerged:\n                logging.warning(\n                    \"Detected overlapping epochs. Merging these in the cast object\"\n                )\n                eventarray.support = eventarray.support.merge()\n\n            self._eventarray = None\n            self._ds = 1 / eventarray.fs\n            self._series_labels = eventarray._series_labels\n            self._bin_centers = eventarray.abscissa_vals\n            tmp = np.insert(np.cumsum(eventarray.lengths), 0, 0)\n            self._binned_support = np.array((tmp[:-1], tmp[1:] - 1)).T\n            self._abscissa.support = eventarray.support\n            try:\n                self._series_ids = (\n                    np.array(eventarray.series_labels).astype(int)\n                ).tolist()\n            except (ValueError, TypeError):\n                self._series_ids = (np.arange(eventarray.n_signals) + 1).tolist()\n            self._data = eventarray._ydata_rowsig\n\n            bins = []\n            for starti, stopi in self._binned_support:\n                bins_edges_in_interval = (\n                    self._bin_centers[starti : stopi + 1] - self._ds / 2\n                ).tolist()\n                bins_edges_in_interval.append(self._bin_centers[stopi] + self._ds / 2)\n                bins.extend(bins_edges_in_interval)\n            self._bins = np.array(bins)\n            return\n\n        if type(eventarray).__name__ == \"BinnedSpikeTrainArray\":\n            # old-style nelpy BinnedSpikeTrainArray object?\n            try:\n                self._eventarray = eventarray._spiketrainarray\n                self._ds = eventarray.ds\n                self._series_labels = eventarray.unit_labels\n                self._bin_centers = eventarray.bin_centers\n                self._binned_support = eventarray.binned_support\n                try:\n                    self._abscissa.support = core.EpochArray(eventarray.support.data)\n                except AttributeError:\n                    self._abscissa.support = core.EpochArray(eventarray.support.time)\n                self._series_ids = eventarray.unit_ids\n                self._data = eventarray.data\n                return\n            except Exception:\n                pass\n\n        if not isinstance(eventarray, EventArray):\n            raise TypeError(\"eventarray must be a nelpy.EventArray object.\")\n\n        self._ds = None\n        self._bin_centers = np.array([])\n        self._event_centers = None\n\n        logging.disable(logging.CRITICAL)\n        kwargs = {\n            \"fs\": eventarray.fs,\n            \"series_ids\": eventarray.series_ids,\n            \"series_labels\": eventarray.series_labels,\n            \"series_tags\": eventarray.series_tags,\n            \"label\": eventarray.label,\n        }\n        logging.disable(0)\n\n        # initialize super so that self.fs is set:\n        self._data = np.zeros((eventarray.n_series, 0))\n        # the above is necessary so that super() can determine\n        # self.n_series when initializing. self.data will\n        # be updated later in __init__ to reflect subsequent changes\n        super().__init__(**kwargs)\n\n        if ds is None:\n            logging.warning(\"no bin size was given, assuming 62.5 ms\")\n            ds = 0.0625\n\n        self._eventarray = eventarray  # TODO: remove this if we don't need it, or decide that it's too wasteful\n        self._abscissa = copy.deepcopy(eventarray._abscissa)\n        self.ds = ds\n\n        self._bin_events(eventarray=eventarray, intervalArray=eventarray.support, ds=ds)\n\n    def __mul__(self, other):\n        \"\"\"Overloaded * operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data * other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T * other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for *: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __rmul__(self, other):\n        \"\"\"Overloaded * operator\"\"\"\n        return self.__mul__(other)\n\n    def __sub__(self, other):\n        \"\"\"Overloaded - operator\"\"\"\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data - other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T - other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for -: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __add__(self, other):\n        \"\"\"Overloaded + operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data + other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T + other).T\n            return neweva\n        elif isinstance(other, type(self)):\n            # TODO: additional checks need to be done, e.g., same series ids...\n            assert self.n_series == other.n_series\n            support = self._abscissa.support + other.support\n\n            newdata = []\n            for series in range(self.n_series):\n                newdata.append(np.append(self.data[series], other.data[series]))\n\n            fs = self.fs\n            if self.fs != other.fs:\n                fs = None\n            return type(self)(newdata, support=support, fs=fs)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __truediv__(self, other):\n        \"\"\"Overloaded / operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data / other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T / other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for /: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def median(self, *, axis=1):\n        \"\"\"Returns the median of each series in BinnedEventArray.\"\"\"\n        try:\n            medians = np.nanmedian(self.data, axis=axis).squeeze()\n            if medians.size == 1:\n                return medians.item()\n            return medians\n        except IndexError:\n            raise IndexError(\"Empty BinnedEventArray; cannot calculate median.\")\n\n    def mean(self, *, axis=1):\n        \"\"\"Returns the mean of each series in BinnedEventArray.\"\"\"\n        try:\n            means = np.nanmean(self.data, axis=axis).squeeze()\n            if means.size == 1:\n                return means.item()\n            return means\n        except IndexError:\n            raise IndexError(\"Empty BinnedEventArray; cannot calculate mean.\")\n\n    def std(self, *, axis=1):\n        \"\"\"Returns the standard deviation of each series in BinnedEventArray.\"\"\"\n        try:\n            stds = np.nanstd(self.data, axis=axis).squeeze()\n            if stds.size == 1:\n                return stds.item()\n            return stds\n        except IndexError:\n            raise IndexError(\n                \"Empty BinnedEventArray; cannot calculate standard deviation\"\n            )\n\n    def center(self, inplace=False):\n        \"\"\"Center data (zero mean).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        return out\n\n    def normalize(self, inplace=False):\n        \"\"\"Normalize data (unit standard deviation).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        std = out.std()\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    def standardize(self, inplace=False):\n        \"\"\"Standardize data (zero mean and unit std deviation).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        std = out.std()\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n\n        return out\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        partitioned = type(self)(\n            core.RegularlySampledAnalogSignalArray(self).partition(\n                ds=ds, n_intervals=n_intervals\n            )\n        )\n        # partitioned.loc = ItemGetter_loc(partitioned)\n        # partitioned.iloc = ItemGetter_iloc(partitioned)\n        return partitioned\n\n        # raise NotImplementedError('workaround: cast to AnalogSignalArray, partition, and cast back to BinnedEventArray')\n\n    def _copy_without_data(self):\n        \"\"\"Returns a copy of the BinnedEventArray, without data.\n        Note: the support is left unchanged, but the binned_support is removed.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._bin_centers = None\n        out._binned_support = None\n        out._bins = None\n        out._data = np.zeros((self.n_series, 0))\n        out._eventarray = out._eventarray._copy_without_data()\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Returns a copy of the BinnedEventArray.\"\"\"\n        newcopy = copy.deepcopy(self)\n        newcopy.__renew__()\n        return newcopy\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        # Summarize labels if too many\n        max_display = 5\n        labels = self._series_labels\n        n = len(labels)\n        if n &lt;= max_display:\n            label_str = str(labels)\n        else:\n            label_str = f\"[{', '.join(map(str, labels[:3]))}, ..., {', '.join(map(str, labels[-2:]))}]\"\n        ustr = f\" {self.n_series} {label_str}\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = f\" ({self._abscissa.support.n_intervals} segments) in\"\n        else:\n            epstr = \" in\"\n        if self.n_bins == 1:\n            bstr = f\" {self.n_bins} bin of width {utils.PrettyDuration(self.ds)}\"\n            dstr = \"\"\n        else:\n            bstr = f\" {self.n_bins} bins of width {utils.PrettyDuration(self.ds)}\"\n            dstr = f\" for a total of {utils.PrettyDuration(self.n_bins * self.ds)}\"\n        return f\"&lt;{self.type_name}{address_str}:{ustr}{epstr}{bstr}&gt;{dstr}\"\n\n    def __iter__(self):\n        \"\"\"BinnedEventArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"BinnedEventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        # TODO: return self.loc[index], and make sure that __getitem__ is updated\n        logging.disable(logging.CRITICAL)\n        support = self._abscissa.support[index]\n        bsupport = self.binned_support[[index], :]\n\n        binnedeventarray = type(self)(empty=True)\n        exclude = [\"_bins\", \"_data\", \"_support\", \"_bin_centers\", \"_binned_support\"]\n        attrs = (x for x in self.__attributes__ if x not in exclude)\n        for attr in attrs:\n            exec(\"binnedeventarray.\" + attr + \" = self.\" + attr)\n        binindices = np.insert(0, 1, np.cumsum(self.lengths + 1))  # indices of bins\n        binstart = binindices[index]\n        binstop = binindices[index + 1]\n        binnedeventarray._bins = self._bins[binstart:binstop]\n        binnedeventarray._data = self._data[:, bsupport[0][0] : bsupport[0][1] + 1]\n        binnedeventarray._abscissa.support = support\n        binnedeventarray._bin_centers = self._bin_centers[\n            bsupport[0][0] : bsupport[0][1] + 1\n        ]\n        binnedeventarray._binned_support = bsupport - bsupport[0, 0]\n        logging.disable(0)\n        self._index += 1\n        binnedeventarray.__renew__()\n        return binnedeventarray\n\n    def empty(self, *, inplace=False):\n        \"\"\"Remove data (but not metadata) from BinnedEventArray.\n\n        Attributes 'data', and 'support' 'binned_support' are all emptied.\n\n        Note: n_series, series_ids, etc. are all preserved.\n        \"\"\"\n        if not inplace:\n            out = self._copy_without_data()\n            out._abscissa.support = type(self._abscissa.support)(empty=True)\n            return out\n        out = self\n        out._data = np.zeros((self.n_series, 0))\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        out._binned_support = None\n        out._bin_centers = None\n        out._bins = None\n        out._eventarray.empty(inplace=True)\n        out.__renew__()\n        return out\n\n    def __getitem__(self, idx):\n        \"\"\"BinnedEventArray index access.\n\n        By default, this method is bound to .loc\n        \"\"\"\n        return self.loc[idx]\n\n    def _restrict(self, intervalslice, seriesslice):\n        # This function should be called only by an itemgetter\n        # because it mutates data.\n        # The itemgetter is responsible for creating copies\n        # of objects\n\n        self._restrict_to_series_subset(seriesslice)\n        self._eventarray._restrict_to_series_subset(seriesslice)\n\n        self._restrict_to_interval(intervalslice)\n        self._eventarray._restrict_to_interval(intervalslice)\n        return self\n\n    def _restrict_to_series_subset(self, idx):\n        # Warning: This function can mutate data\n\n        if isinstance(idx, core.IntervalArray):\n            raise IndexError(\n                \"Slicing is [intervals, signal]; perhaps you have the order reversed?\"\n            )\n\n        # TODO: update tags\n        try:\n            self._data = np.atleast_2d(self.data[idx, :])\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except IndexError:\n            raise IndexError(\n                \"One of more indices were out of bounds for n_series with size {}\".format(\n                    self.n_series\n                )\n            )\n        except Exception:\n            raise TypeError(\"Unsupported indexing type {}\".format(type(idx)))\n\n    def _restrict_to_interval(self, intervalslice):\n        # Warning: This function can mutate data. It should only be called from\n        # _restrict\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                # no restriction on interval\n                return self\n\n        newintervals = self._abscissa.support[intervalslice].merge()\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=True)\n\n        bcenter_inds = []\n        bin_inds = []\n        start = 0\n        bsupport = np.zeros((newintervals.n_intervals, 2), dtype=int)\n        support_intervals = np.zeros((newintervals.n_intervals, 2))\n\n        if not self.isempty:\n            for ii, interval in enumerate(newintervals.data):\n                a_start = interval[0]\n                a_stop = interval[1]\n                frm, to = np.searchsorted(self._bins, (a_start, a_stop))\n                # If bin edges equal a_stop, they should still be included\n                if self._bins[to] &lt;= a_stop:\n                    bin_inds.extend(np.arange(frm, to + 1, step=1))\n                else:\n                    bin_inds.extend(np.arange(frm, to, step=1))\n                    to -= 1\n                support_intervals[ii] = [self._bins[frm], self._bins[to]]\n\n                lind, rind = np.searchsorted(\n                    self._bin_centers, (self._bins[frm], self._bins[to])\n                )\n                # We don't have to worry about an if-else block here unlike\n                # for the bin_inds because the bin_centers can NEVER equal\n                # the bins. Therefore we know every interval looks like\n                # the following:\n                #  first desired bin         last desired bin\n                # |------------------|......|-------------------|\n                #          ^                                         ^\n                #          |                                         |\n                #        lind                                      rind\n                # Since arange is half-open, the indices we actually take\n                # will be such that all bin centers fall within the desired\n                # bin edges.\n                bcenter_inds.extend(np.arange(lind, rind, step=1))\n\n                bsupport[ii] = [start, start + (to - frm - 1)]\n                start += to - frm\n\n            self._bins = self._bins[bin_inds]\n            self._bin_centers = self._bin_centers[bcenter_inds]\n            self._data = np.atleast_2d(self._data[:, bcenter_inds])\n            self._binned_support = bsupport\n\n        self._abscissa.support = type(self._abscissa.support)(support_intervals)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty BinnedEventArray.\"\"\"\n        try:\n            return len(self.bin_centers) == 0\n        except TypeError:\n            return True  # this happens when self.bin_centers is None\n\n    @property\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        try:\n            return utils.PrettyInt(self.data.shape[0])\n        except AttributeError:\n            return 0\n\n    @property\n    def centers(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        logging.warning(\"centers is deprecated. Use bin_centers instead.\")\n        return self.bin_centers\n\n    @property\n    def _abscissa_vals(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        return self._bin_centers\n\n    @property\n    def bin_centers(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        return self._bin_centers\n\n    @property\n    def event_centers(self):\n        \"\"\"(np.array) The centers (in seconds) of each event.\"\"\"\n        if self._event_centers is None:\n            raise NotImplementedError(\"event_centers not yet implemented\")\n            # self._event_centers = midpoints\n        return self._event_centers\n\n    @property\n    def _midpoints(self):\n        \"\"\"(np.array) The centers (in index space) of all events.\n\n        Examples\n        -------\n        ax, img = npl.imagesc(bst.data) # data is of shape (n_series, n_bins)\n        # then _midpoints correspond to the xvals at the center of\n        # each event.\n        ax.plot(bst.event_centers, np.repeat(1, self.n_intervals), marker='o', color='w')\n\n        \"\"\"\n        if self._event_centers is None:\n            midpoints = np.zeros(len(self.lengths))\n            for idx, length in enumerate(self.lengths):\n                midpoints[idx] = np.sum(self.lengths[:idx]) + length / 2\n            self._event_centers = midpoints\n        return self._event_centers\n\n    @property\n    def data(self):\n        \"\"\"(np.array) Event counts in all bins, with shape (n_series, n_bins).\"\"\"\n        return self._data\n\n    @property\n    def bins(self):\n        \"\"\"(np.array) The bin edges (in seconds).\"\"\"\n        return self._bins\n\n    @property\n    def binnedSupport(self):\n        \"\"\"(np.array) The binned support of the BinnedEventArray (in\n        bin IDs) of shape (n_intervals, 2).\n        \"\"\"\n        logging.warning(\"binnedSupport is deprecated. Use bined_support instead.\")\n        return self._binned_support\n\n    @property\n    def binned_support(self):\n        \"\"\"(np.array) The binned support of the BinnedEventArray (in\n        bin IDs) of shape (n_intervals, 2).\n        \"\"\"\n        return self._binned_support\n\n    @property\n    def lengths(self):\n        \"\"\"Lengths of contiguous segments, in number of bins.\"\"\"\n        if self.isempty:\n            return 0\n        return np.atleast_1d(\n            (self.binned_support[:, 1] - self.binned_support[:, 0] + 1).squeeze()\n        )\n\n    @property\n    def eventarray(self):\n        \"\"\"(nelpy.EventArray) The original EventArray associated with\n        the binned data.\n        \"\"\"\n        return self._eventarray\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) The number of bins.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self.bin_centers))\n\n    @property\n    def ds(self):\n        \"\"\"(float) Bin width in seconds.\"\"\"\n        return self._ds\n\n    @ds.setter\n    def ds(self, val):\n        if self._ds is not None:\n            raise AttributeError(\"can't set attribute\")\n        else:\n            try:\n                if val &lt;= 0:\n                    pass\n            except ValueError:\n                raise TypeError(\"bin width must be a scalar\")\n            if val &lt;= 0:\n                raise ValueError(\"bin width must be positive\")\n            self._ds = val\n\n    @staticmethod\n    def _get_bins_inside_interval(interval, ds):\n        \"\"\"(np.array) Return bin edges entirely contained inside an interval.\n\n        Bin edges always start at interval.start, and continue for as many\n        bins as would fit entirely inside the interval.\n\n        NOTE 1: there are (n+1) bin edges associated with n bins.\n\n        WARNING: if an interval is smaller than ds, then no bin will be\n                associated with the particular interval.\n\n        NOTE 2: nelpy uses half-open intervals [a,b), but if the bin\n                width divides b-a, then the bins will cover the entire\n                range. For example, if interval = [0,2) and ds = 1, then\n                bins = [0,1,2], even though [0,2] is not contained in\n                [0,2).\n\n        Parameters\n        ----------\n        interval : IntervalArray\n            IntervalArray containing a single interval with a start, and stop\n        ds : float\n            Time bin width, in seconds.\n\n        Returns\n        -------\n        bins : array\n            Bin edges in an array of shape (n+1,) where n is the number\n            of bins\n        centers : array\n            Bin centers in an array of shape (n,) where n is the number\n            of bins\n        \"\"\"\n\n        if interval.length &lt; ds:\n            logging.warning(\"interval duration is less than bin size: ignoring...\")\n            return None, None\n\n        n = int(np.floor(interval.length / ds))  # number of bins\n\n        # linspace is better than arange for non-integral steps\n        bins = np.linspace(interval.start, interval.start + n * ds, n + 1)\n        centers = bins[:-1] + (ds / 2)\n        return bins, centers\n\n    def _bin_events(self, eventarray, intervalArray, ds):\n        \"\"\"\n        Docstring goes here. TBD. For use with bins that are contained\n        wholly inside the intervals.\n\n        \"\"\"\n        b = []  # bin list\n        c = []  # centers list\n        s = []  # data list\n        for nn in range(eventarray.n_series):\n            s.append([])\n        left_edges = []\n        right_edges = []\n        counter = 0\n        for interval in intervalArray:\n            bins, centers = self._get_bins_inside_interval(interval, ds)\n            if bins is not None:\n                for uu, eventarraydatas in enumerate(eventarray.data):\n                    event_counts, _ = np.histogram(\n                        eventarraydatas,\n                        bins=bins,\n                        density=False,\n                        range=(interval.start, interval.stop),\n                    )  # TODO: is it faster to limit range, or to cut out events?\n                    s[uu].extend(event_counts.tolist())\n                left_edges.append(counter)\n                counter += len(centers) - 1\n                right_edges.append(counter)\n                counter += 1\n                b.extend(bins.tolist())\n                c.extend(centers.tolist())\n        self._bins = np.array(b)\n        self._bin_centers = np.array(c)\n        self._data = np.array(s)\n        le = np.array(left_edges)\n        le = le[:, np.newaxis]\n        re = np.array(right_edges)\n        re = re[:, np.newaxis]\n        self._binned_support = np.hstack((le, re))\n        support_starts = self.bins[np.insert(np.cumsum(self.lengths + 1), 0, 0)[:-1]]\n        support_stops = self.bins[np.insert(np.cumsum(self.lengths + 1) - 1, 0, 0)[1:]]\n        supportdata = np.vstack([support_starts, support_stops]).T\n        self._abscissa.support = type(self._abscissa.support)(\n            supportdata\n        )  # set support to TRUE bin support\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(\n        self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n    ):\n        \"\"\"Smooth BinnedEventArray by convolving with a Gaussian kernel.\n\n        Smoothing is applied in data, and the same smoothing is applied\n        to each series in a BinnedEventArray.\n\n        Smoothing is applied within each interval.\n\n        Parameters\n        ----------\n        sigma : float, optional\n            Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n        truncate : float, optional\n            Bandwidth outside of which the filter value will be zero. Default is 4.0\n        inplace : bool\n            If True the data will be replaced with the smoothed data.\n            Default is False.\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with smoothed data.\n        \"\"\"\n\n        if truncate is None:\n            truncate = 4\n        if sigma is None:\n            sigma = 0.01  # 10 ms default\n\n        fs = 1 / self.ds\n\n        return utils.gaussian_filter(\n            self,\n            fs=fs,\n            sigma=sigma,\n            truncate=truncate,\n            inplace=inplace,\n            within_intervals=within_intervals,\n        )\n\n    @staticmethod\n    def _smooth_array(arr, w=None):\n        \"\"\"Smooth an array by convolving a boxcar, row-wise.\n\n        Parameters\n        ----------\n        w : int, optional\n            Number of bins to include in boxcar window. Default is 10.\n\n        Returns\n        -------\n        smoothed: array\n            Smoothed array with same shape as arr.\n        \"\"\"\n\n        if w is None:\n            w = 10\n\n        if w == 1:  # perform no smoothing\n            return arr\n\n        w = np.min((w, arr.shape[1]))\n\n        smoothed = arr.astype(float)  # copy array and cast to float\n        window = np.ones((w,)) / w\n\n        # smooth per row\n        for rowi, row in enumerate(smoothed):\n            smoothed[rowi, :] = np.convolve(row, window, mode=\"same\")\n\n        if arr.shape[1] != smoothed.shape[1]:\n            raise TypeError(\"Incompatible shape returned!\")\n\n        return smoothed\n\n    @staticmethod\n    def _rebin_array(arr, w):\n        \"\"\"Rebin an array of shape (n_signals, n_bins) into a\n        coarser bin size.\n\n        Parameters\n        ----------\n        arr : array\n            Array with shape (n_signals, n_bins) to re-bin. A copy\n            is returned.\n        w : int\n            Number of original bins to combine into each new bin.\n\n        Returns\n        -------\n        out : array\n            Bnned array with shape (n_signals, n_new_bins)\n        bin_idx : array\n            Array of shape (n_new_bins,) with the indices of the new\n            binned array, relative to the original array.\n        \"\"\"\n        cs = np.cumsum(arr, axis=1)\n        binidx = np.arange(start=w, stop=cs.shape[1] + 1, step=w) - 1\n\n        rebinned = np.hstack(\n            (np.array(cs[:, w - 1], ndmin=2).T, cs[:, binidx[1:]] - cs[:, binidx[:-1]])\n        )\n        # bins = bins[np.insert(binidx+1, 0, 0)]\n        return rebinned, binidx\n\n    def rebin(self, w=None):\n        \"\"\"Rebin the BinnedEventArray into a coarser bin size.\n\n        Parameters\n        ----------\n        w : int, optional\n            number of bins of width bst.ds to bin into new bin of\n            width bst.ds*w. Default is w=1 (no re-binning).\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with coarser resolution.\n        \"\"\"\n\n        if w is None:\n            w = 1\n\n        if not float(w).is_integer:\n            raise ValueError(\"w has to be an integer!\")\n\n        w = int(w)\n\n        bst = self\n        return self._rebin_binnedeventarray(bst, w=w)\n\n    @staticmethod\n    def _rebin_binnedeventarray(bst, w=None):\n        \"\"\"Rebin a BinnedEventArray into a coarser bin size.\n\n        Parameters\n        ----------\n        bst : BinnedEventArray\n            BinnedEventArray to re-bin into a coarser resolution.\n        w : int, optional\n            number of bins of width bst.ds to bin into new bin of\n            width bst.ds*w. Default is w=1 (no re-binning).\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with coarser resolution.\n\n        # FFB! TODO: if w is longer than some event size,\n        # an exception will occur. Handle it! Although I may already\n        # implicitly do that.\n        \"\"\"\n\n        if w is None:\n            w = 1\n\n        if w == 1:\n            return bst\n\n        edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n        newlengths = [0]\n        binedges = np.insert(np.cumsum(bst.lengths + 1), 0, 0)\n        n_events = bst.support.n_intervals\n        newdata = None\n\n        for ii in range(n_events):\n            data = bst.data[:, edges[ii] : edges[ii + 1]]\n            bins = bst.bins[binedges[ii] : binedges[ii + 1]]\n\n            datalen = data.shape[1]\n            if w &lt;= datalen:\n                rebinned, binidx = bst._rebin_array(data, w=w)\n                bins = bins[np.insert(binidx + 1, 0, 0)]\n\n                newlengths.append(rebinned.shape[1])\n\n                if newdata is None:\n                    newdata = rebinned\n                    newbins = bins\n                    newcenters = bins[:-1] + np.diff(bins) / 2\n                    newsupport = np.array([bins[0], bins[-1]])\n                else:\n                    newdata = np.hstack((newdata, rebinned))\n                    newbins = np.hstack((newbins, bins))\n                    newcenters = np.hstack((newcenters, bins[:-1] + np.diff(bins) / 2))\n                    newsupport = np.vstack((newsupport, np.array([bins[0], bins[-1]])))\n            else:\n                pass\n\n        # assemble new binned event series array:\n        newedges = np.cumsum(newlengths)\n        newbst = bst._copy_without_data()\n        abscissa = copy.copy(bst._abscissa)\n        if newdata is not None:\n            newbst._data = newdata\n            newbst._abscissa = abscissa\n            newbst._abscissa.support = type(bst.support)(newsupport)\n            newbst._bins = newbins\n            newbst._bin_centers = newcenters\n            newbst._ds = bst.ds * w\n            newbst._binned_support = np.array((newedges[:-1], newedges[1:] - 1)).T\n        else:\n            logging.warning(\n                \"No events are long enough to contain any bins of width {}\".format(\n                    utils.PrettyDuration(bst.ds)\n                )\n            )\n            newbst._data = None\n            newbst._abscissa = abscissa\n            newbst._abscissa.support = None\n            newbst._binned_support = None\n            newbst._bin_centers = None\n            newbst._bins = None\n\n        newbst.__renew__()\n\n        return newbst\n\n    def bst_from_indices(self, idx):\n        \"\"\"\n        Return a BinnedEventArray from a list of indices.\n\n        bst : BinnedEventArray\n        idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE\n\n        Examples\n        --------\n        idx = [[10, 20]\n            [25, 50]]\n        bst_from_indices(bst, idx=idx)\n        \"\"\"\n\n        idx = np.atleast_2d(idx)\n\n        newbst = self._copy_without_data()\n        ds = self.ds\n        bin_centers_ = []\n        bins_ = []\n        binned_support_ = []\n        support_ = []\n        all_abscissa_vals = []\n\n        n_preceding_bins = 0\n\n        for frm, to in idx:\n            idx_array = np.arange(frm, to + 1).astype(int)\n            all_abscissa_vals.append(idx_array)\n            bin_centers = self.bin_centers[idx_array]\n            bins = np.append(bin_centers - ds / 2, bin_centers[-1] + ds / 2)\n\n            binned_support = [n_preceding_bins, n_preceding_bins + len(bins) - 2]\n            n_preceding_bins += len(bins) - 1\n            support = type(self._abscissa.support)((bins[0], bins[-1]))\n\n            bin_centers_.append(bin_centers)\n            bins_.append(bins)\n            binned_support_.append(binned_support)\n            support_.append(support)\n\n        bin_centers = np.concatenate(bin_centers_)\n        bins = np.concatenate(bins_)\n        binned_support = np.array(binned_support_)\n        support = np.sum(support_)\n        all_abscissa_vals = np.concatenate(all_abscissa_vals)\n\n        newbst._bin_centers = bin_centers\n        newbst._bins = bins\n        newbst._binned_support = binned_support\n        newbst._abscissa.support = support\n        newbst._data = newbst.data[:, all_abscissa_vals]\n\n        newbst.__renew__()\n\n        return newbst\n\n    @property\n    def n_active(self):\n        \"\"\"Number of active series.\n\n        An active series is any series that fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    @property\n    def n_active_per_bin(self):\n        \"\"\"Number of active series per data bin with shape (n_bins,).\"\"\"\n        if self.isempty:\n            return 0\n        # TODO: profile several alternatves. Could use data &gt; 0, or\n        # other numpy methods to get a more efficient implementation:\n        return self.data.clip(max=1).sum(axis=0)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return self.data.sum(axis=1)\n\n    def flatten(self, *, series_id=None, series_label=None):\n        \"\"\"Collapse events across series.\n\n        WARNING! series_tags are thrown away when flattening.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        series_label (str)\n            (series) Label for event series, default is 'flattened'.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n        if series_label is None:\n            series_label = \"flattened\"\n\n        binnedeventarray = self._copy_without_data()\n\n        binnedeventarray._data = np.array(self.data.sum(axis=0), ndmin=2)\n\n        binnedeventarray._bins = self.bins\n        binnedeventarray._abscissa.support = self.support\n        binnedeventarray._bin_centers = self.bin_centers\n        binnedeventarray._binned_support = self.binned_support\n\n        binnedeventarray._series_ids = [series_id]\n        binnedeventarray._series_labels = [series_label]\n        binnedeventarray._series_tags = None\n        binnedeventarray.__renew__()\n\n        return binnedeventarray\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying BinnedEventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying BinnedEventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval(self._abscissa.support)\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.bin_centers","title":"<code>bin_centers</code>  <code>property</code>","text":"<p>(np.array) The bin centers (in seconds).</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.binnedSupport","title":"<code>binnedSupport</code>  <code>property</code>","text":"<p>(np.array) The binned support of the BinnedEventArray (in bin IDs) of shape (n_intervals, 2).</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.binned_support","title":"<code>binned_support</code>  <code>property</code>","text":"<p>(np.array) The binned support of the BinnedEventArray (in bin IDs) of shape (n_intervals, 2).</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.bins","title":"<code>bins</code>  <code>property</code>","text":"<p>(np.array) The bin edges (in seconds).</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.centers","title":"<code>centers</code>  <code>property</code>","text":"<p>(np.array) The bin centers (in seconds).</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>(np.array) Event counts in all bins, with shape (n_series, n_bins).</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.ds","title":"<code>ds</code>  <code>property</code> <code>writable</code>","text":"<p>(float) Bin width in seconds.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.event_centers","title":"<code>event_centers</code>  <code>property</code>","text":"<p>(np.array) The centers (in seconds) of each event.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.eventarray","title":"<code>eventarray</code>  <code>property</code>","text":"<p>(nelpy.EventArray) The original EventArray associated with the binned data.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty BinnedEventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>Lengths of contiguous segments, in number of bins.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>Number of active series.</p> <p>An active series is any series that fired at least one event.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.n_active_per_bin","title":"<code>n_active_per_bin</code>  <code>property</code>","text":"<p>Number of active series per data bin with shape (n_bins,).</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) The number of bins.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.n_series","title":"<code>n_series</code>  <code>property</code>","text":"<p>(int) The number of series.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying BinnedEventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.bst_from_indices","title":"<code>bst_from_indices(idx)</code>","text":"<p>Return a BinnedEventArray from a list of indices.</p> <p>bst : BinnedEventArray idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE</p> <p>Examples:</p> <p>idx = [[10, 20]     [25, 50]] bst_from_indices(bst, idx=idx)</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bst_from_indices(self, idx):\n    \"\"\"\n    Return a BinnedEventArray from a list of indices.\n\n    bst : BinnedEventArray\n    idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE\n\n    Examples\n    --------\n    idx = [[10, 20]\n        [25, 50]]\n    bst_from_indices(bst, idx=idx)\n    \"\"\"\n\n    idx = np.atleast_2d(idx)\n\n    newbst = self._copy_without_data()\n    ds = self.ds\n    bin_centers_ = []\n    bins_ = []\n    binned_support_ = []\n    support_ = []\n    all_abscissa_vals = []\n\n    n_preceding_bins = 0\n\n    for frm, to in idx:\n        idx_array = np.arange(frm, to + 1).astype(int)\n        all_abscissa_vals.append(idx_array)\n        bin_centers = self.bin_centers[idx_array]\n        bins = np.append(bin_centers - ds / 2, bin_centers[-1] + ds / 2)\n\n        binned_support = [n_preceding_bins, n_preceding_bins + len(bins) - 2]\n        n_preceding_bins += len(bins) - 1\n        support = type(self._abscissa.support)((bins[0], bins[-1]))\n\n        bin_centers_.append(bin_centers)\n        bins_.append(bins)\n        binned_support_.append(binned_support)\n        support_.append(support)\n\n    bin_centers = np.concatenate(bin_centers_)\n    bins = np.concatenate(bins_)\n    binned_support = np.array(binned_support_)\n    support = np.sum(support_)\n    all_abscissa_vals = np.concatenate(all_abscissa_vals)\n\n    newbst._bin_centers = bin_centers\n    newbst._bins = bins\n    newbst._binned_support = binned_support\n    newbst._abscissa.support = support\n    newbst._data = newbst.data[:, all_abscissa_vals]\n\n    newbst.__renew__()\n\n    return newbst\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.center","title":"<code>center(inplace=False)</code>","text":"<p>Center data (zero mean).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def center(self, inplace=False):\n    \"\"\"Center data (zero mean).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of the BinnedEventArray.\"\"\"\n    newcopy = copy.deepcopy(self)\n    newcopy.__renew__()\n    return newcopy\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.empty","title":"<code>empty(*, inplace=False)</code>","text":"<p>Remove data (but not metadata) from BinnedEventArray.</p> <p>Attributes 'data', and 'support' 'binned_support' are all emptied.</p> <p>Note: n_series, series_ids, etc. are all preserved.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def empty(self, *, inplace=False):\n    \"\"\"Remove data (but not metadata) from BinnedEventArray.\n\n    Attributes 'data', and 'support' 'binned_support' are all emptied.\n\n    Note: n_series, series_ids, etc. are all preserved.\n    \"\"\"\n    if not inplace:\n        out = self._copy_without_data()\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        return out\n    out = self\n    out._data = np.zeros((self.n_series, 0))\n    out._abscissa.support = type(self._abscissa.support)(empty=True)\n    out._binned_support = None\n    out._bin_centers = None\n    out._bins = None\n    out._eventarray.empty(inplace=True)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.flatten","title":"<code>flatten(*, series_id=None, series_label=None)</code>","text":"<p>Collapse events across series.</p> <p>WARNING! series_tags are thrown away when flattening.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> <code>series_label</code> <p>(series) Label for event series, default is 'flattened'.</p> <code>None</code> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def flatten(self, *, series_id=None, series_label=None):\n    \"\"\"Collapse events across series.\n\n    WARNING! series_tags are thrown away when flattening.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    series_label (str)\n        (series) Label for event series, default is 'flattened'.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n    if series_label is None:\n        series_label = \"flattened\"\n\n    binnedeventarray = self._copy_without_data()\n\n    binnedeventarray._data = np.array(self.data.sum(axis=0), ndmin=2)\n\n    binnedeventarray._bins = self.bins\n    binnedeventarray._abscissa.support = self.support\n    binnedeventarray._bin_centers = self.bin_centers\n    binnedeventarray._binned_support = self.binned_support\n\n    binnedeventarray._series_ids = [series_id]\n    binnedeventarray._series_labels = [series_label]\n    binnedeventarray._series_tags = None\n    binnedeventarray.__renew__()\n\n    return binnedeventarray\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.mean","title":"<code>mean(*, axis=1)</code>","text":"<p>Returns the mean of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def mean(self, *, axis=1):\n    \"\"\"Returns the mean of each series in BinnedEventArray.\"\"\"\n    try:\n        means = np.nanmean(self.data, axis=axis).squeeze()\n        if means.size == 1:\n            return means.item()\n        return means\n    except IndexError:\n        raise IndexError(\"Empty BinnedEventArray; cannot calculate mean.\")\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.median","title":"<code>median(*, axis=1)</code>","text":"<p>Returns the median of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def median(self, *, axis=1):\n    \"\"\"Returns the median of each series in BinnedEventArray.\"\"\"\n    try:\n        medians = np.nanmedian(self.data, axis=axis).squeeze()\n        if medians.size == 1:\n            return medians.item()\n        return medians\n    except IndexError:\n        raise IndexError(\"Empty BinnedEventArray; cannot calculate median.\")\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.normalize","title":"<code>normalize(inplace=False)</code>","text":"<p>Normalize data (unit standard deviation).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def normalize(self, inplace=False):\n    \"\"\"Normalize data (unit standard deviation).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    std = out.std()\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    partitioned = type(self)(\n        core.RegularlySampledAnalogSignalArray(self).partition(\n            ds=ds, n_intervals=n_intervals\n        )\n    )\n    # partitioned.loc = ItemGetter_loc(partitioned)\n    # partitioned.iloc = ItemGetter_iloc(partitioned)\n    return partitioned\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.rebin","title":"<code>rebin(w=None)</code>","text":"<p>Rebin the BinnedEventArray into a coarser bin size.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>int</code> <p>number of bins of width bst.ds to bin into new bin of width bst.ds*w. Default is w=1 (no re-binning).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedEventArray</code> <p>New BinnedEventArray with coarser resolution.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def rebin(self, w=None):\n    \"\"\"Rebin the BinnedEventArray into a coarser bin size.\n\n    Parameters\n    ----------\n    w : int, optional\n        number of bins of width bst.ds to bin into new bin of\n        width bst.ds*w. Default is w=1 (no re-binning).\n\n    Returns\n    -------\n    out : BinnedEventArray\n        New BinnedEventArray with coarser resolution.\n    \"\"\"\n\n    if w is None:\n        w = 1\n\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    w = int(w)\n\n    bst = self\n    return self._rebin_binnedeventarray(bst, w=w)\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.smooth","title":"<code>smooth(*, sigma=None, inplace=False, truncate=None, within_intervals=False)</code>","text":"<p>Smooth BinnedEventArray by convolving with a Gaussian kernel.</p> <p>Smoothing is applied in data, and the same smoothing is applied to each series in a BinnedEventArray.</p> <p>Smoothing is applied within each interval.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedEventArray</code> <p>New BinnedEventArray with smoothed data.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(\n    self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n):\n    \"\"\"Smooth BinnedEventArray by convolving with a Gaussian kernel.\n\n    Smoothing is applied in data, and the same smoothing is applied\n    to each series in a BinnedEventArray.\n\n    Smoothing is applied within each interval.\n\n    Parameters\n    ----------\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n\n    Returns\n    -------\n    out : BinnedEventArray\n        New BinnedEventArray with smoothed data.\n    \"\"\"\n\n    if truncate is None:\n        truncate = 4\n    if sigma is None:\n        sigma = 0.01  # 10 ms default\n\n    fs = 1 / self.ds\n\n    return utils.gaussian_filter(\n        self,\n        fs=fs,\n        sigma=sigma,\n        truncate=truncate,\n        inplace=inplace,\n        within_intervals=within_intervals,\n    )\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.standardize","title":"<code>standardize(inplace=False)</code>","text":"<p>Standardize data (zero mean and unit std deviation).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def standardize(self, inplace=False):\n    \"\"\"Standardize data (zero mean and unit std deviation).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    std = out.std()\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n\n    return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedEventArray.std","title":"<code>std(*, axis=1)</code>","text":"<p>Returns the standard deviation of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def std(self, *, axis=1):\n    \"\"\"Returns the standard deviation of each series in BinnedEventArray.\"\"\"\n    try:\n        stds = np.nanstd(self.data, axis=axis).squeeze()\n        if stds.size == 1:\n            return stds.item()\n        return stds\n    except IndexError:\n        raise IndexError(\n            \"Empty BinnedEventArray; cannot calculate standard deviation\"\n        )\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.BinnedSpikeTrainArray","title":"<code>BinnedSpikeTrainArray</code>","text":"<p>               Bases: <code>BinnedEventArray</code></p> <p>Binned spike train array for analyzing neural spike data.</p> <p>A specialized version of BinnedEventArray designed specifically for spike train analysis. This class bins spike events into discrete time intervals and provides spike train-specific methods and properties through aliased attribute names.</p> <p>Parameters:</p> Name Type Description Default <code>eventarray</code> <code>EventArray or RegularlySampledAnalogSignalArray</code> <p>Input spike train data to be binned.</p> required <code>ds</code> <code>float</code> <p>The bin width, in seconds. Default is 0.0625 (62.5 ms).</p> required <code>empty</code> <code>bool</code> <p>Whether an empty BinnedSpikeTrainArray should be constructed (no data). Default is False.</p> required <code>fs</code> <code>float</code> <p>Sampling rate in Hz. If fs is passed as a parameter, then data is assumed to be in sample numbers instead of actual time values.</p> required <code>support</code> <code>IntervalArray</code> <p>The support (time intervals) over which the spike trains are defined.</p> required <code>unit_ids</code> <code>list of int</code> <p>Unit IDs for each spike train. Default creates sequential IDs starting from 1.</p> required <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Default casts unit_ids to str.</p> required <code>unit_tags</code> <code>optional</code> <p>Tags corresponding to units. Currently accepts any type.</p> required <code>label</code> <code>str</code> <p>Information pertaining to the source of the spike train data. Default is None.</p> required <code>abscissa</code> <code>TemporalAbscissa</code> <p>Object for the time (x-axis) coordinate. Default creates TemporalAbscissa.</p> required <code>ordinate</code> <code>AnalogSignalArrayOrdinate</code> <p>Object for the signal (y-axis) coordinate. Default creates AnalogSignalArrayOrdinate.</p> required <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments passed to the parent BinnedEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BinnedEventArray parent class for additional</code> <code>attributes that are defined there.</code> <code>Spike train-specific attributes (aliases)</code> <code>time</code> <code>array</code> <p>Alias for data. Spike counts in all bins, with shape (n_units, n_bins).</p> <code>n_epochs</code> <code>int</code> <p>Alias for n_intervals. The number of underlying time intervals.</p> <code>n_units</code> <code>int</code> <p>Alias for n_series. The number of units (neurons).</p> <code>n_spikes</code> <code>ndarray</code> <p>Alias for n_events. The number of spikes in each unit.</p> <code>unit_ids</code> <code>list of int</code> <p>Alias for series_ids. Unit IDs contained in the spike train array.</p> <code>unit_labels</code> <code>list of str</code> <p>Alias for series_labels. Labels corresponding to units.</p> <code>unit_tags</code> <p>Alias for series_tags. Tags corresponding to units.</p> <code>Inherited attributes</code> <code>isempty</code> <code>bool</code> <p>Whether the BinnedSpikeTrainArray is empty (no data).</p> <code>bin_centers</code> <code>ndarray</code> <p>The bin centers, in seconds.</p> <code>data</code> <code>np.array, with shape (n_units, n_bins)</code> <p>Spike counts in all bins.</p> <code>bins</code> <code>ndarray</code> <p>The bin edges, in seconds.</p> <code>binned_support</code> <code>np.ndarray, with shape (n_intervals, 2)</code> <p>The binned support of the array (in bin IDs).</p> <code>lengths</code> <code>ndarray</code> <p>Lengths of contiguous segments, in number of bins.</p> <code>eventarray</code> <code>EventArray</code> <p>The original EventArray associated with the binned spike data.</p> <code>n_bins</code> <code>int</code> <p>The number of bins.</p> <code>ds</code> <code>float</code> <p>Bin width, in seconds.</p> <code>n_active</code> <code>int</code> <p>The number of active units. A unit is considered active if it fired at least one spike.</p> <code>n_active_per_bin</code> <code>np.ndarray, with shape (n_bins, )</code> <p>Number of active units per data bin.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the BinnedSpikeTrainArray.</p> <p>Methods:</p> Name Description <code>All methods from BinnedEventArray are available, plus spike train-specific</code> <code>aliases for method names:</code> <code>reorder_units_by_ids</code> <p>Alias for reorder_series_by_ids. Reorder units by their IDs.</p> <code>reorder_units</code> <p>Alias for reorder_series. Reorder units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; # Create a BinnedSpikeTrainArray from spike times\n&gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n&gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n&gt;&gt;&gt; bst = nel.BinnedSpikeTrainArray(sta, ds=0.1)\n&gt;&gt;&gt; print(bst.n_units)\n2\n&gt;&gt;&gt; print(bst.n_bins)\n7\n&gt;&gt;&gt; print(bst.time.shape)  # alias for data\n(2, 7)\n</code></pre> See Also <p>BinnedEventArray : Parent class for general event arrays EventArray : Unbinned event array class SpikeTrainArray : Unbinned spike train array class</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class BinnedSpikeTrainArray(BinnedEventArray):\n    \"\"\"Binned spike train array for analyzing neural spike data.\n\n    A specialized version of BinnedEventArray designed specifically for spike train\n    analysis. This class bins spike events into discrete time intervals and provides\n    spike train-specific methods and properties through aliased attribute names.\n\n    Parameters\n    ----------\n    eventarray : nelpy.EventArray or nelpy.RegularlySampledAnalogSignalArray, optional\n        Input spike train data to be binned.\n    ds : float, optional\n        The bin width, in seconds. Default is 0.0625 (62.5 ms).\n    empty : bool, optional\n        Whether an empty BinnedSpikeTrainArray should be constructed (no data).\n        Default is False.\n    fs : float, optional\n        Sampling rate in Hz. If fs is passed as a parameter, then data\n        is assumed to be in sample numbers instead of actual time values.\n    support : nelpy.IntervalArray, optional\n        The support (time intervals) over which the spike trains are defined.\n    unit_ids : list of int, optional\n        Unit IDs for each spike train. Default creates sequential IDs starting from 1.\n    unit_labels : list of str, optional\n        Labels corresponding to units. Default casts unit_ids to str.\n    unit_tags : optional\n        Tags corresponding to units. Currently accepts any type.\n    label : str, optional\n        Information pertaining to the source of the spike train data.\n        Default is None.\n    abscissa : nelpy.TemporalAbscissa, optional\n        Object for the time (x-axis) coordinate. Default creates TemporalAbscissa.\n    ordinate : nelpy.AnalogSignalArrayOrdinate, optional\n        Object for the signal (y-axis) coordinate. Default creates AnalogSignalArrayOrdinate.\n    **kwargs : optional\n        Additional keyword arguments passed to the parent BinnedEventArray constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BinnedEventArray parent class for additional\n    attributes that are defined there.\n\n    Spike train-specific attributes (aliases):\n    time : np.array\n        Alias for data. Spike counts in all bins, with shape (n_units, n_bins).\n    n_epochs : int\n        Alias for n_intervals. The number of underlying time intervals.\n    n_units : int\n        Alias for n_series. The number of units (neurons).\n    n_spikes : np.ndarray\n        Alias for n_events. The number of spikes in each unit.\n    unit_ids : list of int\n        Alias for series_ids. Unit IDs contained in the spike train array.\n    unit_labels : list of str\n        Alias for series_labels. Labels corresponding to units.\n    unit_tags :\n        Alias for series_tags. Tags corresponding to units.\n\n    Inherited attributes:\n    isempty : bool\n        Whether the BinnedSpikeTrainArray is empty (no data).\n    bin_centers : np.ndarray\n        The bin centers, in seconds.\n    data : np.array, with shape (n_units, n_bins)\n        Spike counts in all bins.\n    bins : np.ndarray\n        The bin edges, in seconds.\n    binned_support : np.ndarray, with shape (n_intervals, 2)\n        The binned support of the array (in bin IDs).\n    lengths : np.ndarray\n        Lengths of contiguous segments, in number of bins.\n    eventarray : nelpy.EventArray\n        The original EventArray associated with the binned spike data.\n    n_bins : int\n        The number of bins.\n    ds : float\n        Bin width, in seconds.\n    n_active : int\n        The number of active units. A unit is considered active if\n        it fired at least one spike.\n    n_active_per_bin : np.ndarray, with shape (n_bins, )\n        Number of active units per data bin.\n    support : nelpy.IntervalArray\n        The support of the BinnedSpikeTrainArray.\n\n    Methods\n    -------\n    All methods from BinnedEventArray are available, plus spike train-specific\n    aliases for method names:\n\n    reorder_units_by_ids(*args, **kwargs)\n        Alias for reorder_series_by_ids. Reorder units by their IDs.\n    reorder_units(*args, **kwargs)\n        Alias for reorder_series. Reorder units.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; # Create a BinnedSpikeTrainArray from spike times\n    &gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n    &gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n    &gt;&gt;&gt; bst = nel.BinnedSpikeTrainArray(sta, ds=0.1)\n    &gt;&gt;&gt; print(bst.n_units)\n    2\n    &gt;&gt;&gt; print(bst.n_bins)\n    7\n    &gt;&gt;&gt; print(bst.time.shape)  # alias for data\n    (2, 7)\n\n    See Also\n    --------\n    BinnedEventArray : Parent class for general event arrays\n    EventArray : Unbinned event array class\n    SpikeTrainArray : Unbinned spike train array class\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        # 'get_event_firing_order' : 'get_spike_firing_order'\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray","title":"<code>EventArray</code>","text":"<p>               Bases: <code>BaseEventArray</code></p> <p>A multiseries eventarray with shared support.</p> <p>Parameters:</p> Name Type Description Default <code>abscissa_vals</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,).</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000.</p> <code>None</code> <code>support</code> <code>IntervalArray</code> <p>IntervalArray on which eventarrays are defined. Default is [0, last event] inclusive.</p> <code>None</code> <code>series_ids</code> <code>list of int</code> <p>Unit IDs.</p> <code>None</code> <code>series_labels</code> <code>list of str</code> <p>Labels corresponding to series. Default casts series_ids to str.</p> <code>None</code> <code>series_tags</code> <code>optional</code> <p>Tags correponding to series. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> <code>None</code> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Whether an empty EventArray should be constructed (no data).</p> <code>False</code> <code>assume_sorted</code> <code>boolean</code> <p>Whether the abscissa values should be treated as sorted (non-decreasing) or not. Significant overhead during RSASA object creation can be removed if this is True, but note that unsorted abscissa values will mess everything up. Default is False</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Additional keyword arguments to forward along to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray superclass for additional</code> <code>attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the EventArray is empty (no data).</p> <code>n_series</code> <code>int</code> <p>The number of series.</p> <code>n_active</code> <code>int</code> <p>The number of active series. A series is considered active if it fired at least one event.</p> <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,).</p> <code>n_events</code> <code>ndarray</code> <p>The number of events in each series.</p> <code>issorted</code> <code>bool</code> <p>Whether the data are sorted.</p> <code>first_event</code> <code>float</code> <p>The time of the very first event, across all series.</p> <code>last_event</code> <code>float</code> <p>The time of the very last event, across all series.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class EventArray(BaseEventArray):\n    \"\"\"A multiseries eventarray with shared support.\n\n    Parameters\n    ----------\n    abscissa_vals : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,).\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000.\n    support : IntervalArray, optional\n        IntervalArray on which eventarrays are defined.\n        Default is [0, last event] inclusive.\n    series_ids : list of int, optional\n        Unit IDs.\n    series_labels : list of str, optional\n        Labels corresponding to series. Default casts series_ids to str.\n    series_tags : optional\n        Tags correponding to series.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    label : str or None, optional\n        Information pertaining to the source of the eventarray.\n    empty : bool, optional\n        Whether an empty EventArray should be constructed (no data).\n    assume_sorted : boolean, optional\n        Whether the abscissa values should be treated as sorted (non-decreasing)\n        or not. Significant overhead during RSASA object creation can be removed\n        if this is True, but note that unsorted abscissa values will mess\n        everything up.\n        Default is False\n    kwargs : optional\n        Additional keyword arguments to forward along to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray superclass for additional\n    attributes that are defined there.\n    isempty : bool\n        Whether the EventArray is empty (no data).\n    n_series : int\n        The number of series.\n    n_active : int\n        The number of active series. A series is considered active if\n        it fired at least one event.\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,).\n    n_events : np.ndarray\n        The number of events in each series.\n    issorted : bool\n        Whether the data are sorted.\n    first_event : np.float\n        The time of the very first event, across all series.\n    last_event : np.float\n        The time of the very last event, across all series.\n    \"\"\"\n\n    __attributes__ = [\"_data\"]\n    __attributes__.extend(BaseEventArray.__attributes__)\n\n    def __init__(\n        self,\n        abscissa_vals=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        series_labels=None,\n        series_tags=None,\n        label=None,\n        empty=False,\n        assume_sorted=None,\n        **kwargs,\n    ):\n        if assume_sorted is None:\n            assume_sorted = False\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.warning(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            data = np.array(data, dtype=object)\n            try:\n                if data.shape[-1] &lt; 2 and np.max(data.shape) &gt; 1:\n                    return True\n                if max(np.array(data).shape[:-1]) &gt; 1 and data.shape[-1] == 1:\n                    return True\n            except (IndexError, TypeError, ValueError):\n                return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            --------\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            try:\n                if isinstance(data[0][0], list) or isinstance(data[0][0], np.ndarray):\n                    logging.info(\"event datas input has too many layers!\")\n                    try:\n                        if max(np.array(data).shape[:-1]) &gt; 1:\n                            #                 singletons = True\n                            return False\n                    except ValueError:\n                        return False\n                    data = np.squeeze(data)\n            except (IndexError, TypeError):\n                pass\n            try:\n                if isinstance(data[1], list) or isinstance(data[1], np.ndarray):\n                    return False\n            except (IndexError, TypeError):\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            if is_single_series(data):\n                return np.array(np.squeeze(data), ndmin=2)\n            if is_singletons(data):\n                data = np.squeeze(data)\n                n = np.max(data.shape)\n                if len(data.shape) == 1:\n                    m = 1\n                else:\n                    m = np.min(data.shape)\n                data = np.reshape(data, (n, m))\n            else:\n                data = np.squeeze(data)\n                if data.dtype == np.dtype(\"O\"):\n                    jagged = True\n                else:\n                    jagged = False\n                if jagged:  # jagged array\n                    # standardize input so that a list of lists is converted\n                    # to an array of arrays:\n                    data = np.array([np.asarray(st) for st in data], dtype=object)\n                else:\n                    data = np.array(data, ndmin=2)\n            return data\n\n        # standardize input data to 2D array\n        data = standardize_to_2d(np.array(abscissa_vals, dtype=object))\n\n        # If user said to assume the absicssa vals are sorted but they actually\n        # aren't, then the mistake will get propagated down. The responsibility\n        # therefore lies on the user whenever he/she uses assume_sorted=True\n        # as a constructor argument\n        for ii, train in enumerate(data):\n            if not assume_sorted:\n                # sort event series, but only if necessary\n                if not utils.is_sorted(train):\n                    data[ii] = np.sort(train)\n            else:\n                data[ii] = np.sort(train)\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n        kwargs[\"series_labels\"] = series_labels\n        kwargs[\"series_tags\"] = series_tags\n        kwargs[\"label\"] = label\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # print(self.type_name, kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            first_spk = np.nanmin(\n                np.array([series[0] for series in data if len(series) != 0])\n            )\n            # BUG: if eventseries is empty np.array([]) then series[-1]\n            # raises an error in the following:\n            # FIX: list[-1] raises an IndexError for an empty list,\n            # whereas list[-1:] returns an empty list.\n            last_spk = np.nanmax(\n                np.array([series[-1:] for series in data if len(series) != 0])\n            )\n            self.support = type(self._abscissa.support)(\n                np.array([first_spk, last_spk + 1 / fs])\n            )\n            # in the above, there's no reason to restrict to support\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            self.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        self._restrict_to_interval(self._abscissa.support, data=data)\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without event datas.\n        Note: the support is left unchanged.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._data = np.array(self.n_series * [None])\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Returns a copy of the EventArray.\"\"\"\n        newcopy = copy.deepcopy(self)\n        newcopy.__renew__()\n        return newcopy\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to EventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty EventArray.\"\"\"\n        try:\n            return np.sum([len(st) for st in self.data]) == 0\n        except TypeError:\n            return True  # this happens when self.data is None\n\n    @property\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        try:\n            return utils.PrettyInt(len(self.data))\n        except TypeError:\n            return 0\n\n    @property\n    def n_active(self):\n        \"\"\"(int) The number of active series.\n\n        A series is considered active if it fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    def flatten(self, *, series_id=None, series_label=None):\n        \"\"\"Collapse events across series.\n\n        WARNING! series_tags are thrown away when flattening.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        series_label (str)\n            (series) Label for event series, default is 'flattened'.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n        if series_label is None:\n            series_label = \"flattened\"\n\n        flattened = self._copy_without_data()\n        flattened._series_ids = [series_id]\n        flattened._series_labels = [series_label]\n        flattened._series_tags = None\n\n        # Efficient: concatenate all events, sort once\n        all_events = np.concatenate(self.data)\n        all_events.sort()\n        flattened._data = np.array([all_events], ndmin=2)\n        flattened.__renew__()\n        return flattened\n\n    def _restrict(self, intervalslice, seriesslice, *, subseriesslice=None):\n        self._restrict_to_series_subset(seriesslice)\n        self._restrict_to_interval(intervalslice)\n        return self\n\n    def _restrict_to_series_subset(self, idx):\n        # Warning: This function can mutate data\n\n        # TODO: Update tags\n        try:\n            self._data = self._data[idx]\n            singleseries = len(self._data) == 1\n            if singleseries:\n                self._data = np.array(self._data[0], ndmin=2)\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except AttributeError:\n            self._data = self._data[idx]\n            singleseries = len(self._data) == 1\n            if singleseries:\n                self._data = np.array(self._data[0], ndmin=2)\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except IndexError:\n            raise IndexError(\n                \"One of more indices were out of bounds for n_series with size {}\".format(\n                    self.n_series\n                )\n            )\n        except Exception:\n            raise TypeError(\"Unsupported indexing type {}\".format(type(idx)))\n\n        return self\n\n    def _restrict_to_interval(self, intervalslice, *, data=None):\n        \"\"\"Return data restricted to an intervalarray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : nelpy.IntervalArray\n        \"\"\"\n\n        # Warning: this function can mutate data\n        # This should be called from _restrict only. That's where\n        # intervalarray is first checked against the support.\n        # This function assumes that has happened already, so\n        # every point in intervalarray is also in the support\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n\n        if data is None:\n            data = self._data\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                # no restriction on interval\n                return self\n\n        newintervals = self._abscissa.support[intervalslice].merge()\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=True)\n\n        issue_warning = False\n        if not self.isempty:\n            for series, evt_data in enumerate(data):\n                indices = []\n                for epdata in newintervals.data:\n                    t_start = epdata[0]\n                    t_stop = epdata[1]\n                    frm, to = np.searchsorted(evt_data, (t_start, t_stop))\n                    indices.append((frm, to))\n                indices = np.array(indices, ndmin=2)\n                if np.diff(indices).sum() &lt; len(evt_data):\n                    issue_warning = True\n                singleseries = len(self._data) == 1\n                if singleseries:\n                    data_list = []\n                    for start, stop in indices:\n                        data_list.extend(evt_data[start:stop])\n                    data = np.array(data_list, ndmin=2)\n                else:\n                    # here we have to do some annoying conversion between\n                    # arrays and lists to fully support jagged array\n                    # mutation\n                    data_list = []\n                    for start, stop in indices:\n                        data_list.extend(evt_data[start:stop])\n                    data_ = data.tolist()  # this creates copy\n                    data_[series] = np.array(data_list)\n                    data = utils.ragged_array(data_)\n            self._data = data\n            if issue_warning:\n                logging.warning(\"ignoring events outside of eventarray support\")\n\n        self._abscissa.support = newintervals\n        return self\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        if self.label is not None:\n            labelstr = \" from %s\" % self.label\n        else:\n            labelstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s%s\" % (\n            self.type_name,\n            address_str,\n            numstr,\n            epstr,\n            fsstr,\n            labelstr,\n        )\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a binned eventarray.\"\"\"\n        return BinnedEventArray(self, ds=ds)\n\n    @property\n    def data(self):\n        \"\"\"Event datas in seconds.\"\"\"\n        return self._data\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.data])\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Sorted EventArray.\"\"\"\n        if self.isempty:\n            return True\n        return np.array([utils.is_sorted(eventarray) for eventarray in self.data]).all()\n\n    def _reorder_series_by_idx(self, neworder, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,)\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            out._series_labels[frm], out._series_labels[to] = (\n                out._series_labels[to],\n                out._series_labels[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def reorder_series(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        raise DeprecationWarning(\n            \"reorder_series has been deprecated. Use reorder_series_by_id(x/s) instead!\"\n        )\n\n    def reorder_series_by_ids(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        neworder = [self.series_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            out._series_labels[frm], out._series_labels[to] = (\n                out._series_labels[to],\n                out._series_labels[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def get_event_firing_order(self):\n        \"\"\"Returns a list of series_ids such that the series are ordered\n        by when they first fire in the EventArray.\n\n        Return\n        ------\n        firing_order : list of series_ids\n        \"\"\"\n\n        first_events = [\n            (ii, series[0]) for (ii, series) in enumerate(self.data) if len(series) != 0\n        ]\n        first_events_series_ids = np.array(self.series_ids)[\n            [fs[0] for fs in first_events]\n        ]\n        first_events_datas = np.array([fs[1] for fs in first_events])\n        sortorder = np.argsort(first_events_datas)\n        first_events_series_ids = first_events_series_ids[sortorder]\n        remaining_ids = list(set(self.series_ids) - set(first_events_series_ids))\n        firing_order = list(first_events_series_ids)\n        firing_order.extend(remaining_ids)\n\n        return firing_order\n\n    @property\n    def first_event(self):\n        \"\"\"Returns the [time of the] first event across all series.\"\"\"\n        first = np.inf\n        for series in self.data:\n            if series[0] &lt; first:\n                first = series[0]\n        return first\n\n    @property\n    def last_event(self):\n        \"\"\"Returns the [time of the] last event across all series.\"\"\"\n        last = -np.inf\n        for series in self.data:\n            if series[-1] &gt; last:\n                last = series[-1]\n        return last\n\n    def empty(self, *, inplace=False):\n        \"\"\"Remove data (but not metadata) from EventArray.\n\n        Attributes 'data', and 'support' are both emptied.\n\n        Note: n_series, series_ids, etc. are all preserved.\n        \"\"\"\n        if not inplace:\n            out = self._copy_without_data()\n            out._abscissa.support = type(self._abscissa.support)(empty=True)\n            return out\n        out = self\n        out._data = np.array(self.n_series * [None])\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        out.__renew__()\n        return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Event datas in seconds.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.first_event","title":"<code>first_event</code>  <code>property</code>","text":"<p>Returns the [time of the] first event across all series.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty EventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Sorted EventArray.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.last_event","title":"<code>last_event</code>  <code>property</code>","text":"<p>Returns the [time of the] last event across all series.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>(int) The number of active series.</p> <p>A series is considered active if it fired at least one event.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.n_series","title":"<code>n_series</code>  <code>property</code>","text":"<p>(int) The number of series.</p>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a binned eventarray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a binned eventarray.\"\"\"\n    return BinnedEventArray(self, ds=ds)\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the EventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of the EventArray.\"\"\"\n    newcopy = copy.deepcopy(self)\n    newcopy.__renew__()\n    return newcopy\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.empty","title":"<code>empty(*, inplace=False)</code>","text":"<p>Remove data (but not metadata) from EventArray.</p> <p>Attributes 'data', and 'support' are both emptied.</p> <p>Note: n_series, series_ids, etc. are all preserved.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def empty(self, *, inplace=False):\n    \"\"\"Remove data (but not metadata) from EventArray.\n\n    Attributes 'data', and 'support' are both emptied.\n\n    Note: n_series, series_ids, etc. are all preserved.\n    \"\"\"\n    if not inplace:\n        out = self._copy_without_data()\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        return out\n    out = self\n    out._data = np.array(self.n_series * [None])\n    out._abscissa.support = type(self._abscissa.support)(empty=True)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.flatten","title":"<code>flatten(*, series_id=None, series_label=None)</code>","text":"<p>Collapse events across series.</p> <p>WARNING! series_tags are thrown away when flattening.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> <code>series_label</code> <p>(series) Label for event series, default is 'flattened'.</p> <code>None</code> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def flatten(self, *, series_id=None, series_label=None):\n    \"\"\"Collapse events across series.\n\n    WARNING! series_tags are thrown away when flattening.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    series_label (str)\n        (series) Label for event series, default is 'flattened'.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n    if series_label is None:\n        series_label = \"flattened\"\n\n    flattened = self._copy_without_data()\n    flattened._series_ids = [series_id]\n    flattened._series_labels = [series_label]\n    flattened._series_tags = None\n\n    # Efficient: concatenate all events, sort once\n    all_events = np.concatenate(self.data)\n    all_events.sort()\n    flattened._data = np.array([all_events], ndmin=2)\n    flattened.__renew__()\n    return flattened\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.get_event_firing_order","title":"<code>get_event_firing_order()</code>","text":"<p>Returns a list of series_ids such that the series are ordered by when they first fire in the EventArray.</p> Return <p>firing_order : list of series_ids</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def get_event_firing_order(self):\n    \"\"\"Returns a list of series_ids such that the series are ordered\n    by when they first fire in the EventArray.\n\n    Return\n    ------\n    firing_order : list of series_ids\n    \"\"\"\n\n    first_events = [\n        (ii, series[0]) for (ii, series) in enumerate(self.data) if len(series) != 0\n    ]\n    first_events_series_ids = np.array(self.series_ids)[\n        [fs[0] for fs in first_events]\n    ]\n    first_events_datas = np.array([fs[1] for fs in first_events])\n    sortorder = np.argsort(first_events_datas)\n    first_events_series_ids = first_events_series_ids[sortorder]\n    remaining_ids = list(set(self.series_ids) - set(first_events_series_ids))\n    firing_order = list(first_events_series_ids)\n    firing_order.extend(remaining_ids)\n\n    return firing_order\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.reorder_series","title":"<code>reorder_series(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def reorder_series(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    raise DeprecationWarning(\n        \"reorder_series has been deprecated. Use reorder_series_by_id(x/s) instead!\"\n    )\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.EventArray.reorder_series_by_ids","title":"<code>reorder_series_by_ids(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def reorder_series_by_ids(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    neworder = [self.series_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        utils.swap_rows(out._data, frm, to)\n        out._series_ids[frm], out._series_ids[to] = (\n            out._series_ids[to],\n            out._series_ids[frm],\n        )\n        out._series_labels[frm], out._series_labels[to] = (\n            out._series_labels[to],\n            out._series_labels[frm],\n        )\n        # TODO: re-build series tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.SpikeTrainArray","title":"<code>SpikeTrainArray</code>","text":"<p>               Bases: <code>EventArray</code></p> <p>A multiseries spike train array with shared support.</p> <p>SpikeTrainArray is a specialized EventArray for handling neural spike train data. It provides unit-specific aliases and methods for analyzing spike timing data across multiple recording units with a common temporal support.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000.</p> required <code>support</code> <code>IntervalArray</code> <p>IntervalArray on which spike trains are defined. Default is [0, last spike] inclusive.</p> required <code>unit_ids</code> <code>list of int</code> <p>Unit IDs. Alias for series_ids.</p> required <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Default casts unit_ids to str. Alias for series_labels.</p> required <code>unit_tags</code> <code>optional</code> <p>Tags corresponding to units. Alias for series_tags. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> required <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the spike train array.</p> required <code>empty</code> <code>bool</code> <p>Whether an empty SpikeTrainArray should be constructed (no data).</p> required <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments forwarded to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray and EventArray superclasses</code> <code>for additional attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the SpikeTrainArray is empty (no data).</p> <code>n_units</code> <code>int</code> <p>The number of units. Alias for n_series.</p> <code>n_active</code> <code>int</code> <p>The number of active units. A unit is considered active if it fired at least one spike.</p> <code>time</code> <code>array of np.array(dtype=np.float64)</code> <p>Spike time data in seconds. Array of length n_units, each entry with shape (n_spikes,). Alias for data.</p> <code>n_spikes</code> <code>ndarray</code> <p>The number of spikes in each unit. Alias for n_events.</p> <code>issorted</code> <code>bool</code> <p>Whether the spike times are sorted.</p> <code>first_spike</code> <code>float</code> <p>The time of the very first spike, across all units.</p> <code>last_spike</code> <code>float</code> <p>The time of the very last spike, across all units.</p> <code>unit_ids</code> <code>list of int</code> <p>Unit IDs. Alias for series_ids.</p> <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Alias for series_labels.</p> <code>unit_tags</code> <p>Tags corresponding to units. Alias for series_tags.</p> <code>n_epochs</code> <code>int</code> <p>The number of epochs/intervals. Alias for n_intervals.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the SpikeTrainArray.</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz).</p> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the spike train array.</p> <p>Methods:</p> Name Description <code>bin</code> <p>Return a BinnedSpikeTrainArray.</p> <code>get_spike_firing_order</code> <p>Returns unit_ids ordered by when they first fire.</p> <code>reorder_units_by_ids</code> <p>Reorder units according to specified unit_ids.</p> <code>flatten</code> <p>Collapse spikes across units into a single unit.</p> <code>partition</code> <p>Returns a SpikeTrainArray whose support has been partitioned.</p> <code>copy</code> <p>Returns a copy of the SpikeTrainArray.</p> <code>empty</code> <p>Remove data (but not metadata) from SpikeTrainArray.</p> <p>Examples:</p> <p>Create a SpikeTrainArray with two units:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n&gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n&gt;&gt;&gt; print(sta.n_units)\n2\n&gt;&gt;&gt; print(sta.n_spikes)\n[3 3]\n</code></pre> <p>Access spike times using the time alias:</p> <pre><code>&gt;&gt;&gt; print(sta.time[0])  # First unit's spike times\n[0.1 0.3 0.7]\n</code></pre> <p>Create a binned version:</p> <pre><code>&gt;&gt;&gt; binned = sta.bin(ds=0.1)  # 100ms bins\n</code></pre> Notes <p>SpikeTrainArray provides neuroscience-specific aliases for EventArray functionality. For example, 'units' instead of 'series', 'spikes' instead of 'events', and 'time' instead of 'data'. This makes the API more intuitive for neuroscience applications while maintaining full compatibility with the underlying EventArray infrastructure.</p> <p>The class automatically handles spike time sorting and provides efficient methods for common spike train analyses. All spike times are stored in seconds and should be non-negative values within the support interval.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class SpikeTrainArray(EventArray):\n    \"\"\"A multiseries spike train array with shared support.\n\n    SpikeTrainArray is a specialized EventArray for handling neural spike train data.\n    It provides unit-specific aliases and methods for analyzing spike timing data\n    across multiple recording units with a common temporal support.\n\n    Parameters\n    ----------\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000.\n    support : IntervalArray, optional\n        IntervalArray on which spike trains are defined.\n        Default is [0, last spike] inclusive.\n    unit_ids : list of int, optional\n        Unit IDs. Alias for series_ids.\n    unit_labels : list of str, optional\n        Labels corresponding to units. Default casts unit_ids to str.\n        Alias for series_labels.\n    unit_tags : optional\n        Tags corresponding to units. Alias for series_tags.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    label : str or None, optional\n        Information pertaining to the source of the spike train array.\n    empty : bool, optional\n        Whether an empty SpikeTrainArray should be constructed (no data).\n    **kwargs : optional\n        Additional keyword arguments forwarded to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray and EventArray superclasses\n    for additional attributes that are defined there.\n\n    isempty : bool\n        Whether the SpikeTrainArray is empty (no data).\n    n_units : int\n        The number of units. Alias for n_series.\n    n_active : int\n        The number of active units. A unit is considered active if\n        it fired at least one spike.\n    time : array of np.array(dtype=np.float64)\n        Spike time data in seconds. Array of length n_units, each entry with\n        shape (n_spikes,). Alias for data.\n    n_spikes : np.ndarray\n        The number of spikes in each unit. Alias for n_events.\n    issorted : bool\n        Whether the spike times are sorted.\n    first_spike : float\n        The time of the very first spike, across all units.\n    last_spike : float\n        The time of the very last spike, across all units.\n    unit_ids : list of int\n        Unit IDs. Alias for series_ids.\n    unit_labels : list of str\n        Labels corresponding to units. Alias for series_labels.\n    unit_tags :\n        Tags corresponding to units. Alias for series_tags.\n    n_epochs : int\n        The number of epochs/intervals. Alias for n_intervals.\n    support : IntervalArray\n        The support of the SpikeTrainArray.\n    fs : float\n        Sampling frequency (Hz).\n    label : str or None\n        Information pertaining to the source of the spike train array.\n\n    Methods\n    -------\n    bin(ds=None)\n        Return a BinnedSpikeTrainArray.\n    get_spike_firing_order()\n        Returns unit_ids ordered by when they first fire.\n    reorder_units_by_ids(neworder, inplace=False)\n        Reorder units according to specified unit_ids.\n    flatten(unit_id=None, unit_label=None)\n        Collapse spikes across units into a single unit.\n    partition(ds=None, n_epochs=None)\n        Returns a SpikeTrainArray whose support has been partitioned.\n    copy()\n        Returns a copy of the SpikeTrainArray.\n    empty(inplace=False)\n        Remove data (but not metadata) from SpikeTrainArray.\n\n    Examples\n    --------\n    Create a SpikeTrainArray with two units:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n    &gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n    &gt;&gt;&gt; print(sta.n_units)\n    2\n    &gt;&gt;&gt; print(sta.n_spikes)\n    [3 3]\n\n    Access spike times using the time alias:\n\n    &gt;&gt;&gt; print(sta.time[0])  # First unit's spike times\n    [0.1 0.3 0.7]\n\n    Create a binned version:\n\n    &gt;&gt;&gt; binned = sta.bin(ds=0.1)  # 100ms bins\n\n    Notes\n    -----\n    SpikeTrainArray provides neuroscience-specific aliases for EventArray\n    functionality. For example, 'units' instead of 'series', 'spikes' instead\n    of 'events', and 'time' instead of 'data'. This makes the API more intuitive\n    for neuroscience applications while maintaining full compatibility with the\n    underlying EventArray infrastructure.\n\n    The class automatically handles spike time sorting and provides efficient\n    methods for common spike train analyses. All spike times are stored in\n    seconds and should be non-negative values within the support interval.\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"units\"\n        kwargs[\"series_label\"] = series_label\n\n        # legacy STA constructor support for backward compatibility\n        kwargs = legacySTAkwargs(**kwargs)\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n\n    # @keyword_equivalence(this_or_that={'n_intervals':'n_epochs'})\n    # def partition(self, ds=None, n_intervals=None, n_epochs=None):\n    #     if n_intervals is None:\n    #         n_intervals = n_epochs\n    #     kwargs = {'ds':ds, 'n_intervals': n_intervals}\n    #     return super().partition(**kwargs)\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n        return BinnedSpikeTrainArray(self, ds=ds)  # TODO #FIXME\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.SpikeTrainArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedSpikeTrainArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n    return BinnedSpikeTrainArray(self, ds=ds)  # TODO #FIXME\n</code></pre>"},{"location":"reference/core/eventarray/#nelpy.core._eventarray.legacySTAkwargs","title":"<code>legacySTAkwargs(**kwargs)</code>","text":"<p>Provide support for legacy SpikeTrainArray kwargs. This function is primarily intended to be a helper for the new STA constructor, not for general-purpose use.</p> <p>kwarg: time        &lt;==&gt; timestamps &lt;==&gt; abscissa_vals kwarg: data        &lt;==&gt; ydata kwarg: unit_ids    &lt;==&gt; series_ids kwarg: unit_labels &lt;==&gt; series_labels kwarg: unit_tags   &lt;==&gt; series_tags</p> <p>Examples:</p> <p>sta = nel.SpikeTrainArray(time=..., ) sta = nel.SpikeTrainArray(timestamps=..., ) sta = nel.SpikeTrainArray(abscissa_vals=..., )</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def legacySTAkwargs(**kwargs):\n    \"\"\"Provide support for legacy SpikeTrainArray\n    kwargs. This function is primarily intended to be\n    a helper for the new STA constructor, not for\n    general-purpose use.\n\n    kwarg: time        &lt;==&gt; timestamps &lt;==&gt; abscissa_vals\n    kwarg: data        &lt;==&gt; ydata\n    kwarg: unit_ids    &lt;==&gt; series_ids\n    kwarg: unit_labels &lt;==&gt; series_labels\n    kwarg: unit_tags   &lt;==&gt; series_tags\n\n    Examples\n    --------\n    sta = nel.SpikeTrainArray(time=..., )\n    sta = nel.SpikeTrainArray(timestamps=..., )\n    sta = nel.SpikeTrainArray(abscissa_vals=..., )\n    \"\"\"\n\n    def only_one_of(*args):\n        num_non_null_args = 0\n        out = None\n        for arg in args:\n            if arg is not None:\n                num_non_null_args += 1\n                out = arg\n        if num_non_null_args &gt; 1:\n            raise ValueError(\"multiple conflicting arguments received\")\n        return out\n\n    # legacy STA constructor support for backward compatibility\n    abscissa_vals = kwargs.pop(\"abscissa_vals\", None)\n    timestamps = kwargs.pop(\"timestamps\", None)\n    time = kwargs.pop(\"time\", None)\n    # only one of the above, otherwise raise exception\n    abscissa_vals = only_one_of(abscissa_vals, timestamps, time)\n    if abscissa_vals is not None:\n        kwargs[\"abscissa_vals\"] = abscissa_vals\n\n    # Other legacy attributes\n    series_ids = kwargs.pop(\"series_ids\", None)\n    unit_ids = kwargs.pop(\"unit_ids\", None)\n    series_ids = only_one_of(series_ids, unit_ids)\n    kwargs[\"series_ids\"] = series_ids\n\n    series_labels = kwargs.pop(\"series_labels\", None)\n    unit_labels = kwargs.pop(\"unit_labels\", None)\n    series_labels = only_one_of(series_labels, unit_labels)\n    kwargs[\"series_labels\"] = series_labels\n\n    series_tags = kwargs.pop(\"series_tags\", None)\n    unit_tags = kwargs.pop(\"unit_tags\", None)\n    series_tags = only_one_of(series_tags, unit_tags)\n    kwargs[\"series_tags\"] = series_tags\n\n    return kwargs\n</code></pre>"},{"location":"reference/core/intervalarray/","title":"IntervalArray","text":""},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.EpochArray","title":"<code>EpochArray</code>","text":"<p>               Bases: <code>IntervalArray</code></p> <p>IntervalArray containing temporal intervals (epochs, in seconds).</p> <p>This class extends <code>IntervalArray</code> to specifically handle time-based intervals, referred to as epochs. It provides aliases for common time-related attributes and uses a <code>PrettyDuration</code> formatter for displaying lengths.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_epochs, 1) or (n_epochs,), the start time for each epoch (which then requires a <code>length</code> to be specified). If shape (n_epochs, 2), the start and stop times for each epoch. Defaults to None, creating an empty <code>EpochArray</code>.</p> required <code>length</code> <code>np.array, float, or None</code> <p>The duration of the epoch (in base units, seconds). If a float, the same duration is assumed for every epoch. Only used if <code>data</code> is a 1D array of start times.</p> required <code>meta</code> <code>dict</code> <p>Metadata associated with the epoch array.</p> required <code>empty</code> <code>bool</code> <p>If True, an empty <code>EpochArray</code> is returned, ignoring <code>data</code> and <code>length</code>. Defaults to False.</p> required <code>domain</code> <code>IntervalArray</code> <p>The domain within which the epochs are defined. If None, it defaults to an infinite domain.</p> required <code>label</code> <code>str</code> <p>A descriptive label for the epoch array.</p> required <p>Attributes:</p> Name Type Description <code>time</code> <code>array</code> <p>Alias for <code>data</code>. The start and stop times for each epoch, with shape (n_epochs, 2).</p> <code>n_epochs</code> <code>int</code> <p>Alias for <code>n_intervals</code>. The number of epochs in the array.</p> <code>duration</code> <code>float</code> <p>Alias for <code>length</code>. The total duration of the [merged] epoch array.</p> <code>durations</code> <code>array</code> <p>Alias for <code>lengths</code>. The duration of each individual epoch.</p> <code>formatter</code> <code>PrettyDuration</code> <p>The formatter used for displaying time durations.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the intervals, which is 's' (seconds) for EpochArray.</p> Notes <p>This class inherits all methods and properties from <code>IntervalArray</code>. Aliases are provided for convenience to make the API more intuitive for temporal data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy.core import EpochArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create an EpochArray from start and stop times\n&gt;&gt;&gt; epochs = EpochArray(data=np.array([[0, 10], [20, 30], [40, 50]]))\n&gt;&gt;&gt; print(epochs)\n&lt;EpochArray at 0x21b641f0950: 3 epochs&gt; of length 30 seconds\n</code></pre> <pre><code>&gt;&gt;&gt; # Create an EpochArray from start times and a common length\n&gt;&gt;&gt; starts = np.array([0, 20, 40])\n&gt;&gt;&gt; length = 5.0\n&gt;&gt;&gt; epochs_with_length = EpochArray(data=starts, length=length)\n&gt;&gt;&gt; print(epochs_with_length)\n&lt;EpochArray at 0x21b631c6050: 3 epochs&gt; of length 15 seconds\n</code></pre> <pre><code>&gt;&gt;&gt; # Accessing aliased attributes\n&gt;&gt;&gt; print(f\"Number of epochs: {epochs.n_epochs}\")\nNumber of epochs: 3\n&gt;&gt;&gt; print(f\"Total duration: {epochs.duration}\")\nTotal duration: 30 seconds\n&gt;&gt;&gt; print(f\"Individual durations: {epochs.durations}\")\nIndividual durations: [10 10 10]\n</code></pre> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class EpochArray(IntervalArray):\n    \"\"\"IntervalArray containing temporal intervals (epochs, in seconds).\n\n    This class extends `IntervalArray` to specifically handle time-based\n    intervals, referred to as epochs. It provides aliases for common\n    time-related attributes and uses a `PrettyDuration` formatter for\n    displaying lengths.\n\n    Parameters\n    ----------\n    data : np.array, optional\n        If shape (n_epochs, 1) or (n_epochs,), the start time for each\n        epoch (which then requires a `length` to be specified).\n        If shape (n_epochs, 2), the start and stop times for each epoch.\n        Defaults to None, creating an empty `EpochArray`.\n    length : np.array, float, or None, optional\n        The duration of the epoch (in base units, seconds). If a float,\n        the same duration is assumed for every epoch. Only used if `data`\n        is a 1D array of start times.\n    meta : dict, optional\n        Metadata associated with the epoch array.\n    empty : bool, optional\n        If True, an empty `EpochArray` is returned, ignoring `data` and `length`.\n        Defaults to False.\n    domain : IntervalArray, optional\n        The domain within which the epochs are defined. If None, it defaults\n        to an infinite domain.\n    label : str, optional\n        A descriptive label for the epoch array.\n\n    Attributes\n    ----------\n    time : np.array\n        Alias for `data`. The start and stop times for each epoch, with shape\n        (n_epochs, 2).\n    n_epochs : int\n        Alias for `n_intervals`. The number of epochs in the array.\n    duration : float\n        Alias for `length`. The total duration of the [merged] epoch array.\n    durations : np.array\n        Alias for `lengths`. The duration of each individual epoch.\n    formatter : formatters.PrettyDuration\n        The formatter used for displaying time durations.\n    base_unit : str\n        The base unit of the intervals, which is 's' (seconds) for EpochArray.\n\n    Notes\n    -----\n    This class inherits all methods and properties from `IntervalArray`.\n    Aliases are provided for convenience to make the API more intuitive\n    for temporal data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy.core import EpochArray\n\n    &gt;&gt;&gt; # Create an EpochArray from start and stop times\n    &gt;&gt;&gt; epochs = EpochArray(data=np.array([[0, 10], [20, 30], [40, 50]]))\n    &gt;&gt;&gt; print(epochs)\n    &lt;EpochArray at 0x21b641f0950: 3 epochs&gt; of length 30 seconds\n\n    &gt;&gt;&gt; # Create an EpochArray from start times and a common length\n    &gt;&gt;&gt; starts = np.array([0, 20, 40])\n    &gt;&gt;&gt; length = 5.0\n    &gt;&gt;&gt; epochs_with_length = EpochArray(data=starts, length=length)\n    &gt;&gt;&gt; print(epochs_with_length)\n    &lt;EpochArray at 0x21b631c6050: 3 epochs&gt; of length 15 seconds\n\n    &gt;&gt;&gt; # Accessing aliased attributes\n    &gt;&gt;&gt; print(f\"Number of epochs: {epochs.n_epochs}\")\n    Number of epochs: 3\n    &gt;&gt;&gt; print(f\"Total duration: {epochs.duration}\")\n    Total duration: 30 seconds\n    &gt;&gt;&gt; print(f\"Individual durations: {epochs.durations}\")\n    Individual durations: [10 10 10]\n    \"\"\"\n\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"duration\": \"length\",\n        \"durations\": \"lengths\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n        self._interval_label = \"epoch\"\n        self.formatter = formatters.PrettyDuration\n        self.base_unit = self.formatter.base_unit\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray","title":"<code>IntervalArray</code>","text":"<p>An array of intervals, where each interval has a start and stop.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_intervals, 1) or (n_intervals,), the start value for each interval (which then requires a length to be specified). If shape (n_intervals, 2), the start and stop values for each interval.</p> <code>None</code> <code>length</code> <code>np.array, float, or None</code> <p>The length of the interval (in base units). If (float) then the same length is assumed for every interval.</p> <code>None</code> <code>meta</code> <code>dict</code> <p>Metadata associated with spiketrain.</p> <code>None</code> <code>domain</code> <code>IntervalArray ??? This is pretty meta @-@</code> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>array</code> <p>The start and stop values for each interval. With shape (n_intervals, 2).</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class IntervalArray:\n    \"\"\"An array of intervals, where each interval has a start and stop.\n\n    Parameters\n    ----------\n    data : np.array\n        If shape (n_intervals, 1) or (n_intervals,), the start value for each\n        interval (which then requires a length to be specified).\n        If shape (n_intervals, 2), the start and stop values for each interval.\n    length : np.array, float, or None, optional\n        The length of the interval (in base units). If (float) then the same\n        length is assumed for every interval.\n    meta : dict, optional\n        Metadata associated with spiketrain.\n    domain : IntervalArray ??? This is pretty meta @-@\n\n    Attributes\n    ----------\n    data : np.array\n        The start and stop values for each interval. With shape (n_intervals, 2).\n    \"\"\"\n\n    __aliases__ = {}\n    __attributes__ = [\"_data\", \"_meta\", \"_domain\"]\n\n    def __init__(\n        self,\n        data=None,\n        *args,\n        length=None,\n        meta=None,\n        empty=False,\n        domain=None,\n        label=None,\n    ):\n        self.__version__ = version.__version__\n\n        self.type_name = self.__class__.__name__\n        self._interval_label = \"interval\"\n        self.formatter = formatters.ArbitraryFormatter\n        self.base_unit = self.formatter.base_unit\n\n        if len(args) &gt; 1:\n            raise TypeError(\n                \"__init__() takes from 1 to 3 positional arguments but 4 were given\"\n            )\n        elif len(args) == 1:\n            data = [data, args[0]]\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        data = np.squeeze(data)  # coerce data into np.array\n\n        # all possible inputs:\n        # 1. single interval, no length    --- OK\n        # 2. single interval and length    --- ERR\n        # 3. multiple intervals, no length --- OK\n        # 4. multiple intervals and length --- ERR\n        # 5. single scalar and length   --- OK\n        # 6. scalar list and duratin list --- OK\n        #\n        # Q. won't np.squeeze make our life difficult?\n        #\n        # Strategy: determine if length was passed. If so, try to see\n        # if data can be coerced into right shape. If not, raise\n        # error.\n        # If length was NOT passed, then do usual checks for intervals.\n\n        if length is not None:  # assume we received scalar starts\n            data = np.array(data, ndmin=1)\n            length = np.squeeze(length).astype(float)\n            if length.ndim == 0:\n                length = length[..., np.newaxis]\n\n            if data.ndim == 2 and length.ndim == 1:\n                raise ValueError(\"length not allowed when using start and stop values\")\n\n            if len(length) &gt; 1:\n                if data.ndim == 1 and data.shape[0] != length.shape[0]:\n                    raise ValueError(\"must have same number of data and length data\")\n            if data.ndim == 1 and length.ndim == 1:\n                stop_interval = data + length\n                data = np.hstack(\n                    (data[..., np.newaxis], stop_interval[..., np.newaxis])\n                )\n        else:  # length was not specified, so assume we recived intervals\n            # Note: if we have an empty array of data with no\n            # dimension, then calling len(data) will return a\n            # TypeError.\n            try:\n                # if no data were received, return an empty IntervalArray:\n                if len(data) == 0:\n                    self.__init__(empty=True)\n                    return\n            except TypeError:\n                logging.warning(\n                    \"unsupported type (\"\n                    + str(type(data))\n                    + \"); creating empty {}\".format(self.type_name)\n                )\n                self.__init__(empty=True)\n                return\n\n            # Only one interval is given eg IntervalArray([3,5,6,10]) with no\n            # length and more than two values:\n            if data.ndim == 1 and len(data) &gt; 2:  # we already know length is None\n                raise TypeError(\n                    \"data of size (n_intervals, ) has to be accompanied by a length\"\n                )\n\n            if data.ndim == 1:  # and length is None:\n                data = np.array([data])\n\n        if data.ndim &gt; 2:\n            raise ValueError(\"data must be a 1D or a 2D vector\")\n\n        try:\n            if data[:, 0].shape[0] != data[:, 1].shape[0]:\n                raise ValueError(\"must have the same number of start and stop values\")\n        except Exception:\n            raise Exception(\"Unhandled {}.__init__ case.\".format(self.type_name))\n\n        # TODO: what if start == stop? what will this break? This situation\n        # can arise automatically when slicing a spike train with one or no\n        # spikes, for example in which case the automatically inferred support\n        # is a delta dirac\n\n        if data.ndim == 2 and np.any(data[:, 1] - data[:, 0] &lt; 0):\n            raise ValueError(\"start must be less than or equal to stop\")\n\n        # potentially assign domain\n        self._domain = domain\n\n        self._data = data\n        self._meta = meta\n        self.label = label\n\n        if not self.issorted:\n            self._sort()\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self.n_intervals &gt; 1:\n            nstr = \"%s %ss\" % (self.n_intervals, self._interval_label)\n        else:\n            nstr = \"1 %s\" % self._interval_label\n        dstr = \"of length {}\".format(self.formatter(self.length))\n        return \"&lt;%s%s: %s&gt; %s\" % (self.type_name, address_str, nstr, dstr)\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without data.\"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._data = np.zeros((self.n_intervals, 2))\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        return out\n\n    def __iter__(self):\n        \"\"\"IntervalArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"IntervalArray iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_intervals - 1:\n            raise StopIteration\n\n        intervalarray = self._copy_without_data()\n        intervalarray._data = np.array([self.data[index, :]])\n        self._index += 1\n        return intervalarray\n\n    def __getitem__(self, *idx):\n        \"\"\"IntervalArray index access.\n\n        Accepts integers, slices, and IntervalArrays.\n        \"\"\"\n        if self.isempty:\n            return self\n\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        if isinstance(idx, type(self)):\n            if idx.isempty:  # case 0:\n                return type(self)(empty=True)\n            return self.intersect(interval=idx)\n        elif isinstance(idx, IntervalArray):\n            raise TypeError(\n                \"Error taking intersection. {} expected, but got {}\".format(\n                    self.type_name, idx.type_name\n                )\n            )\n        else:\n            try:  # works for ints, lists, and slices\n                out = self.copy()\n                out._data = self.data[idx, :]\n            except IndexError:\n                raise IndexError(\"{} index out of range\".format(self.type_name))\n            except Exception:\n                raise TypeError(\"unsupported subscripting type {}\".format(type(idx)))\n        return out\n\n    def __add__(self, other):\n        \"\"\"add length to start and stop of each interval, or join two interval arrays without merging\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            return new.expand(other, direction=\"both\")\n        elif isinstance(other, type(self)):\n            return self.join(other)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __sub__(self, other):\n        \"\"\"subtract length from start and stop of each interval\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            return new.shrink(other, direction=\"both\")\n        elif isinstance(other, type(self)):\n            # A - B = A intersect ~B\n            return self.intersect(~other)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __mul__(self, other):\n        \"\"\"expand (&gt;1) or shrink (&lt;1) interval lengths\"\"\"\n        raise NotImplementedError(\"operator * not yet implemented\")\n\n    def __truediv__(self, other):\n        \"\"\"expand (&gt;1) or shrink (&gt;1) interval lengths\"\"\"\n        raise NotImplementedError(\"operator / not yet implemented\")\n\n    def __lshift__(self, other):\n        \"\"\"shift data to left (&lt;&lt;)\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            new._data = new._data - other\n            if new.domain.is_finite:\n                new.domain._data = new.domain._data - other\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &lt;&lt;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __rshift__(self, other):\n        \"\"\"shift data to right (&gt;&gt;)\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            new._data = new._data + other\n            if new.domain.is_finite:\n                new.domain._data = new.domain._data + other\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &gt;&gt;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __and__(self, other):\n        \"\"\"intersection of interval arrays\"\"\"\n        if isinstance(other, type(self)):\n            new = copy.copy(self)\n            return new.intersect(other, boundaries=True)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &amp;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __or__(self, other):\n        \"\"\"join and merge interval array; set union\"\"\"\n        if isinstance(other, type(self)):\n            new = copy.copy(self)\n            joined = new.join(other)\n            union = joined.merge()\n            return union\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for |: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __invert__(self):\n        \"\"\"complement within self.domain\"\"\"\n        return self.complement()\n\n    def __bool__(self):\n        \"\"\"(bool) Empty IntervalArray\"\"\"\n        return not self.isempty\n\n    def remove_duplicates(self, inplace=False):\n        \"\"\"Remove duplicate intervals.\"\"\"\n        raise NotImplementedError\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, *, ds=None, n_intervals=None):\n        \"\"\"Returns an IntervalArray that has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum length, for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : IntervalArray\n            IntervalArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        if self.isempty:\n            raise ValueError(\"cannot parition an empty object in a meaningful way!\")\n\n        if ds is not None and n_intervals is not None:\n            raise ValueError(\"ds and n_intervals cannot be used together\")\n\n        if n_intervals is not None:\n            assert float(n_intervals).is_integer(), (\n                \"n_intervals must be a positive integer!\"\n            )\n            assert n_intervals &gt; 1, \"n_intervals must be a positive integer &gt; 1\"\n            # determine ds from number of desired points:\n            ds = self.length / n_intervals\n\n        if ds is None:\n            # neither n_intervals nor ds was specified, so assume defaults:\n            n_intervals = 100\n            ds = self.length / n_intervals\n\n        # build list of points at which to esplit the IntervalArray\n        new_starts = []\n        new_stops = []\n        for start, stop in self.data:\n            newxvals = utils.frange(start, stop, step=ds).tolist()\n            # newxvals = np.arange(start, stop, step=ds).tolist()\n            if newxvals[-1] + float_info.epsilon &lt; stop:\n                newxvals.append(stop)\n            newxvals = np.asanyarray(newxvals)\n            new_starts.extend(newxvals[:-1])\n            new_stops.extend(newxvals[1:])\n\n        # now make a new interval array:\n        out = copy.copy(self)\n        out._data = np.hstack(\n            [\n                np.array(new_starts)[..., np.newaxis],\n                np.array(new_stops)[..., np.newaxis],\n            ]\n        )\n        return out\n\n    @property\n    def label(self):\n        \"\"\"Label describing the interval array.\"\"\"\n        if self._label is None:\n            logging.warning(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n\n    def complement(self, domain=None):\n        \"\"\"Complement within domain.\n\n        Parameters\n        ----------\n        domain : IntervalArray, optional\n            IntervalArray specifying entire domain. Default is self.domain.\n\n        Returns\n        -------\n        complement : IntervalArray\n            IntervalArray containing all the nonzero intervals in the\n            complement set.\n        \"\"\"\n\n        if domain is None:\n            domain = self.domain\n\n        # make sure IntervalArray is sorted:\n        if not self.issorted:\n            self._sort()\n        # check that IntervalArray is entirely contained within domain\n        # if (self.start &lt; domain.start) or (self.stop &gt; domain.stop):\n        #     raise ValueError(\"IntervalArray must be entirely contained within domain\")\n\n        # check that IntervalArray is fully merged, or merge it if necessary\n        merged = self.merge()\n        # build complement intervals\n        starts = np.insert(merged.stops, 0, domain.start)\n        stops = np.append(merged.starts, domain.stop)\n        newvalues = np.vstack([starts, stops]).T\n        # remove intervals with zero length\n        lengths = newvalues[:, 1] - newvalues[:, 0]\n        newvalues = newvalues[lengths &gt; 0]\n        complement = copy.copy(self)\n        complement._data = newvalues\n\n        if domain.n_intervals &gt; 1:\n            return complement[domain]\n        try:\n            complement._data[0, 0] = np.max((complement._data[0, 0], domain.start))\n            complement._data[-1, -1] = np.min((complement._data[-1, -1], domain.stop))\n        except IndexError:  # complement is empty\n            return type(self)(empty=True)\n        return complement\n\n    @property\n    def domain(self):\n        \"\"\"domain (in base units) within which support is defined\"\"\"\n        if self._domain is None:\n            self._domain = type(self)([-np.inf, np.inf])\n        return self._domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"domain (in base units) within which support is defined\"\"\"\n        # TODO: add  input validation\n        if isinstance(val, type(self)):\n            self._domain = val\n        elif isinstance(val, (tuple, list)):\n            self._domain = type(self)([val[0], val[1]])\n\n    @property\n    def meta(self):\n        \"\"\"Meta data associated with IntervalArray.\"\"\"\n        if self._meta is None:\n            logging.warning(\"meta data is not available\")\n        return self._meta\n\n    @meta.setter\n    def meta(self, val):\n        self._meta = val\n\n    @property\n    def min(self):\n        \"\"\"Minimum bound of all intervals in IntervalArray.\"\"\"\n        return self.merge().start\n\n    @property\n    def max(self):\n        \"\"\"Maximum bound of all intervals in IntervalArray.\"\"\"\n        return self.merge().stop\n\n    @property\n    def data(self):\n        \"\"\"Interval values [start, stop) in base units.\"\"\"\n        return self._data\n\n    @property\n    def is_finite(self):\n        \"\"\"Is the interval [start, stop) finite.\"\"\"\n        return not (np.isinf(self.start) | np.isinf(self.stop))\n\n    # @property\n    # def _human_readable_posix_intervals(self):\n    #     \"\"\"Interval start and stop values in human readable POSIX time.\n\n    #     This property is left private, because it has not been carefully\n    #     vetted for public API release yet.\n    #     \"\"\"\n    #     import datetime\n    #     n_intervals_zfill = len(str(self.n_intervals))\n    #     for ii, (start, stop) in enumerate(self.time):\n    #         print('[ep ' + str(ii).zfill(n_intervals_zfill) + ']\\t' +\n    #               datetime.datetime.fromtimestamp(\n    #                 int(start)).strftime('%Y-%m-%d %H:%M:%S') + ' -- ' +\n    #               datetime.datetime.fromtimestamp(\n    #                 int(stop)).strftime('%Y-%m-%d %H:%M:%S') + '\\t(' +\n    #               str(utils.PrettyDuration(stop-start)) + ')')\n\n    @property\n    def centers(self):\n        \"\"\"(np.array) The center of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return np.mean(self.data, axis=1)\n\n    @property\n    def lengths(self):\n        \"\"\"(np.array) The length of each interval.\"\"\"\n        if self.isempty:\n            return 0\n        return self.data[:, 1] - self.data[:, 0]\n\n    @property\n    def range(self):\n        \"\"\"return IntervalArray containing range of current IntervalArray.\"\"\"\n        return type(self)([self.start, self.stop])\n\n    @property\n    def length(self):\n        \"\"\"(float) The total length of the [merged] interval array.\"\"\"\n        if self.isempty:\n            return self.formatter(0)\n        merged = self.merge()\n        return self.formatter(np.array(merged.data[:, 1] - merged.data[:, 0]).sum())\n\n    @property\n    def starts(self):\n        \"\"\"(np.array) The start of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 0]\n\n    @property\n    def start(self):\n        \"\"\"(np.array) The start of the first interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 0][0]\n\n    @property\n    def stops(self):\n        \"\"\"(np.array) The stop of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 1]\n\n    @property\n    def stop(self):\n        \"\"\"(np.array) The stop of the last interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 1][-1]\n\n    @property\n    def n_intervals(self):\n        \"\"\"(int) The number of intervals.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self.data[:, 0]))\n\n    def __len__(self):\n        \"\"\"(int) The number of intervals.\"\"\"\n        return self.n_intervals\n\n    @property\n    def ismerged(self):\n        \"\"\"(bool) No overlapping intervals exist.\"\"\"\n        if self.isempty:\n            return True\n        if self.n_intervals == 1:\n            return True\n        if not self.issorted:\n            self._sort()\n        if not utils.is_sorted(self.stops):\n            return False\n\n        return np.all(self.data[1:, 0] - self.data[:-1, 1] &gt; 0)\n\n    def _ismerged(self, overlap=0.0):\n        \"\"\"(bool) No overlapping intervals with overlap &gt;= overlap exist.\"\"\"\n        if self.isempty:\n            return True\n        if self.n_intervals == 1:\n            return True\n        if not self.issorted:\n            self._sort()\n        if not utils.is_sorted(self.stops):\n            return False\n\n        return np.all(self.data[1:, 0] - self.data[:-1, 1] &gt; -overlap)\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Left edges of intervals are sorted in ascending order.\"\"\"\n        if self.isempty:\n            return True\n        return utils.is_sorted(self.starts)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty IntervalArray.\"\"\"\n        try:\n            return len(self.data) == 0\n        except TypeError:\n            return True  # this happens when self.data is None\n\n    def copy(self):\n        \"\"\"(IntervalArray) Returns a copy of the current interval array.\"\"\"\n        newcopy = copy.deepcopy(self)\n        return newcopy\n\n    def _drop_empty_intervals(self):\n        \"\"\"Drops empty intervals. Not in-place, i.e. returns a copy.\"\"\"\n        keep_interval_ids = np.argwhere(self.lengths).squeeze().tolist()\n        return self[keep_interval_ids]\n\n    def intersect(self, interval, *, boundaries=True):\n        \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n        other interval array ('interval').\n        \"\"\"\n\n        if self.isempty or interval.isempty:\n            logging.warning(\"interval intersection is empty\")\n            return type(self)(empty=True)\n\n        new_intervals = []\n\n        # Extract starts and stops and convert to np.array of float64 (for numba)\n        interval_starts_a = np.array(self.starts, dtype=np.float64)\n        interval_stops_a = np.array(self.stops, dtype=np.float64)\n        if interval.data.ndim == 1:\n            interval_starts_b = np.array([interval.data[0]], dtype=np.float64)\n            interval_stops_b = np.array([interval.data[1]], dtype=np.float64)\n        else:\n            interval_starts_b = np.array(interval.data[:, 0], dtype=np.float64)\n            interval_stops_b = np.array(interval.data[:, 1], dtype=np.float64)\n\n        new_starts, new_stops = interval_intersect(\n            interval_starts_a,\n            interval_stops_a,\n            interval_starts_b,\n            interval_stops_b,\n            boundaries,\n        )\n\n        for start, stop in zip(new_starts, new_stops):\n            new_intervals.append([start, stop])\n\n        # convert to np.array of float64\n        new_intervals = np.array(new_intervals, dtype=np.float64)\n\n        out = type(self)(new_intervals)\n        out._domain = self.domain\n        return out\n\n    # def intersect(self, interval, *, boundaries=True):\n    #     \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n    #        other interval array ('interval').\n    #     \"\"\"\n\n    #     this = copy.deepcopy(self)\n    #     new_intervals = []\n    #     for epa in this:\n    #         cand_ep_idx = np.argwhere((interval.starts &lt; epa.stop) &amp; (interval.stops &gt; epa.start)).squeeze()\n    #         if np.size(cand_ep_idx) &gt; 0:\n    #             for epb in interval[cand_ep_idx.tolist()]:\n    #                 new_interval = self._intersect(epa, epb, boundaries=boundaries)\n    #                 if not new_interval.isempty:\n    #                     new_intervals.append([new_interval.start, new_interval.stop])\n    #     out = type(self)(new_intervals)\n    #     out._domain = self.domain\n    #     return out\n\n    # def _intersect(self, intervala, intervalb, *, boundaries=True, meta=None):\n    #     \"\"\"Finds intersection (overlap) between two sets of interval arrays.\n\n    #     TODO: verify if this requires a merged IntervalArray to work properly?\n    #     ISSUE_261: not fixed yet\n\n    #     TODO: domains are not preserved yet! careful consideration is necessary.\n\n    #     Parameters\n    #     ----------\n    #     interval : nelpy.IntervalArray\n    #     boundaries : bool\n    #         If True, limits start, stop to interval start and stop.\n    #     meta : dict, optional\n    #         New dictionary of meta data for interval ontersection.\n\n    #     Returns\n    #     -------\n    #     intersect_intervals : nelpy.IntervalArray\n    #     \"\"\"\n    #     if intervala.isempty or intervalb.isempty:\n    #         logging.warning('interval intersection is empty')\n    #         return type(self)(empty=True)\n\n    #     new_starts = []\n    #     new_stops = []\n    #     interval_a = intervala.merge().copy()\n    #     interval_b = intervalb.merge().copy()\n\n    #     for aa in interval_a.data:\n    #         for bb in interval_b.data:\n    #             if (aa[0] &lt;= bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &lt;= aa[1]):\n    #                 new_starts.append(bb[0])\n    #                 new_stops.append(bb[1])\n    #             elif (aa[0] &lt; bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &gt; aa[1]):\n    #                 new_starts.append(bb[0])\n    #                 if boundaries:\n    #                     new_stops.append(aa[1])\n    #                 else:\n    #                     new_stops.append(bb[1])\n    #             elif (aa[0] &gt; bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &lt; aa[1]):\n    #                 if boundaries:\n    #                     new_starts.append(aa[0])\n    #                 else:\n    #                     new_starts.append(bb[0])\n    #                 new_stops.append(bb[1])\n    #             elif (aa[0] &gt;= bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &gt;= aa[1]):\n    #                 if boundaries:\n    #                     new_starts.append(aa[0])\n    #                     new_stops.append(aa[1])\n    #                 else:\n    #                     new_starts.append(bb[0])\n    #                     new_stops.append(bb[1])\n\n    #     if not boundaries:\n    #         new_starts = np.unique(new_starts)\n    #         new_stops = np.unique(new_stops)\n\n    #     interval_a._data = np.hstack(\n    #         [np.array(new_starts)[..., np.newaxis],\n    #             np.array(new_stops)[..., np.newaxis]])\n\n    #     return interval_a\n\n    def merge(self, *, gap=0.0, overlap=0.0):\n        \"\"\"Merge intervals that are close or overlapping.\n\n        if gap == 0 and overlap == 0:\n            [a, b) U [b, c) = [a, c)\n        if gap == None and overlap &gt; 0:\n            [a, b) U [b, c) = [a, b) U [b, c)\n            [a, b + overlap) U [b, c) = [a, c)\n            [a, b) U [b - overlap, c) = [a, c)\n        if gap &gt; 0 and overlap == None:\n            [a, b) U [b, c) = [a, c)\n            [a, b) U [b + gap, c) = [a, c)\n            [a, b - gap) U [b, c) = [a, c)\n\n        WARNING! Algorithm only works on SORTED intervals.\n\n        Parameters\n        ----------\n        gap : float, optional\n            Amount (in base units) to consider intervals close enough to merge.\n            Defaults to 0.0 (no gap).\n        Returns\n        -------\n        merged_intervals : nelpy.IntervalArray\n        \"\"\"\n\n        if gap &lt; 0:\n            raise ValueError(\"gap cannot be negative\")\n        if overlap &lt; 0:\n            raise ValueError(\"overlap cannot be negative\")\n\n        if self.isempty:\n            return self\n\n        if (self.ismerged) and (gap == 0.0):\n            # already merged\n            return self\n\n        newintervalarray = copy.copy(self)\n\n        if not newintervalarray.issorted:\n            newintervalarray._sort()\n\n        overlap_ = overlap\n\n        while not newintervalarray._ismerged(overlap=overlap) or gap &gt; 0:\n            stops = newintervalarray.stops[:-1] + gap\n            starts = newintervalarray.starts[1:] + overlap_\n            to_merge = (stops - starts) &gt;= 0\n\n            new_starts = [newintervalarray.starts[0]]\n            new_stops = []\n\n            next_stop = newintervalarray.stops[0]\n            for i in range(newintervalarray.data.shape[0] - 1):\n                this_stop = newintervalarray.stops[i]\n                next_stop = max(next_stop, this_stop)\n                if not to_merge[i]:\n                    new_stops.append(next_stop)\n                    new_starts.append(newintervalarray.starts[i + 1])\n\n            new_stops.append(max(newintervalarray.stops[-1], next_stop))\n\n            new_starts = np.array(new_starts)\n            new_stops = np.array(new_stops)\n\n            newintervalarray._data = np.vstack([new_starts, new_stops]).T\n\n            # after one pass, all the gap offsets have been added, and\n            # then we just need to keep merging...\n            gap = 0.0\n            overlap_ = 0.0\n\n        return newintervalarray\n\n    def expand(self, amount, direction=\"both\"):\n        \"\"\"Expands interval by the given amount.\n        Parameters\n        ----------\n        amount : float\n            Amount (in base units) to expand each interval.\n        direction : str\n            Can be 'both', 'start', or 'stop'. This specifies\n            which direction to resize interval.\n        Returns\n        -------\n        expanded_intervals : nelpy.IntervalArray\n        \"\"\"\n        if direction == \"both\":\n            resize_starts = self.data[:, 0] - amount\n            resize_stops = self.data[:, 1] + amount\n        elif direction == \"start\":\n            resize_starts = self.data[:, 0] - amount\n            resize_stops = self.data[:, 1]\n        elif direction == \"stop\":\n            resize_starts = self.data[:, 0]\n            resize_stops = self.data[:, 1] + amount\n        else:\n            raise ValueError(\"direction must be 'both', 'start', or 'stop'\")\n\n        newintervalarray = copy.copy(self)\n\n        newintervalarray._data = np.hstack(\n            (resize_starts[..., np.newaxis], resize_stops[..., np.newaxis])\n        )\n\n        return newintervalarray\n\n    def shrink(self, amount, direction=\"both\"):\n        \"\"\"Shrinks interval by the given amount.\n        Parameters\n        ----------\n        amount : float\n            Amount (in base units) to shrink each interval.\n        direction : str\n            Can be 'both', 'start', or 'stop'. This specifies\n            which direction to resize interval.\n        Returns\n        -------\n        shrinked_intervals : nelpy.IntervalArray\n        \"\"\"\n        both_limit = min(self.lengths / 2)\n        if amount &gt; both_limit and direction == \"both\":\n            raise ValueError(\"shrink amount too large\")\n\n        single_limit = min(self.lengths)\n        if amount &gt; single_limit and direction != \"both\":\n            raise ValueError(\"shrink amount too large\")\n\n        return self.expand(-amount, direction)\n\n    def join(self, interval, meta=None):\n        \"\"\"Combines [and merges] two sets of intervals. Intervals can have\n        different sampling rates.\n\n        Parameters\n        ----------\n        interval : nelpy.IntervalArray\n        meta : dict, optional\n            New meta data dictionary describing the joined intervals.\n\n        Returns\n        -------\n        joined_intervals : nelpy.IntervalArray\n        \"\"\"\n\n        if self.isempty:\n            return interval\n        if interval.isempty:\n            return self\n\n        newintervalarray = copy.copy(self)\n\n        join_starts = np.concatenate((self.data[:, 0], interval.data[:, 0]))\n        join_stops = np.concatenate((self.data[:, 1], interval.data[:, 1]))\n\n        newintervalarray._data = np.hstack(\n            (join_starts[..., np.newaxis], join_stops[..., np.newaxis])\n        )\n        if not newintervalarray.issorted:\n            newintervalarray._sort()\n        # if not newintervalarray.ismerged:\n        #     newintervalarray = newintervalarray.merge()\n        return newintervalarray\n\n    def __contains__(self, value):\n        \"\"\"Checks whether value is in any interval.\n\n        #TODO: add support for when value is an IntervalArray (could be easy with intersection)\n\n        Parameters\n        ----------\n        intervals: nelpy.IntervalArray\n        value: float or int\n\n        Returns\n        -------\n        boolean\n\n        \"\"\"\n        # TODO: consider vectorizing this loop, which should increase\n        # speed, but also greatly increase memory? Alternatively, if we\n        # could assume something about intervals being sorted, this can\n        # also be made much faster than the current O(N)\n        for start, stop in zip(self.starts, self.stops):\n            if start &lt;= value &lt;= stop:\n                return True\n        return False\n\n    def _sort(self):\n        \"\"\"Sort intervals by interval starts\"\"\"\n        sort_idx = np.argsort(self.data[:, 0])\n        self._data = self._data[sort_idx]\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.centers","title":"<code>centers</code>  <code>property</code>","text":"<p>(np.array) The center of each interval.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Interval values [start, stop) in base units.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>domain (in base units) within which support is defined</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.is_finite","title":"<code>is_finite</code>  <code>property</code>","text":"<p>Is the interval [start, stop) finite.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty IntervalArray.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.ismerged","title":"<code>ismerged</code>  <code>property</code>","text":"<p>(bool) No overlapping intervals exist.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Left edges of intervals are sorted in ascending order.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label describing the interval array.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.length","title":"<code>length</code>  <code>property</code>","text":"<p>(float) The total length of the [merged] interval array.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>(np.array) The length of each interval.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.max","title":"<code>max</code>  <code>property</code>","text":"<p>Maximum bound of all intervals in IntervalArray.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.meta","title":"<code>meta</code>  <code>property</code> <code>writable</code>","text":"<p>Meta data associated with IntervalArray.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.min","title":"<code>min</code>  <code>property</code>","text":"<p>Minimum bound of all intervals in IntervalArray.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.n_intervals","title":"<code>n_intervals</code>  <code>property</code>","text":"<p>(int) The number of intervals.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.range","title":"<code>range</code>  <code>property</code>","text":"<p>return IntervalArray containing range of current IntervalArray.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.start","title":"<code>start</code>  <code>property</code>","text":"<p>(np.array) The start of the first interval.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>(np.array) The start of each interval.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.stop","title":"<code>stop</code>  <code>property</code>","text":"<p>(np.array) The stop of the last interval.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.stops","title":"<code>stops</code>  <code>property</code>","text":"<p>(np.array) The stop of each interval.</p>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.complement","title":"<code>complement(domain=None)</code>","text":"<p>Complement within domain.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>IntervalArray</code> <p>IntervalArray specifying entire domain. Default is self.domain.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>complement</code> <code>IntervalArray</code> <p>IntervalArray containing all the nonzero intervals in the complement set.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def complement(self, domain=None):\n    \"\"\"Complement within domain.\n\n    Parameters\n    ----------\n    domain : IntervalArray, optional\n        IntervalArray specifying entire domain. Default is self.domain.\n\n    Returns\n    -------\n    complement : IntervalArray\n        IntervalArray containing all the nonzero intervals in the\n        complement set.\n    \"\"\"\n\n    if domain is None:\n        domain = self.domain\n\n    # make sure IntervalArray is sorted:\n    if not self.issorted:\n        self._sort()\n    # check that IntervalArray is entirely contained within domain\n    # if (self.start &lt; domain.start) or (self.stop &gt; domain.stop):\n    #     raise ValueError(\"IntervalArray must be entirely contained within domain\")\n\n    # check that IntervalArray is fully merged, or merge it if necessary\n    merged = self.merge()\n    # build complement intervals\n    starts = np.insert(merged.stops, 0, domain.start)\n    stops = np.append(merged.starts, domain.stop)\n    newvalues = np.vstack([starts, stops]).T\n    # remove intervals with zero length\n    lengths = newvalues[:, 1] - newvalues[:, 0]\n    newvalues = newvalues[lengths &gt; 0]\n    complement = copy.copy(self)\n    complement._data = newvalues\n\n    if domain.n_intervals &gt; 1:\n        return complement[domain]\n    try:\n        complement._data[0, 0] = np.max((complement._data[0, 0], domain.start))\n        complement._data[-1, -1] = np.min((complement._data[-1, -1], domain.stop))\n    except IndexError:  # complement is empty\n        return type(self)(empty=True)\n    return complement\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.copy","title":"<code>copy()</code>","text":"<p>(IntervalArray) Returns a copy of the current interval array.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def copy(self):\n    \"\"\"(IntervalArray) Returns a copy of the current interval array.\"\"\"\n    newcopy = copy.deepcopy(self)\n    return newcopy\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.expand","title":"<code>expand(amount, direction='both')</code>","text":"<p>Expands interval by the given amount.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>float</code> <p>Amount (in base units) to expand each interval.</p> required <code>direction</code> <code>str</code> <p>Can be 'both', 'start', or 'stop'. This specifies which direction to resize interval.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>expanded_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def expand(self, amount, direction=\"both\"):\n    \"\"\"Expands interval by the given amount.\n    Parameters\n    ----------\n    amount : float\n        Amount (in base units) to expand each interval.\n    direction : str\n        Can be 'both', 'start', or 'stop'. This specifies\n        which direction to resize interval.\n    Returns\n    -------\n    expanded_intervals : nelpy.IntervalArray\n    \"\"\"\n    if direction == \"both\":\n        resize_starts = self.data[:, 0] - amount\n        resize_stops = self.data[:, 1] + amount\n    elif direction == \"start\":\n        resize_starts = self.data[:, 0] - amount\n        resize_stops = self.data[:, 1]\n    elif direction == \"stop\":\n        resize_starts = self.data[:, 0]\n        resize_stops = self.data[:, 1] + amount\n    else:\n        raise ValueError(\"direction must be 'both', 'start', or 'stop'\")\n\n    newintervalarray = copy.copy(self)\n\n    newintervalarray._data = np.hstack(\n        (resize_starts[..., np.newaxis], resize_stops[..., np.newaxis])\n    )\n\n    return newintervalarray\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.intersect","title":"<code>intersect(interval, *, boundaries=True)</code>","text":"<p>Returns intersection (overlap) between current IntervalArray (self) and other interval array ('interval').</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def intersect(self, interval, *, boundaries=True):\n    \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n    other interval array ('interval').\n    \"\"\"\n\n    if self.isempty or interval.isempty:\n        logging.warning(\"interval intersection is empty\")\n        return type(self)(empty=True)\n\n    new_intervals = []\n\n    # Extract starts and stops and convert to np.array of float64 (for numba)\n    interval_starts_a = np.array(self.starts, dtype=np.float64)\n    interval_stops_a = np.array(self.stops, dtype=np.float64)\n    if interval.data.ndim == 1:\n        interval_starts_b = np.array([interval.data[0]], dtype=np.float64)\n        interval_stops_b = np.array([interval.data[1]], dtype=np.float64)\n    else:\n        interval_starts_b = np.array(interval.data[:, 0], dtype=np.float64)\n        interval_stops_b = np.array(interval.data[:, 1], dtype=np.float64)\n\n    new_starts, new_stops = interval_intersect(\n        interval_starts_a,\n        interval_stops_a,\n        interval_starts_b,\n        interval_stops_b,\n        boundaries,\n    )\n\n    for start, stop in zip(new_starts, new_stops):\n        new_intervals.append([start, stop])\n\n    # convert to np.array of float64\n    new_intervals = np.array(new_intervals, dtype=np.float64)\n\n    out = type(self)(new_intervals)\n    out._domain = self.domain\n    return out\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.join","title":"<code>join(interval, meta=None)</code>","text":"<p>Combines [and merges] two sets of intervals. Intervals can have different sampling rates.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>IntervalArray</code> required <code>meta</code> <code>dict</code> <p>New meta data dictionary describing the joined intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>joined_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def join(self, interval, meta=None):\n    \"\"\"Combines [and merges] two sets of intervals. Intervals can have\n    different sampling rates.\n\n    Parameters\n    ----------\n    interval : nelpy.IntervalArray\n    meta : dict, optional\n        New meta data dictionary describing the joined intervals.\n\n    Returns\n    -------\n    joined_intervals : nelpy.IntervalArray\n    \"\"\"\n\n    if self.isempty:\n        return interval\n    if interval.isempty:\n        return self\n\n    newintervalarray = copy.copy(self)\n\n    join_starts = np.concatenate((self.data[:, 0], interval.data[:, 0]))\n    join_stops = np.concatenate((self.data[:, 1], interval.data[:, 1]))\n\n    newintervalarray._data = np.hstack(\n        (join_starts[..., np.newaxis], join_stops[..., np.newaxis])\n    )\n    if not newintervalarray.issorted:\n        newintervalarray._sort()\n    # if not newintervalarray.ismerged:\n    #     newintervalarray = newintervalarray.merge()\n    return newintervalarray\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.merge","title":"<code>merge(*, gap=0.0, overlap=0.0)</code>","text":"<p>Merge intervals that are close or overlapping.</p> <p>if gap == 0 and overlap == 0:     [a, b) U [b, c) = [a, c) if gap == None and overlap &gt; 0:     [a, b) U [b, c) = [a, b) U [b, c)     [a, b + overlap) U [b, c) = [a, c)     [a, b) U [b - overlap, c) = [a, c) if gap &gt; 0 and overlap == None:     [a, b) U [b, c) = [a, c)     [a, b) U [b + gap, c) = [a, c)     [a, b - gap) U [b, c) = [a, c)</p> <p>WARNING! Algorithm only works on SORTED intervals.</p> <p>Parameters:</p> Name Type Description Default <code>gap</code> <code>float</code> <p>Amount (in base units) to consider intervals close enough to merge. Defaults to 0.0 (no gap).</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>merged_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def merge(self, *, gap=0.0, overlap=0.0):\n    \"\"\"Merge intervals that are close or overlapping.\n\n    if gap == 0 and overlap == 0:\n        [a, b) U [b, c) = [a, c)\n    if gap == None and overlap &gt; 0:\n        [a, b) U [b, c) = [a, b) U [b, c)\n        [a, b + overlap) U [b, c) = [a, c)\n        [a, b) U [b - overlap, c) = [a, c)\n    if gap &gt; 0 and overlap == None:\n        [a, b) U [b, c) = [a, c)\n        [a, b) U [b + gap, c) = [a, c)\n        [a, b - gap) U [b, c) = [a, c)\n\n    WARNING! Algorithm only works on SORTED intervals.\n\n    Parameters\n    ----------\n    gap : float, optional\n        Amount (in base units) to consider intervals close enough to merge.\n        Defaults to 0.0 (no gap).\n    Returns\n    -------\n    merged_intervals : nelpy.IntervalArray\n    \"\"\"\n\n    if gap &lt; 0:\n        raise ValueError(\"gap cannot be negative\")\n    if overlap &lt; 0:\n        raise ValueError(\"overlap cannot be negative\")\n\n    if self.isempty:\n        return self\n\n    if (self.ismerged) and (gap == 0.0):\n        # already merged\n        return self\n\n    newintervalarray = copy.copy(self)\n\n    if not newintervalarray.issorted:\n        newintervalarray._sort()\n\n    overlap_ = overlap\n\n    while not newintervalarray._ismerged(overlap=overlap) or gap &gt; 0:\n        stops = newintervalarray.stops[:-1] + gap\n        starts = newintervalarray.starts[1:] + overlap_\n        to_merge = (stops - starts) &gt;= 0\n\n        new_starts = [newintervalarray.starts[0]]\n        new_stops = []\n\n        next_stop = newintervalarray.stops[0]\n        for i in range(newintervalarray.data.shape[0] - 1):\n            this_stop = newintervalarray.stops[i]\n            next_stop = max(next_stop, this_stop)\n            if not to_merge[i]:\n                new_stops.append(next_stop)\n                new_starts.append(newintervalarray.starts[i + 1])\n\n        new_stops.append(max(newintervalarray.stops[-1], next_stop))\n\n        new_starts = np.array(new_starts)\n        new_stops = np.array(new_stops)\n\n        newintervalarray._data = np.vstack([new_starts, new_stops]).T\n\n        # after one pass, all the gap offsets have been added, and\n        # then we just need to keep merging...\n        gap = 0.0\n        overlap_ = 0.0\n\n    return newintervalarray\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.partition","title":"<code>partition(*, ds=None, n_intervals=None)</code>","text":"<p>Returns an IntervalArray that has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum length, for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalArray</code> <p>IntervalArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, *, ds=None, n_intervals=None):\n    \"\"\"Returns an IntervalArray that has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum length, for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : IntervalArray\n        IntervalArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    if self.isempty:\n        raise ValueError(\"cannot parition an empty object in a meaningful way!\")\n\n    if ds is not None and n_intervals is not None:\n        raise ValueError(\"ds and n_intervals cannot be used together\")\n\n    if n_intervals is not None:\n        assert float(n_intervals).is_integer(), (\n            \"n_intervals must be a positive integer!\"\n        )\n        assert n_intervals &gt; 1, \"n_intervals must be a positive integer &gt; 1\"\n        # determine ds from number of desired points:\n        ds = self.length / n_intervals\n\n    if ds is None:\n        # neither n_intervals nor ds was specified, so assume defaults:\n        n_intervals = 100\n        ds = self.length / n_intervals\n\n    # build list of points at which to esplit the IntervalArray\n    new_starts = []\n    new_stops = []\n    for start, stop in self.data:\n        newxvals = utils.frange(start, stop, step=ds).tolist()\n        # newxvals = np.arange(start, stop, step=ds).tolist()\n        if newxvals[-1] + float_info.epsilon &lt; stop:\n            newxvals.append(stop)\n        newxvals = np.asanyarray(newxvals)\n        new_starts.extend(newxvals[:-1])\n        new_stops.extend(newxvals[1:])\n\n    # now make a new interval array:\n    out = copy.copy(self)\n    out._data = np.hstack(\n        [\n            np.array(new_starts)[..., np.newaxis],\n            np.array(new_stops)[..., np.newaxis],\n        ]\n    )\n    return out\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.remove_duplicates","title":"<code>remove_duplicates(inplace=False)</code>","text":"<p>Remove duplicate intervals.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def remove_duplicates(self, inplace=False):\n    \"\"\"Remove duplicate intervals.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.IntervalArray.shrink","title":"<code>shrink(amount, direction='both')</code>","text":"<p>Shrinks interval by the given amount.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>float</code> <p>Amount (in base units) to shrink each interval.</p> required <code>direction</code> <code>str</code> <p>Can be 'both', 'start', or 'stop'. This specifies which direction to resize interval.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>shrinked_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def shrink(self, amount, direction=\"both\"):\n    \"\"\"Shrinks interval by the given amount.\n    Parameters\n    ----------\n    amount : float\n        Amount (in base units) to shrink each interval.\n    direction : str\n        Can be 'both', 'start', or 'stop'. This specifies\n        which direction to resize interval.\n    Returns\n    -------\n    shrinked_intervals : nelpy.IntervalArray\n    \"\"\"\n    both_limit = min(self.lengths / 2)\n    if amount &gt; both_limit and direction == \"both\":\n        raise ValueError(\"shrink amount too large\")\n\n    single_limit = min(self.lengths)\n    if amount &gt; single_limit and direction != \"both\":\n        raise ValueError(\"shrink amount too large\")\n\n    return self.expand(-amount, direction)\n</code></pre>"},{"location":"reference/core/intervalarray/#nelpy.core._intervalarray.SpaceArray","title":"<code>SpaceArray</code>","text":"<p>               Bases: <code>IntervalArray</code></p> <p>IntervalArray containing spatial intervals (in centimeters).</p> <p>This class extends <code>IntervalArray</code> to specifically handle space-based intervals, such as linear or 2D spatial regions. It provides a formatter for displaying spatial lengths and can be used for spatial segmentation in behavioral or neural data analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_intervals, 1) or (n_intervals,), the start position for each interval (which then requires a <code>length</code> to be specified). If shape (n_intervals, 2), the start and stop positions for each interval. Defaults to None, creating an empty <code>SpaceArray</code>.</p> required <code>length</code> <code>np.array, float, or None</code> <p>The length of the interval (in base units, centimeters). If a float, the same length is assumed for every interval. Only used if <code>data</code> is a 1D array of start positions.</p> required <code>meta</code> <code>dict</code> <p>Metadata associated with the spatial intervals.</p> required <code>empty</code> <code>bool</code> <p>If True, an empty <code>SpaceArray</code> is returned, ignoring <code>data</code> and <code>length</code>. Defaults to False.</p> required <code>domain</code> <code>IntervalArray</code> <p>The domain within which the spatial intervals are defined. If None, it defaults to an infinite domain.</p> required <code>label</code> <code>str</code> <p>A descriptive label for the space array.</p> required <p>Attributes:</p> Name Type Description <code>data</code> <code>array</code> <p>The start and stop positions for each interval, with shape (n_intervals, 2).</p> <code>n_intervals</code> <code>int</code> <p>The number of spatial intervals in the array.</p> <code>lengths</code> <code>array</code> <p>The length of each spatial interval (in centimeters).</p> <code>formatter</code> <code>PrettySpace</code> <p>The formatter used for displaying spatial lengths.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the intervals, which is 'cm' for SpaceArray.</p> Notes <p>This class inherits all methods and properties from <code>IntervalArray</code>. It is intended for use with spatial data, such as segmenting a linear track or defining regions of interest in a behavioral arena.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy.core import SpaceArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a SpaceArray from start and stop positions\n&gt;&gt;&gt; regions = SpaceArray(data=np.array([[0, 50], [100, 150]]))\n&gt;&gt;&gt; print(regions)\n&lt;SpaceArray at 0x...: 2 intervals&gt; of length 100 cm\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a SpaceArray from start positions and a common length\n&gt;&gt;&gt; starts = np.array([0, 100])\n&gt;&gt;&gt; length = 25.0\n&gt;&gt;&gt; regions_with_length = SpaceArray(data=starts, length=length)\n&gt;&gt;&gt; print(regions_with_length)\n&lt;SpaceArray at 0x...: 2 intervals&gt; of length 50 cm\n</code></pre> <pre><code>&gt;&gt;&gt; # Accessing attributes\n&gt;&gt;&gt; print(f\"Number of regions: {regions.n_intervals}\")\nNumber of regions: 2\n&gt;&gt;&gt; print(f\"Lengths: {regions.lengths}\")\nLengths: [50 50]\n</code></pre> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class SpaceArray(IntervalArray):\n    \"\"\"\n    IntervalArray containing spatial intervals (in centimeters).\n\n    This class extends `IntervalArray` to specifically handle space-based\n    intervals, such as linear or 2D spatial regions. It provides a formatter\n    for displaying spatial lengths and can be used for spatial segmentation\n    in behavioral or neural data analysis.\n\n    Parameters\n    ----------\n    data : np.array, optional\n        If shape (n_intervals, 1) or (n_intervals,), the start position for each\n        interval (which then requires a `length` to be specified).\n        If shape (n_intervals, 2), the start and stop positions for each interval.\n        Defaults to None, creating an empty `SpaceArray`.\n    length : np.array, float, or None, optional\n        The length of the interval (in base units, centimeters). If a float,\n        the same length is assumed for every interval. Only used if `data`\n        is a 1D array of start positions.\n    meta : dict, optional\n        Metadata associated with the spatial intervals.\n    empty : bool, optional\n        If True, an empty `SpaceArray` is returned, ignoring `data` and `length`.\n        Defaults to False.\n    domain : IntervalArray, optional\n        The domain within which the spatial intervals are defined. If None, it defaults\n        to an infinite domain.\n    label : str, optional\n        A descriptive label for the space array.\n\n    Attributes\n    ----------\n    data : np.array\n        The start and stop positions for each interval, with shape (n_intervals, 2).\n    n_intervals : int\n        The number of spatial intervals in the array.\n    lengths : np.array\n        The length of each spatial interval (in centimeters).\n    formatter : formatters.PrettySpace\n        The formatter used for displaying spatial lengths.\n    base_unit : str\n        The base unit of the intervals, which is 'cm' for SpaceArray.\n\n    Notes\n    -----\n    This class inherits all methods and properties from `IntervalArray`.\n    It is intended for use with spatial data, such as segmenting a linear track\n    or defining regions of interest in a behavioral arena.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy.core import SpaceArray\n\n    &gt;&gt;&gt; # Create a SpaceArray from start and stop positions\n    &gt;&gt;&gt; regions = SpaceArray(data=np.array([[0, 50], [100, 150]]))\n    &gt;&gt;&gt; print(regions)\n    &lt;SpaceArray at 0x...: 2 intervals&gt; of length 100 cm\n\n    &gt;&gt;&gt; # Create a SpaceArray from start positions and a common length\n    &gt;&gt;&gt; starts = np.array([0, 100])\n    &gt;&gt;&gt; length = 25.0\n    &gt;&gt;&gt; regions_with_length = SpaceArray(data=starts, length=length)\n    &gt;&gt;&gt; print(regions_with_length)\n    &lt;SpaceArray at 0x...: 2 intervals&gt; of length 50 cm\n\n    &gt;&gt;&gt; # Accessing attributes\n    &gt;&gt;&gt; print(f\"Number of regions: {regions.n_intervals}\")\n    Number of regions: 2\n    &gt;&gt;&gt; print(f\"Lengths: {regions.lengths}\")\n    Lengths: [50 50]\n    \"\"\"\n\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n        self.formatter = formatters.PrettySpace\n        self.base_unit = self.formatter.base_unit\n</code></pre>"},{"location":"reference/core/valeventarray/","title":"ValeventArray","text":"Notes <p>non-binned EVAs have multiple timeseries as abscissa binned EVAs only have a single timeseries as abscissa</p> <p>ISASA = Irregularly Sampled AnalogSignalArray</p> <p>eva - not callable     - fully binnable --&gt; beva</p> <p>beva - callable --&gt; veva (really ISASA)      - binnable      - castable to ASA and back</p> <p>veva - not callable      - somewhat binnable --&gt; bveva      - make stateful --&gt; sveva      - .data; .values</p> <p>bveva - callable --&gt; veva (not exactly an ISASA anymore, due to each signal being ND)       - castable to beva (2d matrix, instead of list of 2d matrices); not complete!</p> <p>sveva - callable --&gt; veva       - somewhat binnable --&gt; bveva       - .data; .values; .states</p> <p>Remarks: I planned on having a list of arrays, and a list of timeseries for the          ValueEventArrays.</p> <p>ValueEventArrays have data structure     nSeries : nEvents x nValues [(i, j, k) --&gt; i: j=f(i), k] BinnedValueEventArrays have data structure     nSeries x nBins x nValues [(i, j, k) --&gt; i x j x k] and each of these structures are wrapped in intervals[data] pseudo encapsulation</p> <p>BaseValueEventArray (ABC)   --- veva (base)   --- bveva (base)   --- sveva (veva)</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray","title":"<code>BaseValueEventArray</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for ValueEventArray and BinnedValueEventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class BaseValueEventArray(ABC):\n    \"\"\"Base class for ValueEventArray and BinnedValueEventArray.\"\"\"\n\n    __aliases__ = {}\n    __attributes__ = [\"_fs\", \"_series_ids\"]\n\n    def __init__(\n        self,\n        *,\n        fs=None,\n        series_ids=None,\n        empty=False,\n        abscissa=None,\n        ordinate=None,\n        **kwargs,\n    ):\n        self.__version__ = version.__version__\n        self.type_name = self.__class__.__name__\n        if abscissa is None:\n            abscissa = core.Abscissa()  # TODO: integrate into constructor?\n        if ordinate is None:\n            ordinate = core.Ordinate()  # TODO: integrate into constructor?\n        self._abscissa = abscissa\n        self._ordinate = ordinate\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"series\"\n        self._series_label = series_label\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self.loc = ItemGetter_loc(self)\n            self.iloc = ItemGetter_iloc(self)\n            return\n\n        # set initial fs to None\n        self._fs = None\n        # then attempt to update the fs; this does input validation:\n        self.fs = fs\n\n        # WARNING! we need to ensure that self.n_series can work BEFORE\n        # we can set self.series_ids or self.series_labels, since those\n        # setters check that the lengths of the inputs are consistent\n        # with self.n_series.\n\n        # inherit series IDs if available, otherwise initialize to default\n        if series_ids is None:\n            series_ids = list(range(1, self.n_series + 1))\n\n        series_ids = np.array(series_ids, ndmin=1)  # standardize series_ids\n\n        self.series_ids = series_ids\n\n        self.loc = ItemGetter_loc(self)\n        self.iloc = ItemGetter_iloc(self)\n\n    def __renew__(self):\n        \"\"\"Re-attach slicers and indexers.\"\"\"\n        self.loc = ItemGetter_loc(self)\n        self.iloc = ItemGetter_iloc(self)\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        return \"&lt;BaseValueEventArray\" + address_str + \"&gt;\"\n\n    @abstractmethod\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n        return\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty EventArray.\"\"\"\n        try:\n            return np.sum([len(st) for st in self.data]) == 0\n        except TypeError:\n            return True  # this happens when self.data is None\n\n    @property\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        try:\n            return utils.PrettyInt(len(self.data))\n        except TypeError:\n            return 0\n\n    @abstractmethod\n    def n_values(self):\n        \"\"\"(int) The number of values associated with each event series.\"\"\"\n        return\n\n    @property\n    def n_intervals(self):\n        if self.isempty:\n            return 0\n        \"\"\"(int) The number of underlying intervals.\"\"\"\n        return self._abscissa.support.n_intervals\n\n    @property\n    def series_ids(self):\n        \"\"\"Unit IDs contained in the BaseEventArray.\"\"\"\n        return self._series_ids\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without event datas.\"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._data = None\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        return out\n\n    def copy(self):\n        \"\"\"Returns a copy of the EventArray.\"\"\"\n        newcopy = copy.deepcopy(self)\n        newcopy.__renew__()\n        return newcopy\n\n    @property\n    def data(self):\n        \"\"\"Event datas in seconds.\"\"\"\n        return self._data\n\n    @property\n    def first_event(self):\n        \"\"\"Returns the [time of the] first event across all series.\"\"\"\n        first = np.inf\n        for series in self.data:\n            if series[0, 0] &lt; first:\n                first = series[0, 0]\n        return first\n\n    @property\n    def last_event(self):\n        \"\"\"Returns the [time of the] last event across all series.\"\"\"\n        last = -np.inf\n        for series in self.data:\n            if series[-1, 0] &gt; last:\n                last = series[-1, 0]\n        return last\n\n    @series_ids.setter\n    def series_ids(self, val):\n        if val is None:\n            self._series_ids = None\n            return\n\n        if len(val) != self.n_series:\n            raise TypeError(\"series_ids must be of length n_series\")\n\n        # Convert to list of integers, handling various input types\n        try:\n            series_ids = []\n            for item in val:\n                if hasattr(item, \"item\") and item.size == 1:\n                    # Handle numpy scalars\n                    series_ids.append(int(item.item()))\n                elif hasattr(item, \"__int__\") and not hasattr(item, \"__len__\"):\n                    # Handle other numeric types (but not arrays)\n                    series_ids.append(int(item))\n                elif hasattr(item, \"__len__\") and len(item) == 1:\n                    # Handle single-element arrays/lists\n                    series_ids.append(int(item[0]))\n                else:\n                    raise TypeError(\n                        f\"Cannot convert {type(item)} with shape {getattr(item, 'shape', 'unknown')} to int\"\n                    )\n\n            # Check for duplicates\n            if len(set(series_ids)) &lt; len(series_ids):\n                raise TypeError(\"duplicate series_ids are not allowed\")\n\n            self._series_ids = series_ids\n        except (TypeError, ValueError) as e:\n            raise TypeError(f\"series_ids must be int-like: {e}\")\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    @property\n    def fs(self):\n        \"\"\"(float) Sampling rate.\"\"\"\n        return self._fs\n\n    @fs.setter\n    def fs(self, val):\n        \"\"\"(float) Sampling rate.\"\"\"\n        if self._fs == val:\n            return\n        try:\n            if val &lt;= 0:\n                raise ValueError(\"sampling rate must be positive\")\n        except TypeError:\n            raise TypeError(\"sampling rate must be a scalar\")\n        self._fs = val\n\n    @property\n    def label(self):\n        \"\"\"Label pertaining to the source of the event series.\"\"\"\n        if self._label is None:\n            logging.warning(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n\n    def _series_subset(self, series_list):\n        \"\"\"Return a BaseEventArray restricted to a subset of series.\n\n        Parameters\n        ----------\n        series_list : array-like\n            Array or list of series_ids.\n        \"\"\"\n        return self.loc[:, series_list]\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n\n    @staticmethod\n    def _standardize_kwargs(**kwargs):\n        \"\"\"Provide support for easier ValueEventArray keyword arguments.\n\n        kwarg: time &lt;==&gt; timestamps &lt;==&gt; abscissa_vals &lt;==&gt; events\n        kwarg: data &lt;==&gt; values &lt;==&gt; marks\n\n        Examples\n        --------\n        veva = nel.ValueEventArray(time=..., )\n        veva = nel.ValueEventArray(timestamps=..., )\n        veva = nel.ValueEventArray(abscissa_vals=..., data=... )\n        veva = nel.ValueEventArray(events=..., values=... )\n        \"\"\"\n\n        def only_one_of(*args):\n            num_non_null_args = 0\n            out = None\n            for arg in args:\n                if arg is not None:\n                    num_non_null_args += 1\n                    out = arg\n            if num_non_null_args &gt; 1:\n                raise ValueError(\"multiple conflicting arguments received\")\n            return out\n\n        abscissa_vals = kwargs.pop(\"abscissa_vals\", None)\n        timestamps = kwargs.pop(\"timestamps\", None)\n        time = kwargs.pop(\"time\", None)\n        events = kwargs.pop(\"events\", None)\n        data = kwargs.pop(\"data\", None)\n        values = kwargs.pop(\"values\", None)\n        marks = kwargs.pop(\"marks\", None)\n\n        # only one of the above, else raise exception\n        events = only_one_of(abscissa_vals, timestamps, time, events)\n        values = only_one_of(data, values, marks)\n\n        if events is not None:\n            kwargs[\"events\"] = events\n\n        if values is not None:\n            kwargs[\"values\"] = values\n\n        return kwargs\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Event datas in seconds.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the underlying EventArray.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.first_event","title":"<code>first_event</code>  <code>property</code>","text":"<p>Returns the [time of the] first event across all series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.fs","title":"<code>fs</code>  <code>property</code> <code>writable</code>","text":"<p>(float) Sampling rate.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty EventArray.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label pertaining to the source of the event series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.last_event","title":"<code>last_event</code>  <code>property</code>","text":"<p>Returns the [time of the] last event across all series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.n_series","title":"<code>n_series</code>  <code>property</code>","text":"<p>(int) The number of series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.series_ids","title":"<code>series_ids</code>  <code>property</code> <code>writable</code>","text":"<p>Unit IDs contained in the BaseEventArray.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying EventArray.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the EventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of the EventArray.\"\"\"\n    newcopy = copy.deepcopy(self)\n    newcopy.__renew__()\n    return newcopy\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.n_values","title":"<code>n_values()</code>  <code>abstractmethod</code>","text":"<p>(int) The number of values associated with each event series.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@abstractmethod\ndef n_values(self):\n    \"\"\"(int) The number of values associated with each event series.\"\"\"\n    return\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BaseValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>  <code>abstractmethod</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@abstractmethod\n@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n    return\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BinnedValueEventArray","title":"<code>BinnedValueEventArray</code>","text":"<p>               Bases: <code>BaseValueEventArray</code></p> <p>A binned representation of ValueEventArray data.</p> <p>This class creates a time-binned version of a ValueEventArray by aggregating event values within specified time bins. For each support interval, bins are defined globally (not per-series) using np.linspace as in BinnedEventArray. All series use the same bins for a given interval.</p> <p>Parameters:</p> Name Type Description Default <code>vea</code> <code>ValueEventArray</code> <p>The source ValueEventArray to be binned.</p> required <code>ds</code> <code>float</code> <p>Bin size in seconds. Bins are created with left-inclusive, right-exclusive boundaries (e.g., [t, t+ds)).</p> required <code>method</code> <code>str or callable</code> <p>Aggregation method for combining values within each bin: - \"sum\": Sum all values in the bin - \"mean\": Average of all values in the bin - \"median\": Median of all values in the bin - \"min\": Minimum value in the bin - \"max\": Maximum value in the bin - callable: Custom aggregation function that takes an array and returns a scalar</p> <code>\"mean\"</code> <p>Attributes:</p> Name Type Description <code>vea</code> <code>ValueEventArray</code> <p>The original ValueEventArray used to create this binned version.</p> <code>ds</code> <code>float</code> <p>The bin size in seconds.</p> <code>method</code> <code>str or callable</code> <p>The aggregation method used for binning.</p> <code>data</code> <code>ndarray</code> <p>Binned data array of shape (n_series, n_bins, n_values) where: - n_series: Number of event series - n_bins: Total number of bins across all intervals - n_values: Number of values per event</p> <code>bins</code> <code>ndarray</code> <p>Array of bin edges used for binning.</p> <code>bin_centers</code> <code>ndarray</code> <p>Array of bin center times.</p> <code>n_series</code> <code>int</code> <p>Number of event series.</p> <code>n_bins</code> <code>int</code> <p>Total number of bins.</p> <code>n_values</code> <code>int</code> <p>Number of values per event.</p> <code>fs</code> <code>float or None</code> <p>Sampling frequency (inherited from source ValueEventArray).</p> <code>support</code> <code>IntervalArray</code> <p>Support intervals (inherited from source ValueEventArray).</p> Notes <ul> <li>Binning is performed per-series, with each series having its own bin   boundaries starting from its minimum event time within each interval.</li> <li>Bins use left-inclusive, right-exclusive boundaries [t, t+ds).</li> <li>Events outside the support intervals are ignored.</li> <li>Empty bins are filled with NaN values.</li> <li>The resulting data maintains the same number of series and values as   the original ValueEventArray.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n&gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; vea = nel.ValueEventArray(events=events, values=values, fs=10)\n&gt;&gt;&gt; bvea = nel.BinnedValueEventArray(vea, ds=0.5, method=\"sum\")\n&gt;&gt;&gt; print(bvea.data.shape)  # (2, n_bins, 1)\n&gt;&gt;&gt; print(bvea.bin_centers)  # Array of bin center times\n</code></pre> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class BinnedValueEventArray(BaseValueEventArray):\n    \"\"\"A binned representation of ValueEventArray data.\n\n    This class creates a time-binned version of a ValueEventArray by aggregating\n    event values within specified time bins. For each support interval, bins are\n    defined globally (not per-series) using np.linspace as in BinnedEventArray.\n    All series use the same bins for a given interval.\n\n    Parameters\n    ----------\n    vea : ValueEventArray\n        The source ValueEventArray to be binned.\n    ds : float\n        Bin size in seconds. Bins are created with left-inclusive, right-exclusive\n        boundaries (e.g., [t, t+ds)).\n    method : str or callable, default=\"mean\"\n        Aggregation method for combining values within each bin:\n        - \"sum\": Sum all values in the bin\n        - \"mean\": Average of all values in the bin\n        - \"median\": Median of all values in the bin\n        - \"min\": Minimum value in the bin\n        - \"max\": Maximum value in the bin\n        - callable: Custom aggregation function that takes an array and returns a scalar\n\n    Attributes\n    ----------\n    vea : ValueEventArray\n        The original ValueEventArray used to create this binned version.\n    ds : float\n        The bin size in seconds.\n    method : str or callable\n        The aggregation method used for binning.\n    data : np.ndarray\n        Binned data array of shape (n_series, n_bins, n_values) where:\n        - n_series: Number of event series\n        - n_bins: Total number of bins across all intervals\n        - n_values: Number of values per event\n    bins : np.ndarray\n        Array of bin edges used for binning.\n    bin_centers : np.ndarray\n        Array of bin center times.\n    n_series : int\n        Number of event series.\n    n_bins : int\n        Total number of bins.\n    n_values : int\n        Number of values per event.\n    fs : float or None\n        Sampling frequency (inherited from source ValueEventArray).\n    support : IntervalArray\n        Support intervals (inherited from source ValueEventArray).\n\n    Notes\n    -----\n    - Binning is performed per-series, with each series having its own bin\n      boundaries starting from its minimum event time within each interval.\n    - Bins use left-inclusive, right-exclusive boundaries [t, t+ds).\n    - Events outside the support intervals are ignored.\n    - Empty bins are filled with NaN values.\n    - The resulting data maintains the same number of series and values as\n      the original ValueEventArray.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n    &gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n    &gt;&gt;&gt; vea = nel.ValueEventArray(events=events, values=values, fs=10)\n    &gt;&gt;&gt; bvea = nel.BinnedValueEventArray(vea, ds=0.5, method=\"sum\")\n    &gt;&gt;&gt; print(bvea.data.shape)  # (2, n_bins, 1)\n    &gt;&gt;&gt; print(bvea.bin_centers)  # Array of bin center times\n    \"\"\"\n\n    def __init__(self, vea, ds, method=\"mean\"):\n        # Set _data to a placeholder with the correct number of series and values\n        n_series = vea.n_series\n        n_values = (\n            max(vea.n_values)\n            if hasattr(vea, \"n_values\") and len(vea.n_values) &gt; 0\n            else 1\n        )\n        self._data = np.empty((n_series, 0, n_values))\n        # Call base class constructor with metadata from vea, but don't copy series_ids\n        super().__init__(\n            fs=getattr(vea, \"fs\", None),\n            series_ids=None,  # Let the base class handle default series_ids\n            abscissa=getattr(vea, \"_abscissa\", None),\n            ordinate=getattr(vea, \"_ordinate\", None),\n            empty=False,\n        )\n        self.vea = vea\n        self.ds = ds\n        self.method = method\n        self._bin()\n\n    def _bin(self):\n        \"\"\"Perform binning operation on the ValueEventArray data, matching BinnedEventArray logic.\n\n        For each support interval, bins are defined globally (not per-series) using np.linspace as in BinnedEventArray.\n        All series use the same bins for a given interval. np.histogram is used for each value column for each series.\n        Aggregation is performed using the specified method (sum, mean, etc.).\n        \"\"\"\n        support = getattr(self.vea, \"support\", None)\n        if support is None:\n            # Fallback: use all events if no support is defined\n            all_events = np.concatenate([np.asarray(ev) for ev in self.vea.events])\n            if all_events.size == 0:\n                self._data = np.zeros((self.vea.n_series, 0, self.vea.n_values[0]))\n                self._bins = np.array([])\n                self._bin_centers = np.array([])\n                return\n            tmin = np.min(all_events)\n            tmax = np.max(all_events)\n            intervals = [(tmin, tmax)]\n        else:\n            intervals = list(zip(support.starts, support.stops))\n\n        n_series = self.vea.n_series\n        n_values = max(self.vea.n_values)\n        all_binned = []\n        all_bins = []\n        all_bin_centers = []\n\n        if isinstance(self.method, str):\n            method_str = self.method\n            if method_str in (\"sum\", \"mean\", \"min\", \"max\", \"median\"):\n                use_numba = True\n            else:\n                use_numba = False\n        elif callable(self.method):\n            method_str = None\n            use_numba = False\n        else:\n            raise ValueError(\"method must be a string or callable\")\n\n        for start, stop in intervals:\n            interval_length = stop - start\n            if interval_length &lt; self.ds:\n                continue\n            n_bins = int(np.floor(interval_length / self.ds))\n            bins = np.linspace(start, start + n_bins * self.ds, n_bins + 1)\n            bin_centers = bins[:-1] + (self.ds / 2)\n            binned = np.full((n_series, n_bins, n_values), np.nan)\n\n            if use_numba and n_values == 1:\n                # Convert to numba.typed.List to avoid reflected list warning\n                try:\n                    events_list = NumbaList()\n                    for ev in self.vea.events:\n                        events_list.append(np.asarray(ev))\n                    values_list = NumbaList()\n                    for val in self.vea.values:\n                        values_list.append(np.asarray(val))\n                except Exception:\n                    # Fallback to regular list if numba.typed.List is not available\n                    events_list = [np.asarray(ev) for ev in self.vea.events]\n                    values_list = [np.asarray(val) for val in self.vea.values]\n                binned_agg = _bin_ragged_agg_numba(\n                    events_list, values_list, bins, method_str\n                )\n                binned[:, :, 0] = binned_agg\n            else:\n                for i, (ev, val) in enumerate(zip(self.vea.events, self.vea.values)):\n                    ev = np.asarray(ev)\n                    val = np.asarray(val)\n                    mask_interval = (ev &gt;= start) &amp; (ev &lt; stop)\n                    ev_in = ev[mask_interval]\n                    val_in = val[mask_interval]\n                    if ev_in.size == 0:\n                        continue\n                    for v in range(n_values):\n                        if val_in.ndim == 1:\n                            vals = val_in\n                        else:\n                            vals = val_in[:, v]\n                        if method_str == \"sum\":\n                            hist, _ = np.histogram(ev_in, bins=bins, weights=vals)\n                            binned[i, :, v] = hist\n                        else:\n                            inds = np.digitize(ev_in, bins, right=False) - 1\n                            for b in range(n_bins):\n                                vals_in_bin = vals[inds == b]\n                                if vals_in_bin.size &gt; 0:\n                                    if method_str == \"mean\":\n                                        binned[i, b, v] = np.mean(vals_in_bin)\n                                    elif method_str == \"min\":\n                                        binned[i, b, v] = np.min(vals_in_bin)\n                                    elif method_str == \"max\":\n                                        binned[i, b, v] = np.max(vals_in_bin)\n                                    elif method_str == \"median\":\n                                        binned[i, b, v] = np.median(vals_in_bin)\n                                    else:\n                                        binned[i, b, v] = self.method(vals_in_bin)\n            all_binned.append(binned)\n            all_bins.append(bins)\n            all_bin_centers.append(bin_centers)\n\n        if all_binned:\n            self._data = np.concatenate(all_binned, axis=1)\n            self._bins = np.concatenate(\n                [b[:-1] for b in all_bins] + [all_bins[-1][-1:]]\n            )\n            self._bin_centers = np.concatenate(all_bin_centers)\n        else:\n            self._data = np.zeros((n_series, 0, n_values))\n            self._bins = np.array([])\n            self._bin_centers = np.array([])\n\n    @property\n    def data(self):\n        return self._data\n\n    @property\n    def bins(self):\n        return self._bins\n\n    @property\n    def bin_centers(self):\n        return self._bin_centers\n\n    def __repr__(self):\n        return f\"&lt;BinnedValueEventArray: {self.n_series} series, {self.n_bins} bins, {self.n_values} values, ds={self.ds}&gt;\"\n\n    def __getitem__(self, idx):\n        # Slicing by IntervalArray/EpochArray\n        if hasattr(idx, \"starts\") and hasattr(idx, \"stops\"):\n            mask = np.zeros_like(self.bin_centers, dtype=bool)\n            for start, stop in zip(idx.starts, idx.stops):\n                mask |= (self.bin_centers &gt;= start) &amp; (self.bin_centers &lt; stop)\n            new_obj = copy.copy(self)\n            new_obj._data = self.data[:, mask, :]\n            # Adjust bins and bin_centers\n            # bins: keep edges that bracket the selected bin_centers\n            bin_indices = np.where(mask)[0]\n            if len(bin_indices) == 0:\n                new_obj._bins = np.array([])\n                new_obj._bin_centers = np.array([])\n            else:\n                new_obj._bins = self.bins[bin_indices[0] : bin_indices[-1] + 2]\n                new_obj._bin_centers = self.bin_centers[mask]\n            return new_obj\n        else:\n            raise NotImplementedError(\n                \"Only slicing by IntervalArray/EpochArray is supported.\"\n            )\n\n    @property\n    def n_series(self):\n        return self.data.shape[0]\n\n    @property\n    def n_bins(self):\n        return self.data.shape[1]\n\n    @property\n    def n_values(self):\n        return self.data.shape[2]\n\n    @property\n    def fs(self):\n        return getattr(self, \"_fs\", None)\n\n    @fs.setter\n    def fs(self, value):\n        self._fs = value\n\n    @property\n    def support(self):\n        return getattr(self.vea, \"support\", None)\n\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BinnedValueEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_intervals : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BinnedValueEventArray\n            BinnedValueEventArray that has been partitioned.\n        \"\"\"\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n        return out\n\n    @staticmethod\n    def _to_2d_array(arr):\n        \"\"\"Convert array to 2D numpy array, handling object arrays properly.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.dtype == object:\n            # Handle object arrays by extracting the actual data\n            if arr.size == 1:\n                # Single element object array\n                return np.atleast_2d(arr[0])\n            else:\n                # Multiple element object array - concatenate\n                flattened = []\n                for item in arr:\n                    if isinstance(item, np.ndarray):\n                        flattened.append(item)\n                    else:\n                        flattened.append(np.array(item))\n                if flattened:\n                    return np.vstack(flattened)\n                else:\n                    return np.array([]).reshape(0, 0)\n        else:\n            return np.atleast_2d(arr)\n\n    def smooth(\n        self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n    ):\n        \"\"\"Smooth BinnedValueEventArray by convolving with a Gaussian kernel.\n\n        Smoothing is applied in data, and the same smoothing is applied\n        to each series in a BinnedValueEventArray.\n\n        Smoothing is applied within each interval by default.\n\n        Parameters\n        ----------\n        sigma : float, optional\n            Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n        truncate : float, optional\n            Bandwidth outside of which the filter value will be zero. Default is 4.0\n        inplace : bool, optional\n            If True the data will be replaced with the smoothed data. Default is False.\n        within_intervals : bool, optional\n            If True, smooth within each interval. If False, smooth across intervals. Default is False.\n\n        Returns\n        -------\n        out : BinnedValueEventArray\n            New BinnedValueEventArray with smoothed data.\n        \"\"\"\n        from .. import utils  # local import to avoid circular import\n\n        if truncate is None:\n            truncate = 4\n        if sigma is None:\n            sigma = 0.01  # 10 ms default\n        fs = 1 / self.ds if self.ds else None\n        return utils.gaussian_filter(\n            self,\n            fs=fs,\n            sigma=sigma,\n            truncate=truncate,\n            inplace=inplace,\n            within_intervals=within_intervals,\n        )\n\n    @property\n    def _abscissa_vals(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        return self._bin_centers\n\n    @property\n    def lengths(self):\n        \"\"\"Lengths of contiguous segments, in number of bins.\"\"\"\n        if (\n            self.n_bins == 0\n            or self.support is None\n            or getattr(self.support, \"isempty\", False)\n        ):\n            return 0\n        # Each interval: count how many bin_centers fall within it\n        lengths = []\n        for start, stop in zip(self.support.starts, self.support.stops):\n            mask = (self.bin_centers &gt;= start) &amp; (self.bin_centers &lt; stop)\n            lengths.append(np.sum(mask))\n        return np.array(lengths)\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BinnedValueEventArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>Lengths of contiguous segments, in number of bins.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BinnedValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BinnedValueEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_intervals</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedValueEventArray</code> <p>BinnedValueEventArray that has been partitioned.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BinnedValueEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_intervals : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BinnedValueEventArray\n        BinnedValueEventArray that has been partitioned.\n    \"\"\"\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.BinnedValueEventArray.smooth","title":"<code>smooth(*, sigma=None, inplace=False, truncate=None, within_intervals=False)</code>","text":"<p>Smooth BinnedValueEventArray by convolving with a Gaussian kernel.</p> <p>Smoothing is applied in data, and the same smoothing is applied to each series in a BinnedValueEventArray.</p> <p>Smoothing is applied within each interval by default.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <code>within_intervals</code> <code>bool</code> <p>If True, smooth within each interval. If False, smooth across intervals. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedValueEventArray</code> <p>New BinnedValueEventArray with smoothed data.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def smooth(\n    self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n):\n    \"\"\"Smooth BinnedValueEventArray by convolving with a Gaussian kernel.\n\n    Smoothing is applied in data, and the same smoothing is applied\n    to each series in a BinnedValueEventArray.\n\n    Smoothing is applied within each interval by default.\n\n    Parameters\n    ----------\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n    inplace : bool, optional\n        If True the data will be replaced with the smoothed data. Default is False.\n    within_intervals : bool, optional\n        If True, smooth within each interval. If False, smooth across intervals. Default is False.\n\n    Returns\n    -------\n    out : BinnedValueEventArray\n        New BinnedValueEventArray with smoothed data.\n    \"\"\"\n    from .. import utils  # local import to avoid circular import\n\n    if truncate is None:\n        truncate = 4\n    if sigma is None:\n        sigma = 0.01  # 10 ms default\n    fs = 1 / self.ds if self.ds else None\n    return utils.gaussian_filter(\n        self,\n        fs=fs,\n        sigma=sigma,\n        truncate=truncate,\n        inplace=inplace,\n        within_intervals=within_intervals,\n    )\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.IntervalSeriesSlicer","title":"<code>IntervalSeriesSlicer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class IntervalSeriesSlicer(object):\n    def __init__(self):\n        pass\n\n    def __getitem__(self, *args):\n        \"\"\"intervals (e.g. epochs), series (e.g. units)\"\"\"\n        # by default, keep all series\n        seriesslice = slice(None, None, None)\n        if isinstance(*args, int):\n            intervalslice = args[0]\n        elif isinstance(*args, core.IntervalArray):\n            intervalslice = args[0]\n        else:\n            try:\n                slices = np.s_[args]\n                slices = slices[0]\n                if len(slices) &gt; 2:\n                    raise IndexError(\n                        \"only [intervals, series] slicing is supported at this time!\"\n                    )\n                elif len(slices) == 2:\n                    intervalslice, seriesslice = slices\n                else:\n                    intervalslice = slices[0]\n            except TypeError:\n                # only interval to slice:\n                intervalslice = slices\n\n        return intervalslice, seriesslice\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ItemGetter_iloc","title":"<code>ItemGetter_iloc</code>","text":"<p>               Bases: <code>object</code></p> <p>.iloc is primarily integer position based (from 0 to length-1 of the axis).</p> <p>.iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with python/numpy slice semantics).</p> <p>Allowed inputs are:     - An integer e.g. 5     - A list or array of integers [4, 3, 0]     - A slice object with ints 1:7</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class ItemGetter_iloc(object):\n    \"\"\".iloc is primarily integer position based (from 0 to length-1\n    of the axis).\n\n    .iloc will raise IndexError if a requested indexer is\n    out-of-bounds, except slice indexers which allow out-of-bounds\n    indexing. (this conforms with python/numpy slice semantics).\n\n    Allowed inputs are:\n        - An integer e.g. 5\n        - A list or array of integers [4, 3, 0]\n        - A slice object with ints 1:7\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = IntervalSeriesSlicer()[idx]\n        out = copy.copy(self.obj)\n        if isinstance(seriesslice, int):\n            seriesslice = [seriesslice]\n        out._data = out._data[[seriesslice]]\n        singleseries = len(out._data) == 1\n        if singleseries:\n            out._data = np.array(out._data[[0]], ndmin=2)\n        out._series_ids = list(\n            np.atleast_1d(np.atleast_1d(out._series_ids)[seriesslice])\n        )\n\n        # TODO: update tags\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                out.loc = ItemGetter_loc(out)\n                out.iloc = ItemGetter_iloc(out)\n                return out\n        out = out._intervalslicer(intervalslice)\n        out.loc = ItemGetter_loc(out)\n        out.iloc = ItemGetter_iloc(out)\n        return out\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ItemGetter_loc","title":"<code>ItemGetter_loc</code>","text":"<p>               Bases: <code>object</code></p> <p>.loc is primarily label based (that is, series_id based)</p> <p>.loc will raise KeyError when the items are not found.</p> <p>Allowed inputs are:     - A single label, e.g. 5 or 'a', (note that 5 is interpreted         as a label of the index. This use is not an integer         position along the index)     - A list or array of labels ['a', 'b', 'c']     - A slice object with labels 'a':'f', (note that contrary to         usual python slices, both the start and the stop are         included!)</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class ItemGetter_loc(object):\n    \"\"\".loc is primarily label based (that is, series_id based)\n\n    .loc will raise KeyError when the items are not found.\n\n    Allowed inputs are:\n        - A single label, e.g. 5 or 'a', (note that 5 is interpreted\n            as a label of the index. This use is not an integer\n            position along the index)\n        - A list or array of labels ['a', 'b', 'c']\n        - A slice object with labels 'a':'f', (note that contrary to\n            usual python slices, both the start and the stop are\n            included!)\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = IntervalSeriesSlicer()[idx]\n\n        # first convert series slice into list\n        if isinstance(seriesslice, slice):\n            start = seriesslice.start\n            stop = seriesslice.stop\n            istep = seriesslice.step\n            try:\n                if start is None:\n                    istart = 0\n                else:\n                    istart = self.obj._series_ids.index(start)\n            except ValueError:\n                raise KeyError(\n                    \"series_id {} could not be found in BaseEventArray!\".format(start)\n                )\n            try:\n                if stop is None:\n                    istop = self.obj.n_series\n                else:\n                    istop = self.obj._series_ids.index(stop) + 1\n            except ValueError:\n                raise KeyError(\n                    \"series_id {} could not be found in BaseEventArray!\".format(stop)\n                )\n            if istep is None:\n                istep = 1\n            if istep &lt; 0:\n                istop -= 1\n                istart -= 1\n                istart, istop = istop, istart\n            series_idx_list = list(range(istart, istop, istep))\n        else:\n            series_idx_list = []\n            seriesslice = np.atleast_1d(seriesslice)\n            for series in seriesslice:\n                try:\n                    uidx = self.obj.series_ids.index(series)\n                except ValueError:\n                    raise KeyError(\n                        \"series_id {} could not be found in BaseEventArray!\".format(\n                            series\n                        )\n                    )\n                else:\n                    series_idx_list.append(uidx)\n\n        if not isinstance(series_idx_list, list):\n            series_idx_list = list(series_idx_list)\n\n        out = copy.copy(self.obj)\n        try:\n            out._data = out._data[[series_idx_list]]\n            singleseries = len(out._data) == 1\n        except AttributeError:\n            out._data = out._data[[series_idx_list]]\n            singleseries = len(out._data) == 1\n\n        if singleseries:\n            out._data = np.array([out._data[0]], ndmin=2)\n        out._series_ids = list(\n            np.atleast_1d(np.atleast_1d(out._series_ids)[[series_idx_list]])\n        )\n        # TODO: update tags\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                out.loc = ItemGetter_loc(out)\n                out.iloc = ItemGetter_iloc(out)\n                return out\n        out = out._intervalslicer(intervalslice)\n        out.loc = ItemGetter_loc(out)\n        out.iloc = ItemGetter_iloc(out)\n        return out\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.MarkedSpikeTrainArray","title":"<code>MarkedSpikeTrainArray</code>","text":"<p>               Bases: <code>ValueEventArray</code></p> <p>MarkedSpikeTrainArray for storing spike times with associated marks (e.g., waveform features).</p> <p>This class extends ValueEventArray to support marks for each spike event, such as tetrode features or other metadata.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>array - like</code> <p>Spike times or event times.</p> required <code>marks</code> <code>array - like</code> <p>Associated marks/features for each event.</p> required <code>support</code> <code>IntervalArray</code> <p>Support intervals for the spike train.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> required <code>series_label</code> <code>str</code> <p>Label for the series (e.g., 'tetrodes').</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>events</code> <code>array - like</code> <p>Spike times or event times.</p> <code>marks</code> <code>array - like</code> <p>Associated marks/features for each event.</p> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the spike train.</p> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> <code>series_label</code> <code>str</code> <p>Label for the series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; msta = MarkedSpikeTrainArray(events=spike_times, marks=features, fs=30000)\n&gt;&gt;&gt; msta.events\narray([...])\n&gt;&gt;&gt; msta.marks\narray([...])\n</code></pre> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class MarkedSpikeTrainArray(ValueEventArray):\n    \"\"\"\n    MarkedSpikeTrainArray for storing spike times with associated marks (e.g., waveform features).\n\n    This class extends ValueEventArray to support marks for each spike event, such as tetrode features or other metadata.\n\n    Parameters\n    ----------\n    events : array-like\n        Spike times or event times.\n    marks : array-like\n        Associated marks/features for each event.\n    support : nelpy.IntervalArray, optional\n        Support intervals for the spike train.\n    fs : float, optional\n        Sampling frequency in Hz.\n    series_label : str, optional\n        Label for the series (e.g., 'tetrodes').\n    **kwargs :\n        Additional keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    events : array-like\n        Spike times or event times.\n    marks : array-like\n        Associated marks/features for each event.\n    support : nelpy.IntervalArray\n        Support intervals for the spike train.\n    fs : float\n        Sampling frequency in Hz.\n    series_label : str\n        Label for the series.\n\n    Examples\n    --------\n    &gt;&gt;&gt; msta = MarkedSpikeTrainArray(events=spike_times, marks=features, fs=30000)\n    &gt;&gt;&gt; msta.events\n    array([...])\n    &gt;&gt;&gt; msta.marks\n    array([...])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"n_marks\": \"n_values\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n        \"marks\": \"values\",\n        \"spikes\": \"events\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"tetrodes\"\n        kwargs[\"series_label\"] = series_label\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n\n    # @keyword_equivalence(this_or_that={'n_intervals':'n_epochs'})\n    # def partition(self, ds=None, n_intervals=None, n_epochs=None):\n    #     if n_intervals is None:\n    #         n_intervals = n_epochs\n    #     kwargs = {'ds':ds, 'n_intervals': n_intervals}\n    #     return super().partition(**kwargs)\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n        raise NotImplementedError\n        return BinnedMarkedSpikeTrainArray(self, ds=ds)  # noqa: F821\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.MarkedSpikeTrainArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedSpikeTrainArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n    raise NotImplementedError\n    return BinnedMarkedSpikeTrainArray(self, ds=ds)  # noqa: F821\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray","title":"<code>StatefulValueEventArray</code>","text":"<p>               Bases: <code>BaseValueEventArray</code></p> <p>StatefulValueEventArray for storing events with associated values and states.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>array - like</code> <p>Event times for each series. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>values</code> <code>array - like</code> <p>Values associated with each event. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>states</code> <code>array - like</code> <p>States associated with each event. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the events. If None, inferred from events.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Default is 30000.</p> <code>None</code> <code>series_ids</code> <code>list</code> <p>List of series IDs. If None, defaults to [1, ..., n_series].</p> <code>None</code> <code>empty</code> <code>bool</code> <p>If True, create an empty object.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>events</code> <code>ndarray</code> <p>Event times for each series. Ragged array, shape (n_series, n_events_i).</p> <code>values</code> <code>ndarray</code> <p>Values for each event. Ragged array, shape (n_series, n_events_i).</p> <code>states</code> <code>ndarray</code> <p>States for each event. Ragged array, shape (n_series, n_events_i).</p> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the events.</p> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> <code>n_series</code> <code>int</code> <p>Number of series.</p> <code>n_events</code> <code>ndarray</code> <p>Number of events in each series.</p> <code>series_ids</code> <code>list</code> <p>List of series IDs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n&gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; states = [[10, 20, 30], [40, 50, 60]]\n&gt;&gt;&gt; sveva = nel.StatefulValueEventArray(\n...     events=events, values=values, states=states, fs=10\n... )\n&gt;&gt;&gt; sveva.n_series\n2\n&gt;&gt;&gt; sveva.n_events\narray([3, 3])\n&gt;&gt;&gt; sveva.events[0]\narray([0.1, 0.5, 1.0])\n&gt;&gt;&gt; sveva.values[0]\narray([1, 2, 3])\n&gt;&gt;&gt; sveva.states[0]\narray([10, 20, 30])\n</code></pre> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class StatefulValueEventArray(BaseValueEventArray):\n    \"\"\"\n    StatefulValueEventArray for storing events with associated values and states.\n\n    Parameters\n    ----------\n    events : array-like\n        Event times for each series. List of arrays, shape (n_series, n_events_i).\n    values : array-like\n        Values associated with each event. List of arrays, shape (n_series, n_events_i).\n    states : array-like\n        States associated with each event. List of arrays, shape (n_series, n_events_i).\n    support : nelpy.IntervalArray, optional\n        Support intervals for the events. If None, inferred from events.\n    fs : float, optional\n        Sampling frequency in Hz. Default is 30000.\n    series_ids : list, optional\n        List of series IDs. If None, defaults to [1, ..., n_series].\n    empty : bool, optional\n        If True, create an empty object.\n    **kwargs\n        Additional keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    events : np.ndarray\n        Event times for each series. Ragged array, shape (n_series, n_events_i).\n    values : np.ndarray\n        Values for each event. Ragged array, shape (n_series, n_events_i).\n    states : np.ndarray\n        States for each event. Ragged array, shape (n_series, n_events_i).\n    support : nelpy.IntervalArray\n        Support intervals for the events.\n    fs : float\n        Sampling frequency in Hz.\n    n_series : int\n        Number of series.\n    n_events : np.ndarray\n        Number of events in each series.\n    series_ids : list\n        List of series IDs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n    &gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n    &gt;&gt;&gt; states = [[10, 20, 30], [40, 50, 60]]\n    &gt;&gt;&gt; sveva = nel.StatefulValueEventArray(\n    ...     events=events, values=values, states=states, fs=10\n    ... )\n    &gt;&gt;&gt; sveva.n_series\n    2\n    &gt;&gt;&gt; sveva.n_events\n    array([3, 3])\n    &gt;&gt;&gt; sveva.events[0]\n    array([0.1, 0.5, 1.0])\n    &gt;&gt;&gt; sveva.values[0]\n    array([1, 2, 3])\n    &gt;&gt;&gt; sveva.states[0]\n    array([10, 20, 30])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"n_marks\": \"n_values\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n        \"marks\": \"values\",\n        \"spikes\": \"events\",\n    }\n\n    def __init__(\n        self,\n        events=None,\n        values=None,\n        states=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        # add class-specific aliases to existing aliases:\n        # self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        # print('in init')\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        # print('non-stateful preprocessing')\n        self._val_init(\n            events=events,\n            values=values,\n            states=states,\n            fs=fs,\n            support=support,\n            series_ids=series_ids,\n            empty=empty,\n            **kwargs,\n        )\n\n        # print('making stateful')\n        data = self._make_stateful(data=self.data)\n        self._data = data\n\n    def _val_init(\n        self,\n        events=None,\n        values=None,\n        states=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        #############################################\n        #            standardize kwargs             #\n        #############################################\n        if events is not None:\n            kwargs[\"events\"] = events\n        if values is not None:\n            kwargs[\"values\"] = values\n        if states is not None:\n            kwargs[\"states\"] = states\n        kwargs = self._standardize_kwargs(**kwargs)\n        events = kwargs.pop(\"events\", None)\n        values = kwargs.pop(\"values\", None)\n        states = kwargs.pop(\"states\", None)\n        #############################################\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.info(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            data = np.array(data)\n            try:\n                if data.shape[-1] &lt; 2 and np.max(data.shape) &gt; 1:\n                    return True\n                if max(np.array(data).shape[:-1]) &gt; 1 and data.shape[-1] == 1:\n                    return True\n            except (IndexError, TypeError, ValueError):\n                return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            ========\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            try:\n                if isinstance(data[0][0], list) or isinstance(data[0][0], np.ndarray):\n                    logging.info(\"event datas input has too many layers!\")\n                    try:\n                        if max(np.array(data).shape[:-1]) &gt; 1:\n                            #                 singletons = True\n                            return False\n                    except ValueError:\n                        return False\n                    data = np.squeeze(data)\n            except (IndexError, TypeError):\n                pass\n            try:\n                if isinstance(data[1], list) or isinstance(data[1], np.ndarray):\n                    return False\n            except (IndexError, TypeError):\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            # Handle ragged input: list/tuple or np.ndarray of dtype=object\n            is_ragged = False\n            if isinstance(data, (list, tuple)):\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            elif isinstance(data, np.ndarray) and data.dtype == object:\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            if is_ragged:\n                return utils.ragged_array([np.array(st, ndmin=1) for st in data])\n            # Only here, if not ragged, use np.array/np.squeeze\n            return np.array(np.squeeze(data), ndmin=2)\n\n        def standardize_values_to_2d(data):\n            data = standardize_to_2d(data)\n            for ii, series in enumerate(data):\n                if len(series.shape) == 2:\n                    pass\n                else:\n                    for xx in series:\n                        if len(np.atleast_1d(xx)) &gt; 1:\n                            raise ValueError(\n                                \"each series must have a fixed number of values; mismatch in series {}\".format(\n                                    ii\n                                )\n                            )\n            return data\n\n        events = standardize_to_2d(events)\n        values = standardize_values_to_2d(values)\n        states = standardize_to_2d(states)\n\n        data = []\n        for a, v, s in zip(events, values, states):\n            data.append(np.vstack((a, v.T, s.T)).T)\n        data = np.array(data)\n\n        # sort event series, but only if necessary:\n        for ii, train in enumerate(events):\n            if not utils.is_sorted(train):\n                sortidx = np.argsort(train)\n                data[ii] = (data[ii])[sortidx, :]\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            self._abscissa.support = type(self._abscissa.support)(\n                np.array([self.first_event, self.last_event + 1 / fs])\n            )\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            self._abscissa.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        data = self._restrict_to_interval_array_fast(\n            intervalarray=self.support, data=data\n        )\n\n        self._data = data\n        return\n\n    @property\n    def data(self):\n        \"\"\"Event datas in seconds.\"\"\"\n        return self._data\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to ValueEventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_value_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_value_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    def _intervalslicer(self, idx):\n        \"\"\"Helper function to restrict object to EpochArray.\"\"\"\n        # if self.isempty:\n        #     return self\n\n        if isinstance(idx, core.IntervalArray):\n            if idx.isempty:\n                return type(self)(empty=True)\n            support = self._abscissa.support.intersect(\n                interval=idx, boundaries=True\n            )  # what if fs of slicing interval is different?\n            if support.isempty:\n                return type(self)(empty=True)\n\n            logging.disable(logging.CRITICAL)\n            data = self._restrict_to_interval_array_value_fast(\n                intervalarray=support, data=self.data, copyover=True\n            )\n            eventarray = self._copy_without_data()\n            eventarray._data = data\n            eventarray._abscissa.support = support\n            eventarray.__renew__()\n            logging.disable(0)\n            return eventarray\n        elif isinstance(idx, int):\n            eventarray = self._copy_without_data()\n            support = self._abscissa.support[idx]\n            eventarray._abscissa.support = support\n            if (idx &gt;= self._abscissa.support.n_intervals) or idx &lt; (\n                -self._abscissa.support.n_intervals\n            ):\n                eventarray.__renew__()\n                return eventarray\n            else:\n                data = self._restrict_to_interval_array_value_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                return eventarray\n        else:  # most likely slice indexing\n            try:\n                logging.disable(logging.CRITICAL)\n                support = self._abscissa.support[idx]\n                data = self._restrict_to_interval_array_value_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray = self._copy_without_data()\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                logging.disable(0)\n                return eventarray\n            except Exception:\n                raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    @staticmethod\n    def _restrict_to_interval_array_fast(intervalarray, data, copyover=True):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                frm, to = np.searchsorted(evt_data[:, 0], (t_start, t_stop))\n                indices.append((frm, to))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n        return data\n\n    def _restrict_to_interval_array_value_fast(\n        self, intervalarray, data, copyover=True\n    ):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        # plan of action\n        # create pseudo events supporting each interval\n        # then restrict existing data (pseudo and real events)\n        # then merge in all pseudo events that don't exist yet\n        starts = intervalarray.starts\n        stops = intervalarray.stops\n\n        kinds = []\n        events = []\n        states = []\n\n        for series in data:\n            tvect = series[:, 0].astype(float)\n            statevals = series[:, 2:]\n\n            kind = []\n            state = []\n\n            for start in starts:\n                idx = np.max((np.searchsorted(tvect, start, side=\"right\") - 1, 0))\n                kind.append(0)\n                state.append(statevals[[idx]])\n\n            for stop in stops:\n                idx = np.max((np.searchsorted(tvect, stop, side=\"right\") - 1, 0))\n                kind.append(2)\n                state.append(statevals[[idx]])\n\n            states.append(np.array(state).squeeze())  ## squeeze???\n            events.append(np.hstack((starts, stops)))\n            kinds.append(np.array(kind))\n\n        pseudodata = []\n        for e, k, s in zip(events, kinds, states):\n            pseudodata.append(np.vstack((e, k, s.T)).T)\n\n        pseudodata = utils.ragged_array(pseudodata)\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                frm, to = np.searchsorted(evt_data[:, 0], (t_start, t_stop))\n                indices.append((frm, to))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n\n        # now add in all pseudo events that don't already exist in data\n\n        kinds = []\n        events = []\n        states = []\n\n        for pseries, series in zip(pseudodata, data):\n            ptvect = pseries[:, 0].astype(float)\n            pkind = pseries[:, 1].astype(int)\n            pstatevals = pseries[:, 2:]\n\n            try:\n                tvect = series[:, 0].astype(float)\n                kind = series[:, 1]\n                statevals = series[:, 2:]\n            except IndexError:\n                tvect = np.zeros((0))\n                kind = np.zeros((0))\n                statevals = np.zeros((0,))\n\n            for tt, kk, psv in zip(ptvect, pkind, pstatevals):\n                # print(tt, kk, psv)\n                idx = np.searchsorted(tvect, tt, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                try:\n                    if tt == tvect[idx2]:\n                        pass\n                        # print('pseudo event {} not necessary...'.format(tt))\n                    else:\n                        # print('pseudo event {} necessary...'.format(tt))\n                        kind = np.insert(kind, idx, kk)\n                        tvect = np.insert(tvect, idx, tt)\n                        statevals = np.insert(statevals, idx, psv, axis=0)\n                except IndexError:\n                    kind = np.insert(kind, idx, kk)\n                    tvect = np.insert(tvect, idx, tt)\n                    statevals = np.insert(statevals, idx, psv, axis=0)\n\n            states.append(np.array(statevals).squeeze())\n            events.append(tvect)\n            kinds.append(kind)\n\n        # print(states)\n        # print(tvect)\n        # print(kinds)\n\n        data = []\n        for e, k, s in zip(events, kinds, states):\n            data.append(np.vstack((e, k, s.T)).T)\n\n        data = utils.ragged_array(data)\n\n        return data\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedValueEventArray.\"\"\"\n        raise NotImplementedError\n        return BinnedValueEventArray(self, ds=ds)\n\n    def __call__(self, *args):\n        \"\"\"StatefulValueEventArray callable method; by default returns state values\"\"\"\n        values = []\n        for events, vals in zip(self.state_events, self.state_values):\n            idx = np.searchsorted(events, args, side=\"right\") - 1\n            idx[idx &lt; 0] = 0\n            values.append(vals[[idx]])\n        values = np.asarray(values)\n        return values\n\n    def _make_stateful(self, data, intervalarray=None, initial_state=np.nan):\n        \"\"\"\n        [i, e0, e1, e2, ..., f] for every epoch\n\n        matrix of size (n_values x (n_epochs*2 + n_events) )\n        matrix of size (nSeries: n_values x (n_epochs*2 + n_events) )\n\n        needs to change when calling loc, iloc, restrict, getitem, ...\n\n        TODO: initial_state is not used yet!!!\n        \"\"\"\n        kinds = []\n        events = []\n        states = []\n\n        if intervalarray is None:\n            intervalarray = self.support\n\n        for series in data:\n            starts = intervalarray.starts\n            stops = intervalarray.stops\n            tvect = series[:, 0].astype(float)\n            statevals = series[:, 1:]\n            kind = np.ones(tvect.size).astype(int)\n\n            for start in starts:\n                idx = np.searchsorted(tvect, start, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                if start == tvect[idx2]:\n                    continue\n                else:\n                    kind = np.insert(kind, idx, 0)\n                    tvect = np.insert(tvect, idx, start)\n                    statevals = np.insert(statevals, idx, statevals[idx2], axis=0)\n\n            for stop in stops:\n                idx = np.searchsorted(tvect, stop, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                if stop == tvect[idx2]:\n                    continue\n                else:\n                    kind = np.insert(kind, idx, 2)\n                    tvect = np.insert(tvect, idx, stop)\n                    statevals = np.insert(statevals, idx, statevals[idx2], axis=0)\n\n            states.append(statevals)\n            events.append(tvect)\n            kinds.append(kind)\n\n        data = []\n        for e, k, s in zip(events, kinds, states):\n            data.append(np.vstack((e, k, s.T)).T)\n        data = utils.ragged_array(data)\n\n        return data\n\n    @property\n    def n_values(self):\n        \"\"\"(int) The number of values associated with each event series.\"\"\"\n        if self.isempty:\n            return 0\n        n_values = []\n        for series in self.data:\n            n_values.append(series.squeeze().shape[1] - 2)\n        return n_values\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s\" % (self.type_name, address_str, numstr, epstr, fsstr)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.events])\n\n    @property\n    def events(self):\n        events = []\n        for series, kinds in zip(self.state_events, self.state_kinds):\n            keep_idx = np.argwhere(kinds == 1)\n            events.append(series[keep_idx].squeeze())\n        return np.asarray(events)\n\n    @property\n    def values(self):\n        values = []\n        for series, kinds in zip(self.state_values, self.state_kinds):\n            keep_idx = np.argwhere(kinds == 1)\n            values.append(series[keep_idx].squeeze())\n        return np.asarray(values)\n\n    @property\n    def state_events(self):\n        events = []\n        for series in self.data:\n            events.append(series[:, 0].squeeze())\n\n        return np.asarray(events)\n\n    @property\n    def state_values(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 2:].squeeze())\n\n        return np.asarray(values)\n\n    @property\n    def state_kinds(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 1].squeeze())\n\n        return np.asarray(values)\n\n    def _plot(self, *args, **kwargs):\n        if self.n_series &gt; 1:\n            raise NotImplementedError\n        if np.any(np.array(self.n_values) &gt; 1):\n            raise NotImplementedError\n\n        import matplotlib.pyplot as plt\n\n        events = self.state_events.squeeze()\n        values = self.state_values.squeeze()\n        kinds = self.state_kinds.squeeze()\n\n        for (a, b), val, (ka, kb) in zip(\n            utils.pairwise(events), values, utils.pairwise(kinds)\n        ):\n            if kb == 1:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"b\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n            if ka == 1:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"g\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n            if kb == 1:\n                plt.plot(b, val, \"o\", color=\"k\", markerfacecolor=\"w\", lw=1.5, mew=1.5)\n            if ka == 1:\n                plt.plot(a, val, \"o\", color=\"k\", markerfacecolor=\"k\", lw=1.5, mew=1.5)\n            if ka == 0 and kb == 2:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"r\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Event datas in seconds.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the underlying EventArray.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray.n_values","title":"<code>n_values</code>  <code>property</code>","text":"<p>(int) The number of values associated with each event series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying EventArray.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedValueEventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedValueEventArray.\"\"\"\n    raise NotImplementedError\n    return BinnedValueEventArray(self, ds=ds)\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.StatefulValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray","title":"<code>ValueEventArray</code>","text":"<p>               Bases: <code>BaseValueEventArray</code></p> <p>A multiseries eventarray with shared support.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,)</p> required <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000</p> <code>None</code> <code>support</code> <code>EpochArray</code> <p>EpochArray on which eventarrays are defined. Default is [0, last event] inclusive.</p> <code>None</code> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> required <code>cell_type</code> <code>list (of length n_series) of str or other</code> <p>Identified cell type indicator, e.g., 'pyr', 'int'.</p> required <code>series_ids</code> <code>list (of length n_series) of indices corresponding to</code> <p>curated data. If no series_ids are specified, then [1,...,n_series] will be used. WARNING! The first series will have index 1, not 0!</p> <code>None</code> <code>meta</code> <code>dict</code> <p>Metadata associated with eventarray.</p> required <p>Attributes:</p> Name Type Description <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,)</p> <code>support</code> <code>EpochArray on which eventarray is defined.</code> <code>n_events</code> <code>np.array(dtype=np.int) of shape (n_series,)</code> <p>Number of events in each series.</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz).</p> <code>cell_types</code> <code>np.array of str or other</code> <p>Identified cell type for each series.</p> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> <code>meta</code> <code>dict</code> <p>Metadata associated with eventseries.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class ValueEventArray(BaseValueEventArray):\n    \"\"\"A multiseries eventarray with shared support.\n\n    Parameters\n    ----------\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,)\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000\n    support : EpochArray, optional\n        EpochArray on which eventarrays are defined.\n        Default is [0, last event] inclusive.\n    label : str or None, optional\n        Information pertaining to the source of the eventarray.\n    cell_type : list (of length n_series) of str or other, optional\n        Identified cell type indicator, e.g., 'pyr', 'int'.\n    series_ids : list (of length n_series) of indices corresponding to\n        curated data. If no series_ids are specified, then [1,...,n_series]\n        will be used. WARNING! The first series will have index 1, not 0!\n    meta : dict\n        Metadata associated with eventarray.\n\n    Attributes\n    ----------\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,)\n    support : EpochArray on which eventarray is defined.\n    n_events: np.array(dtype=np.int) of shape (n_series,)\n        Number of events in each series.\n    fs: float\n        Sampling frequency (Hz).\n    cell_types : np.array of str or other\n        Identified cell type for each series.\n    label : str or None\n        Information pertaining to the source of the eventarray.\n    meta : dict\n        Metadata associated with eventseries.\n    \"\"\"\n\n    __attributes__ = [\"_data\"]\n    __attributes__.extend(BaseValueEventArray.__attributes__)\n\n    def __init__(\n        self,\n        events=None,\n        values=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        self._val_init(\n            events=events,\n            values=values,\n            fs=fs,\n            support=support,\n            series_ids=series_ids,\n            empty=empty,\n            **kwargs,\n        )\n\n    def _val_init(\n        self,\n        events=None,\n        values=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        #############################################\n        #            standardize kwargs             #\n        #############################################\n        if events is not None:\n            kwargs[\"events\"] = events\n        if values is not None:\n            kwargs[\"values\"] = values\n        kwargs = self._standardize_kwargs(**kwargs)\n        events = kwargs.pop(\"events\", None)\n        values = kwargs.pop(\"values\", None)\n        #############################################\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.info(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            # Avoid np.array on jagged input\n            if isinstance(data, (list, tuple)) and len(data) &gt; 1:\n                # If all elements are scalars or 1-element lists/arrays\n                try:\n                    if all(\n                        (not hasattr(x, \"__len__\") or len(np.atleast_1d(x)) == 1)\n                        for x in data\n                    ):\n                        return True\n                except Exception:\n                    return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            --------\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            # Avoid np.array on jagged input\n            try:\n                # If first element is a list/array and has more than 1 element, not single series\n                if hasattr(data[0], \"__len__\") and len(data[0]) &gt; 1:\n                    # If data[0][0] is also a list/array, too many layers\n                    if hasattr(data[0][0], \"__len__\"):\n                        return False\n                # If second element exists and is a list/array, not single series\n                if len(data) &gt; 1 and hasattr(data[1], \"__len__\"):\n                    return False\n            except Exception:\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            # Handle ragged input: list/tuple or np.ndarray of dtype=object\n            is_ragged = False\n            if isinstance(data, (list, tuple)):\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            elif isinstance(data, np.ndarray) and data.dtype == object:\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            if is_ragged:\n                return utils.ragged_array([np.array(st, ndmin=1) for st in data])\n            # Only here, if not ragged, use np.array/np.squeeze\n            return np.array(np.squeeze(data), ndmin=2)\n\n        def standardize_values_to_2d(data):\n            data = standardize_to_2d(data)\n            for ii, series in enumerate(data):\n                if len(series.shape) == 2:\n                    pass\n                else:\n                    for xx in series:\n                        if len(np.atleast_1d(xx)) &gt; 1:\n                            raise ValueError(\n                                \"each series must have a fixed number of values; mismatch in series {}\".format(\n                                    ii\n                                )\n                            )\n            return data\n\n        events = standardize_to_2d(events)\n        values = standardize_values_to_2d(values)\n\n        data = []\n        for a, v in zip(events, values):\n            data.append(np.vstack((a, v.T)).T)\n        # Use ragged_array to support jagged arrays (multi-series with different numbers of events)\n        data = utils.ragged_array(data)\n\n        # sort event series, but only if necessary:\n        for ii, train in enumerate(events):\n            if not utils.is_sorted(train):\n                sortidx = np.argsort(train)\n                data[ii] = (data[ii])[sortidx, :]\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # print(self.type_name, kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            self.support = type(self._abscissa.support)(\n                np.array([self.first_event, self.last_event + 1 / fs])\n            )\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            # print('restricting, here')\n            self.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        data = self._restrict_to_interval_array_fast(\n            intervalarray=self.support, data=data\n        )\n\n        self._data = data\n        return\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def _intervalslicer(self, idx):\n        \"\"\"Helper function to restrict object to EpochArray.\"\"\"\n        # if self.isempty:\n        #     return self\n\n        if isinstance(idx, core.IntervalArray):\n            if idx.isempty:\n                return type(self)(empty=True)\n            support = self._abscissa.support.intersect(\n                interval=idx, boundaries=True\n            )  # what if fs of slicing interval is different?\n            if support.isempty:\n                return type(self)(empty=True)\n\n            logging.disable(logging.CRITICAL)\n            data = self._restrict_to_interval_array_fast(\n                intervalarray=support, data=self.data, copyover=True\n            )\n            eventarray = self._copy_without_data()\n            eventarray._data = data\n            eventarray._abscissa.support = support\n            eventarray.__renew__()\n            logging.disable(0)\n            return eventarray\n        elif isinstance(idx, int):\n            eventarray = self._copy_without_data()\n            support = self._abscissa.support[idx]\n            eventarray._abscissa.support = support\n            if (idx &gt;= self._abscissa.support.n_intervals) or idx &lt; (\n                -self._abscissa.support.n_intervals\n            ):\n                eventarray.__renew__()\n                return eventarray\n            else:\n                data = self._restrict_to_interval_array_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                return eventarray\n        else:  # most likely slice indexing\n            try:\n                logging.disable(logging.CRITICAL)\n                support = self._abscissa.support[idx]\n                data = self._restrict_to_interval_array_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray = self._copy_without_data()\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                logging.disable(0)\n                return eventarray\n            except Exception:\n                raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to ValueEventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def n_active(self):\n        \"\"\"(int) The number of active series.\n\n        A series is considered active if it fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    @property\n    def events(self):\n        events = []\n        for series in self.data:\n            events.append(series[:, 0].squeeze())\n        return utils.ragged_array(events)\n\n    @property\n    def values(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 1:].squeeze())\n        return utils.ragged_array(values)\n\n    def flatten(self, *, series_id=None):\n        \"\"\"Collapse events across series.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n\n        flattened = self._copy_without_data()\n\n        flattened._series_ids = [series_id]\n\n        raise NotImplementedError\n        alldatas = self.data[0]\n        for series in range(1, self.n_series):\n            alldatas = utils.linear_merge(alldatas, self.data[series])\n\n        flattened._data = np.array(list(alldatas), ndmin=2)\n        flattened.__renew__()\n        return flattened\n\n    @staticmethod\n    def _restrict_to_interval_array_fast(intervalarray, data, copyover=True):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            evt_data = ValueEventArray._to_2d_array(evt_data)\n            if evt_data.size == 0 or evt_data.shape[1] &lt; 1:\n                if singleseries:\n                    data = np.array([[]])\n                else:\n                    data_ = data.tolist()\n                    data_[series] = np.array([])\n                    data = utils.ragged_array(data_)\n                continue\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                # Ensure we have a proper 1D array of event times\n                if evt_data.ndim &gt; 1:\n                    event_times = evt_data[:, 0].flatten()\n                else:\n                    event_times = evt_data.flatten()\n                # Ensure event_times is a proper 1D array and not an object array\n                if event_times.dtype == object:\n                    # Handle object array by extracting the actual values\n                    event_times = np.array(\n                        [\n                            float(t) if hasattr(t, \"__float__\") else t\n                            for t in event_times\n                        ]\n                    )\n                # Ensure event_times is a proper 1D array\n                if event_times.size == 0:\n                    indices.append((0, 0))\n                else:\n                    try:\n                        frm, to = np.searchsorted(event_times, (t_start, t_stop))\n                        indices.append((frm, to))\n                    except (ValueError, TypeError):\n                        # Fallback: handle case where searchsorted fails\n                        indices.append((0, 0))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n        return data\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s\" % (self.type_name, address_str, numstr, epstr, fsstr)\n\n    def bin(self, *, ds=None, method=\"mean\", **kwargs):\n        \"\"\"Return a binned value event array.\n\n        method in [sum, mean, median, min, max] or a custom function.\n        Additional keyword arguments are passed to BinnedValueEventArray.\n        \"\"\"\n        return BinnedValueEventArray(self, ds=ds, method=method, **kwargs)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.data])\n\n    @property\n    def n_values(self):\n        \"\"\"(int) The number of values associated with each event series.\"\"\"\n        if self.isempty:\n            return 0\n        n_values = []\n        for series in self.data:\n            n_values.append(series.squeeze().shape[1] - 1)\n        return n_values\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Sorted EventArray.\"\"\"\n        if self.isempty:\n            return True\n        return np.array(\n            [utils.is_sorted(eventarray[:, 0]) for eventarray in self.data]\n        ).all()\n\n    def _reorder_series_by_idx(self, neworder, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,)\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n        out.__renew__()\n\n        return out\n\n    def reorder_series_by_ids(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        neworder = [self.series_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def make_stateful(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def _to_2d_array(arr):\n        \"\"\"Convert array to 2D numpy array, handling object arrays properly.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.dtype == object:\n            # Handle object arrays by extracting the actual data\n            if arr.size == 1:\n                # Single element object array\n                return np.atleast_2d(arr[0])\n            else:\n                # Multiple element object array - concatenate\n                flattened = []\n                for item in arr:\n                    if isinstance(item, np.ndarray):\n                        flattened.append(item)\n                    else:\n                        flattened.append(np.array(item))\n                if flattened:\n                    return np.vstack(flattened)\n                else:\n                    return np.array([]).reshape(0, 0)\n        else:\n            return np.atleast_2d(arr)\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Sorted EventArray.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>(int) The number of active series.</p> <p>A series is considered active if it fired at least one event.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.n_values","title":"<code>n_values</code>  <code>property</code>","text":"<p>(int) The number of values associated with each event series.</p>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.bin","title":"<code>bin(*, ds=None, method='mean', **kwargs)</code>","text":"<p>Return a binned value event array.</p> <p>method in [sum, mean, median, min, max] or a custom function. Additional keyword arguments are passed to BinnedValueEventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None, method=\"mean\", **kwargs):\n    \"\"\"Return a binned value event array.\n\n    method in [sum, mean, median, min, max] or a custom function.\n    Additional keyword arguments are passed to BinnedValueEventArray.\n    \"\"\"\n    return BinnedValueEventArray(self, ds=ds, method=method, **kwargs)\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.flatten","title":"<code>flatten(*, series_id=None)</code>","text":"<p>Collapse events across series.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def flatten(self, *, series_id=None):\n    \"\"\"Collapse events across series.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n\n    flattened = self._copy_without_data()\n\n    flattened._series_ids = [series_id]\n\n    raise NotImplementedError\n    alldatas = self.data[0]\n    for series in range(1, self.n_series):\n        alldatas = utils.linear_merge(alldatas, self.data[series])\n\n    flattened._data = np.array(list(alldatas), ndmin=2)\n    flattened.__renew__()\n    return flattened\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/core/valeventarray/#nelpy.core._valeventarray.ValueEventArray.reorder_series_by_ids","title":"<code>reorder_series_by_ids(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def reorder_series_by_ids(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    neworder = [self.series_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        utils.swap_rows(out._data, frm, to)\n        out._series_ids[frm], out._series_ids[to] = (\n            out._series_ids[to],\n            out._series_ids[frm],\n        )\n        # TODO: re-build series tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/","title":"nelpy.all","text":""},{"location":"reference/nelpy/all/#nelpy.all--nelpy-full-api","title":"nelpy full API","text":"<p><code>nelpy</code> is a neuroelectrophysiology object model and data analysis suite based on the python-vdmlab project (https://github.com/mvdm/vandermeerlab), and inspired by the neuralensemble.org NEO project (see http://neo.readthedocs.io/en/0.4.0/core.html).</p>"},{"location":"reference/nelpy/all/#nelpy.all.Abscissa","title":"<code>Abscissa</code>","text":"<p>An abscissa (x-axis) object for core nelpy data containers.</p> <p>Parameters:</p> Name Type Description Default <code>support</code> <code>IntervalArray</code> <p>The support associated with the abscissa. Default is an empty IntervalArray.</p> <code>None</code> <code>is_wrapping</code> <code>bool</code> <p>Whether or not the abscissa is wrapping (continuous). Default is False.</p> <code>False</code> <code>labelstring</code> <code>str</code> <p>String template for the abscissa label. Default is '{}'.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>support</code> <code>IntervalArray</code> <p>The support associated with the abscissa.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the abscissa, inherited from support.</p> <code>is_wrapping</code> <code>bool</code> <p>Whether the abscissa is wrapping.</p> <code>label</code> <code>str</code> <p>The formatted label for the abscissa.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class Abscissa:\n    \"\"\"\n    An abscissa (x-axis) object for core nelpy data containers.\n\n    Parameters\n    ----------\n    support : nelpy.IntervalArray, optional\n        The support associated with the abscissa. Default is an empty IntervalArray.\n    is_wrapping : bool, optional\n        Whether or not the abscissa is wrapping (continuous). Default is False.\n    labelstring : str, optional\n        String template for the abscissa label. Default is '{}'.\n\n    Attributes\n    ----------\n    support : nelpy.IntervalArray\n        The support associated with the abscissa.\n    base_unit : str\n        The base unit of the abscissa, inherited from support.\n    is_wrapping : bool\n        Whether the abscissa is wrapping.\n    label : str\n        The formatted label for the abscissa.\n    \"\"\"\n\n    def __init__(self, support=None, is_wrapping=False, labelstring=None):\n        # TODO: add label support\n        if support is None:\n            support = core.IntervalArray(empty=True)\n        if labelstring is None:\n            labelstring = \"{}\"\n\n        self.formatter = formatters.ArbitraryFormatter\n        self.support = support\n        self.base_unit = self.support.base_unit\n        self._labelstring = labelstring\n        self.is_wrapping = is_wrapping\n\n    @property\n    def label(self):\n        \"\"\"\n        Get the abscissa label.\n\n        Returns\n        -------\n        label : str\n            The formatted abscissa label.\n        \"\"\"\n        return self._labelstring.format(self.base_unit)\n\n    @label.setter\n    def label(self, val):\n        \"\"\"\n        Set the abscissa label string template.\n\n        Parameters\n        ----------\n        val : str\n            String template for the abscissa label.\n        \"\"\"\n        if val is None:\n            val = \"{}\"\n        try:  # cast to str:\n            labelstring = str(val)\n        except TypeError:\n            raise TypeError(\"cannot convert label to string\")\n        else:\n            labelstring = val\n        self._labelstring = labelstring\n\n    def __repr__(self):\n        return \"Abscissa(base_unit={}, is_wrapping={}) on domain [{}, {})\".format(\n            self.base_unit, self.is_wrapping, self.domain.start, self.domain.stop\n        )\n\n    @property\n    def domain(self):\n        \"\"\"Domain (in base units) on which abscissa is defined.\"\"\"\n        return self.support.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"Domain (in base units) on which abscissa is defined.\"\"\"\n        # val can be an IntervalArray type, or (start, stop)\n        self.support.domain = val\n        self.support = self.support[self.support.domain]\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.Abscissa.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>Domain (in base units) on which abscissa is defined.</p>"},{"location":"reference/nelpy/all/#nelpy.all.Abscissa.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Get the abscissa label.</p> <p>Returns:</p> Name Type Description <code>label</code> <code>str</code> <p>The formatted abscissa label.</p>"},{"location":"reference/nelpy/all/#nelpy.all.AnalogSignalArray","title":"<code>AnalogSignalArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>Array of continuous analog signals with regular sampling rates.</p> <p>This class extends RegularlySampledAnalogSignalArray with additional aliases and legacy support for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of signal data with shape (n_signals, n_samples). Default is empty array.</p> required <code>abscissa_vals</code> <code>ndarray</code> <p>Time values corresponding to samples, with shape (n_samples,). Default is None.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Default is None.</p> required <code>step</code> <code>float</code> <p>Sampling interval in seconds. Default is None.</p> required <code>merge_sample_gap</code> <code>float</code> <p>Maximum gap between samples to merge intervals (seconds). Default is 0.</p> required <code>support</code> <code>IntervalArray</code> <p>Time intervals where signal is defined. Default is None.</p> required <code>in_core</code> <code>bool</code> <p>Whether to keep data in core memory. Default is True.</p> required <code>labels</code> <code>array - like</code> <p>Labels for each signal. Default is None.</p> required <code>empty</code> <code>bool</code> <p>If True, creates empty array. Default is False.</p> required <code>abscissa</code> <code>AnalogSignalArrayAbscissa</code> <p>Abscissa object. Default is created from support.</p> required <code>ordinate</code> <code>AnalogSignalArrayOrdinate</code> <p>Ordinate object. Default is empty.</p> required Aliases <p>time : abscissa_vals     Alias for time values.</p> <p>n_epochs : n_intervals     Alias for number of intervals. ydata : data     Legacy alias for data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy import AnalogSignalArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a simple sine wave signal\n&gt;&gt;&gt; time = np.linspace(0, 1, 1000)  # 1 second of data\n&gt;&gt;&gt; signal = np.sin(2 * np.pi * 5 * time)  # 5 Hz sine wave\n</code></pre> <pre><code>&gt;&gt;&gt; # Create AnalogSignalArray with default parameters\n&gt;&gt;&gt; asa = AnalogSignalArray(\n...     data=signal[np.newaxis, :], abscissa_vals=time, fs=1000\n... )  # 1 kHz sampling\n</code></pre> <pre><code>&gt;&gt;&gt; # Access data using different aliases\n&gt;&gt;&gt; print(asa.data.shape)  # (1, 1000)\n&gt;&gt;&gt; print(asa.ydata.shape)  # same as data (legacy alias)\n&gt;&gt;&gt; print(asa.time.shape)  # (1000,) alias for abscissa_vals\n</code></pre> <pre><code>&gt;&gt;&gt; # Plot the signal (requires matplotlib)\n&gt;&gt;&gt; # asa.plot()\n</code></pre> <pre><code>&gt;&gt;&gt; # Create multi-channel signal with labels\n&gt;&gt;&gt; signals = np.vstack([signal, np.cos(2 * np.pi * 5 * time)])  # add cosine wave\n&gt;&gt;&gt; asa2 = AnalogSignalArray(\n...     data=signals, abscissa_vals=time, fs=1000, labels=[\"sine\", \"cosine\"]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Access individual channels\n&gt;&gt;&gt; sine_channel = asa2[:, 0]\n&gt;&gt;&gt; cosine_channel = asa2[:, 1]\n</code></pre> Notes <ul> <li>Inherits all attributes and methods from RegularlySampledAnalogSignalArray</li> <li>Provides backward compatibility with legacy parameter names</li> <li>Automatically handles abscissa and ordinate objects if not provided</li> </ul> See Also <p>RegularlySampledAnalogSignalArray : Parent class with core functionality</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class AnalogSignalArray(RegularlySampledAnalogSignalArray):\n    \"\"\"Array of continuous analog signals with regular sampling rates.\n\n    This class extends RegularlySampledAnalogSignalArray with additional aliases\n    and legacy support for backward compatibility.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Array of signal data with shape (n_signals, n_samples).\n        Default is empty array.\n    abscissa_vals : np.ndarray, optional\n        Time values corresponding to samples, with shape (n_samples,).\n        Default is None.\n    fs : float, optional\n        Sampling frequency in Hz. Default is None.\n    step : float, optional\n        Sampling interval in seconds. Default is None.\n    merge_sample_gap : float, optional\n        Maximum gap between samples to merge intervals (seconds).\n        Default is 0.\n    support : nelpy.IntervalArray, optional\n        Time intervals where signal is defined. Default is None.\n    in_core : bool, optional\n        Whether to keep data in core memory. Default is True.\n    labels : array-like, optional\n        Labels for each signal. Default is None.\n    empty : bool, optional\n        If True, creates empty array. Default is False.\n    abscissa : nelpy.core.AnalogSignalArrayAbscissa, optional\n        Abscissa object. Default is created from support.\n    ordinate : nelpy.core.AnalogSignalArrayOrdinate, optional\n        Ordinate object. Default is empty.\n\n    Aliases\n    -------\n    time : abscissa_vals\n        Alias for time values.\n\n    n_epochs : n_intervals\n        Alias for number of intervals.\n    ydata : data\n        Legacy alias for data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy import AnalogSignalArray\n\n    &gt;&gt;&gt; # Create a simple sine wave signal\n    &gt;&gt;&gt; time = np.linspace(0, 1, 1000)  # 1 second of data\n    &gt;&gt;&gt; signal = np.sin(2 * np.pi * 5 * time)  # 5 Hz sine wave\n\n    &gt;&gt;&gt; # Create AnalogSignalArray with default parameters\n    &gt;&gt;&gt; asa = AnalogSignalArray(\n    ...     data=signal[np.newaxis, :], abscissa_vals=time, fs=1000\n    ... )  # 1 kHz sampling\n\n    &gt;&gt;&gt; # Access data using different aliases\n    &gt;&gt;&gt; print(asa.data.shape)  # (1, 1000)\n    &gt;&gt;&gt; print(asa.ydata.shape)  # same as data (legacy alias)\n    &gt;&gt;&gt; print(asa.time.shape)  # (1000,) alias for abscissa_vals\n\n    &gt;&gt;&gt; # Plot the signal (requires matplotlib)\n    &gt;&gt;&gt; # asa.plot()\n\n    &gt;&gt;&gt; # Create multi-channel signal with labels\n    &gt;&gt;&gt; signals = np.vstack([signal, np.cos(2 * np.pi * 5 * time)])  # add cosine wave\n    &gt;&gt;&gt; asa2 = AnalogSignalArray(\n    ...     data=signals, abscissa_vals=time, fs=1000, labels=[\"sine\", \"cosine\"]\n    ... )\n\n    &gt;&gt;&gt; # Access individual channels\n    &gt;&gt;&gt; sine_channel = asa2[:, 0]\n    &gt;&gt;&gt; cosine_channel = asa2[:, 1]\n\n    Notes\n    -----\n    - Inherits all attributes and methods from RegularlySampledAnalogSignalArray\n    - Provides backward compatibility with legacy parameter names\n    - Automatically handles abscissa and ordinate objects if not provided\n\n    See Also\n    --------\n    RegularlySampledAnalogSignalArray : Parent class with core functionality\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"abscissa_vals\",\n        \"_time\": \"_abscissa_vals\",\n        \"n_epochs\": \"n_intervals\",\n        \"ydata\": \"data\",  # legacy support\n        \"_ydata\": \"_data\",  # legacy support\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        # legacy ASA constructor support for backward compatibility\n        kwargs = legacyASAkwargs(**kwargs)\n\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        abscissa = kwargs.get(\n            \"abscissa\", core.AnalogSignalArrayAbscissa(support=support)\n        )\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.AnalogSignalArrayAbscissa","title":"<code>AnalogSignalArrayAbscissa</code>","text":"<p>               Bases: <code>Abscissa</code></p> <p>Abscissa for AnalogSignalArray.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class AnalogSignalArrayAbscissa(Abscissa):\n    \"\"\"Abscissa for AnalogSignalArray.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        labelstring = kwargs.get(\n            \"labelstring\", \"time ({})\"\n        )  # TODO FIXME after unit inheritance; inherit from formatter?\n\n        kwargs[\"support\"] = support\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n\n        self.formatter = self.support.formatter\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.AnalogSignalArrayOrdinate","title":"<code>AnalogSignalArrayOrdinate</code>","text":"<p>               Bases: <code>Ordinate</code></p> <p>Ordinate for AnalogSignalArray.</p> <p>Examples:</p> <p>nel.AnalogSignalArrayOrdinate(base_unit='uV')</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class AnalogSignalArrayOrdinate(Ordinate):\n    \"\"\"Ordinate for AnalogSignalArray.\n\n    Examples\n    -------\n    nel.AnalogSignalArrayOrdinate(base_unit='uV')\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        base_unit = kwargs.get(\"base_unit\", \"V\")\n        labelstring = kwargs.get(\"labelstring\", \"voltage ({})\")\n\n        kwargs[\"base_unit\"] = base_unit\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray","title":"<code>BinnedEventArray</code>","text":"<p>               Bases: <code>BaseEventArray</code></p> <p>BinnedEventArray.</p> <p>Parameters:</p> Name Type Description Default <code>eventarray</code> <code>EventArray or RegularlySampledAnalogSignalArray</code> <p>Input data.</p> <code>None</code> <code>ds</code> <code>float</code> <p>The bin width, in seconds. Default is 0.0625 (62.5 ms)</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Whether an empty BinnedEventArray should be constructed (no data).</p> <code>False</code> <code>fs</code> <code>float</code> <p>Sampling rate in Hz. If fs is passed as a parameter, then data is assumed to be in sample numbers instead of actual data.</p> required <code>kwargs</code> <code>optional</code> <p>Additional keyword arguments to forward along to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray superclass for additional</code> <code>attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the BinnedEventArray is empty (no data).</p> <code>n_series</code> <code>int</code> <p>The number of series.</p> <code>bin_centers</code> <code>ndarray</code> <p>The bin centers, in seconds.</p> <code>event_centers</code> <code>ndarray</code> <p>The centers of each event, in seconds.</p> <code>data</code> <code>np.array, with shape (n_series, n_bins)</code> <p>Event counts in all bins.</p> <code>bins</code> <code>ndarray</code> <p>The bin edges, in seconds.</p> <code>binned_support</code> <code>np.ndarray, with shape (n_intervals, 2)</code> <p>The binned support of the BinnedEventArray (in bin IDs).</p> <code>lengths</code> <code>ndarray</code> <p>Lengths of contiguous segments, in number of bins.</p> <code>eventarray</code> <code>EventArray</code> <p>The original eventarray associated with the binned data.</p> <code>n_bins</code> <code>int</code> <p>The number of bins.</p> <code>ds</code> <code>float</code> <p>Bin width, in seconds.</p> <code>n_active</code> <code>int</code> <p>The number of active series. A series is considered active if it fired at least one event.</p> <code>n_active_per_bin</code> <code>np.ndarray, with shape (n_bins, )</code> <p>Number of active series per data bin.</p> <code>n_events</code> <code>ndarray</code> <p>The number of events in each series.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class BinnedEventArray(BaseEventArray):\n    \"\"\"BinnedEventArray.\n\n    Parameters\n    ----------\n    eventarray : nelpy.EventArray or nelpy.RegularlySampledAnalogSignalArray\n        Input data.\n    ds : float\n        The bin width, in seconds.\n        Default is 0.0625 (62.5 ms)\n    empty : bool, optional\n        Whether an empty BinnedEventArray should be constructed (no data).\n    fs : float, optional\n        Sampling rate in Hz. If fs is passed as a parameter, then data\n        is assumed to be in sample numbers instead of actual data.\n    kwargs : optional\n        Additional keyword arguments to forward along to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray superclass for additional\n    attributes that are defined there.\n    isempty : bool\n        Whether the BinnedEventArray is empty (no data).\n    n_series : int\n        The number of series.\n    bin_centers : np.ndarray\n        The bin centers, in seconds.\n    event_centers : np.ndarray\n        The centers of each event, in seconds.\n    data : np.array, with shape (n_series, n_bins)\n        Event counts in all bins.\n    bins : np.ndarray\n        The bin edges, in seconds.\n    binned_support : np.ndarray, with shape (n_intervals, 2)\n        The binned support of the BinnedEventArray (in\n        bin IDs).\n    lengths : np.ndarray\n        Lengths of contiguous segments, in number of bins.\n    eventarray : nelpy.EventArray\n        The original eventarray associated with the binned data.\n    n_bins : int\n        The number of bins.\n    ds : float\n        Bin width, in seconds.\n    n_active : int\n        The number of active series. A series is considered active if\n        it fired at least one event.\n    n_active_per_bin : np.ndarray, with shape (n_bins, )\n        Number of active series per data bin.\n    n_events : np.ndarray\n        The number of events in each series.\n    support : nelpy.IntervalArray\n        The support of the BinnedEventArray.\n    \"\"\"\n\n    __attributes__ = [\n        \"_ds\",\n        \"_bins\",\n        \"_data\",\n        \"_bin_centers\",\n        \"_binned_support\",\n        \"_eventarray\",\n    ]\n    __attributes__.extend(BaseEventArray.__attributes__)\n\n    def __init__(self, eventarray=None, *, ds=None, empty=False, **kwargs):\n        super().__init__(empty=True)\n\n        # if an empty object is requested, return it:\n        if empty:\n            # super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self._event_centers = None\n            return\n\n        # handle casting other nelpy objects to BinnedEventArray:\n        if isinstance(eventarray, core.RegularlySampledAnalogSignalArray):\n            if eventarray.isempty:\n                for attr in self.__attributes__:\n                    exec(\"self.\" + attr + \" = None\")\n                self._abscissa.support = type(eventarray._abscissa.support)(empty=True)\n                self._event_centers = None\n                return\n            eventarray = eventarray.copy()  # Note: this is a deep copy\n            n_empty_epochs = np.sum(eventarray.support.lengths == 0)\n            if n_empty_epochs &gt; 0:\n                logging.warning(\n                    \"Detected {} empty epochs. Removing these in the cast object\".format(\n                        n_empty_epochs\n                    )\n                )\n                eventarray.support = eventarray.support._drop_empty_intervals()\n            if not eventarray.support.ismerged:\n                logging.warning(\n                    \"Detected overlapping epochs. Merging these in the cast object\"\n                )\n                eventarray.support = eventarray.support.merge()\n\n            self._eventarray = None\n            self._ds = 1 / eventarray.fs\n            self._series_labels = eventarray._series_labels\n            self._bin_centers = eventarray.abscissa_vals\n            tmp = np.insert(np.cumsum(eventarray.lengths), 0, 0)\n            self._binned_support = np.array((tmp[:-1], tmp[1:] - 1)).T\n            self._abscissa.support = eventarray.support\n            try:\n                self._series_ids = (\n                    np.array(eventarray.series_labels).astype(int)\n                ).tolist()\n            except (ValueError, TypeError):\n                self._series_ids = (np.arange(eventarray.n_signals) + 1).tolist()\n            self._data = eventarray._ydata_rowsig\n\n            bins = []\n            for starti, stopi in self._binned_support:\n                bins_edges_in_interval = (\n                    self._bin_centers[starti : stopi + 1] - self._ds / 2\n                ).tolist()\n                bins_edges_in_interval.append(self._bin_centers[stopi] + self._ds / 2)\n                bins.extend(bins_edges_in_interval)\n            self._bins = np.array(bins)\n            return\n\n        if type(eventarray).__name__ == \"BinnedSpikeTrainArray\":\n            # old-style nelpy BinnedSpikeTrainArray object?\n            try:\n                self._eventarray = eventarray._spiketrainarray\n                self._ds = eventarray.ds\n                self._series_labels = eventarray.unit_labels\n                self._bin_centers = eventarray.bin_centers\n                self._binned_support = eventarray.binned_support\n                try:\n                    self._abscissa.support = core.EpochArray(eventarray.support.data)\n                except AttributeError:\n                    self._abscissa.support = core.EpochArray(eventarray.support.time)\n                self._series_ids = eventarray.unit_ids\n                self._data = eventarray.data\n                return\n            except Exception:\n                pass\n\n        if not isinstance(eventarray, EventArray):\n            raise TypeError(\"eventarray must be a nelpy.EventArray object.\")\n\n        self._ds = None\n        self._bin_centers = np.array([])\n        self._event_centers = None\n\n        logging.disable(logging.CRITICAL)\n        kwargs = {\n            \"fs\": eventarray.fs,\n            \"series_ids\": eventarray.series_ids,\n            \"series_labels\": eventarray.series_labels,\n            \"series_tags\": eventarray.series_tags,\n            \"label\": eventarray.label,\n        }\n        logging.disable(0)\n\n        # initialize super so that self.fs is set:\n        self._data = np.zeros((eventarray.n_series, 0))\n        # the above is necessary so that super() can determine\n        # self.n_series when initializing. self.data will\n        # be updated later in __init__ to reflect subsequent changes\n        super().__init__(**kwargs)\n\n        if ds is None:\n            logging.warning(\"no bin size was given, assuming 62.5 ms\")\n            ds = 0.0625\n\n        self._eventarray = eventarray  # TODO: remove this if we don't need it, or decide that it's too wasteful\n        self._abscissa = copy.deepcopy(eventarray._abscissa)\n        self.ds = ds\n\n        self._bin_events(eventarray=eventarray, intervalArray=eventarray.support, ds=ds)\n\n    def __mul__(self, other):\n        \"\"\"Overloaded * operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data * other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T * other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for *: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __rmul__(self, other):\n        \"\"\"Overloaded * operator\"\"\"\n        return self.__mul__(other)\n\n    def __sub__(self, other):\n        \"\"\"Overloaded - operator\"\"\"\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data - other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T - other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for -: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __add__(self, other):\n        \"\"\"Overloaded + operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data + other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T + other).T\n            return neweva\n        elif isinstance(other, type(self)):\n            # TODO: additional checks need to be done, e.g., same series ids...\n            assert self.n_series == other.n_series\n            support = self._abscissa.support + other.support\n\n            newdata = []\n            for series in range(self.n_series):\n                newdata.append(np.append(self.data[series], other.data[series]))\n\n            fs = self.fs\n            if self.fs != other.fs:\n                fs = None\n            return type(self)(newdata, support=support, fs=fs)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __truediv__(self, other):\n        \"\"\"Overloaded / operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data / other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T / other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for /: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def median(self, *, axis=1):\n        \"\"\"Returns the median of each series in BinnedEventArray.\"\"\"\n        try:\n            medians = np.nanmedian(self.data, axis=axis).squeeze()\n            if medians.size == 1:\n                return medians.item()\n            return medians\n        except IndexError:\n            raise IndexError(\"Empty BinnedEventArray; cannot calculate median.\")\n\n    def mean(self, *, axis=1):\n        \"\"\"Returns the mean of each series in BinnedEventArray.\"\"\"\n        try:\n            means = np.nanmean(self.data, axis=axis).squeeze()\n            if means.size == 1:\n                return means.item()\n            return means\n        except IndexError:\n            raise IndexError(\"Empty BinnedEventArray; cannot calculate mean.\")\n\n    def std(self, *, axis=1):\n        \"\"\"Returns the standard deviation of each series in BinnedEventArray.\"\"\"\n        try:\n            stds = np.nanstd(self.data, axis=axis).squeeze()\n            if stds.size == 1:\n                return stds.item()\n            return stds\n        except IndexError:\n            raise IndexError(\n                \"Empty BinnedEventArray; cannot calculate standard deviation\"\n            )\n\n    def center(self, inplace=False):\n        \"\"\"Center data (zero mean).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        return out\n\n    def normalize(self, inplace=False):\n        \"\"\"Normalize data (unit standard deviation).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        std = out.std()\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    def standardize(self, inplace=False):\n        \"\"\"Standardize data (zero mean and unit std deviation).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        std = out.std()\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n\n        return out\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        partitioned = type(self)(\n            core.RegularlySampledAnalogSignalArray(self).partition(\n                ds=ds, n_intervals=n_intervals\n            )\n        )\n        # partitioned.loc = ItemGetter_loc(partitioned)\n        # partitioned.iloc = ItemGetter_iloc(partitioned)\n        return partitioned\n\n        # raise NotImplementedError('workaround: cast to AnalogSignalArray, partition, and cast back to BinnedEventArray')\n\n    def _copy_without_data(self):\n        \"\"\"Returns a copy of the BinnedEventArray, without data.\n        Note: the support is left unchanged, but the binned_support is removed.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._bin_centers = None\n        out._binned_support = None\n        out._bins = None\n        out._data = np.zeros((self.n_series, 0))\n        out._eventarray = out._eventarray._copy_without_data()\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Returns a copy of the BinnedEventArray.\"\"\"\n        newcopy = copy.deepcopy(self)\n        newcopy.__renew__()\n        return newcopy\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        # Summarize labels if too many\n        max_display = 5\n        labels = self._series_labels\n        n = len(labels)\n        if n &lt;= max_display:\n            label_str = str(labels)\n        else:\n            label_str = f\"[{', '.join(map(str, labels[:3]))}, ..., {', '.join(map(str, labels[-2:]))}]\"\n        ustr = f\" {self.n_series} {label_str}\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = f\" ({self._abscissa.support.n_intervals} segments) in\"\n        else:\n            epstr = \" in\"\n        if self.n_bins == 1:\n            bstr = f\" {self.n_bins} bin of width {utils.PrettyDuration(self.ds)}\"\n            dstr = \"\"\n        else:\n            bstr = f\" {self.n_bins} bins of width {utils.PrettyDuration(self.ds)}\"\n            dstr = f\" for a total of {utils.PrettyDuration(self.n_bins * self.ds)}\"\n        return f\"&lt;{self.type_name}{address_str}:{ustr}{epstr}{bstr}&gt;{dstr}\"\n\n    def __iter__(self):\n        \"\"\"BinnedEventArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"BinnedEventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        # TODO: return self.loc[index], and make sure that __getitem__ is updated\n        logging.disable(logging.CRITICAL)\n        support = self._abscissa.support[index]\n        bsupport = self.binned_support[[index], :]\n\n        binnedeventarray = type(self)(empty=True)\n        exclude = [\"_bins\", \"_data\", \"_support\", \"_bin_centers\", \"_binned_support\"]\n        attrs = (x for x in self.__attributes__ if x not in exclude)\n        for attr in attrs:\n            exec(\"binnedeventarray.\" + attr + \" = self.\" + attr)\n        binindices = np.insert(0, 1, np.cumsum(self.lengths + 1))  # indices of bins\n        binstart = binindices[index]\n        binstop = binindices[index + 1]\n        binnedeventarray._bins = self._bins[binstart:binstop]\n        binnedeventarray._data = self._data[:, bsupport[0][0] : bsupport[0][1] + 1]\n        binnedeventarray._abscissa.support = support\n        binnedeventarray._bin_centers = self._bin_centers[\n            bsupport[0][0] : bsupport[0][1] + 1\n        ]\n        binnedeventarray._binned_support = bsupport - bsupport[0, 0]\n        logging.disable(0)\n        self._index += 1\n        binnedeventarray.__renew__()\n        return binnedeventarray\n\n    def empty(self, *, inplace=False):\n        \"\"\"Remove data (but not metadata) from BinnedEventArray.\n\n        Attributes 'data', and 'support' 'binned_support' are all emptied.\n\n        Note: n_series, series_ids, etc. are all preserved.\n        \"\"\"\n        if not inplace:\n            out = self._copy_without_data()\n            out._abscissa.support = type(self._abscissa.support)(empty=True)\n            return out\n        out = self\n        out._data = np.zeros((self.n_series, 0))\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        out._binned_support = None\n        out._bin_centers = None\n        out._bins = None\n        out._eventarray.empty(inplace=True)\n        out.__renew__()\n        return out\n\n    def __getitem__(self, idx):\n        \"\"\"BinnedEventArray index access.\n\n        By default, this method is bound to .loc\n        \"\"\"\n        return self.loc[idx]\n\n    def _restrict(self, intervalslice, seriesslice):\n        # This function should be called only by an itemgetter\n        # because it mutates data.\n        # The itemgetter is responsible for creating copies\n        # of objects\n\n        self._restrict_to_series_subset(seriesslice)\n        self._eventarray._restrict_to_series_subset(seriesslice)\n\n        self._restrict_to_interval(intervalslice)\n        self._eventarray._restrict_to_interval(intervalslice)\n        return self\n\n    def _restrict_to_series_subset(self, idx):\n        # Warning: This function can mutate data\n\n        if isinstance(idx, core.IntervalArray):\n            raise IndexError(\n                \"Slicing is [intervals, signal]; perhaps you have the order reversed?\"\n            )\n\n        # TODO: update tags\n        try:\n            self._data = np.atleast_2d(self.data[idx, :])\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except IndexError:\n            raise IndexError(\n                \"One of more indices were out of bounds for n_series with size {}\".format(\n                    self.n_series\n                )\n            )\n        except Exception:\n            raise TypeError(\"Unsupported indexing type {}\".format(type(idx)))\n\n    def _restrict_to_interval(self, intervalslice):\n        # Warning: This function can mutate data. It should only be called from\n        # _restrict\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                # no restriction on interval\n                return self\n\n        newintervals = self._abscissa.support[intervalslice].merge()\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=True)\n\n        bcenter_inds = []\n        bin_inds = []\n        start = 0\n        bsupport = np.zeros((newintervals.n_intervals, 2), dtype=int)\n        support_intervals = np.zeros((newintervals.n_intervals, 2))\n\n        if not self.isempty:\n            for ii, interval in enumerate(newintervals.data):\n                a_start = interval[0]\n                a_stop = interval[1]\n                frm, to = np.searchsorted(self._bins, (a_start, a_stop))\n                # If bin edges equal a_stop, they should still be included\n                if self._bins[to] &lt;= a_stop:\n                    bin_inds.extend(np.arange(frm, to + 1, step=1))\n                else:\n                    bin_inds.extend(np.arange(frm, to, step=1))\n                    to -= 1\n                support_intervals[ii] = [self._bins[frm], self._bins[to]]\n\n                lind, rind = np.searchsorted(\n                    self._bin_centers, (self._bins[frm], self._bins[to])\n                )\n                # We don't have to worry about an if-else block here unlike\n                # for the bin_inds because the bin_centers can NEVER equal\n                # the bins. Therefore we know every interval looks like\n                # the following:\n                #  first desired bin         last desired bin\n                # |------------------|......|-------------------|\n                #          ^                                         ^\n                #          |                                         |\n                #        lind                                      rind\n                # Since arange is half-open, the indices we actually take\n                # will be such that all bin centers fall within the desired\n                # bin edges.\n                bcenter_inds.extend(np.arange(lind, rind, step=1))\n\n                bsupport[ii] = [start, start + (to - frm - 1)]\n                start += to - frm\n\n            self._bins = self._bins[bin_inds]\n            self._bin_centers = self._bin_centers[bcenter_inds]\n            self._data = np.atleast_2d(self._data[:, bcenter_inds])\n            self._binned_support = bsupport\n\n        self._abscissa.support = type(self._abscissa.support)(support_intervals)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty BinnedEventArray.\"\"\"\n        try:\n            return len(self.bin_centers) == 0\n        except TypeError:\n            return True  # this happens when self.bin_centers is None\n\n    @property\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        try:\n            return utils.PrettyInt(self.data.shape[0])\n        except AttributeError:\n            return 0\n\n    @property\n    def centers(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        logging.warning(\"centers is deprecated. Use bin_centers instead.\")\n        return self.bin_centers\n\n    @property\n    def _abscissa_vals(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        return self._bin_centers\n\n    @property\n    def bin_centers(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        return self._bin_centers\n\n    @property\n    def event_centers(self):\n        \"\"\"(np.array) The centers (in seconds) of each event.\"\"\"\n        if self._event_centers is None:\n            raise NotImplementedError(\"event_centers not yet implemented\")\n            # self._event_centers = midpoints\n        return self._event_centers\n\n    @property\n    def _midpoints(self):\n        \"\"\"(np.array) The centers (in index space) of all events.\n\n        Examples\n        -------\n        ax, img = npl.imagesc(bst.data) # data is of shape (n_series, n_bins)\n        # then _midpoints correspond to the xvals at the center of\n        # each event.\n        ax.plot(bst.event_centers, np.repeat(1, self.n_intervals), marker='o', color='w')\n\n        \"\"\"\n        if self._event_centers is None:\n            midpoints = np.zeros(len(self.lengths))\n            for idx, length in enumerate(self.lengths):\n                midpoints[idx] = np.sum(self.lengths[:idx]) + length / 2\n            self._event_centers = midpoints\n        return self._event_centers\n\n    @property\n    def data(self):\n        \"\"\"(np.array) Event counts in all bins, with shape (n_series, n_bins).\"\"\"\n        return self._data\n\n    @property\n    def bins(self):\n        \"\"\"(np.array) The bin edges (in seconds).\"\"\"\n        return self._bins\n\n    @property\n    def binnedSupport(self):\n        \"\"\"(np.array) The binned support of the BinnedEventArray (in\n        bin IDs) of shape (n_intervals, 2).\n        \"\"\"\n        logging.warning(\"binnedSupport is deprecated. Use bined_support instead.\")\n        return self._binned_support\n\n    @property\n    def binned_support(self):\n        \"\"\"(np.array) The binned support of the BinnedEventArray (in\n        bin IDs) of shape (n_intervals, 2).\n        \"\"\"\n        return self._binned_support\n\n    @property\n    def lengths(self):\n        \"\"\"Lengths of contiguous segments, in number of bins.\"\"\"\n        if self.isempty:\n            return 0\n        return np.atleast_1d(\n            (self.binned_support[:, 1] - self.binned_support[:, 0] + 1).squeeze()\n        )\n\n    @property\n    def eventarray(self):\n        \"\"\"(nelpy.EventArray) The original EventArray associated with\n        the binned data.\n        \"\"\"\n        return self._eventarray\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) The number of bins.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self.bin_centers))\n\n    @property\n    def ds(self):\n        \"\"\"(float) Bin width in seconds.\"\"\"\n        return self._ds\n\n    @ds.setter\n    def ds(self, val):\n        if self._ds is not None:\n            raise AttributeError(\"can't set attribute\")\n        else:\n            try:\n                if val &lt;= 0:\n                    pass\n            except ValueError:\n                raise TypeError(\"bin width must be a scalar\")\n            if val &lt;= 0:\n                raise ValueError(\"bin width must be positive\")\n            self._ds = val\n\n    @staticmethod\n    def _get_bins_inside_interval(interval, ds):\n        \"\"\"(np.array) Return bin edges entirely contained inside an interval.\n\n        Bin edges always start at interval.start, and continue for as many\n        bins as would fit entirely inside the interval.\n\n        NOTE 1: there are (n+1) bin edges associated with n bins.\n\n        WARNING: if an interval is smaller than ds, then no bin will be\n                associated with the particular interval.\n\n        NOTE 2: nelpy uses half-open intervals [a,b), but if the bin\n                width divides b-a, then the bins will cover the entire\n                range. For example, if interval = [0,2) and ds = 1, then\n                bins = [0,1,2], even though [0,2] is not contained in\n                [0,2).\n\n        Parameters\n        ----------\n        interval : IntervalArray\n            IntervalArray containing a single interval with a start, and stop\n        ds : float\n            Time bin width, in seconds.\n\n        Returns\n        -------\n        bins : array\n            Bin edges in an array of shape (n+1,) where n is the number\n            of bins\n        centers : array\n            Bin centers in an array of shape (n,) where n is the number\n            of bins\n        \"\"\"\n\n        if interval.length &lt; ds:\n            logging.warning(\"interval duration is less than bin size: ignoring...\")\n            return None, None\n\n        n = int(np.floor(interval.length / ds))  # number of bins\n\n        # linspace is better than arange for non-integral steps\n        bins = np.linspace(interval.start, interval.start + n * ds, n + 1)\n        centers = bins[:-1] + (ds / 2)\n        return bins, centers\n\n    def _bin_events(self, eventarray, intervalArray, ds):\n        \"\"\"\n        Docstring goes here. TBD. For use with bins that are contained\n        wholly inside the intervals.\n\n        \"\"\"\n        b = []  # bin list\n        c = []  # centers list\n        s = []  # data list\n        for nn in range(eventarray.n_series):\n            s.append([])\n        left_edges = []\n        right_edges = []\n        counter = 0\n        for interval in intervalArray:\n            bins, centers = self._get_bins_inside_interval(interval, ds)\n            if bins is not None:\n                for uu, eventarraydatas in enumerate(eventarray.data):\n                    event_counts, _ = np.histogram(\n                        eventarraydatas,\n                        bins=bins,\n                        density=False,\n                        range=(interval.start, interval.stop),\n                    )  # TODO: is it faster to limit range, or to cut out events?\n                    s[uu].extend(event_counts.tolist())\n                left_edges.append(counter)\n                counter += len(centers) - 1\n                right_edges.append(counter)\n                counter += 1\n                b.extend(bins.tolist())\n                c.extend(centers.tolist())\n        self._bins = np.array(b)\n        self._bin_centers = np.array(c)\n        self._data = np.array(s)\n        le = np.array(left_edges)\n        le = le[:, np.newaxis]\n        re = np.array(right_edges)\n        re = re[:, np.newaxis]\n        self._binned_support = np.hstack((le, re))\n        support_starts = self.bins[np.insert(np.cumsum(self.lengths + 1), 0, 0)[:-1]]\n        support_stops = self.bins[np.insert(np.cumsum(self.lengths + 1) - 1, 0, 0)[1:]]\n        supportdata = np.vstack([support_starts, support_stops]).T\n        self._abscissa.support = type(self._abscissa.support)(\n            supportdata\n        )  # set support to TRUE bin support\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(\n        self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n    ):\n        \"\"\"Smooth BinnedEventArray by convolving with a Gaussian kernel.\n\n        Smoothing is applied in data, and the same smoothing is applied\n        to each series in a BinnedEventArray.\n\n        Smoothing is applied within each interval.\n\n        Parameters\n        ----------\n        sigma : float, optional\n            Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n        truncate : float, optional\n            Bandwidth outside of which the filter value will be zero. Default is 4.0\n        inplace : bool\n            If True the data will be replaced with the smoothed data.\n            Default is False.\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with smoothed data.\n        \"\"\"\n\n        if truncate is None:\n            truncate = 4\n        if sigma is None:\n            sigma = 0.01  # 10 ms default\n\n        fs = 1 / self.ds\n\n        return utils.gaussian_filter(\n            self,\n            fs=fs,\n            sigma=sigma,\n            truncate=truncate,\n            inplace=inplace,\n            within_intervals=within_intervals,\n        )\n\n    @staticmethod\n    def _smooth_array(arr, w=None):\n        \"\"\"Smooth an array by convolving a boxcar, row-wise.\n\n        Parameters\n        ----------\n        w : int, optional\n            Number of bins to include in boxcar window. Default is 10.\n\n        Returns\n        -------\n        smoothed: array\n            Smoothed array with same shape as arr.\n        \"\"\"\n\n        if w is None:\n            w = 10\n\n        if w == 1:  # perform no smoothing\n            return arr\n\n        w = np.min((w, arr.shape[1]))\n\n        smoothed = arr.astype(float)  # copy array and cast to float\n        window = np.ones((w,)) / w\n\n        # smooth per row\n        for rowi, row in enumerate(smoothed):\n            smoothed[rowi, :] = np.convolve(row, window, mode=\"same\")\n\n        if arr.shape[1] != smoothed.shape[1]:\n            raise TypeError(\"Incompatible shape returned!\")\n\n        return smoothed\n\n    @staticmethod\n    def _rebin_array(arr, w):\n        \"\"\"Rebin an array of shape (n_signals, n_bins) into a\n        coarser bin size.\n\n        Parameters\n        ----------\n        arr : array\n            Array with shape (n_signals, n_bins) to re-bin. A copy\n            is returned.\n        w : int\n            Number of original bins to combine into each new bin.\n\n        Returns\n        -------\n        out : array\n            Bnned array with shape (n_signals, n_new_bins)\n        bin_idx : array\n            Array of shape (n_new_bins,) with the indices of the new\n            binned array, relative to the original array.\n        \"\"\"\n        cs = np.cumsum(arr, axis=1)\n        binidx = np.arange(start=w, stop=cs.shape[1] + 1, step=w) - 1\n\n        rebinned = np.hstack(\n            (np.array(cs[:, w - 1], ndmin=2).T, cs[:, binidx[1:]] - cs[:, binidx[:-1]])\n        )\n        # bins = bins[np.insert(binidx+1, 0, 0)]\n        return rebinned, binidx\n\n    def rebin(self, w=None):\n        \"\"\"Rebin the BinnedEventArray into a coarser bin size.\n\n        Parameters\n        ----------\n        w : int, optional\n            number of bins of width bst.ds to bin into new bin of\n            width bst.ds*w. Default is w=1 (no re-binning).\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with coarser resolution.\n        \"\"\"\n\n        if w is None:\n            w = 1\n\n        if not float(w).is_integer:\n            raise ValueError(\"w has to be an integer!\")\n\n        w = int(w)\n\n        bst = self\n        return self._rebin_binnedeventarray(bst, w=w)\n\n    @staticmethod\n    def _rebin_binnedeventarray(bst, w=None):\n        \"\"\"Rebin a BinnedEventArray into a coarser bin size.\n\n        Parameters\n        ----------\n        bst : BinnedEventArray\n            BinnedEventArray to re-bin into a coarser resolution.\n        w : int, optional\n            number of bins of width bst.ds to bin into new bin of\n            width bst.ds*w. Default is w=1 (no re-binning).\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with coarser resolution.\n\n        # FFB! TODO: if w is longer than some event size,\n        # an exception will occur. Handle it! Although I may already\n        # implicitly do that.\n        \"\"\"\n\n        if w is None:\n            w = 1\n\n        if w == 1:\n            return bst\n\n        edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n        newlengths = [0]\n        binedges = np.insert(np.cumsum(bst.lengths + 1), 0, 0)\n        n_events = bst.support.n_intervals\n        newdata = None\n\n        for ii in range(n_events):\n            data = bst.data[:, edges[ii] : edges[ii + 1]]\n            bins = bst.bins[binedges[ii] : binedges[ii + 1]]\n\n            datalen = data.shape[1]\n            if w &lt;= datalen:\n                rebinned, binidx = bst._rebin_array(data, w=w)\n                bins = bins[np.insert(binidx + 1, 0, 0)]\n\n                newlengths.append(rebinned.shape[1])\n\n                if newdata is None:\n                    newdata = rebinned\n                    newbins = bins\n                    newcenters = bins[:-1] + np.diff(bins) / 2\n                    newsupport = np.array([bins[0], bins[-1]])\n                else:\n                    newdata = np.hstack((newdata, rebinned))\n                    newbins = np.hstack((newbins, bins))\n                    newcenters = np.hstack((newcenters, bins[:-1] + np.diff(bins) / 2))\n                    newsupport = np.vstack((newsupport, np.array([bins[0], bins[-1]])))\n            else:\n                pass\n\n        # assemble new binned event series array:\n        newedges = np.cumsum(newlengths)\n        newbst = bst._copy_without_data()\n        abscissa = copy.copy(bst._abscissa)\n        if newdata is not None:\n            newbst._data = newdata\n            newbst._abscissa = abscissa\n            newbst._abscissa.support = type(bst.support)(newsupport)\n            newbst._bins = newbins\n            newbst._bin_centers = newcenters\n            newbst._ds = bst.ds * w\n            newbst._binned_support = np.array((newedges[:-1], newedges[1:] - 1)).T\n        else:\n            logging.warning(\n                \"No events are long enough to contain any bins of width {}\".format(\n                    utils.PrettyDuration(bst.ds)\n                )\n            )\n            newbst._data = None\n            newbst._abscissa = abscissa\n            newbst._abscissa.support = None\n            newbst._binned_support = None\n            newbst._bin_centers = None\n            newbst._bins = None\n\n        newbst.__renew__()\n\n        return newbst\n\n    def bst_from_indices(self, idx):\n        \"\"\"\n        Return a BinnedEventArray from a list of indices.\n\n        bst : BinnedEventArray\n        idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE\n\n        Examples\n        --------\n        idx = [[10, 20]\n            [25, 50]]\n        bst_from_indices(bst, idx=idx)\n        \"\"\"\n\n        idx = np.atleast_2d(idx)\n\n        newbst = self._copy_without_data()\n        ds = self.ds\n        bin_centers_ = []\n        bins_ = []\n        binned_support_ = []\n        support_ = []\n        all_abscissa_vals = []\n\n        n_preceding_bins = 0\n\n        for frm, to in idx:\n            idx_array = np.arange(frm, to + 1).astype(int)\n            all_abscissa_vals.append(idx_array)\n            bin_centers = self.bin_centers[idx_array]\n            bins = np.append(bin_centers - ds / 2, bin_centers[-1] + ds / 2)\n\n            binned_support = [n_preceding_bins, n_preceding_bins + len(bins) - 2]\n            n_preceding_bins += len(bins) - 1\n            support = type(self._abscissa.support)((bins[0], bins[-1]))\n\n            bin_centers_.append(bin_centers)\n            bins_.append(bins)\n            binned_support_.append(binned_support)\n            support_.append(support)\n\n        bin_centers = np.concatenate(bin_centers_)\n        bins = np.concatenate(bins_)\n        binned_support = np.array(binned_support_)\n        support = np.sum(support_)\n        all_abscissa_vals = np.concatenate(all_abscissa_vals)\n\n        newbst._bin_centers = bin_centers\n        newbst._bins = bins\n        newbst._binned_support = binned_support\n        newbst._abscissa.support = support\n        newbst._data = newbst.data[:, all_abscissa_vals]\n\n        newbst.__renew__()\n\n        return newbst\n\n    @property\n    def n_active(self):\n        \"\"\"Number of active series.\n\n        An active series is any series that fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    @property\n    def n_active_per_bin(self):\n        \"\"\"Number of active series per data bin with shape (n_bins,).\"\"\"\n        if self.isempty:\n            return 0\n        # TODO: profile several alternatves. Could use data &gt; 0, or\n        # other numpy methods to get a more efficient implementation:\n        return self.data.clip(max=1).sum(axis=0)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return self.data.sum(axis=1)\n\n    def flatten(self, *, series_id=None, series_label=None):\n        \"\"\"Collapse events across series.\n\n        WARNING! series_tags are thrown away when flattening.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        series_label (str)\n            (series) Label for event series, default is 'flattened'.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n        if series_label is None:\n            series_label = \"flattened\"\n\n        binnedeventarray = self._copy_without_data()\n\n        binnedeventarray._data = np.array(self.data.sum(axis=0), ndmin=2)\n\n        binnedeventarray._bins = self.bins\n        binnedeventarray._abscissa.support = self.support\n        binnedeventarray._bin_centers = self.bin_centers\n        binnedeventarray._binned_support = self.binned_support\n\n        binnedeventarray._series_ids = [series_id]\n        binnedeventarray._series_labels = [series_label]\n        binnedeventarray._series_tags = None\n        binnedeventarray.__renew__()\n\n        return binnedeventarray\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying BinnedEventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying BinnedEventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval(self._abscissa.support)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.bin_centers","title":"<code>bin_centers</code>  <code>property</code>","text":"<p>(np.array) The bin centers (in seconds).</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.binnedSupport","title":"<code>binnedSupport</code>  <code>property</code>","text":"<p>(np.array) The binned support of the BinnedEventArray (in bin IDs) of shape (n_intervals, 2).</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.binned_support","title":"<code>binned_support</code>  <code>property</code>","text":"<p>(np.array) The binned support of the BinnedEventArray (in bin IDs) of shape (n_intervals, 2).</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.bins","title":"<code>bins</code>  <code>property</code>","text":"<p>(np.array) The bin edges (in seconds).</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.centers","title":"<code>centers</code>  <code>property</code>","text":"<p>(np.array) The bin centers (in seconds).</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>(np.array) Event counts in all bins, with shape (n_series, n_bins).</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.ds","title":"<code>ds</code>  <code>property</code> <code>writable</code>","text":"<p>(float) Bin width in seconds.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.event_centers","title":"<code>event_centers</code>  <code>property</code>","text":"<p>(np.array) The centers (in seconds) of each event.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.eventarray","title":"<code>eventarray</code>  <code>property</code>","text":"<p>(nelpy.EventArray) The original EventArray associated with the binned data.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty BinnedEventArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>Lengths of contiguous segments, in number of bins.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>Number of active series.</p> <p>An active series is any series that fired at least one event.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.n_active_per_bin","title":"<code>n_active_per_bin</code>  <code>property</code>","text":"<p>Number of active series per data bin with shape (n_bins,).</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) The number of bins.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.n_series","title":"<code>n_series</code>  <code>property</code>","text":"<p>(int) The number of series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying BinnedEventArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.bst_from_indices","title":"<code>bst_from_indices(idx)</code>","text":"<p>Return a BinnedEventArray from a list of indices.</p> <p>bst : BinnedEventArray idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE</p> <p>Examples:</p> <p>idx = [[10, 20]     [25, 50]] bst_from_indices(bst, idx=idx)</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bst_from_indices(self, idx):\n    \"\"\"\n    Return a BinnedEventArray from a list of indices.\n\n    bst : BinnedEventArray\n    idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE\n\n    Examples\n    --------\n    idx = [[10, 20]\n        [25, 50]]\n    bst_from_indices(bst, idx=idx)\n    \"\"\"\n\n    idx = np.atleast_2d(idx)\n\n    newbst = self._copy_without_data()\n    ds = self.ds\n    bin_centers_ = []\n    bins_ = []\n    binned_support_ = []\n    support_ = []\n    all_abscissa_vals = []\n\n    n_preceding_bins = 0\n\n    for frm, to in idx:\n        idx_array = np.arange(frm, to + 1).astype(int)\n        all_abscissa_vals.append(idx_array)\n        bin_centers = self.bin_centers[idx_array]\n        bins = np.append(bin_centers - ds / 2, bin_centers[-1] + ds / 2)\n\n        binned_support = [n_preceding_bins, n_preceding_bins + len(bins) - 2]\n        n_preceding_bins += len(bins) - 1\n        support = type(self._abscissa.support)((bins[0], bins[-1]))\n\n        bin_centers_.append(bin_centers)\n        bins_.append(bins)\n        binned_support_.append(binned_support)\n        support_.append(support)\n\n    bin_centers = np.concatenate(bin_centers_)\n    bins = np.concatenate(bins_)\n    binned_support = np.array(binned_support_)\n    support = np.sum(support_)\n    all_abscissa_vals = np.concatenate(all_abscissa_vals)\n\n    newbst._bin_centers = bin_centers\n    newbst._bins = bins\n    newbst._binned_support = binned_support\n    newbst._abscissa.support = support\n    newbst._data = newbst.data[:, all_abscissa_vals]\n\n    newbst.__renew__()\n\n    return newbst\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.center","title":"<code>center(inplace=False)</code>","text":"<p>Center data (zero mean).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def center(self, inplace=False):\n    \"\"\"Center data (zero mean).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of the BinnedEventArray.\"\"\"\n    newcopy = copy.deepcopy(self)\n    newcopy.__renew__()\n    return newcopy\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.empty","title":"<code>empty(*, inplace=False)</code>","text":"<p>Remove data (but not metadata) from BinnedEventArray.</p> <p>Attributes 'data', and 'support' 'binned_support' are all emptied.</p> <p>Note: n_series, series_ids, etc. are all preserved.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def empty(self, *, inplace=False):\n    \"\"\"Remove data (but not metadata) from BinnedEventArray.\n\n    Attributes 'data', and 'support' 'binned_support' are all emptied.\n\n    Note: n_series, series_ids, etc. are all preserved.\n    \"\"\"\n    if not inplace:\n        out = self._copy_without_data()\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        return out\n    out = self\n    out._data = np.zeros((self.n_series, 0))\n    out._abscissa.support = type(self._abscissa.support)(empty=True)\n    out._binned_support = None\n    out._bin_centers = None\n    out._bins = None\n    out._eventarray.empty(inplace=True)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.flatten","title":"<code>flatten(*, series_id=None, series_label=None)</code>","text":"<p>Collapse events across series.</p> <p>WARNING! series_tags are thrown away when flattening.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> <code>series_label</code> <p>(series) Label for event series, default is 'flattened'.</p> <code>None</code> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def flatten(self, *, series_id=None, series_label=None):\n    \"\"\"Collapse events across series.\n\n    WARNING! series_tags are thrown away when flattening.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    series_label (str)\n        (series) Label for event series, default is 'flattened'.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n    if series_label is None:\n        series_label = \"flattened\"\n\n    binnedeventarray = self._copy_without_data()\n\n    binnedeventarray._data = np.array(self.data.sum(axis=0), ndmin=2)\n\n    binnedeventarray._bins = self.bins\n    binnedeventarray._abscissa.support = self.support\n    binnedeventarray._bin_centers = self.bin_centers\n    binnedeventarray._binned_support = self.binned_support\n\n    binnedeventarray._series_ids = [series_id]\n    binnedeventarray._series_labels = [series_label]\n    binnedeventarray._series_tags = None\n    binnedeventarray.__renew__()\n\n    return binnedeventarray\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.mean","title":"<code>mean(*, axis=1)</code>","text":"<p>Returns the mean of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def mean(self, *, axis=1):\n    \"\"\"Returns the mean of each series in BinnedEventArray.\"\"\"\n    try:\n        means = np.nanmean(self.data, axis=axis).squeeze()\n        if means.size == 1:\n            return means.item()\n        return means\n    except IndexError:\n        raise IndexError(\"Empty BinnedEventArray; cannot calculate mean.\")\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.median","title":"<code>median(*, axis=1)</code>","text":"<p>Returns the median of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def median(self, *, axis=1):\n    \"\"\"Returns the median of each series in BinnedEventArray.\"\"\"\n    try:\n        medians = np.nanmedian(self.data, axis=axis).squeeze()\n        if medians.size == 1:\n            return medians.item()\n        return medians\n    except IndexError:\n        raise IndexError(\"Empty BinnedEventArray; cannot calculate median.\")\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.normalize","title":"<code>normalize(inplace=False)</code>","text":"<p>Normalize data (unit standard deviation).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def normalize(self, inplace=False):\n    \"\"\"Normalize data (unit standard deviation).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    std = out.std()\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    partitioned = type(self)(\n        core.RegularlySampledAnalogSignalArray(self).partition(\n            ds=ds, n_intervals=n_intervals\n        )\n    )\n    # partitioned.loc = ItemGetter_loc(partitioned)\n    # partitioned.iloc = ItemGetter_iloc(partitioned)\n    return partitioned\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.rebin","title":"<code>rebin(w=None)</code>","text":"<p>Rebin the BinnedEventArray into a coarser bin size.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>int</code> <p>number of bins of width bst.ds to bin into new bin of width bst.ds*w. Default is w=1 (no re-binning).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedEventArray</code> <p>New BinnedEventArray with coarser resolution.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def rebin(self, w=None):\n    \"\"\"Rebin the BinnedEventArray into a coarser bin size.\n\n    Parameters\n    ----------\n    w : int, optional\n        number of bins of width bst.ds to bin into new bin of\n        width bst.ds*w. Default is w=1 (no re-binning).\n\n    Returns\n    -------\n    out : BinnedEventArray\n        New BinnedEventArray with coarser resolution.\n    \"\"\"\n\n    if w is None:\n        w = 1\n\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    w = int(w)\n\n    bst = self\n    return self._rebin_binnedeventarray(bst, w=w)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.smooth","title":"<code>smooth(*, sigma=None, inplace=False, truncate=None, within_intervals=False)</code>","text":"<p>Smooth BinnedEventArray by convolving with a Gaussian kernel.</p> <p>Smoothing is applied in data, and the same smoothing is applied to each series in a BinnedEventArray.</p> <p>Smoothing is applied within each interval.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedEventArray</code> <p>New BinnedEventArray with smoothed data.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(\n    self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n):\n    \"\"\"Smooth BinnedEventArray by convolving with a Gaussian kernel.\n\n    Smoothing is applied in data, and the same smoothing is applied\n    to each series in a BinnedEventArray.\n\n    Smoothing is applied within each interval.\n\n    Parameters\n    ----------\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n\n    Returns\n    -------\n    out : BinnedEventArray\n        New BinnedEventArray with smoothed data.\n    \"\"\"\n\n    if truncate is None:\n        truncate = 4\n    if sigma is None:\n        sigma = 0.01  # 10 ms default\n\n    fs = 1 / self.ds\n\n    return utils.gaussian_filter(\n        self,\n        fs=fs,\n        sigma=sigma,\n        truncate=truncate,\n        inplace=inplace,\n        within_intervals=within_intervals,\n    )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.standardize","title":"<code>standardize(inplace=False)</code>","text":"<p>Standardize data (zero mean and unit std deviation).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def standardize(self, inplace=False):\n    \"\"\"Standardize data (zero mean and unit std deviation).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    std = out.std()\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedEventArray.std","title":"<code>std(*, axis=1)</code>","text":"<p>Returns the standard deviation of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def std(self, *, axis=1):\n    \"\"\"Returns the standard deviation of each series in BinnedEventArray.\"\"\"\n    try:\n        stds = np.nanstd(self.data, axis=axis).squeeze()\n        if stds.size == 1:\n            return stds.item()\n        return stds\n    except IndexError:\n        raise IndexError(\n            \"Empty BinnedEventArray; cannot calculate standard deviation\"\n        )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.BinnedSpikeTrainArray","title":"<code>BinnedSpikeTrainArray</code>","text":"<p>               Bases: <code>BinnedEventArray</code></p> <p>Binned spike train array for analyzing neural spike data.</p> <p>A specialized version of BinnedEventArray designed specifically for spike train analysis. This class bins spike events into discrete time intervals and provides spike train-specific methods and properties through aliased attribute names.</p> <p>Parameters:</p> Name Type Description Default <code>eventarray</code> <code>EventArray or RegularlySampledAnalogSignalArray</code> <p>Input spike train data to be binned.</p> required <code>ds</code> <code>float</code> <p>The bin width, in seconds. Default is 0.0625 (62.5 ms).</p> required <code>empty</code> <code>bool</code> <p>Whether an empty BinnedSpikeTrainArray should be constructed (no data). Default is False.</p> required <code>fs</code> <code>float</code> <p>Sampling rate in Hz. If fs is passed as a parameter, then data is assumed to be in sample numbers instead of actual time values.</p> required <code>support</code> <code>IntervalArray</code> <p>The support (time intervals) over which the spike trains are defined.</p> required <code>unit_ids</code> <code>list of int</code> <p>Unit IDs for each spike train. Default creates sequential IDs starting from 1.</p> required <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Default casts unit_ids to str.</p> required <code>unit_tags</code> <code>optional</code> <p>Tags corresponding to units. Currently accepts any type.</p> required <code>label</code> <code>str</code> <p>Information pertaining to the source of the spike train data. Default is None.</p> required <code>abscissa</code> <code>TemporalAbscissa</code> <p>Object for the time (x-axis) coordinate. Default creates TemporalAbscissa.</p> required <code>ordinate</code> <code>AnalogSignalArrayOrdinate</code> <p>Object for the signal (y-axis) coordinate. Default creates AnalogSignalArrayOrdinate.</p> required <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments passed to the parent BinnedEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BinnedEventArray parent class for additional</code> <code>attributes that are defined there.</code> <code>Spike train-specific attributes (aliases)</code> <code>time</code> <code>array</code> <p>Alias for data. Spike counts in all bins, with shape (n_units, n_bins).</p> <code>n_epochs</code> <code>int</code> <p>Alias for n_intervals. The number of underlying time intervals.</p> <code>n_units</code> <code>int</code> <p>Alias for n_series. The number of units (neurons).</p> <code>n_spikes</code> <code>ndarray</code> <p>Alias for n_events. The number of spikes in each unit.</p> <code>unit_ids</code> <code>list of int</code> <p>Alias for series_ids. Unit IDs contained in the spike train array.</p> <code>unit_labels</code> <code>list of str</code> <p>Alias for series_labels. Labels corresponding to units.</p> <code>unit_tags</code> <p>Alias for series_tags. Tags corresponding to units.</p> <code>Inherited attributes</code> <code>isempty</code> <code>bool</code> <p>Whether the BinnedSpikeTrainArray is empty (no data).</p> <code>bin_centers</code> <code>ndarray</code> <p>The bin centers, in seconds.</p> <code>data</code> <code>np.array, with shape (n_units, n_bins)</code> <p>Spike counts in all bins.</p> <code>bins</code> <code>ndarray</code> <p>The bin edges, in seconds.</p> <code>binned_support</code> <code>np.ndarray, with shape (n_intervals, 2)</code> <p>The binned support of the array (in bin IDs).</p> <code>lengths</code> <code>ndarray</code> <p>Lengths of contiguous segments, in number of bins.</p> <code>eventarray</code> <code>EventArray</code> <p>The original EventArray associated with the binned spike data.</p> <code>n_bins</code> <code>int</code> <p>The number of bins.</p> <code>ds</code> <code>float</code> <p>Bin width, in seconds.</p> <code>n_active</code> <code>int</code> <p>The number of active units. A unit is considered active if it fired at least one spike.</p> <code>n_active_per_bin</code> <code>np.ndarray, with shape (n_bins, )</code> <p>Number of active units per data bin.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the BinnedSpikeTrainArray.</p> <p>Methods:</p> Name Description <code>All methods from BinnedEventArray are available, plus spike train-specific</code> <code>aliases for method names:</code> <code>reorder_units_by_ids</code> <p>Alias for reorder_series_by_ids. Reorder units by their IDs.</p> <code>reorder_units</code> <p>Alias for reorder_series. Reorder units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; # Create a BinnedSpikeTrainArray from spike times\n&gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n&gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n&gt;&gt;&gt; bst = nel.BinnedSpikeTrainArray(sta, ds=0.1)\n&gt;&gt;&gt; print(bst.n_units)\n2\n&gt;&gt;&gt; print(bst.n_bins)\n7\n&gt;&gt;&gt; print(bst.time.shape)  # alias for data\n(2, 7)\n</code></pre> See Also <p>BinnedEventArray : Parent class for general event arrays EventArray : Unbinned event array class SpikeTrainArray : Unbinned spike train array class</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class BinnedSpikeTrainArray(BinnedEventArray):\n    \"\"\"Binned spike train array for analyzing neural spike data.\n\n    A specialized version of BinnedEventArray designed specifically for spike train\n    analysis. This class bins spike events into discrete time intervals and provides\n    spike train-specific methods and properties through aliased attribute names.\n\n    Parameters\n    ----------\n    eventarray : nelpy.EventArray or nelpy.RegularlySampledAnalogSignalArray, optional\n        Input spike train data to be binned.\n    ds : float, optional\n        The bin width, in seconds. Default is 0.0625 (62.5 ms).\n    empty : bool, optional\n        Whether an empty BinnedSpikeTrainArray should be constructed (no data).\n        Default is False.\n    fs : float, optional\n        Sampling rate in Hz. If fs is passed as a parameter, then data\n        is assumed to be in sample numbers instead of actual time values.\n    support : nelpy.IntervalArray, optional\n        The support (time intervals) over which the spike trains are defined.\n    unit_ids : list of int, optional\n        Unit IDs for each spike train. Default creates sequential IDs starting from 1.\n    unit_labels : list of str, optional\n        Labels corresponding to units. Default casts unit_ids to str.\n    unit_tags : optional\n        Tags corresponding to units. Currently accepts any type.\n    label : str, optional\n        Information pertaining to the source of the spike train data.\n        Default is None.\n    abscissa : nelpy.TemporalAbscissa, optional\n        Object for the time (x-axis) coordinate. Default creates TemporalAbscissa.\n    ordinate : nelpy.AnalogSignalArrayOrdinate, optional\n        Object for the signal (y-axis) coordinate. Default creates AnalogSignalArrayOrdinate.\n    **kwargs : optional\n        Additional keyword arguments passed to the parent BinnedEventArray constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BinnedEventArray parent class for additional\n    attributes that are defined there.\n\n    Spike train-specific attributes (aliases):\n    time : np.array\n        Alias for data. Spike counts in all bins, with shape (n_units, n_bins).\n    n_epochs : int\n        Alias for n_intervals. The number of underlying time intervals.\n    n_units : int\n        Alias for n_series. The number of units (neurons).\n    n_spikes : np.ndarray\n        Alias for n_events. The number of spikes in each unit.\n    unit_ids : list of int\n        Alias for series_ids. Unit IDs contained in the spike train array.\n    unit_labels : list of str\n        Alias for series_labels. Labels corresponding to units.\n    unit_tags :\n        Alias for series_tags. Tags corresponding to units.\n\n    Inherited attributes:\n    isempty : bool\n        Whether the BinnedSpikeTrainArray is empty (no data).\n    bin_centers : np.ndarray\n        The bin centers, in seconds.\n    data : np.array, with shape (n_units, n_bins)\n        Spike counts in all bins.\n    bins : np.ndarray\n        The bin edges, in seconds.\n    binned_support : np.ndarray, with shape (n_intervals, 2)\n        The binned support of the array (in bin IDs).\n    lengths : np.ndarray\n        Lengths of contiguous segments, in number of bins.\n    eventarray : nelpy.EventArray\n        The original EventArray associated with the binned spike data.\n    n_bins : int\n        The number of bins.\n    ds : float\n        Bin width, in seconds.\n    n_active : int\n        The number of active units. A unit is considered active if\n        it fired at least one spike.\n    n_active_per_bin : np.ndarray, with shape (n_bins, )\n        Number of active units per data bin.\n    support : nelpy.IntervalArray\n        The support of the BinnedSpikeTrainArray.\n\n    Methods\n    -------\n    All methods from BinnedEventArray are available, plus spike train-specific\n    aliases for method names:\n\n    reorder_units_by_ids(*args, **kwargs)\n        Alias for reorder_series_by_ids. Reorder units by their IDs.\n    reorder_units(*args, **kwargs)\n        Alias for reorder_series. Reorder units.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; # Create a BinnedSpikeTrainArray from spike times\n    &gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n    &gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n    &gt;&gt;&gt; bst = nel.BinnedSpikeTrainArray(sta, ds=0.1)\n    &gt;&gt;&gt; print(bst.n_units)\n    2\n    &gt;&gt;&gt; print(bst.n_bins)\n    7\n    &gt;&gt;&gt; print(bst.time.shape)  # alias for data\n    (2, 7)\n\n    See Also\n    --------\n    BinnedEventArray : Parent class for general event arrays\n    EventArray : Unbinned event array class\n    SpikeTrainArray : Unbinned spike train array class\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        # 'get_event_firing_order' : 'get_spike_firing_order'\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EpochArray","title":"<code>EpochArray</code>","text":"<p>               Bases: <code>IntervalArray</code></p> <p>IntervalArray containing temporal intervals (epochs, in seconds).</p> <p>This class extends <code>IntervalArray</code> to specifically handle time-based intervals, referred to as epochs. It provides aliases for common time-related attributes and uses a <code>PrettyDuration</code> formatter for displaying lengths.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_epochs, 1) or (n_epochs,), the start time for each epoch (which then requires a <code>length</code> to be specified). If shape (n_epochs, 2), the start and stop times for each epoch. Defaults to None, creating an empty <code>EpochArray</code>.</p> required <code>length</code> <code>np.array, float, or None</code> <p>The duration of the epoch (in base units, seconds). If a float, the same duration is assumed for every epoch. Only used if <code>data</code> is a 1D array of start times.</p> required <code>meta</code> <code>dict</code> <p>Metadata associated with the epoch array.</p> required <code>empty</code> <code>bool</code> <p>If True, an empty <code>EpochArray</code> is returned, ignoring <code>data</code> and <code>length</code>. Defaults to False.</p> required <code>domain</code> <code>IntervalArray</code> <p>The domain within which the epochs are defined. If None, it defaults to an infinite domain.</p> required <code>label</code> <code>str</code> <p>A descriptive label for the epoch array.</p> required <p>Attributes:</p> Name Type Description <code>time</code> <code>array</code> <p>Alias for <code>data</code>. The start and stop times for each epoch, with shape (n_epochs, 2).</p> <code>n_epochs</code> <code>int</code> <p>Alias for <code>n_intervals</code>. The number of epochs in the array.</p> <code>duration</code> <code>float</code> <p>Alias for <code>length</code>. The total duration of the [merged] epoch array.</p> <code>durations</code> <code>array</code> <p>Alias for <code>lengths</code>. The duration of each individual epoch.</p> <code>formatter</code> <code>PrettyDuration</code> <p>The formatter used for displaying time durations.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the intervals, which is 's' (seconds) for EpochArray.</p> Notes <p>This class inherits all methods and properties from <code>IntervalArray</code>. Aliases are provided for convenience to make the API more intuitive for temporal data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy.core import EpochArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create an EpochArray from start and stop times\n&gt;&gt;&gt; epochs = EpochArray(data=np.array([[0, 10], [20, 30], [40, 50]]))\n&gt;&gt;&gt; print(epochs)\n&lt;EpochArray at 0x21b641f0950: 3 epochs&gt; of length 30 seconds\n</code></pre> <pre><code>&gt;&gt;&gt; # Create an EpochArray from start times and a common length\n&gt;&gt;&gt; starts = np.array([0, 20, 40])\n&gt;&gt;&gt; length = 5.0\n&gt;&gt;&gt; epochs_with_length = EpochArray(data=starts, length=length)\n&gt;&gt;&gt; print(epochs_with_length)\n&lt;EpochArray at 0x21b631c6050: 3 epochs&gt; of length 15 seconds\n</code></pre> <pre><code>&gt;&gt;&gt; # Accessing aliased attributes\n&gt;&gt;&gt; print(f\"Number of epochs: {epochs.n_epochs}\")\nNumber of epochs: 3\n&gt;&gt;&gt; print(f\"Total duration: {epochs.duration}\")\nTotal duration: 30 seconds\n&gt;&gt;&gt; print(f\"Individual durations: {epochs.durations}\")\nIndividual durations: [10 10 10]\n</code></pre> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class EpochArray(IntervalArray):\n    \"\"\"IntervalArray containing temporal intervals (epochs, in seconds).\n\n    This class extends `IntervalArray` to specifically handle time-based\n    intervals, referred to as epochs. It provides aliases for common\n    time-related attributes and uses a `PrettyDuration` formatter for\n    displaying lengths.\n\n    Parameters\n    ----------\n    data : np.array, optional\n        If shape (n_epochs, 1) or (n_epochs,), the start time for each\n        epoch (which then requires a `length` to be specified).\n        If shape (n_epochs, 2), the start and stop times for each epoch.\n        Defaults to None, creating an empty `EpochArray`.\n    length : np.array, float, or None, optional\n        The duration of the epoch (in base units, seconds). If a float,\n        the same duration is assumed for every epoch. Only used if `data`\n        is a 1D array of start times.\n    meta : dict, optional\n        Metadata associated with the epoch array.\n    empty : bool, optional\n        If True, an empty `EpochArray` is returned, ignoring `data` and `length`.\n        Defaults to False.\n    domain : IntervalArray, optional\n        The domain within which the epochs are defined. If None, it defaults\n        to an infinite domain.\n    label : str, optional\n        A descriptive label for the epoch array.\n\n    Attributes\n    ----------\n    time : np.array\n        Alias for `data`. The start and stop times for each epoch, with shape\n        (n_epochs, 2).\n    n_epochs : int\n        Alias for `n_intervals`. The number of epochs in the array.\n    duration : float\n        Alias for `length`. The total duration of the [merged] epoch array.\n    durations : np.array\n        Alias for `lengths`. The duration of each individual epoch.\n    formatter : formatters.PrettyDuration\n        The formatter used for displaying time durations.\n    base_unit : str\n        The base unit of the intervals, which is 's' (seconds) for EpochArray.\n\n    Notes\n    -----\n    This class inherits all methods and properties from `IntervalArray`.\n    Aliases are provided for convenience to make the API more intuitive\n    for temporal data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy.core import EpochArray\n\n    &gt;&gt;&gt; # Create an EpochArray from start and stop times\n    &gt;&gt;&gt; epochs = EpochArray(data=np.array([[0, 10], [20, 30], [40, 50]]))\n    &gt;&gt;&gt; print(epochs)\n    &lt;EpochArray at 0x21b641f0950: 3 epochs&gt; of length 30 seconds\n\n    &gt;&gt;&gt; # Create an EpochArray from start times and a common length\n    &gt;&gt;&gt; starts = np.array([0, 20, 40])\n    &gt;&gt;&gt; length = 5.0\n    &gt;&gt;&gt; epochs_with_length = EpochArray(data=starts, length=length)\n    &gt;&gt;&gt; print(epochs_with_length)\n    &lt;EpochArray at 0x21b631c6050: 3 epochs&gt; of length 15 seconds\n\n    &gt;&gt;&gt; # Accessing aliased attributes\n    &gt;&gt;&gt; print(f\"Number of epochs: {epochs.n_epochs}\")\n    Number of epochs: 3\n    &gt;&gt;&gt; print(f\"Total duration: {epochs.duration}\")\n    Total duration: 30 seconds\n    &gt;&gt;&gt; print(f\"Individual durations: {epochs.durations}\")\n    Individual durations: [10 10 10]\n    \"\"\"\n\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"duration\": \"length\",\n        \"durations\": \"lengths\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n        self._interval_label = \"epoch\"\n        self.formatter = formatters.PrettyDuration\n        self.base_unit = self.formatter.base_unit\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray","title":"<code>EventArray</code>","text":"<p>               Bases: <code>BaseEventArray</code></p> <p>A multiseries eventarray with shared support.</p> <p>Parameters:</p> Name Type Description Default <code>abscissa_vals</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,).</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000.</p> <code>None</code> <code>support</code> <code>IntervalArray</code> <p>IntervalArray on which eventarrays are defined. Default is [0, last event] inclusive.</p> <code>None</code> <code>series_ids</code> <code>list of int</code> <p>Unit IDs.</p> <code>None</code> <code>series_labels</code> <code>list of str</code> <p>Labels corresponding to series. Default casts series_ids to str.</p> <code>None</code> <code>series_tags</code> <code>optional</code> <p>Tags correponding to series. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> <code>None</code> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Whether an empty EventArray should be constructed (no data).</p> <code>False</code> <code>assume_sorted</code> <code>boolean</code> <p>Whether the abscissa values should be treated as sorted (non-decreasing) or not. Significant overhead during RSASA object creation can be removed if this is True, but note that unsorted abscissa values will mess everything up. Default is False</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Additional keyword arguments to forward along to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray superclass for additional</code> <code>attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the EventArray is empty (no data).</p> <code>n_series</code> <code>int</code> <p>The number of series.</p> <code>n_active</code> <code>int</code> <p>The number of active series. A series is considered active if it fired at least one event.</p> <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,).</p> <code>n_events</code> <code>ndarray</code> <p>The number of events in each series.</p> <code>issorted</code> <code>bool</code> <p>Whether the data are sorted.</p> <code>first_event</code> <code>float</code> <p>The time of the very first event, across all series.</p> <code>last_event</code> <code>float</code> <p>The time of the very last event, across all series.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class EventArray(BaseEventArray):\n    \"\"\"A multiseries eventarray with shared support.\n\n    Parameters\n    ----------\n    abscissa_vals : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,).\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000.\n    support : IntervalArray, optional\n        IntervalArray on which eventarrays are defined.\n        Default is [0, last event] inclusive.\n    series_ids : list of int, optional\n        Unit IDs.\n    series_labels : list of str, optional\n        Labels corresponding to series. Default casts series_ids to str.\n    series_tags : optional\n        Tags correponding to series.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    label : str or None, optional\n        Information pertaining to the source of the eventarray.\n    empty : bool, optional\n        Whether an empty EventArray should be constructed (no data).\n    assume_sorted : boolean, optional\n        Whether the abscissa values should be treated as sorted (non-decreasing)\n        or not. Significant overhead during RSASA object creation can be removed\n        if this is True, but note that unsorted abscissa values will mess\n        everything up.\n        Default is False\n    kwargs : optional\n        Additional keyword arguments to forward along to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray superclass for additional\n    attributes that are defined there.\n    isempty : bool\n        Whether the EventArray is empty (no data).\n    n_series : int\n        The number of series.\n    n_active : int\n        The number of active series. A series is considered active if\n        it fired at least one event.\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,).\n    n_events : np.ndarray\n        The number of events in each series.\n    issorted : bool\n        Whether the data are sorted.\n    first_event : np.float\n        The time of the very first event, across all series.\n    last_event : np.float\n        The time of the very last event, across all series.\n    \"\"\"\n\n    __attributes__ = [\"_data\"]\n    __attributes__.extend(BaseEventArray.__attributes__)\n\n    def __init__(\n        self,\n        abscissa_vals=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        series_labels=None,\n        series_tags=None,\n        label=None,\n        empty=False,\n        assume_sorted=None,\n        **kwargs,\n    ):\n        if assume_sorted is None:\n            assume_sorted = False\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.warning(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            data = np.array(data, dtype=object)\n            try:\n                if data.shape[-1] &lt; 2 and np.max(data.shape) &gt; 1:\n                    return True\n                if max(np.array(data).shape[:-1]) &gt; 1 and data.shape[-1] == 1:\n                    return True\n            except (IndexError, TypeError, ValueError):\n                return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            --------\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            try:\n                if isinstance(data[0][0], list) or isinstance(data[0][0], np.ndarray):\n                    logging.info(\"event datas input has too many layers!\")\n                    try:\n                        if max(np.array(data).shape[:-1]) &gt; 1:\n                            #                 singletons = True\n                            return False\n                    except ValueError:\n                        return False\n                    data = np.squeeze(data)\n            except (IndexError, TypeError):\n                pass\n            try:\n                if isinstance(data[1], list) or isinstance(data[1], np.ndarray):\n                    return False\n            except (IndexError, TypeError):\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            if is_single_series(data):\n                return np.array(np.squeeze(data), ndmin=2)\n            if is_singletons(data):\n                data = np.squeeze(data)\n                n = np.max(data.shape)\n                if len(data.shape) == 1:\n                    m = 1\n                else:\n                    m = np.min(data.shape)\n                data = np.reshape(data, (n, m))\n            else:\n                data = np.squeeze(data)\n                if data.dtype == np.dtype(\"O\"):\n                    jagged = True\n                else:\n                    jagged = False\n                if jagged:  # jagged array\n                    # standardize input so that a list of lists is converted\n                    # to an array of arrays:\n                    data = np.array([np.asarray(st) for st in data], dtype=object)\n                else:\n                    data = np.array(data, ndmin=2)\n            return data\n\n        # standardize input data to 2D array\n        data = standardize_to_2d(np.array(abscissa_vals, dtype=object))\n\n        # If user said to assume the absicssa vals are sorted but they actually\n        # aren't, then the mistake will get propagated down. The responsibility\n        # therefore lies on the user whenever he/she uses assume_sorted=True\n        # as a constructor argument\n        for ii, train in enumerate(data):\n            if not assume_sorted:\n                # sort event series, but only if necessary\n                if not utils.is_sorted(train):\n                    data[ii] = np.sort(train)\n            else:\n                data[ii] = np.sort(train)\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n        kwargs[\"series_labels\"] = series_labels\n        kwargs[\"series_tags\"] = series_tags\n        kwargs[\"label\"] = label\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # print(self.type_name, kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            first_spk = np.nanmin(\n                np.array([series[0] for series in data if len(series) != 0])\n            )\n            # BUG: if eventseries is empty np.array([]) then series[-1]\n            # raises an error in the following:\n            # FIX: list[-1] raises an IndexError for an empty list,\n            # whereas list[-1:] returns an empty list.\n            last_spk = np.nanmax(\n                np.array([series[-1:] for series in data if len(series) != 0])\n            )\n            self.support = type(self._abscissa.support)(\n                np.array([first_spk, last_spk + 1 / fs])\n            )\n            # in the above, there's no reason to restrict to support\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            self.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        self._restrict_to_interval(self._abscissa.support, data=data)\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without event datas.\n        Note: the support is left unchanged.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._data = np.array(self.n_series * [None])\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Returns a copy of the EventArray.\"\"\"\n        newcopy = copy.deepcopy(self)\n        newcopy.__renew__()\n        return newcopy\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to EventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty EventArray.\"\"\"\n        try:\n            return np.sum([len(st) for st in self.data]) == 0\n        except TypeError:\n            return True  # this happens when self.data is None\n\n    @property\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        try:\n            return utils.PrettyInt(len(self.data))\n        except TypeError:\n            return 0\n\n    @property\n    def n_active(self):\n        \"\"\"(int) The number of active series.\n\n        A series is considered active if it fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    def flatten(self, *, series_id=None, series_label=None):\n        \"\"\"Collapse events across series.\n\n        WARNING! series_tags are thrown away when flattening.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        series_label (str)\n            (series) Label for event series, default is 'flattened'.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n        if series_label is None:\n            series_label = \"flattened\"\n\n        flattened = self._copy_without_data()\n        flattened._series_ids = [series_id]\n        flattened._series_labels = [series_label]\n        flattened._series_tags = None\n\n        # Efficient: concatenate all events, sort once\n        all_events = np.concatenate(self.data)\n        all_events.sort()\n        flattened._data = np.array([all_events], ndmin=2)\n        flattened.__renew__()\n        return flattened\n\n    def _restrict(self, intervalslice, seriesslice, *, subseriesslice=None):\n        self._restrict_to_series_subset(seriesslice)\n        self._restrict_to_interval(intervalslice)\n        return self\n\n    def _restrict_to_series_subset(self, idx):\n        # Warning: This function can mutate data\n\n        # TODO: Update tags\n        try:\n            self._data = self._data[idx]\n            singleseries = len(self._data) == 1\n            if singleseries:\n                self._data = np.array(self._data[0], ndmin=2)\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except AttributeError:\n            self._data = self._data[idx]\n            singleseries = len(self._data) == 1\n            if singleseries:\n                self._data = np.array(self._data[0], ndmin=2)\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except IndexError:\n            raise IndexError(\n                \"One of more indices were out of bounds for n_series with size {}\".format(\n                    self.n_series\n                )\n            )\n        except Exception:\n            raise TypeError(\"Unsupported indexing type {}\".format(type(idx)))\n\n        return self\n\n    def _restrict_to_interval(self, intervalslice, *, data=None):\n        \"\"\"Return data restricted to an intervalarray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : nelpy.IntervalArray\n        \"\"\"\n\n        # Warning: this function can mutate data\n        # This should be called from _restrict only. That's where\n        # intervalarray is first checked against the support.\n        # This function assumes that has happened already, so\n        # every point in intervalarray is also in the support\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n\n        if data is None:\n            data = self._data\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                # no restriction on interval\n                return self\n\n        newintervals = self._abscissa.support[intervalslice].merge()\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=True)\n\n        issue_warning = False\n        if not self.isempty:\n            for series, evt_data in enumerate(data):\n                indices = []\n                for epdata in newintervals.data:\n                    t_start = epdata[0]\n                    t_stop = epdata[1]\n                    frm, to = np.searchsorted(evt_data, (t_start, t_stop))\n                    indices.append((frm, to))\n                indices = np.array(indices, ndmin=2)\n                if np.diff(indices).sum() &lt; len(evt_data):\n                    issue_warning = True\n                singleseries = len(self._data) == 1\n                if singleseries:\n                    data_list = []\n                    for start, stop in indices:\n                        data_list.extend(evt_data[start:stop])\n                    data = np.array(data_list, ndmin=2)\n                else:\n                    # here we have to do some annoying conversion between\n                    # arrays and lists to fully support jagged array\n                    # mutation\n                    data_list = []\n                    for start, stop in indices:\n                        data_list.extend(evt_data[start:stop])\n                    data_ = data.tolist()  # this creates copy\n                    data_[series] = np.array(data_list)\n                    data = utils.ragged_array(data_)\n            self._data = data\n            if issue_warning:\n                logging.warning(\"ignoring events outside of eventarray support\")\n\n        self._abscissa.support = newintervals\n        return self\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        if self.label is not None:\n            labelstr = \" from %s\" % self.label\n        else:\n            labelstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s%s\" % (\n            self.type_name,\n            address_str,\n            numstr,\n            epstr,\n            fsstr,\n            labelstr,\n        )\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a binned eventarray.\"\"\"\n        return BinnedEventArray(self, ds=ds)\n\n    @property\n    def data(self):\n        \"\"\"Event datas in seconds.\"\"\"\n        return self._data\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.data])\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Sorted EventArray.\"\"\"\n        if self.isempty:\n            return True\n        return np.array([utils.is_sorted(eventarray) for eventarray in self.data]).all()\n\n    def _reorder_series_by_idx(self, neworder, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,)\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            out._series_labels[frm], out._series_labels[to] = (\n                out._series_labels[to],\n                out._series_labels[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def reorder_series(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        raise DeprecationWarning(\n            \"reorder_series has been deprecated. Use reorder_series_by_id(x/s) instead!\"\n        )\n\n    def reorder_series_by_ids(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        neworder = [self.series_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            out._series_labels[frm], out._series_labels[to] = (\n                out._series_labels[to],\n                out._series_labels[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def get_event_firing_order(self):\n        \"\"\"Returns a list of series_ids such that the series are ordered\n        by when they first fire in the EventArray.\n\n        Return\n        ------\n        firing_order : list of series_ids\n        \"\"\"\n\n        first_events = [\n            (ii, series[0]) for (ii, series) in enumerate(self.data) if len(series) != 0\n        ]\n        first_events_series_ids = np.array(self.series_ids)[\n            [fs[0] for fs in first_events]\n        ]\n        first_events_datas = np.array([fs[1] for fs in first_events])\n        sortorder = np.argsort(first_events_datas)\n        first_events_series_ids = first_events_series_ids[sortorder]\n        remaining_ids = list(set(self.series_ids) - set(first_events_series_ids))\n        firing_order = list(first_events_series_ids)\n        firing_order.extend(remaining_ids)\n\n        return firing_order\n\n    @property\n    def first_event(self):\n        \"\"\"Returns the [time of the] first event across all series.\"\"\"\n        first = np.inf\n        for series in self.data:\n            if series[0] &lt; first:\n                first = series[0]\n        return first\n\n    @property\n    def last_event(self):\n        \"\"\"Returns the [time of the] last event across all series.\"\"\"\n        last = -np.inf\n        for series in self.data:\n            if series[-1] &gt; last:\n                last = series[-1]\n        return last\n\n    def empty(self, *, inplace=False):\n        \"\"\"Remove data (but not metadata) from EventArray.\n\n        Attributes 'data', and 'support' are both emptied.\n\n        Note: n_series, series_ids, etc. are all preserved.\n        \"\"\"\n        if not inplace:\n            out = self._copy_without_data()\n            out._abscissa.support = type(self._abscissa.support)(empty=True)\n            return out\n        out = self\n        out._data = np.array(self.n_series * [None])\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        out.__renew__()\n        return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Event datas in seconds.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.first_event","title":"<code>first_event</code>  <code>property</code>","text":"<p>Returns the [time of the] first event across all series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty EventArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Sorted EventArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.last_event","title":"<code>last_event</code>  <code>property</code>","text":"<p>Returns the [time of the] last event across all series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>(int) The number of active series.</p> <p>A series is considered active if it fired at least one event.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.n_series","title":"<code>n_series</code>  <code>property</code>","text":"<p>(int) The number of series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a binned eventarray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a binned eventarray.\"\"\"\n    return BinnedEventArray(self, ds=ds)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the EventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of the EventArray.\"\"\"\n    newcopy = copy.deepcopy(self)\n    newcopy.__renew__()\n    return newcopy\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.empty","title":"<code>empty(*, inplace=False)</code>","text":"<p>Remove data (but not metadata) from EventArray.</p> <p>Attributes 'data', and 'support' are both emptied.</p> <p>Note: n_series, series_ids, etc. are all preserved.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def empty(self, *, inplace=False):\n    \"\"\"Remove data (but not metadata) from EventArray.\n\n    Attributes 'data', and 'support' are both emptied.\n\n    Note: n_series, series_ids, etc. are all preserved.\n    \"\"\"\n    if not inplace:\n        out = self._copy_without_data()\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        return out\n    out = self\n    out._data = np.array(self.n_series * [None])\n    out._abscissa.support = type(self._abscissa.support)(empty=True)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.flatten","title":"<code>flatten(*, series_id=None, series_label=None)</code>","text":"<p>Collapse events across series.</p> <p>WARNING! series_tags are thrown away when flattening.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> <code>series_label</code> <p>(series) Label for event series, default is 'flattened'.</p> <code>None</code> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def flatten(self, *, series_id=None, series_label=None):\n    \"\"\"Collapse events across series.\n\n    WARNING! series_tags are thrown away when flattening.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    series_label (str)\n        (series) Label for event series, default is 'flattened'.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n    if series_label is None:\n        series_label = \"flattened\"\n\n    flattened = self._copy_without_data()\n    flattened._series_ids = [series_id]\n    flattened._series_labels = [series_label]\n    flattened._series_tags = None\n\n    # Efficient: concatenate all events, sort once\n    all_events = np.concatenate(self.data)\n    all_events.sort()\n    flattened._data = np.array([all_events], ndmin=2)\n    flattened.__renew__()\n    return flattened\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.get_event_firing_order","title":"<code>get_event_firing_order()</code>","text":"<p>Returns a list of series_ids such that the series are ordered by when they first fire in the EventArray.</p> Return <p>firing_order : list of series_ids</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def get_event_firing_order(self):\n    \"\"\"Returns a list of series_ids such that the series are ordered\n    by when they first fire in the EventArray.\n\n    Return\n    ------\n    firing_order : list of series_ids\n    \"\"\"\n\n    first_events = [\n        (ii, series[0]) for (ii, series) in enumerate(self.data) if len(series) != 0\n    ]\n    first_events_series_ids = np.array(self.series_ids)[\n        [fs[0] for fs in first_events]\n    ]\n    first_events_datas = np.array([fs[1] for fs in first_events])\n    sortorder = np.argsort(first_events_datas)\n    first_events_series_ids = first_events_series_ids[sortorder]\n    remaining_ids = list(set(self.series_ids) - set(first_events_series_ids))\n    firing_order = list(first_events_series_ids)\n    firing_order.extend(remaining_ids)\n\n    return firing_order\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.reorder_series","title":"<code>reorder_series(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def reorder_series(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    raise DeprecationWarning(\n        \"reorder_series has been deprecated. Use reorder_series_by_id(x/s) instead!\"\n    )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.EventArray.reorder_series_by_ids","title":"<code>reorder_series_by_ids(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def reorder_series_by_ids(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    neworder = [self.series_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        utils.swap_rows(out._data, frm, to)\n        out._series_ids[frm], out._series_ids[to] = (\n            out._series_ids[to],\n            out._series_ids[frm],\n        )\n        out._series_labels[frm], out._series_labels[to] = (\n            out._series_labels[to],\n            out._series_labels[frm],\n        )\n        # TODO: re-build series tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IMUSensorArray","title":"<code>IMUSensorArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>Array for storing IMU (Inertial Measurement Unit) sensor data with regular sampling rates.</p> <p>This class extends RegularlySampledAnalogSignalArray for IMU-specific data, such as accelerometer, gyroscope, and magnetometer signals.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>__aliases__</code> <code>dict</code> <p>Dictionary of class-specific aliases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; imu = IMUSensorArray(data=[[0, 1, 2], [3, 4, 5]], fs=100)\n&gt;&gt;&gt; imu.data\narray([[0, 1, 2],\n       [3, 4, 5]])\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class IMUSensorArray(RegularlySampledAnalogSignalArray):\n    \"\"\"\n    Array for storing IMU (Inertial Measurement Unit) sensor data with regular sampling rates.\n\n    This class extends RegularlySampledAnalogSignalArray for IMU-specific data, such as accelerometer, gyroscope, and magnetometer signals.\n\n    Parameters\n    ----------\n    *args :\n        Positional arguments passed to the parent class.\n    **kwargs :\n        Keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    __aliases__ : dict\n        Dictionary of class-specific aliases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; imu = IMUSensorArray(data=[[0, 1, 2], [3, 4, 5]], fs=100)\n    &gt;&gt;&gt; imu.data\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray","title":"<code>IntervalArray</code>","text":"<p>An array of intervals, where each interval has a start and stop.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_intervals, 1) or (n_intervals,), the start value for each interval (which then requires a length to be specified). If shape (n_intervals, 2), the start and stop values for each interval.</p> <code>None</code> <code>length</code> <code>np.array, float, or None</code> <p>The length of the interval (in base units). If (float) then the same length is assumed for every interval.</p> <code>None</code> <code>meta</code> <code>dict</code> <p>Metadata associated with spiketrain.</p> <code>None</code> <code>domain</code> <code>IntervalArray ??? This is pretty meta @-@</code> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>array</code> <p>The start and stop values for each interval. With shape (n_intervals, 2).</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class IntervalArray:\n    \"\"\"An array of intervals, where each interval has a start and stop.\n\n    Parameters\n    ----------\n    data : np.array\n        If shape (n_intervals, 1) or (n_intervals,), the start value for each\n        interval (which then requires a length to be specified).\n        If shape (n_intervals, 2), the start and stop values for each interval.\n    length : np.array, float, or None, optional\n        The length of the interval (in base units). If (float) then the same\n        length is assumed for every interval.\n    meta : dict, optional\n        Metadata associated with spiketrain.\n    domain : IntervalArray ??? This is pretty meta @-@\n\n    Attributes\n    ----------\n    data : np.array\n        The start and stop values for each interval. With shape (n_intervals, 2).\n    \"\"\"\n\n    __aliases__ = {}\n    __attributes__ = [\"_data\", \"_meta\", \"_domain\"]\n\n    def __init__(\n        self,\n        data=None,\n        *args,\n        length=None,\n        meta=None,\n        empty=False,\n        domain=None,\n        label=None,\n    ):\n        self.__version__ = version.__version__\n\n        self.type_name = self.__class__.__name__\n        self._interval_label = \"interval\"\n        self.formatter = formatters.ArbitraryFormatter\n        self.base_unit = self.formatter.base_unit\n\n        if len(args) &gt; 1:\n            raise TypeError(\n                \"__init__() takes from 1 to 3 positional arguments but 4 were given\"\n            )\n        elif len(args) == 1:\n            data = [data, args[0]]\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        data = np.squeeze(data)  # coerce data into np.array\n\n        # all possible inputs:\n        # 1. single interval, no length    --- OK\n        # 2. single interval and length    --- ERR\n        # 3. multiple intervals, no length --- OK\n        # 4. multiple intervals and length --- ERR\n        # 5. single scalar and length   --- OK\n        # 6. scalar list and duratin list --- OK\n        #\n        # Q. won't np.squeeze make our life difficult?\n        #\n        # Strategy: determine if length was passed. If so, try to see\n        # if data can be coerced into right shape. If not, raise\n        # error.\n        # If length was NOT passed, then do usual checks for intervals.\n\n        if length is not None:  # assume we received scalar starts\n            data = np.array(data, ndmin=1)\n            length = np.squeeze(length).astype(float)\n            if length.ndim == 0:\n                length = length[..., np.newaxis]\n\n            if data.ndim == 2 and length.ndim == 1:\n                raise ValueError(\"length not allowed when using start and stop values\")\n\n            if len(length) &gt; 1:\n                if data.ndim == 1 and data.shape[0] != length.shape[0]:\n                    raise ValueError(\"must have same number of data and length data\")\n            if data.ndim == 1 and length.ndim == 1:\n                stop_interval = data + length\n                data = np.hstack(\n                    (data[..., np.newaxis], stop_interval[..., np.newaxis])\n                )\n        else:  # length was not specified, so assume we recived intervals\n            # Note: if we have an empty array of data with no\n            # dimension, then calling len(data) will return a\n            # TypeError.\n            try:\n                # if no data were received, return an empty IntervalArray:\n                if len(data) == 0:\n                    self.__init__(empty=True)\n                    return\n            except TypeError:\n                logging.warning(\n                    \"unsupported type (\"\n                    + str(type(data))\n                    + \"); creating empty {}\".format(self.type_name)\n                )\n                self.__init__(empty=True)\n                return\n\n            # Only one interval is given eg IntervalArray([3,5,6,10]) with no\n            # length and more than two values:\n            if data.ndim == 1 and len(data) &gt; 2:  # we already know length is None\n                raise TypeError(\n                    \"data of size (n_intervals, ) has to be accompanied by a length\"\n                )\n\n            if data.ndim == 1:  # and length is None:\n                data = np.array([data])\n\n        if data.ndim &gt; 2:\n            raise ValueError(\"data must be a 1D or a 2D vector\")\n\n        try:\n            if data[:, 0].shape[0] != data[:, 1].shape[0]:\n                raise ValueError(\"must have the same number of start and stop values\")\n        except Exception:\n            raise Exception(\"Unhandled {}.__init__ case.\".format(self.type_name))\n\n        # TODO: what if start == stop? what will this break? This situation\n        # can arise automatically when slicing a spike train with one or no\n        # spikes, for example in which case the automatically inferred support\n        # is a delta dirac\n\n        if data.ndim == 2 and np.any(data[:, 1] - data[:, 0] &lt; 0):\n            raise ValueError(\"start must be less than or equal to stop\")\n\n        # potentially assign domain\n        self._domain = domain\n\n        self._data = data\n        self._meta = meta\n        self.label = label\n\n        if not self.issorted:\n            self._sort()\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self.n_intervals &gt; 1:\n            nstr = \"%s %ss\" % (self.n_intervals, self._interval_label)\n        else:\n            nstr = \"1 %s\" % self._interval_label\n        dstr = \"of length {}\".format(self.formatter(self.length))\n        return \"&lt;%s%s: %s&gt; %s\" % (self.type_name, address_str, nstr, dstr)\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without data.\"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._data = np.zeros((self.n_intervals, 2))\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        return out\n\n    def __iter__(self):\n        \"\"\"IntervalArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"IntervalArray iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_intervals - 1:\n            raise StopIteration\n\n        intervalarray = self._copy_without_data()\n        intervalarray._data = np.array([self.data[index, :]])\n        self._index += 1\n        return intervalarray\n\n    def __getitem__(self, *idx):\n        \"\"\"IntervalArray index access.\n\n        Accepts integers, slices, and IntervalArrays.\n        \"\"\"\n        if self.isempty:\n            return self\n\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        if isinstance(idx, type(self)):\n            if idx.isempty:  # case 0:\n                return type(self)(empty=True)\n            return self.intersect(interval=idx)\n        elif isinstance(idx, IntervalArray):\n            raise TypeError(\n                \"Error taking intersection. {} expected, but got {}\".format(\n                    self.type_name, idx.type_name\n                )\n            )\n        else:\n            try:  # works for ints, lists, and slices\n                out = self.copy()\n                out._data = self.data[idx, :]\n            except IndexError:\n                raise IndexError(\"{} index out of range\".format(self.type_name))\n            except Exception:\n                raise TypeError(\"unsupported subscripting type {}\".format(type(idx)))\n        return out\n\n    def __add__(self, other):\n        \"\"\"add length to start and stop of each interval, or join two interval arrays without merging\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            return new.expand(other, direction=\"both\")\n        elif isinstance(other, type(self)):\n            return self.join(other)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __sub__(self, other):\n        \"\"\"subtract length from start and stop of each interval\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            return new.shrink(other, direction=\"both\")\n        elif isinstance(other, type(self)):\n            # A - B = A intersect ~B\n            return self.intersect(~other)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __mul__(self, other):\n        \"\"\"expand (&gt;1) or shrink (&lt;1) interval lengths\"\"\"\n        raise NotImplementedError(\"operator * not yet implemented\")\n\n    def __truediv__(self, other):\n        \"\"\"expand (&gt;1) or shrink (&gt;1) interval lengths\"\"\"\n        raise NotImplementedError(\"operator / not yet implemented\")\n\n    def __lshift__(self, other):\n        \"\"\"shift data to left (&lt;&lt;)\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            new._data = new._data - other\n            if new.domain.is_finite:\n                new.domain._data = new.domain._data - other\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &lt;&lt;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __rshift__(self, other):\n        \"\"\"shift data to right (&gt;&gt;)\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            new._data = new._data + other\n            if new.domain.is_finite:\n                new.domain._data = new.domain._data + other\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &gt;&gt;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __and__(self, other):\n        \"\"\"intersection of interval arrays\"\"\"\n        if isinstance(other, type(self)):\n            new = copy.copy(self)\n            return new.intersect(other, boundaries=True)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &amp;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __or__(self, other):\n        \"\"\"join and merge interval array; set union\"\"\"\n        if isinstance(other, type(self)):\n            new = copy.copy(self)\n            joined = new.join(other)\n            union = joined.merge()\n            return union\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for |: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __invert__(self):\n        \"\"\"complement within self.domain\"\"\"\n        return self.complement()\n\n    def __bool__(self):\n        \"\"\"(bool) Empty IntervalArray\"\"\"\n        return not self.isempty\n\n    def remove_duplicates(self, inplace=False):\n        \"\"\"Remove duplicate intervals.\"\"\"\n        raise NotImplementedError\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, *, ds=None, n_intervals=None):\n        \"\"\"Returns an IntervalArray that has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum length, for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : IntervalArray\n            IntervalArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        if self.isempty:\n            raise ValueError(\"cannot parition an empty object in a meaningful way!\")\n\n        if ds is not None and n_intervals is not None:\n            raise ValueError(\"ds and n_intervals cannot be used together\")\n\n        if n_intervals is not None:\n            assert float(n_intervals).is_integer(), (\n                \"n_intervals must be a positive integer!\"\n            )\n            assert n_intervals &gt; 1, \"n_intervals must be a positive integer &gt; 1\"\n            # determine ds from number of desired points:\n            ds = self.length / n_intervals\n\n        if ds is None:\n            # neither n_intervals nor ds was specified, so assume defaults:\n            n_intervals = 100\n            ds = self.length / n_intervals\n\n        # build list of points at which to esplit the IntervalArray\n        new_starts = []\n        new_stops = []\n        for start, stop in self.data:\n            newxvals = utils.frange(start, stop, step=ds).tolist()\n            # newxvals = np.arange(start, stop, step=ds).tolist()\n            if newxvals[-1] + float_info.epsilon &lt; stop:\n                newxvals.append(stop)\n            newxvals = np.asanyarray(newxvals)\n            new_starts.extend(newxvals[:-1])\n            new_stops.extend(newxvals[1:])\n\n        # now make a new interval array:\n        out = copy.copy(self)\n        out._data = np.hstack(\n            [\n                np.array(new_starts)[..., np.newaxis],\n                np.array(new_stops)[..., np.newaxis],\n            ]\n        )\n        return out\n\n    @property\n    def label(self):\n        \"\"\"Label describing the interval array.\"\"\"\n        if self._label is None:\n            logging.warning(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n\n    def complement(self, domain=None):\n        \"\"\"Complement within domain.\n\n        Parameters\n        ----------\n        domain : IntervalArray, optional\n            IntervalArray specifying entire domain. Default is self.domain.\n\n        Returns\n        -------\n        complement : IntervalArray\n            IntervalArray containing all the nonzero intervals in the\n            complement set.\n        \"\"\"\n\n        if domain is None:\n            domain = self.domain\n\n        # make sure IntervalArray is sorted:\n        if not self.issorted:\n            self._sort()\n        # check that IntervalArray is entirely contained within domain\n        # if (self.start &lt; domain.start) or (self.stop &gt; domain.stop):\n        #     raise ValueError(\"IntervalArray must be entirely contained within domain\")\n\n        # check that IntervalArray is fully merged, or merge it if necessary\n        merged = self.merge()\n        # build complement intervals\n        starts = np.insert(merged.stops, 0, domain.start)\n        stops = np.append(merged.starts, domain.stop)\n        newvalues = np.vstack([starts, stops]).T\n        # remove intervals with zero length\n        lengths = newvalues[:, 1] - newvalues[:, 0]\n        newvalues = newvalues[lengths &gt; 0]\n        complement = copy.copy(self)\n        complement._data = newvalues\n\n        if domain.n_intervals &gt; 1:\n            return complement[domain]\n        try:\n            complement._data[0, 0] = np.max((complement._data[0, 0], domain.start))\n            complement._data[-1, -1] = np.min((complement._data[-1, -1], domain.stop))\n        except IndexError:  # complement is empty\n            return type(self)(empty=True)\n        return complement\n\n    @property\n    def domain(self):\n        \"\"\"domain (in base units) within which support is defined\"\"\"\n        if self._domain is None:\n            self._domain = type(self)([-np.inf, np.inf])\n        return self._domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"domain (in base units) within which support is defined\"\"\"\n        # TODO: add  input validation\n        if isinstance(val, type(self)):\n            self._domain = val\n        elif isinstance(val, (tuple, list)):\n            self._domain = type(self)([val[0], val[1]])\n\n    @property\n    def meta(self):\n        \"\"\"Meta data associated with IntervalArray.\"\"\"\n        if self._meta is None:\n            logging.warning(\"meta data is not available\")\n        return self._meta\n\n    @meta.setter\n    def meta(self, val):\n        self._meta = val\n\n    @property\n    def min(self):\n        \"\"\"Minimum bound of all intervals in IntervalArray.\"\"\"\n        return self.merge().start\n\n    @property\n    def max(self):\n        \"\"\"Maximum bound of all intervals in IntervalArray.\"\"\"\n        return self.merge().stop\n\n    @property\n    def data(self):\n        \"\"\"Interval values [start, stop) in base units.\"\"\"\n        return self._data\n\n    @property\n    def is_finite(self):\n        \"\"\"Is the interval [start, stop) finite.\"\"\"\n        return not (np.isinf(self.start) | np.isinf(self.stop))\n\n    # @property\n    # def _human_readable_posix_intervals(self):\n    #     \"\"\"Interval start and stop values in human readable POSIX time.\n\n    #     This property is left private, because it has not been carefully\n    #     vetted for public API release yet.\n    #     \"\"\"\n    #     import datetime\n    #     n_intervals_zfill = len(str(self.n_intervals))\n    #     for ii, (start, stop) in enumerate(self.time):\n    #         print('[ep ' + str(ii).zfill(n_intervals_zfill) + ']\\t' +\n    #               datetime.datetime.fromtimestamp(\n    #                 int(start)).strftime('%Y-%m-%d %H:%M:%S') + ' -- ' +\n    #               datetime.datetime.fromtimestamp(\n    #                 int(stop)).strftime('%Y-%m-%d %H:%M:%S') + '\\t(' +\n    #               str(utils.PrettyDuration(stop-start)) + ')')\n\n    @property\n    def centers(self):\n        \"\"\"(np.array) The center of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return np.mean(self.data, axis=1)\n\n    @property\n    def lengths(self):\n        \"\"\"(np.array) The length of each interval.\"\"\"\n        if self.isempty:\n            return 0\n        return self.data[:, 1] - self.data[:, 0]\n\n    @property\n    def range(self):\n        \"\"\"return IntervalArray containing range of current IntervalArray.\"\"\"\n        return type(self)([self.start, self.stop])\n\n    @property\n    def length(self):\n        \"\"\"(float) The total length of the [merged] interval array.\"\"\"\n        if self.isempty:\n            return self.formatter(0)\n        merged = self.merge()\n        return self.formatter(np.array(merged.data[:, 1] - merged.data[:, 0]).sum())\n\n    @property\n    def starts(self):\n        \"\"\"(np.array) The start of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 0]\n\n    @property\n    def start(self):\n        \"\"\"(np.array) The start of the first interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 0][0]\n\n    @property\n    def stops(self):\n        \"\"\"(np.array) The stop of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 1]\n\n    @property\n    def stop(self):\n        \"\"\"(np.array) The stop of the last interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 1][-1]\n\n    @property\n    def n_intervals(self):\n        \"\"\"(int) The number of intervals.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self.data[:, 0]))\n\n    def __len__(self):\n        \"\"\"(int) The number of intervals.\"\"\"\n        return self.n_intervals\n\n    @property\n    def ismerged(self):\n        \"\"\"(bool) No overlapping intervals exist.\"\"\"\n        if self.isempty:\n            return True\n        if self.n_intervals == 1:\n            return True\n        if not self.issorted:\n            self._sort()\n        if not utils.is_sorted(self.stops):\n            return False\n\n        return np.all(self.data[1:, 0] - self.data[:-1, 1] &gt; 0)\n\n    def _ismerged(self, overlap=0.0):\n        \"\"\"(bool) No overlapping intervals with overlap &gt;= overlap exist.\"\"\"\n        if self.isempty:\n            return True\n        if self.n_intervals == 1:\n            return True\n        if not self.issorted:\n            self._sort()\n        if not utils.is_sorted(self.stops):\n            return False\n\n        return np.all(self.data[1:, 0] - self.data[:-1, 1] &gt; -overlap)\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Left edges of intervals are sorted in ascending order.\"\"\"\n        if self.isempty:\n            return True\n        return utils.is_sorted(self.starts)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty IntervalArray.\"\"\"\n        try:\n            return len(self.data) == 0\n        except TypeError:\n            return True  # this happens when self.data is None\n\n    def copy(self):\n        \"\"\"(IntervalArray) Returns a copy of the current interval array.\"\"\"\n        newcopy = copy.deepcopy(self)\n        return newcopy\n\n    def _drop_empty_intervals(self):\n        \"\"\"Drops empty intervals. Not in-place, i.e. returns a copy.\"\"\"\n        keep_interval_ids = np.argwhere(self.lengths).squeeze().tolist()\n        return self[keep_interval_ids]\n\n    def intersect(self, interval, *, boundaries=True):\n        \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n        other interval array ('interval').\n        \"\"\"\n\n        if self.isempty or interval.isempty:\n            logging.warning(\"interval intersection is empty\")\n            return type(self)(empty=True)\n\n        new_intervals = []\n\n        # Extract starts and stops and convert to np.array of float64 (for numba)\n        interval_starts_a = np.array(self.starts, dtype=np.float64)\n        interval_stops_a = np.array(self.stops, dtype=np.float64)\n        if interval.data.ndim == 1:\n            interval_starts_b = np.array([interval.data[0]], dtype=np.float64)\n            interval_stops_b = np.array([interval.data[1]], dtype=np.float64)\n        else:\n            interval_starts_b = np.array(interval.data[:, 0], dtype=np.float64)\n            interval_stops_b = np.array(interval.data[:, 1], dtype=np.float64)\n\n        new_starts, new_stops = interval_intersect(\n            interval_starts_a,\n            interval_stops_a,\n            interval_starts_b,\n            interval_stops_b,\n            boundaries,\n        )\n\n        for start, stop in zip(new_starts, new_stops):\n            new_intervals.append([start, stop])\n\n        # convert to np.array of float64\n        new_intervals = np.array(new_intervals, dtype=np.float64)\n\n        out = type(self)(new_intervals)\n        out._domain = self.domain\n        return out\n\n    # def intersect(self, interval, *, boundaries=True):\n    #     \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n    #        other interval array ('interval').\n    #     \"\"\"\n\n    #     this = copy.deepcopy(self)\n    #     new_intervals = []\n    #     for epa in this:\n    #         cand_ep_idx = np.argwhere((interval.starts &lt; epa.stop) &amp; (interval.stops &gt; epa.start)).squeeze()\n    #         if np.size(cand_ep_idx) &gt; 0:\n    #             for epb in interval[cand_ep_idx.tolist()]:\n    #                 new_interval = self._intersect(epa, epb, boundaries=boundaries)\n    #                 if not new_interval.isempty:\n    #                     new_intervals.append([new_interval.start, new_interval.stop])\n    #     out = type(self)(new_intervals)\n    #     out._domain = self.domain\n    #     return out\n\n    # def _intersect(self, intervala, intervalb, *, boundaries=True, meta=None):\n    #     \"\"\"Finds intersection (overlap) between two sets of interval arrays.\n\n    #     TODO: verify if this requires a merged IntervalArray to work properly?\n    #     ISSUE_261: not fixed yet\n\n    #     TODO: domains are not preserved yet! careful consideration is necessary.\n\n    #     Parameters\n    #     ----------\n    #     interval : nelpy.IntervalArray\n    #     boundaries : bool\n    #         If True, limits start, stop to interval start and stop.\n    #     meta : dict, optional\n    #         New dictionary of meta data for interval ontersection.\n\n    #     Returns\n    #     -------\n    #     intersect_intervals : nelpy.IntervalArray\n    #     \"\"\"\n    #     if intervala.isempty or intervalb.isempty:\n    #         logging.warning('interval intersection is empty')\n    #         return type(self)(empty=True)\n\n    #     new_starts = []\n    #     new_stops = []\n    #     interval_a = intervala.merge().copy()\n    #     interval_b = intervalb.merge().copy()\n\n    #     for aa in interval_a.data:\n    #         for bb in interval_b.data:\n    #             if (aa[0] &lt;= bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &lt;= aa[1]):\n    #                 new_starts.append(bb[0])\n    #                 new_stops.append(bb[1])\n    #             elif (aa[0] &lt; bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &gt; aa[1]):\n    #                 new_starts.append(bb[0])\n    #                 if boundaries:\n    #                     new_stops.append(aa[1])\n    #                 else:\n    #                     new_stops.append(bb[1])\n    #             elif (aa[0] &gt; bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &lt; aa[1]):\n    #                 if boundaries:\n    #                     new_starts.append(aa[0])\n    #                 else:\n    #                     new_starts.append(bb[0])\n    #                 new_stops.append(bb[1])\n    #             elif (aa[0] &gt;= bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &gt;= aa[1]):\n    #                 if boundaries:\n    #                     new_starts.append(aa[0])\n    #                     new_stops.append(aa[1])\n    #                 else:\n    #                     new_starts.append(bb[0])\n    #                     new_stops.append(bb[1])\n\n    #     if not boundaries:\n    #         new_starts = np.unique(new_starts)\n    #         new_stops = np.unique(new_stops)\n\n    #     interval_a._data = np.hstack(\n    #         [np.array(new_starts)[..., np.newaxis],\n    #             np.array(new_stops)[..., np.newaxis]])\n\n    #     return interval_a\n\n    def merge(self, *, gap=0.0, overlap=0.0):\n        \"\"\"Merge intervals that are close or overlapping.\n\n        if gap == 0 and overlap == 0:\n            [a, b) U [b, c) = [a, c)\n        if gap == None and overlap &gt; 0:\n            [a, b) U [b, c) = [a, b) U [b, c)\n            [a, b + overlap) U [b, c) = [a, c)\n            [a, b) U [b - overlap, c) = [a, c)\n        if gap &gt; 0 and overlap == None:\n            [a, b) U [b, c) = [a, c)\n            [a, b) U [b + gap, c) = [a, c)\n            [a, b - gap) U [b, c) = [a, c)\n\n        WARNING! Algorithm only works on SORTED intervals.\n\n        Parameters\n        ----------\n        gap : float, optional\n            Amount (in base units) to consider intervals close enough to merge.\n            Defaults to 0.0 (no gap).\n        Returns\n        -------\n        merged_intervals : nelpy.IntervalArray\n        \"\"\"\n\n        if gap &lt; 0:\n            raise ValueError(\"gap cannot be negative\")\n        if overlap &lt; 0:\n            raise ValueError(\"overlap cannot be negative\")\n\n        if self.isempty:\n            return self\n\n        if (self.ismerged) and (gap == 0.0):\n            # already merged\n            return self\n\n        newintervalarray = copy.copy(self)\n\n        if not newintervalarray.issorted:\n            newintervalarray._sort()\n\n        overlap_ = overlap\n\n        while not newintervalarray._ismerged(overlap=overlap) or gap &gt; 0:\n            stops = newintervalarray.stops[:-1] + gap\n            starts = newintervalarray.starts[1:] + overlap_\n            to_merge = (stops - starts) &gt;= 0\n\n            new_starts = [newintervalarray.starts[0]]\n            new_stops = []\n\n            next_stop = newintervalarray.stops[0]\n            for i in range(newintervalarray.data.shape[0] - 1):\n                this_stop = newintervalarray.stops[i]\n                next_stop = max(next_stop, this_stop)\n                if not to_merge[i]:\n                    new_stops.append(next_stop)\n                    new_starts.append(newintervalarray.starts[i + 1])\n\n            new_stops.append(max(newintervalarray.stops[-1], next_stop))\n\n            new_starts = np.array(new_starts)\n            new_stops = np.array(new_stops)\n\n            newintervalarray._data = np.vstack([new_starts, new_stops]).T\n\n            # after one pass, all the gap offsets have been added, and\n            # then we just need to keep merging...\n            gap = 0.0\n            overlap_ = 0.0\n\n        return newintervalarray\n\n    def expand(self, amount, direction=\"both\"):\n        \"\"\"Expands interval by the given amount.\n        Parameters\n        ----------\n        amount : float\n            Amount (in base units) to expand each interval.\n        direction : str\n            Can be 'both', 'start', or 'stop'. This specifies\n            which direction to resize interval.\n        Returns\n        -------\n        expanded_intervals : nelpy.IntervalArray\n        \"\"\"\n        if direction == \"both\":\n            resize_starts = self.data[:, 0] - amount\n            resize_stops = self.data[:, 1] + amount\n        elif direction == \"start\":\n            resize_starts = self.data[:, 0] - amount\n            resize_stops = self.data[:, 1]\n        elif direction == \"stop\":\n            resize_starts = self.data[:, 0]\n            resize_stops = self.data[:, 1] + amount\n        else:\n            raise ValueError(\"direction must be 'both', 'start', or 'stop'\")\n\n        newintervalarray = copy.copy(self)\n\n        newintervalarray._data = np.hstack(\n            (resize_starts[..., np.newaxis], resize_stops[..., np.newaxis])\n        )\n\n        return newintervalarray\n\n    def shrink(self, amount, direction=\"both\"):\n        \"\"\"Shrinks interval by the given amount.\n        Parameters\n        ----------\n        amount : float\n            Amount (in base units) to shrink each interval.\n        direction : str\n            Can be 'both', 'start', or 'stop'. This specifies\n            which direction to resize interval.\n        Returns\n        -------\n        shrinked_intervals : nelpy.IntervalArray\n        \"\"\"\n        both_limit = min(self.lengths / 2)\n        if amount &gt; both_limit and direction == \"both\":\n            raise ValueError(\"shrink amount too large\")\n\n        single_limit = min(self.lengths)\n        if amount &gt; single_limit and direction != \"both\":\n            raise ValueError(\"shrink amount too large\")\n\n        return self.expand(-amount, direction)\n\n    def join(self, interval, meta=None):\n        \"\"\"Combines [and merges] two sets of intervals. Intervals can have\n        different sampling rates.\n\n        Parameters\n        ----------\n        interval : nelpy.IntervalArray\n        meta : dict, optional\n            New meta data dictionary describing the joined intervals.\n\n        Returns\n        -------\n        joined_intervals : nelpy.IntervalArray\n        \"\"\"\n\n        if self.isempty:\n            return interval\n        if interval.isempty:\n            return self\n\n        newintervalarray = copy.copy(self)\n\n        join_starts = np.concatenate((self.data[:, 0], interval.data[:, 0]))\n        join_stops = np.concatenate((self.data[:, 1], interval.data[:, 1]))\n\n        newintervalarray._data = np.hstack(\n            (join_starts[..., np.newaxis], join_stops[..., np.newaxis])\n        )\n        if not newintervalarray.issorted:\n            newintervalarray._sort()\n        # if not newintervalarray.ismerged:\n        #     newintervalarray = newintervalarray.merge()\n        return newintervalarray\n\n    def __contains__(self, value):\n        \"\"\"Checks whether value is in any interval.\n\n        #TODO: add support for when value is an IntervalArray (could be easy with intersection)\n\n        Parameters\n        ----------\n        intervals: nelpy.IntervalArray\n        value: float or int\n\n        Returns\n        -------\n        boolean\n\n        \"\"\"\n        # TODO: consider vectorizing this loop, which should increase\n        # speed, but also greatly increase memory? Alternatively, if we\n        # could assume something about intervals being sorted, this can\n        # also be made much faster than the current O(N)\n        for start, stop in zip(self.starts, self.stops):\n            if start &lt;= value &lt;= stop:\n                return True\n        return False\n\n    def _sort(self):\n        \"\"\"Sort intervals by interval starts\"\"\"\n        sort_idx = np.argsort(self.data[:, 0])\n        self._data = self._data[sort_idx]\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.centers","title":"<code>centers</code>  <code>property</code>","text":"<p>(np.array) The center of each interval.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Interval values [start, stop) in base units.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>domain (in base units) within which support is defined</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.is_finite","title":"<code>is_finite</code>  <code>property</code>","text":"<p>Is the interval [start, stop) finite.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty IntervalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.ismerged","title":"<code>ismerged</code>  <code>property</code>","text":"<p>(bool) No overlapping intervals exist.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Left edges of intervals are sorted in ascending order.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label describing the interval array.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.length","title":"<code>length</code>  <code>property</code>","text":"<p>(float) The total length of the [merged] interval array.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>(np.array) The length of each interval.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.max","title":"<code>max</code>  <code>property</code>","text":"<p>Maximum bound of all intervals in IntervalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.meta","title":"<code>meta</code>  <code>property</code> <code>writable</code>","text":"<p>Meta data associated with IntervalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.min","title":"<code>min</code>  <code>property</code>","text":"<p>Minimum bound of all intervals in IntervalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.n_intervals","title":"<code>n_intervals</code>  <code>property</code>","text":"<p>(int) The number of intervals.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.range","title":"<code>range</code>  <code>property</code>","text":"<p>return IntervalArray containing range of current IntervalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.start","title":"<code>start</code>  <code>property</code>","text":"<p>(np.array) The start of the first interval.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>(np.array) The start of each interval.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.stop","title":"<code>stop</code>  <code>property</code>","text":"<p>(np.array) The stop of the last interval.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.stops","title":"<code>stops</code>  <code>property</code>","text":"<p>(np.array) The stop of each interval.</p>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.complement","title":"<code>complement(domain=None)</code>","text":"<p>Complement within domain.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>IntervalArray</code> <p>IntervalArray specifying entire domain. Default is self.domain.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>complement</code> <code>IntervalArray</code> <p>IntervalArray containing all the nonzero intervals in the complement set.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def complement(self, domain=None):\n    \"\"\"Complement within domain.\n\n    Parameters\n    ----------\n    domain : IntervalArray, optional\n        IntervalArray specifying entire domain. Default is self.domain.\n\n    Returns\n    -------\n    complement : IntervalArray\n        IntervalArray containing all the nonzero intervals in the\n        complement set.\n    \"\"\"\n\n    if domain is None:\n        domain = self.domain\n\n    # make sure IntervalArray is sorted:\n    if not self.issorted:\n        self._sort()\n    # check that IntervalArray is entirely contained within domain\n    # if (self.start &lt; domain.start) or (self.stop &gt; domain.stop):\n    #     raise ValueError(\"IntervalArray must be entirely contained within domain\")\n\n    # check that IntervalArray is fully merged, or merge it if necessary\n    merged = self.merge()\n    # build complement intervals\n    starts = np.insert(merged.stops, 0, domain.start)\n    stops = np.append(merged.starts, domain.stop)\n    newvalues = np.vstack([starts, stops]).T\n    # remove intervals with zero length\n    lengths = newvalues[:, 1] - newvalues[:, 0]\n    newvalues = newvalues[lengths &gt; 0]\n    complement = copy.copy(self)\n    complement._data = newvalues\n\n    if domain.n_intervals &gt; 1:\n        return complement[domain]\n    try:\n        complement._data[0, 0] = np.max((complement._data[0, 0], domain.start))\n        complement._data[-1, -1] = np.min((complement._data[-1, -1], domain.stop))\n    except IndexError:  # complement is empty\n        return type(self)(empty=True)\n    return complement\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.copy","title":"<code>copy()</code>","text":"<p>(IntervalArray) Returns a copy of the current interval array.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def copy(self):\n    \"\"\"(IntervalArray) Returns a copy of the current interval array.\"\"\"\n    newcopy = copy.deepcopy(self)\n    return newcopy\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.expand","title":"<code>expand(amount, direction='both')</code>","text":"<p>Expands interval by the given amount.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>float</code> <p>Amount (in base units) to expand each interval.</p> required <code>direction</code> <code>str</code> <p>Can be 'both', 'start', or 'stop'. This specifies which direction to resize interval.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>expanded_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def expand(self, amount, direction=\"both\"):\n    \"\"\"Expands interval by the given amount.\n    Parameters\n    ----------\n    amount : float\n        Amount (in base units) to expand each interval.\n    direction : str\n        Can be 'both', 'start', or 'stop'. This specifies\n        which direction to resize interval.\n    Returns\n    -------\n    expanded_intervals : nelpy.IntervalArray\n    \"\"\"\n    if direction == \"both\":\n        resize_starts = self.data[:, 0] - amount\n        resize_stops = self.data[:, 1] + amount\n    elif direction == \"start\":\n        resize_starts = self.data[:, 0] - amount\n        resize_stops = self.data[:, 1]\n    elif direction == \"stop\":\n        resize_starts = self.data[:, 0]\n        resize_stops = self.data[:, 1] + amount\n    else:\n        raise ValueError(\"direction must be 'both', 'start', or 'stop'\")\n\n    newintervalarray = copy.copy(self)\n\n    newintervalarray._data = np.hstack(\n        (resize_starts[..., np.newaxis], resize_stops[..., np.newaxis])\n    )\n\n    return newintervalarray\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.intersect","title":"<code>intersect(interval, *, boundaries=True)</code>","text":"<p>Returns intersection (overlap) between current IntervalArray (self) and other interval array ('interval').</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def intersect(self, interval, *, boundaries=True):\n    \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n    other interval array ('interval').\n    \"\"\"\n\n    if self.isempty or interval.isempty:\n        logging.warning(\"interval intersection is empty\")\n        return type(self)(empty=True)\n\n    new_intervals = []\n\n    # Extract starts and stops and convert to np.array of float64 (for numba)\n    interval_starts_a = np.array(self.starts, dtype=np.float64)\n    interval_stops_a = np.array(self.stops, dtype=np.float64)\n    if interval.data.ndim == 1:\n        interval_starts_b = np.array([interval.data[0]], dtype=np.float64)\n        interval_stops_b = np.array([interval.data[1]], dtype=np.float64)\n    else:\n        interval_starts_b = np.array(interval.data[:, 0], dtype=np.float64)\n        interval_stops_b = np.array(interval.data[:, 1], dtype=np.float64)\n\n    new_starts, new_stops = interval_intersect(\n        interval_starts_a,\n        interval_stops_a,\n        interval_starts_b,\n        interval_stops_b,\n        boundaries,\n    )\n\n    for start, stop in zip(new_starts, new_stops):\n        new_intervals.append([start, stop])\n\n    # convert to np.array of float64\n    new_intervals = np.array(new_intervals, dtype=np.float64)\n\n    out = type(self)(new_intervals)\n    out._domain = self.domain\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.join","title":"<code>join(interval, meta=None)</code>","text":"<p>Combines [and merges] two sets of intervals. Intervals can have different sampling rates.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>IntervalArray</code> required <code>meta</code> <code>dict</code> <p>New meta data dictionary describing the joined intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>joined_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def join(self, interval, meta=None):\n    \"\"\"Combines [and merges] two sets of intervals. Intervals can have\n    different sampling rates.\n\n    Parameters\n    ----------\n    interval : nelpy.IntervalArray\n    meta : dict, optional\n        New meta data dictionary describing the joined intervals.\n\n    Returns\n    -------\n    joined_intervals : nelpy.IntervalArray\n    \"\"\"\n\n    if self.isempty:\n        return interval\n    if interval.isempty:\n        return self\n\n    newintervalarray = copy.copy(self)\n\n    join_starts = np.concatenate((self.data[:, 0], interval.data[:, 0]))\n    join_stops = np.concatenate((self.data[:, 1], interval.data[:, 1]))\n\n    newintervalarray._data = np.hstack(\n        (join_starts[..., np.newaxis], join_stops[..., np.newaxis])\n    )\n    if not newintervalarray.issorted:\n        newintervalarray._sort()\n    # if not newintervalarray.ismerged:\n    #     newintervalarray = newintervalarray.merge()\n    return newintervalarray\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.merge","title":"<code>merge(*, gap=0.0, overlap=0.0)</code>","text":"<p>Merge intervals that are close or overlapping.</p> <p>if gap == 0 and overlap == 0:     [a, b) U [b, c) = [a, c) if gap == None and overlap &gt; 0:     [a, b) U [b, c) = [a, b) U [b, c)     [a, b + overlap) U [b, c) = [a, c)     [a, b) U [b - overlap, c) = [a, c) if gap &gt; 0 and overlap == None:     [a, b) U [b, c) = [a, c)     [a, b) U [b + gap, c) = [a, c)     [a, b - gap) U [b, c) = [a, c)</p> <p>WARNING! Algorithm only works on SORTED intervals.</p> <p>Parameters:</p> Name Type Description Default <code>gap</code> <code>float</code> <p>Amount (in base units) to consider intervals close enough to merge. Defaults to 0.0 (no gap).</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>merged_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def merge(self, *, gap=0.0, overlap=0.0):\n    \"\"\"Merge intervals that are close or overlapping.\n\n    if gap == 0 and overlap == 0:\n        [a, b) U [b, c) = [a, c)\n    if gap == None and overlap &gt; 0:\n        [a, b) U [b, c) = [a, b) U [b, c)\n        [a, b + overlap) U [b, c) = [a, c)\n        [a, b) U [b - overlap, c) = [a, c)\n    if gap &gt; 0 and overlap == None:\n        [a, b) U [b, c) = [a, c)\n        [a, b) U [b + gap, c) = [a, c)\n        [a, b - gap) U [b, c) = [a, c)\n\n    WARNING! Algorithm only works on SORTED intervals.\n\n    Parameters\n    ----------\n    gap : float, optional\n        Amount (in base units) to consider intervals close enough to merge.\n        Defaults to 0.0 (no gap).\n    Returns\n    -------\n    merged_intervals : nelpy.IntervalArray\n    \"\"\"\n\n    if gap &lt; 0:\n        raise ValueError(\"gap cannot be negative\")\n    if overlap &lt; 0:\n        raise ValueError(\"overlap cannot be negative\")\n\n    if self.isempty:\n        return self\n\n    if (self.ismerged) and (gap == 0.0):\n        # already merged\n        return self\n\n    newintervalarray = copy.copy(self)\n\n    if not newintervalarray.issorted:\n        newintervalarray._sort()\n\n    overlap_ = overlap\n\n    while not newintervalarray._ismerged(overlap=overlap) or gap &gt; 0:\n        stops = newintervalarray.stops[:-1] + gap\n        starts = newintervalarray.starts[1:] + overlap_\n        to_merge = (stops - starts) &gt;= 0\n\n        new_starts = [newintervalarray.starts[0]]\n        new_stops = []\n\n        next_stop = newintervalarray.stops[0]\n        for i in range(newintervalarray.data.shape[0] - 1):\n            this_stop = newintervalarray.stops[i]\n            next_stop = max(next_stop, this_stop)\n            if not to_merge[i]:\n                new_stops.append(next_stop)\n                new_starts.append(newintervalarray.starts[i + 1])\n\n        new_stops.append(max(newintervalarray.stops[-1], next_stop))\n\n        new_starts = np.array(new_starts)\n        new_stops = np.array(new_stops)\n\n        newintervalarray._data = np.vstack([new_starts, new_stops]).T\n\n        # after one pass, all the gap offsets have been added, and\n        # then we just need to keep merging...\n        gap = 0.0\n        overlap_ = 0.0\n\n    return newintervalarray\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.partition","title":"<code>partition(*, ds=None, n_intervals=None)</code>","text":"<p>Returns an IntervalArray that has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum length, for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalArray</code> <p>IntervalArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, *, ds=None, n_intervals=None):\n    \"\"\"Returns an IntervalArray that has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum length, for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : IntervalArray\n        IntervalArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    if self.isempty:\n        raise ValueError(\"cannot parition an empty object in a meaningful way!\")\n\n    if ds is not None and n_intervals is not None:\n        raise ValueError(\"ds and n_intervals cannot be used together\")\n\n    if n_intervals is not None:\n        assert float(n_intervals).is_integer(), (\n            \"n_intervals must be a positive integer!\"\n        )\n        assert n_intervals &gt; 1, \"n_intervals must be a positive integer &gt; 1\"\n        # determine ds from number of desired points:\n        ds = self.length / n_intervals\n\n    if ds is None:\n        # neither n_intervals nor ds was specified, so assume defaults:\n        n_intervals = 100\n        ds = self.length / n_intervals\n\n    # build list of points at which to esplit the IntervalArray\n    new_starts = []\n    new_stops = []\n    for start, stop in self.data:\n        newxvals = utils.frange(start, stop, step=ds).tolist()\n        # newxvals = np.arange(start, stop, step=ds).tolist()\n        if newxvals[-1] + float_info.epsilon &lt; stop:\n            newxvals.append(stop)\n        newxvals = np.asanyarray(newxvals)\n        new_starts.extend(newxvals[:-1])\n        new_stops.extend(newxvals[1:])\n\n    # now make a new interval array:\n    out = copy.copy(self)\n    out._data = np.hstack(\n        [\n            np.array(new_starts)[..., np.newaxis],\n            np.array(new_stops)[..., np.newaxis],\n        ]\n    )\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.remove_duplicates","title":"<code>remove_duplicates(inplace=False)</code>","text":"<p>Remove duplicate intervals.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def remove_duplicates(self, inplace=False):\n    \"\"\"Remove duplicate intervals.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.IntervalArray.shrink","title":"<code>shrink(amount, direction='both')</code>","text":"<p>Shrinks interval by the given amount.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>float</code> <p>Amount (in base units) to shrink each interval.</p> required <code>direction</code> <code>str</code> <p>Can be 'both', 'start', or 'stop'. This specifies which direction to resize interval.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>shrinked_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def shrink(self, amount, direction=\"both\"):\n    \"\"\"Shrinks interval by the given amount.\n    Parameters\n    ----------\n    amount : float\n        Amount (in base units) to shrink each interval.\n    direction : str\n        Can be 'both', 'start', or 'stop'. This specifies\n        which direction to resize interval.\n    Returns\n    -------\n    shrinked_intervals : nelpy.IntervalArray\n    \"\"\"\n    both_limit = min(self.lengths / 2)\n    if amount &gt; both_limit and direction == \"both\":\n        raise ValueError(\"shrink amount too large\")\n\n    single_limit = min(self.lengths)\n    if amount &gt; single_limit and direction != \"both\":\n        raise ValueError(\"shrink amount too large\")\n\n    return self.expand(-amount, direction)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.MarkedSpikeTrainArray","title":"<code>MarkedSpikeTrainArray</code>","text":"<p>               Bases: <code>ValueEventArray</code></p> <p>MarkedSpikeTrainArray for storing spike times with associated marks (e.g., waveform features).</p> <p>This class extends ValueEventArray to support marks for each spike event, such as tetrode features or other metadata.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>array - like</code> <p>Spike times or event times.</p> required <code>marks</code> <code>array - like</code> <p>Associated marks/features for each event.</p> required <code>support</code> <code>IntervalArray</code> <p>Support intervals for the spike train.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> required <code>series_label</code> <code>str</code> <p>Label for the series (e.g., 'tetrodes').</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>events</code> <code>array - like</code> <p>Spike times or event times.</p> <code>marks</code> <code>array - like</code> <p>Associated marks/features for each event.</p> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the spike train.</p> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> <code>series_label</code> <code>str</code> <p>Label for the series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; msta = MarkedSpikeTrainArray(events=spike_times, marks=features, fs=30000)\n&gt;&gt;&gt; msta.events\narray([...])\n&gt;&gt;&gt; msta.marks\narray([...])\n</code></pre> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class MarkedSpikeTrainArray(ValueEventArray):\n    \"\"\"\n    MarkedSpikeTrainArray for storing spike times with associated marks (e.g., waveform features).\n\n    This class extends ValueEventArray to support marks for each spike event, such as tetrode features or other metadata.\n\n    Parameters\n    ----------\n    events : array-like\n        Spike times or event times.\n    marks : array-like\n        Associated marks/features for each event.\n    support : nelpy.IntervalArray, optional\n        Support intervals for the spike train.\n    fs : float, optional\n        Sampling frequency in Hz.\n    series_label : str, optional\n        Label for the series (e.g., 'tetrodes').\n    **kwargs :\n        Additional keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    events : array-like\n        Spike times or event times.\n    marks : array-like\n        Associated marks/features for each event.\n    support : nelpy.IntervalArray\n        Support intervals for the spike train.\n    fs : float\n        Sampling frequency in Hz.\n    series_label : str\n        Label for the series.\n\n    Examples\n    --------\n    &gt;&gt;&gt; msta = MarkedSpikeTrainArray(events=spike_times, marks=features, fs=30000)\n    &gt;&gt;&gt; msta.events\n    array([...])\n    &gt;&gt;&gt; msta.marks\n    array([...])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"n_marks\": \"n_values\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n        \"marks\": \"values\",\n        \"spikes\": \"events\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"tetrodes\"\n        kwargs[\"series_label\"] = series_label\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n\n    # @keyword_equivalence(this_or_that={'n_intervals':'n_epochs'})\n    # def partition(self, ds=None, n_intervals=None, n_epochs=None):\n    #     if n_intervals is None:\n    #         n_intervals = n_epochs\n    #     kwargs = {'ds':ds, 'n_intervals': n_intervals}\n    #     return super().partition(**kwargs)\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n        raise NotImplementedError\n        return BinnedMarkedSpikeTrainArray(self, ds=ds)  # noqa: F821\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.MarkedSpikeTrainArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedSpikeTrainArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n    raise NotImplementedError\n    return BinnedMarkedSpikeTrainArray(self, ds=ds)  # noqa: F821\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.MinimalExampleArray","title":"<code>MinimalExampleArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>MinimalExampleArray is a custom example subclass of RegularlySampledAnalogSignalArray.</p> <p>This class demonstrates how to extend RegularlySampledAnalogSignalArray with custom aliases and methods.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>__aliases__</code> <code>dict</code> <p>Dictionary of class-specific aliases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n&gt;&gt;&gt; arr.custom_func()\nWoot! We have some special skillz!\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class MinimalExampleArray(RegularlySampledAnalogSignalArray):\n    \"\"\"\n    MinimalExampleArray is a custom example subclass of RegularlySampledAnalogSignalArray.\n\n    This class demonstrates how to extend RegularlySampledAnalogSignalArray with custom aliases and methods.\n\n    Parameters\n    ----------\n    *args :\n        Positional arguments passed to the parent class.\n    **kwargs :\n        Keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    __aliases__ : dict\n        Dictionary of class-specific aliases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n    &gt;&gt;&gt; arr.custom_func()\n    Woot! We have some special skillz!\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n    def custom_func(self):\n        \"\"\"\n        Print a custom message demonstrating a special method.\n\n        Examples\n        --------\n        &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n        &gt;&gt;&gt; arr.custom_func()\n        Woot! We have some special skillz!\n        \"\"\"\n        print(\"Woot! We have some special skillz!\")\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.MinimalExampleArray.custom_func","title":"<code>custom_func()</code>","text":"<p>Print a custom message demonstrating a special method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n&gt;&gt;&gt; arr.custom_func()\nWoot! We have some special skillz!\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def custom_func(self):\n    \"\"\"\n    Print a custom message demonstrating a special method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n    &gt;&gt;&gt; arr.custom_func()\n    Woot! We have some special skillz!\n    \"\"\"\n    print(\"Woot! We have some special skillz!\")\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.Ordinate","title":"<code>Ordinate</code>","text":"<p>An ordinate (y-axis) object for core nelpy data containers.</p> <p>Parameters:</p> Name Type Description Default <code>base_unit</code> <code>str</code> <p>The base unit for the ordinate. Default is ''.</p> <code>None</code> <code>is_linking</code> <code>bool</code> <p>Whether the ordinate is linking. Default is False.</p> <code>False</code> <code>is_wrapping</code> <code>bool</code> <p>Whether the ordinate is wrapping. Default is False.</p> <code>False</code> <code>labelstring</code> <code>str</code> <p>String template for the ordinate label. Default is '{}'.</p> <code>None</code> <code>_range</code> <code>IntervalArray</code> <p>The range of the ordinate. Default is [-inf, inf].</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>base_unit</code> <code>str</code> <p>The base unit for the ordinate.</p> <code>is_linking</code> <code>bool</code> <p>Whether the ordinate is linking.</p> <code>is_wrapping</code> <code>bool</code> <p>Whether the ordinate is wrapping.</p> <code>label</code> <code>str</code> <p>The formatted label for the ordinate.</p> <code>range</code> <code>IntervalArray</code> <p>The range of the ordinate.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class Ordinate:\n    \"\"\"\n    An ordinate (y-axis) object for core nelpy data containers.\n\n    Parameters\n    ----------\n    base_unit : str, optional\n        The base unit for the ordinate. Default is ''.\n    is_linking : bool, optional\n        Whether the ordinate is linking. Default is False.\n    is_wrapping : bool, optional\n        Whether the ordinate is wrapping. Default is False.\n    labelstring : str, optional\n        String template for the ordinate label. Default is '{}'.\n    _range : nelpy.IntervalArray, optional\n        The range of the ordinate. Default is [-inf, inf].\n\n    Attributes\n    ----------\n    base_unit : str\n        The base unit for the ordinate.\n    is_linking : bool\n        Whether the ordinate is linking.\n    is_wrapping : bool\n        Whether the ordinate is wrapping.\n    label : str\n        The formatted label for the ordinate.\n    range : nelpy.IntervalArray\n        The range of the ordinate.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_unit=None,\n        is_linking=False,\n        is_wrapping=False,\n        labelstring=None,\n        _range=None,\n    ):\n        # TODO: add label support\n\n        if base_unit is None:\n            base_unit = \"\"\n        if labelstring is None:\n            labelstring = \"{}\"\n\n        if _range is None:\n            _range = core.IntervalArray([-inf, inf])\n\n        self.base_unit = base_unit\n        self._labelstring = labelstring\n        self.is_linking = is_linking\n        self.is_wrapping = is_wrapping\n        self._is_wrapped = None  # intialize to unknown (None) state\n        self._range = _range\n\n    @property\n    def label(self):\n        \"\"\"\n        Get the ordinate label.\n\n        Returns\n        -------\n        label : str\n            The formatted ordinate label.\n        \"\"\"\n        return self._labelstring.format(self.base_unit)\n\n    @label.setter\n    def label(self, val):\n        \"\"\"\n        Set the ordinate label string template.\n\n        Parameters\n        ----------\n        val : str\n            String template for the ordinate label.\n        \"\"\"\n        if val is None:\n            val = \"{}\"\n        try:  # cast to str:\n            labelstring = str(val)\n        except TypeError:\n            raise TypeError(\"cannot convert label to string\")\n        else:\n            labelstring = val\n        self._labelstring = labelstring\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the Ordinate object.\n\n        Returns\n        -------\n        repr_str : str\n            String representation of the Ordinate.\n        \"\"\"\n        return \"Ordinate(base_unit={}, is_linking={}, is_wrapping={})\".format(\n            self.base_unit, self.is_linking, self.is_wrapping\n        )\n\n    @property\n    def range(self):\n        \"\"\"\n        Get the range (in ordinate base units) on which ordinate is defined.\n\n        Returns\n        -------\n        range : nelpy.IntervalArray\n            The range of the ordinate.\n        \"\"\"\n        return self._range\n\n    @range.setter\n    def range(self, val):\n        \"\"\"Range (in ordinate base units) on which ordinate is defined.\"\"\"\n        # val can be an IntervalArray type, or (start, stop)\n        if isinstance(val, type(self.range)):\n            self._range = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self.range.domain\n            self._range = type(self.range)([val[0], val[1]])\n            self._range.domain = prev_domain\n        else:\n            raise TypeError(\"range must be of type {}\".format(str(type(self.range))))\n\n        self._range = self.range[self.range.domain]\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.Ordinate.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Get the ordinate label.</p> <p>Returns:</p> Name Type Description <code>label</code> <code>str</code> <p>The formatted ordinate label.</p>"},{"location":"reference/nelpy/all/#nelpy.all.Ordinate.range","title":"<code>range</code>  <code>property</code> <code>writable</code>","text":"<p>Get the range (in ordinate base units) on which ordinate is defined.</p> <p>Returns:</p> Name Type Description <code>range</code> <code>IntervalArray</code> <p>The range of the ordinate.</p>"},{"location":"reference/nelpy/all/#nelpy.all.PositionArray","title":"<code>PositionArray</code>","text":"<p>               Bases: <code>AnalogSignalArray</code></p> <p>An array for storing position data in 1D or 2D space.</p> <p>PositionArray is a specialized subclass of AnalogSignalArray designed to handle position tracking data. It provides convenient access to x and y coordinates, supports both 1D and 2D positional data, and includes spatial boundary information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like or None</code> <p>Position data with shape (n_signals, n_samples). For 1D position data, n_signals should be 1. For 2D position data, n_signals should be 2, where the first row contains x-coordinates and the second row contains y-coordinates. Can also be specified using the alias 'posdata'.</p> required <code>timestamps</code> <code>array_like or None</code> <p>Time stamps corresponding to each sample in data. If None, timestamps are automatically generated based on fs and start time.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Used to generate timestamps if not provided.</p> required <code>support</code> <code>EpochArray or None</code> <p>EpochArray defining the time intervals over which the position data is valid.</p> required <code>label</code> <code>str</code> <p>Descriptive label for the position array.</p> required <code>xlim</code> <code>tuple or None</code> <p>Spatial boundaries for x-coordinate as (min_x, max_x).</p> required <code>ylim</code> <code>tuple or None</code> <p>Spatial boundaries for y-coordinate as (min_y, max_y).</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>X-coordinates as a 1D numpy array.</p> <code>y</code> <code>ndarray</code> <p>Y-coordinates as a 1D numpy array (only available for 2D data).</p> <code>is_1d</code> <code>bool</code> <p>True if position data is 1-dimensional.</p> <code>is_2d</code> <code>bool</code> <p>True if position data is 2-dimensional.</p> <code>xlim</code> <code>tuple or None</code> <p>Spatial boundaries for x-coordinate (only for 2D data).</p> <code>ylim</code> <code>tuple or None</code> <p>Spatial boundaries for y-coordinate (only for 2D data).</p> <p>Examples:</p> <p>Create a 1D position array:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; # 1D position data (e.g., position on a linear track)\n&gt;&gt;&gt; x_pos = np.linspace(0, 100, 1000)  # 100 cm track\n&gt;&gt;&gt; timestamps = np.linspace(0, 10, 1000)  # 10 seconds\n&gt;&gt;&gt; pos_1d = nel.PositionArray(\n...     data=x_pos[np.newaxis, :],\n...     timestamps=timestamps,\n...     label=\"Linear track position\",\n... )\n&gt;&gt;&gt; print(f\"1D position: {pos_1d.is_1d}\")\n&gt;&gt;&gt; print(f\"X range: {pos_1d.x.min():.1f} to {pos_1d.x.max():.1f} cm\")\n</code></pre> <p>Create a 2D position array:</p> <pre><code>&gt;&gt;&gt; # 2D position data (e.g., open field behavior)\n&gt;&gt;&gt; t = np.linspace(0, 2 * np.pi, 1000)\n&gt;&gt;&gt; x_pos = 50 + 30 * np.cos(t)  # circular trajectory\n&gt;&gt;&gt; y_pos = 50 + 30 * np.sin(t)\n&gt;&gt;&gt; pos_data = np.vstack([x_pos, y_pos])\n&gt;&gt;&gt; pos_2d = nel.PositionArray(\n...     posdata=pos_data,\n...     fs=100,  # 100 Hz sampling\n...     xlim=(0, 100),\n...     ylim=(0, 100),\n...     label=\"Open field position\",\n... )\n&gt;&gt;&gt; print(f\"2D position: {pos_2d.is_2d}\")\n&gt;&gt;&gt; print(f\"X position shape: {pos_2d.x.shape}\")\n&gt;&gt;&gt; print(f\"Y position shape: {pos_2d.y.shape}\")\n&gt;&gt;&gt; print(f\"Spatial bounds: x={pos_2d.xlim}, y={pos_2d.ylim}\")\n</code></pre> <p>Access position data:</p> <pre><code>&gt;&gt;&gt; # Get position at specific time\n&gt;&gt;&gt; time_idx = 500\n&gt;&gt;&gt; if pos_2d.is_2d:\n...     x_at_time = pos_2d.x[time_idx]\n...     y_at_time = pos_2d.y[time_idx]\n...     print(f\"Position at sample {time_idx}: ({x_at_time:.1f}, {y_at_time:.1f})\")\n</code></pre> Notes <ul> <li>For 2D position data, the first row of data should contain x-coordinates   and the second row should contain y-coordinates.</li> <li>The xlim and ylim parameters are only meaningful for 2D position data.</li> <li>Attempting to access y-coordinates or spatial limits on 1D data will   raise a ValueError.</li> <li>The 'posdata' alias can be used interchangeably with 'data' parameter.</li> </ul> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class PositionArray(AnalogSignalArray):\n    \"\"\"An array for storing position data in 1D or 2D space.\n\n    PositionArray is a specialized subclass of AnalogSignalArray designed to\n    handle position tracking data. It provides convenient access to x and y\n    coordinates, supports both 1D and 2D positional data, and includes\n    spatial boundary information.\n\n    Parameters\n    ----------\n    data : array_like or None, optional\n        Position data with shape (n_signals, n_samples). For 1D position data,\n        n_signals should be 1. For 2D position data, n_signals should be 2,\n        where the first row contains x-coordinates and the second row contains\n        y-coordinates. Can also be specified using the alias 'posdata'.\n    timestamps : array_like or None, optional\n        Time stamps corresponding to each sample in data. If None, timestamps\n        are automatically generated based on fs and start time.\n    fs : float, optional\n        Sampling frequency in Hz. Used to generate timestamps if not provided.\n    support : EpochArray or None, optional\n        EpochArray defining the time intervals over which the position data\n        is valid.\n    label : str, optional\n        Descriptive label for the position array.\n    xlim : tuple or None, optional\n        Spatial boundaries for x-coordinate as (min_x, max_x).\n    ylim : tuple or None, optional\n        Spatial boundaries for y-coordinate as (min_y, max_y).\n\n    Attributes\n    ----------\n    x : ndarray\n        X-coordinates as a 1D numpy array.\n    y : ndarray\n        Y-coordinates as a 1D numpy array (only available for 2D data).\n    is_1d : bool\n        True if position data is 1-dimensional.\n    is_2d : bool\n        True if position data is 2-dimensional.\n    xlim : tuple or None\n        Spatial boundaries for x-coordinate (only for 2D data).\n    ylim : tuple or None\n        Spatial boundaries for y-coordinate (only for 2D data).\n\n    Examples\n    --------\n    Create a 1D position array:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; # 1D position data (e.g., position on a linear track)\n    &gt;&gt;&gt; x_pos = np.linspace(0, 100, 1000)  # 100 cm track\n    &gt;&gt;&gt; timestamps = np.linspace(0, 10, 1000)  # 10 seconds\n    &gt;&gt;&gt; pos_1d = nel.PositionArray(\n    ...     data=x_pos[np.newaxis, :],\n    ...     timestamps=timestamps,\n    ...     label=\"Linear track position\",\n    ... )\n    &gt;&gt;&gt; print(f\"1D position: {pos_1d.is_1d}\")\n    &gt;&gt;&gt; print(f\"X range: {pos_1d.x.min():.1f} to {pos_1d.x.max():.1f} cm\")\n\n    Create a 2D position array:\n\n    &gt;&gt;&gt; # 2D position data (e.g., open field behavior)\n    &gt;&gt;&gt; t = np.linspace(0, 2 * np.pi, 1000)\n    &gt;&gt;&gt; x_pos = 50 + 30 * np.cos(t)  # circular trajectory\n    &gt;&gt;&gt; y_pos = 50 + 30 * np.sin(t)\n    &gt;&gt;&gt; pos_data = np.vstack([x_pos, y_pos])\n    &gt;&gt;&gt; pos_2d = nel.PositionArray(\n    ...     posdata=pos_data,\n    ...     fs=100,  # 100 Hz sampling\n    ...     xlim=(0, 100),\n    ...     ylim=(0, 100),\n    ...     label=\"Open field position\",\n    ... )\n    &gt;&gt;&gt; print(f\"2D position: {pos_2d.is_2d}\")\n    &gt;&gt;&gt; print(f\"X position shape: {pos_2d.x.shape}\")\n    &gt;&gt;&gt; print(f\"Y position shape: {pos_2d.y.shape}\")\n    &gt;&gt;&gt; print(f\"Spatial bounds: x={pos_2d.xlim}, y={pos_2d.ylim}\")\n\n    Access position data:\n\n    &gt;&gt;&gt; # Get position at specific time\n    &gt;&gt;&gt; time_idx = 500\n    &gt;&gt;&gt; if pos_2d.is_2d:\n    ...     x_at_time = pos_2d.x[time_idx]\n    ...     y_at_time = pos_2d.y[time_idx]\n    ...     print(f\"Position at sample {time_idx}: ({x_at_time:.1f}, {y_at_time:.1f})\")\n\n    Notes\n    -----\n    - For 2D position data, the first row of data should contain x-coordinates\n      and the second row should contain y-coordinates.\n    - The xlim and ylim parameters are only meaningful for 2D position data.\n    - Attempting to access y-coordinates or spatial limits on 1D data will\n      raise a ValueError.\n    - The 'posdata' alias can be used interchangeably with 'data' parameter.\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\"posdata\": \"data\"}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        xlim = kwargs.pop(\"xlim\", None)\n        ylim = kwargs.pop(\"ylim\", None)\n        super().__init__(*args, **kwargs)\n        self._xlim = xlim\n        self._ylim = ylim\n\n    @property\n    def is_2d(self):\n        try:\n            return self.n_signals == 2\n        except IndexError:\n            return False\n\n    @property\n    def is_1d(self):\n        try:\n            return self.n_signals == 1\n        except IndexError:\n            return False\n\n    @property\n    def x(self):\n        \"\"\"return x-values, as numpy array.\"\"\"\n        return self.data[0, :]\n\n    @property\n    def y(self):\n        \"\"\"return y-values, as numpy array.\"\"\"\n        if self.is_2d:\n            return self.data[1, :]\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so y-values are undefined!\"\n        )\n\n    @property\n    def xlim(self):\n        if self.is_2d:\n            return self._xlim\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so xlim is not undefined!\"\n        )\n\n    @xlim.setter\n    def xlim(self, val):\n        if self.is_2d:\n            self._xlim = xlim  # noqa: F821\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so xlim cannot be defined!\"\n        )\n\n    @property\n    def ylim(self):\n        if self.is_2d:\n            return self._ylim\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so ylim is not undefined!\"\n        )\n\n    @ylim.setter\n    def ylim(self, val):\n        if self.is_2d:\n            self._ylim = ylim  # noqa: F821\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so ylim cannot be defined!\"\n        )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.PositionArray.x","title":"<code>x</code>  <code>property</code>","text":"<p>return x-values, as numpy array.</p>"},{"location":"reference/nelpy/all/#nelpy.all.PositionArray.y","title":"<code>y</code>  <code>property</code>","text":"<p>return y-values, as numpy array.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray","title":"<code>RegularlySampledAnalogSignalArray</code>","text":"<p>Continuous analog signal(s) with regular sampling rates (irregular sampling rates can be corrected with operations on the support) and same support. NOTE: data that is not equal dimensionality will NOT work and error/warning messages may/may not be sent out. Assumes abscissa_vals are identical for all signals passed through and are therefore expected to be 1-dimensional.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray, with shape (n_signals, n_samples).</code> <p>Data samples.</p> <code>[]</code> <code>abscissa_vals</code> <code>np.ndarray, with shape (n_samples, ).</code> <p>The abscissa coordinate values. Currently we assume that (1) these values are timestamps, and (2) the timestamps are sampled regularly (we rely on these assumptions to generate intervals). Irregular sampling rates can be corrected with operations on the support.</p> <code>None</code> <code>fs</code> <code>float</code> <p>The sampling rate. abscissa_vals are still expected to be in units of time and fs is expected to be in the corresponding sampling rate (e.g. abscissa_vals in seconds, fs in Hz). Default is 1 Hz.</p> <code>None</code> <code>step</code> <code>float</code> <p>The sampling interval of the data, in seconds. Default is None. specifies step size of samples passed as tdata if fs is given, default is None. If not passed it is inferred by the minimum difference in between samples of tdata passed in (based on if FS is passed). e.g. decimated data would have sample numbers every ten samples so step=10</p> <code>None</code> <code>merge_sample_gap</code> <code>float</code> <p>Optional merging of gaps between support intervals. If intervals are within a certain amount of time, gap, they will be merged as one interval. Example use case is when there is a dropped sample</p> <code>0</code> <code>support</code> <code>IntervalArray</code> <p>Where the data are defined. Default is [0, last abscissa value] inclusive.</p> <code>None</code> <code>in_core</code> <code>bool</code> <p>Whether the abscissa values should be treated as residing in core memory. During RSASA construction, np.diff() is called, so for large data, passing in in_core=True might help. In that case, a slower but much smaller memory footprint function is used.</p> <code>True</code> <code>labels</code> <code>np.array, dtype=np.str</code> <p>Labels for each of the signals. If fewer labels than signals are passed in, labels are padded with None's to match the number of signals. If more labels than signals are passed in, labels are truncated to match the number of signals. Default is None.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Return an empty RegularlySampledAnalogSignalArray if true else false. Default is false.</p> <code>False</code> <code>abscissa</code> <code>optional</code> <p>The object handling the abscissa values. It is recommended to leave this parameter alone and let nelpy take care of this. Default is a nelpy.core.Abscissa object.</p> <code>None</code> <code>ordinate</code> <code>optional</code> <p>The object handling the ordinate values. It is recommended to leave this parameter alone and let nelpy take care of this. Default is a nelpy.core.Ordinate object.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>np.ndarray, with shape (n_signals, n_samples)</code> <p>The underlying data.</p> <code>abscissa_vals</code> <code>np.ndarray, with shape (n_samples, )</code> <p>The values of the abscissa coordinate.</p> <code>is1d</code> <code>bool</code> <p>Whether there is only 1 signal in the RSASA</p> <code>iswrapped</code> <code>bool</code> <p>Whether the RSASA's data is wrapping.</p> <code>base_unit</code> <code>string</code> <p>Base unit of the abscissa.</p> <code>signals</code> <code>list</code> <p>A list of RegularlySampledAnalogSignalArrays, each RSASA containing a single signal (channel). WARNING: this method creates a copy of each signal, so is not particularly efficient at this time.</p> <code>isreal</code> <code>bool</code> <p>Whether ALL of the values in the RSASA's data are real.</p> <code>iscomplex</code> <code>bool</code> <p>Whether ANY values in the data are complex.</p> <code>abs</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is the absolute value of the original original RSASA's (potentially complex) data.</p> <code>phase</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the phase angle (in radians) of the original RSASA's data.</p> <code>real</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the real part of the original RSASA's data.</p> <code>imag</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the imaginary part of the original RSASA's data.</p> <code>lengths</code> <code>list</code> <p>The number of samples in each interval.</p> <code>labels</code> <code>list</code> <p>The labels corresponding to each signal.</p> <code>n_signals</code> <code>int</code> <p>The number of signals in the RSASA.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the RSASA.</p> <code>domain</code> <code>IntervalArray</code> <p>The domain of the RSASA.</p> <code>range</code> <code>IntervalArray</code> <p>The range of the RSASA's data.</p> <code>step</code> <code>float</code> <p>The sampling interval of the RSASA. Currently the units are in seconds.</p> <code>fs</code> <code>float</code> <p>The sampling frequency of the RSASA. Currently the units are in Hz.</p> <code>isempty</code> <code>bool</code> <p>Whether the underlying data has zero length, i.e. 0 samples</p> <code>n_bytes</code> <code>int</code> <p>Approximate number of bytes taken up by the RSASA.</p> <code>n_intervals</code> <code>int</code> <p>The number of underlying intervals in the RSASA.</p> <code>n_samples</code> <code>int</code> <p>The number of abscissa values in the RSASA.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class RegularlySampledAnalogSignalArray:\n    \"\"\"Continuous analog signal(s) with regular sampling rates (irregular\n    sampling rates can be corrected with operations on the support) and same\n    support. NOTE: data that is not equal dimensionality will NOT work\n    and error/warning messages may/may not be sent out. Assumes abscissa_vals\n    are identical for all signals passed through and are therefore expected\n    to be 1-dimensional.\n\n    Parameters\n    ----------\n    data : np.ndarray, with shape (n_signals, n_samples).\n        Data samples.\n    abscissa_vals : np.ndarray, with shape (n_samples, ).\n        The abscissa coordinate values. Currently we assume that (1) these values\n        are timestamps, and (2) the timestamps are sampled regularly (we rely on\n        these assumptions to generate intervals). Irregular sampling rates can be\n        corrected with operations on the support.\n    fs : float, optional\n        The sampling rate. abscissa_vals are still expected to be in units of\n        time and fs is expected to be in the corresponding sampling rate (e.g.\n        abscissa_vals in seconds, fs in Hz).\n        Default is 1 Hz.\n    step : float, optional\n        The sampling interval of the data, in seconds.\n        Default is None.\n        specifies step size of samples passed as tdata if fs is given,\n        default is None. If not passed it is inferred by the minimum\n        difference in between samples of tdata passed in (based on if FS\n        is passed). e.g. decimated data would have sample numbers every\n        ten samples so step=10\n    merge_sample_gap : float, optional\n        Optional merging of gaps between support intervals. If intervals are within\n        a certain amount of time, gap, they will be merged as one interval. Example\n        use case is when there is a dropped sample\n    support : nelpy.IntervalArray, optional\n        Where the data are defined. Default is [0, last abscissa value] inclusive.\n    in_core : bool, optional\n        Whether the abscissa values should be treated as residing in core memory.\n        During RSASA construction, np.diff() is called, so for large data, passing\n        in in_core=True might help. In that case, a slower but much smaller memory\n        footprint function is used.\n    labels : np.array, dtype=np.str\n        Labels for each of the signals. If fewer labels than signals are passed in,\n        labels are padded with None's to match the number of signals. If more labels\n        than signals are passed in, labels are truncated to match the number of\n        signals.\n        Default is None.\n    empty : bool, optional\n        Return an empty RegularlySampledAnalogSignalArray if true else false.\n        Default is false.\n    abscissa : optional\n        The object handling the abscissa values. It is recommended to leave\n        this parameter alone and let nelpy take care of this.\n        Default is a nelpy.core.Abscissa object.\n    ordinate : optional\n        The object handling the ordinate values. It is recommended to leave\n        this parameter alone and let nelpy take care of this.\n        Default is a nelpy.core.Ordinate object.\n\n    Attributes\n    ----------\n    data : np.ndarray, with shape (n_signals, n_samples)\n        The underlying data.\n    abscissa_vals : np.ndarray, with shape (n_samples, )\n        The values of the abscissa coordinate.\n    is1d : bool\n        Whether there is only 1 signal in the RSASA\n    iswrapped : bool\n        Whether the RSASA's data is wrapping.\n    base_unit : string\n        Base unit of the abscissa.\n    signals : list\n        A list of RegularlySampledAnalogSignalArrays, each RSASA containing\n        a single signal (channel).\n        WARNING: this method creates a copy of each signal, so is not\n        particularly efficient at this time.\n    isreal : bool\n        Whether ALL of the values in the RSASA's data are real.\n    iscomplex : bool\n        Whether ANY values in the data are complex.\n    abs : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is the absolute value of the original\n        original RSASA's (potentially complex) data.\n    phase : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the phase angle (in radians) of\n        the original RSASA's data.\n    real : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the real part of the original\n        RSASA's data.\n    imag : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the imaginary part of the\n        original RSASA's data.\n    lengths : list\n        The number of samples in each interval.\n    labels : list\n        The labels corresponding to each signal.\n    n_signals : int\n        The number of signals in the RSASA.\n    support : nelpy.IntervalArray\n        The support of the RSASA.\n    domain : nelpy.IntervalArray\n        The domain of the RSASA.\n    range : nelpy.IntervalArray\n        The range of the RSASA's data.\n    step : float\n        The sampling interval of the RSASA. Currently the units are\n        in seconds.\n    fs : float\n        The sampling frequency of the RSASA. Currently the units are\n        in Hz.\n    isempty : bool\n        Whether the underlying data has zero length, i.e. 0 samples\n    n_bytes : int\n        Approximate number of bytes taken up by the RSASA.\n    n_intervals : int\n        The number of underlying intervals in the RSASA.\n    n_samples : int\n        The number of abscissa values in the RSASA.\n    \"\"\"\n\n    __aliases__ = {}\n\n    __attributes__ = [\n        \"_data\",\n        \"_abscissa_vals\",\n        \"_fs\",\n        \"_support\",\n        \"_interp\",\n        \"_step\",\n        \"_labels\",\n    ]\n\n    @rsasa_init_wrapper\n    def __init__(\n        self,\n        data=[],\n        *,\n        abscissa_vals=None,\n        fs=None,\n        step=None,\n        merge_sample_gap=0,\n        support=None,\n        in_core=True,\n        labels=None,\n        empty=False,\n        abscissa=None,\n        ordinate=None,\n    ):\n        self._intervalsignalslicer = IntervalSignalSlicer(self)\n        self._intervaldata = DataSlicer(self)\n        self._intervaltime = AbscissaSlicer(self)\n\n        self.type_name = self.__class__.__name__\n        if abscissa is None:\n            abscissa = core.Abscissa()  # TODO: integrate into constructor?\n        if ordinate is None:\n            ordinate = core.Ordinate()  # TODO: integrate into constructor?\n\n        self._abscissa = abscissa\n        self._ordinate = ordinate\n\n        # TODO: #FIXME abscissa and ordinate domain, range, and supports should be integrated and/or coerced with support\n\n        self.__version__ = version.__version__\n\n        # cast derivatives of RegularlySampledAnalogSignalArray back into RegularlySampledAnalogSignalArray:\n        # if isinstance(data, auxiliary.PositionArray):\n        if isinstance(data, RegularlySampledAnalogSignalArray):\n            self.__dict__ = copy.deepcopy(data.__dict__)\n            # if self._has_changed:\n            # self.__renew__()\n            self.__renew__()\n            return\n\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self._data = np.array([])\n            self._abscissa_vals = np.array([])\n            self.__bake__()\n            return\n\n        self._step = step\n        self._fs = fs\n\n        # Note; if we have an empty array of data with no dimension,\n        # then calling len(data) will return a TypeError\n        try:\n            # if no data are given return empty RegularlySampledAnalogSignalArray\n            if data.size == 0:\n                self.__init__(empty=True)\n                return\n        except TypeError:\n            logging.warning(\n                \"unsupported type; creating empty RegularlySampledAnalogSignalArray\"\n            )\n            self.__init__(empty=True)\n            return\n\n        # Note: if both abscissa_vals and data are given and dimensionality does not\n        # match, then TypeError!\n\n        abscissa_vals = np.squeeze(abscissa_vals).astype(float)\n        if abscissa_vals.shape[0] != data.shape[1]:\n            # self.__init__([],empty=True)\n            raise TypeError(\n                \"abscissa_vals and data size mismatch! Note: data \"\n                \"is expected to have rows containing signals\"\n            )\n        # data is not sorted and user wants it to be\n        # TODO: use faster is_sort from jagular\n        if not utils.is_sorted(abscissa_vals):\n            logging.warning(\"Data is _not_ sorted! Data will be sorted automatically.\")\n            ind = np.argsort(abscissa_vals)\n            abscissa_vals = abscissa_vals[ind]\n            data = np.take(a=data, indices=ind, axis=-1)\n\n        self._data = data\n        self._abscissa_vals = abscissa_vals\n\n        # handle labels\n        if labels is not None:\n            labels = np.asarray(labels, dtype=str)\n            # label size doesn't match\n            if labels.shape[0] &gt; data.shape[0]:\n                logging.warning(\n                    \"More labels than data! Labels are truncated to size of data\"\n                )\n                labels = labels[0 : data.shape[0]]\n            elif labels.shape[0] &lt; data.shape[0]:\n                logging.warning(\n                    \"Fewer labels than abscissa_vals! Labels are filled with \"\n                    \"None to match data shape\"\n                )\n                for i in range(labels.shape[0], data.shape[0]):\n                    labels.append(None)\n        self._labels = labels\n\n        # Alright, let's handle all the possible parameter cases!\n        if support is not None:\n            self._restrict_to_interval_array_fast(intervalarray=support)\n        else:\n            logging.warning(\n                \"creating support from abscissa_vals and sampling rate, fs!\"\n            )\n            self._abscissa.support = type(self._abscissa.support)(\n                utils.get_contiguous_segments(\n                    self._abscissa_vals, step=self._step, fs=fs, in_core=in_core\n                )\n            )\n            if merge_sample_gap &gt; 0:\n                self._abscissa.support = self._abscissa.support.merge(\n                    gap=merge_sample_gap\n                )\n\n        if np.abs((self.fs - self._estimate_fs()) / self.fs) &gt; 0.01:\n            logging.warning(\"estimated fs and provided fs differ by more than 1%\")\n\n    def __bake__(self):\n        \"\"\"Fix object as-is, and bake a new hash.\n\n        For example, if a label has changed, or if an interp has been attached,\n        then the object's hash will change, and it needs to be baked\n        again for efficiency / consistency.\n        \"\"\"\n        self._stored_hash_ = self.__hash__()\n\n    # def _has_changed_data(self):\n    #     \"\"\"Compute hash on abscissa_vals and data and compare to cached hash.\"\"\"\n    #     return self.data.__hash__ elf._data_hash_\n\n    def _has_changed(self):\n        \"\"\"Compute hash on current object, and compare to previously stored hash\"\"\"\n        return self.__hash__() == self._stored_hash_\n\n    def __renew__(self):\n        \"\"\"Re-attach data slicers.\"\"\"\n        self._intervalsignalslicer = IntervalSignalSlicer(self)\n        self._intervaldata = DataSlicer(self)\n        self._intervaltime = AbscissaSlicer(self)\n        self._interp = None\n        self.__bake__()\n\n    def __call__(self, x):\n        \"\"\"RegularlySampledAnalogSignalArray callable method. Returns\n        interpolated data at requested points. Note that points falling\n        outside the support will not be interpolated.\n\n        Parameters\n        ----------\n        x : np.ndarray, list, or tuple, with length n_requested_samples\n            Points at which to interpolate the RSASA's data\n\n        Returns\n        -------\n        A np.ndarray with shape (n_signals, n_samples). If all the requested\n        points lie in the support, then n_samples = n_requested_samples.\n        Otherwise n_samples &lt; n_requested_samples.\n        \"\"\"\n\n        return self.asarray(at=x).yvals\n\n    def center(self, inplace=False):\n        \"\"\"\n        Center the data to have zero mean along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The centered signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; centered = asa.center()\n        &gt;&gt;&gt; centered.mean()\n        0.0\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        return out\n\n    def normalize(self, inplace=False):\n        \"\"\"\n        Normalize the data to have unit standard deviation along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The normalized signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; normalized = asa.normalize()\n        &gt;&gt;&gt; normalized.std()\n        1.0\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        std = np.atleast_1d(out.std())\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    def standardize(self, inplace=False):\n        \"\"\"\n        Standardize the data to zero mean and unit standard deviation along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The standardized signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; standardized = asa.standardize()\n        &gt;&gt;&gt; standardized.mean(), standardized.std()\n        (0.0, 1.0)\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        std = np.atleast_1d(out.std())\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    @property\n    def is_1d(self):\n        try:\n            return self.n_signals == 1\n        except IndexError:\n            return False\n\n    @property\n    def is_wrapped(self):\n        if np.any(self.max() &gt; self._ordinate.range.stop) | np.any(\n            self.min() &lt; self._ordinate.range.min\n        ):\n            self._ordinate._is_wrapped = False\n        else:\n            self._ordinate._is_wrapped = True\n\n        # if self._ordinate._is_wrapped is None:\n        #     if np.any(self.max() &gt; self._ordinate.range.stop) | np.any(self.min() &lt; self._ordinate.range.min):\n        #         self._ordinate._is_wrapped = False\n        #     else:\n        #         self._ordinate._is_wrapped = True\n        return self._ordinate._is_wrapped\n\n    def _wrap(self, arr, vmin, vmax):\n        \"\"\"Wrap array within finite range.\"\"\"\n        if np.isinf(vmax - vmin):\n            raise ValueError(\"range has to be finite!\")\n        return ((arr - vmin) % (vmax - vmin)) + vmin\n\n    def wrap(self, inplace=False):\n        \"\"\"\n        Wrap the ordinate values within the finite range defined by the ordinate's range.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The wrapped signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; wrapped = asa.wrap()\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        out.data = np.atleast_2d(\n            out._wrap(out.data, out._ordinate.range.min, out._ordinate.range.max)\n        )\n        # out._is_wrapped = True\n        return out\n\n    def _unwrap(self, arr, vmin, vmax):\n        \"\"\"Unwrap 2D array (with one signal per row) by minimizing total displacement.\"\"\"\n        d = vmax - vmin\n        dh = d / 2\n\n        lin = copy.deepcopy(arr) - vmin\n        n_signals, n_samples = arr.shape\n        for ii in range(1, n_samples):\n            h1 = lin[:, ii] - lin[:, ii - 1] &gt;= dh\n            lin[h1, ii:] = lin[h1, ii:] - d\n            h2 = lin[:, ii] - lin[:, ii - 1] &lt; -dh\n            lin[h2, ii:] = lin[h2, ii:] + d\n        return np.atleast_2d(lin + vmin)\n\n    def unwrap(self, inplace=False):\n        \"\"\"\n        Unwrap the ordinate values by minimizing total displacement, useful for phase data.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The unwrapped signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; unwrapped = asa.unwrap()\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        out.data = np.atleast_2d(\n            out._unwrap(out._data, out._ordinate.range.min, out._ordinate.range.max)\n        )\n        # out._is_wrapped = False\n        return out\n\n    def _crossvals(self):\n        \"\"\"Return all abscissa values where the orinate crosses.\n\n        Note that this can return multiple values close in succession\n        if the signal oscillates around the maximum or minimum range.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def base_unit(self):\n        \"\"\"Base unit of the abscissa.\"\"\"\n        return self._abscissa.base_unit\n\n    def _data_interval_indices(self):\n        \"\"\"\n        Get the start and stop indices for each interval in the analog signal array.\n\n        Returns\n        -------\n        indices : np.ndarray\n            Array of shape (n_intervals, 2), where each row contains the start and stop indices for an interval.\n        \"\"\"\n        tmp = np.insert(np.cumsum(self.lengths), 0, 0)\n        indices = np.vstack((tmp[:-1], tmp[1:])).T\n        return indices\n\n    def ddt(self, rectify=False):\n        \"\"\"Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n        asa.data = f(t)\n        asa.ddt = d/dt (asa.data)\n\n        Parameters\n        ----------\n        rectify : boolean, optional\n            If True, the absolute value of the derivative will be returned.\n            Default is False.\n\n        Returns\n        -------\n        ddt : RegularlySampledAnalogSignalArray\n            Time derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n        Note\n        ----\n        Second order central differences are used here, and it is assumed that\n        the signals are sampled uniformly. If the signals are not uniformly\n        sampled, it is recommended to resample the signal before computing the\n        derivative.\n        \"\"\"\n        ddt = utils.ddt_asa(self, rectify=rectify)\n        return ddt\n\n    @property\n    def signals(self):\n        \"\"\"Returns a list of RegularlySampledAnalogSignalArrays, each array containing\n        a single signal (channel).\n\n        WARNING: this method creates a copy of each signal, so is not\n        particularly efficient at this time.\n\n        Examples\n        --------\n        &gt;&gt;&gt; for channel in lfp.signals:\n            print(channel)\n        \"\"\"\n        signals = []\n        for ii in range(self.n_signals):\n            signals.append(self[:, ii])\n        return signals\n        # return np.asanyarray(signals).squeeze()\n\n    @property\n    def isreal(self):\n        \"\"\"Returns True if entire signal is real.\"\"\"\n        return np.all(np.isreal(self.data))\n        # return np.isrealobj(self._data)\n\n    @property\n    def iscomplex(self):\n        \"\"\"Returns True if any part of the signal is complex.\"\"\"\n        return np.any(np.iscomplex(self.data))\n        # return np.iscomplexobj(self._data)\n\n    @property\n    def abs(self):\n        \"\"\"RegularlySampledAnalogSignalArray with absolute value of (potentially complex) data.\"\"\"\n        out = self.copy()\n        out._data = np.abs(self.data)\n        return out\n\n    @property\n    def angle(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only phase angle (in radians) of data.\"\"\"\n        out = self.copy()\n        out._data = np.angle(self.data)\n        return out\n\n    @property\n    def imag(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only imaginary part of data.\"\"\"\n        out = self.copy()\n        out._data = self.data.imag\n        return out\n\n    @property\n    def real(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only real part of data.\"\"\"\n        out = self.copy()\n        out._data = self.data.real\n        return out\n\n    def __mul__(self, other):\n        \"\"\"overloaded * operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data * other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T * other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to multiply.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data * other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for *: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __add__(self, other):\n        \"\"\"overloaded + operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data + other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T + other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to add.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data + other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __sub__(self, other):\n        \"\"\"overloaded - operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data - other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T - other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to subtract.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data - other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for -: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def zscore(self):\n        \"\"\"\n        Normalize each signal in the array using z-scores (zero mean, unit variance).\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            New object with z-scored data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; zscored = asa.zscore()\n        \"\"\"\n        out = self.copy()\n        out._data = zscore(out._data, axis=1)\n        return out\n\n    def __truediv__(self, other):\n        \"\"\"overloaded / operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data / other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T / other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to divide.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data / other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for /: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __lshift__(self, val):\n        \"\"\"shift abscissa and support to left (&lt;&lt;)\"\"\"\n        if isinstance(val, numbers.Number):\n            new = self.copy()\n            new._abscissa_vals -= val\n            new._abscissa.support = new._abscissa.support &lt;&lt; val\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &lt;&lt;: {} and {}\".format(\n                    str(type(self)), str(type(val))\n                )\n            )\n\n    def __rshift__(self, val):\n        \"\"\"shift abscissa and support to right (&gt;&gt;)\"\"\"\n        if isinstance(val, numbers.Number):\n            new = self.copy()\n            new._abscissa_vals += val\n            new._abscissa.support = new._abscissa.support &gt;&gt; val\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &gt;&gt;: {} and {}\".format(\n                    str(type(self)), str(type(val))\n                )\n            )\n\n    def __len__(self):\n        return self.n_intervals\n\n    def _drop_empty_intervals(self):\n        \"\"\"Drops empty intervals from support. In-place.\"\"\"\n        keep_interval_ids = np.argwhere(self.lengths).squeeze().tolist()\n        self._abscissa.support = self._abscissa.support[keep_interval_ids]\n        return self\n\n    def _estimate_fs(self, abscissa_vals=None):\n        \"\"\"Estimate the sampling rate of the data.\"\"\"\n        if abscissa_vals is None:\n            abscissa_vals = self._abscissa_vals\n        return 1.0 / np.median(np.diff(abscissa_vals))\n\n    def downsample(self, *, fs_out, aafilter=True, inplace=False, **kwargs):\n        \"\"\"Downsamples the RegularlySampledAnalogSignalArray\n\n        Parameters\n        ----------\n        fs_out : float, optional\n            Desired output sampling rate in Hz\n        aafilter : boolean, optional\n            Whether to apply an anti-aliasing filter before performing the actual\n            downsampling. Default is True\n        inplace : boolean, optional\n            If True, the output ASA will replace the input ASA. Default is False\n        kwargs :\n            Other keyword arguments are passed to sosfiltfilt() in the `filtering`\n            module\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The downsampled RegularlySampledAnalogSignalArray\n        \"\"\"\n\n        if not fs_out &lt; self._fs:\n            raise ValueError(\"fs_out must be less than current sampling rate!\")\n\n        if aafilter:\n            fh = fs_out / 2.0\n            out = filtering.sosfiltfilt(self, fl=None, fh=fh, inplace=inplace, **kwargs)\n\n        downsampled = out.simplify(ds=1 / fs_out)\n        out._data = downsampled._data\n        out._abscissa_vals = downsampled._abscissa_vals\n        out._fs = fs_out\n\n        out.__renew__()\n        return out\n\n    def add_signal(self, signal, label=None):\n        \"\"\"Docstring goes here.\n        Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?\n        \"\"\"\n        # TODO: add functionality to check that supports are the same, etc.\n        if isinstance(signal, RegularlySampledAnalogSignalArray):\n            signal = signal.data\n\n        signal = np.squeeze(signal)\n        if signal.ndim &gt; 1:\n            raise TypeError(\"Can only add one signal at a time!\")\n        if self.data.ndim == 1:\n            self._data = np.vstack(\n                [np.array(self.data, ndmin=2), np.array(signal, ndmin=2)]\n            )\n        else:\n            self._data = np.vstack([self.data, np.array(signal, ndmin=2)])\n        if label is None:\n            logging.warning(\"None label appended\")\n        self._labels = np.append(self._labels, label)\n        return self\n\n    def _restrict_to_interval_array_fast(self, *, intervalarray=None, update=True):\n        \"\"\"Restrict self._abscissa_vals and self._data to an IntervalArray. If no\n        IntervalArray is specified, self._abscissa.support is used.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray, optional\n                IntervalArray on which to restrict AnalogSignal. Default is\n                self._abscissa.support\n        update : bool, optional\n                Overwrite self._abscissa.support with intervalarray if True (default).\n        \"\"\"\n        if intervalarray is None:\n            intervalarray = self._abscissa.support\n            update = False  # support did not change; no need to update\n\n        try:\n            if intervalarray.isempty:\n                logging.warning(\"Support specified is empty\")\n                # self.__init__([],empty=True)\n                exclude = [\"_support\", \"_data\", \"_fs\", \"_step\"]\n                attrs = (x for x in self.__attributes__ if x not in exclude)\n                logging.disable(logging.CRITICAL)\n                for attr in attrs:\n                    exec(\"self.\" + attr + \" = None\")\n                logging.disable(0)\n                self._data = np.zeros([0, self.data.shape[0]])\n                self._data[:] = np.nan\n                self._abscissa.support = intervalarray\n                return\n        except AttributeError:\n            raise AttributeError(\"IntervalArray expected\")\n\n        indices = []\n        for interval in intervalarray.merge().data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            frm, to = np.searchsorted(self._abscissa_vals, (a_start, a_stop + 1e-10))\n            indices.append((frm, to))\n        indices = np.array(indices, ndmin=2)\n        if np.diff(indices).sum() &lt; len(self._abscissa_vals):\n            logging.warning(\"ignoring signal outside of support\")\n        # check if only one interval and interval is already bounds of data\n        # if so, we don't need to do anything\n        if len(indices) == 1:\n            if indices[0, 0] == 0 and indices[0, 1] == len(self._abscissa_vals):\n                if update:\n                    self._abscissa.support = intervalarray\n                    return\n        try:\n            data_list = []\n            for start, stop in indices:\n                data_list.append(self._data[:, start:stop])\n            self._data = np.hstack(data_list)\n        except IndexError:\n            self._data = np.zeros([0, self.data.shape[0]])\n            self._data[:] = np.nan\n        time_list = []\n        for start, stop in indices:\n            time_list.extend(self._abscissa_vals[start:stop])\n        self._abscissa_vals = np.array(time_list)\n        if update:\n            self._abscissa.support = intervalarray\n\n    def _restrict_to_interval_array(self, *, intervalarray=None, update=True):\n        \"\"\"Restrict self._abscissa_vals and self._data to an IntervalArray. If no\n        IntervalArray is specified, self._abscissa.support is used.\n\n        This function is quite slow, as it checks each sample for inclusion.\n        It does this in a vectorized form, which is fast for small or moderately\n        sized objects, but the memory penalty can be large, and it becomes very\n        slow for large objects. Consequently, _restrict_to_interval_array_fast\n        should be used when possible.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray, optional\n                IntervalArray on which to restrict AnalogSignal. Default is\n                self._abscissa.support\n        update : bool, optional\n                Overwrite self._abscissa.support with intervalarray if True (default).\n        \"\"\"\n        if intervalarray is None:\n            intervalarray = self._abscissa.support\n            update = False  # support did not change; no need to update\n\n        try:\n            if intervalarray.isempty:\n                logging.warning(\"Support specified is empty\")\n                # self.__init__([],empty=True)\n                exclude = [\"_support\", \"_data\", \"_fs\", \"_step\"]\n                attrs = (x for x in self.__attributes__ if x not in exclude)\n                logging.disable(logging.CRITICAL)\n                for attr in attrs:\n                    exec(\"self.\" + attr + \" = None\")\n                logging.disable(0)\n                self._data = np.zeros([0, self.data.shape[0]])\n                self._data[:] = np.nan\n                self._abscissa.support = intervalarray\n                return\n        except AttributeError:\n            raise AttributeError(\"IntervalArray expected\")\n\n        indices = []\n        for interval in intervalarray.merge().data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            indices.append(\n                (self._abscissa_vals &gt;= a_start) &amp; (self._abscissa_vals &lt; a_stop)\n            )\n        indices = np.any(np.column_stack(indices), axis=1)\n        if np.count_nonzero(indices) &lt; len(self._abscissa_vals):\n            logging.warning(\"ignoring signal outside of support\")\n        try:\n            self._data = self.data[:, indices]\n        except IndexError:\n            self._data = np.zeros([0, self.data.shape[0]])\n            self._data[:] = np.nan\n        self._abscissa_vals = self._abscissa_vals[indices]\n        if update:\n            self._abscissa.support = intervalarray\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(\n        self,\n        *,\n        fs=None,\n        sigma=None,\n        truncate=None,\n        inplace=False,\n        mode=None,\n        cval=None,\n        within_intervals=False,\n    ):\n        \"\"\"Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.\n\n        Smoothing is applied along the abscissa, and the same smoothing is applied to each\n        signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n        Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n        Parameters\n        ----------\n        obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n        fs : float, optional\n            Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n            be inferred.\n        sigma : float, optional\n            Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n            (50 ms if base_unit=seconds).\n        truncate : float, optional\n            Bandwidth outside of which the filter value will be zero. Default is 4.0.\n        inplace : bool\n            If True the data will be replaced with the smoothed data.\n            Default is False.\n        mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to 'constant'. Default is\n            'reflect'.\n        cval : scalar, optional\n            Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n        within_intervals : boolean, optional\n            If True, then smooth within each epoch. Otherwise smooth across epochs.\n            Default is False.\n            Note that when mode = 'wrap', then smoothing within epochs aren't affected\n            by wrapping.\n\n        Returns\n        -------\n        out : same type as obj\n            An object with smoothed data is returned.\n\n        \"\"\"\n\n        if sigma is None:\n            sigma = 0.05\n        if truncate is None:\n            truncate = 4\n\n        kwargs = {\n            \"inplace\": inplace,\n            \"fs\": fs,\n            \"sigma\": sigma,\n            \"truncate\": truncate,\n            \"mode\": mode,\n            \"cval\": cval,\n            \"within_intervals\": within_intervals,\n        }\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        if self._ordinate.is_wrapping:\n            ord_is_wrapped = self.is_wrapped\n\n            if ord_is_wrapped:\n                out = out.unwrap()\n\n        # case 1: abs.wrapping=False, ord.linking=False, ord.wrapping=False\n        if (\n            not self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            pass\n\n        # case 2: abs.wrapping=False, ord.linking=False, ord.wrapping=True\n        elif (\n            not self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            pass\n\n        # case 3: abs.wrapping=False, ord.linking=True, ord.wrapping=False\n        elif (\n            not self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 4: abs.wrapping=False, ord.linking=True, ord.wrapping=True\n        elif (\n            not self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 5: abs.wrapping=True, ord.linking=False, ord.wrapping=False\n        elif (\n            self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            if mode is None:\n                kwargs[\"mode\"] = \"wrap\"\n\n        # case 6: abs.wrapping=True, ord.linking=False, ord.wrapping=True\n        elif (\n            self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            # (1) unwrap ordinate (abscissa wrap=False)\n            # (2) smooth unwrapped ordinate (absissa wrap=False)\n            # (3) repeat unwrapped signal based on conditions from (2):\n            # if smoothed wrapped ordinate samples\n            # HH ==&gt; SSS (this must be done on a per-signal basis!!!) H = high; L = low; S = same\n            # LL ==&gt; SSS (the vertical offset must be such that neighbors have smallest displacement)\n            # LH ==&gt; LSH\n            # HL ==&gt; HSL\n            # (4) smooth expanded and unwrapped ordinate (abscissa wrap=False)\n            # (5) cut out orignal signal\n\n            # (1)\n            kwargs[\"mode\"] = \"reflect\"\n            L = out._ordinate.range.max - out._ordinate.range.min\n            D = out.domain.length\n\n            tmp = utils.gaussian_filter(out.unwrap(), **kwargs)\n            # (2) (3)\n            n_reps = int(np.ceil((sigma * truncate) / float(D)))\n\n            smooth_data = []\n            for ss, signal in enumerate(tmp.signals):\n                # signal = signal.wrap()\n                offset = (\n                    float((signal._data[:, -1] - signal._data[:, 0]) // (L / 2)) * L\n                )\n                # print(offset)\n                # left_high = signal._data[:,0] &gt;= out._ordinate.range.min + L/2\n                # right_high = signal._data[:,-1] &gt;= out._ordinate.range.min + L/2\n                # signal = signal.unwrap()\n\n                expanded = signal.copy()\n                for nn in range(n_reps):\n                    expanded = expanded.join((signal &lt;&lt; D * (nn + 1)) - offset).join(\n                        (signal &gt;&gt; D * (nn + 1)) + offset\n                    )\n                    # print(expanded)\n                    # if left_high == right_high:\n                    #     print('extending flat! signal {}'.format(ss))\n                    #     expanded = expanded.join(signal &lt;&lt; D*(nn+1)).join(signal &gt;&gt; D*(nn+1))\n                    # elif left_high &lt; right_high:\n                    #     print('extending LSH! signal {}'.format(ss))\n                    #     # LSH\n                    #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))-L).join((signal &gt;&gt; D*(nn+1))+L)\n                    # else:\n                    #     # HSL\n                    #     print('extending HSL! signal {}'.format(ss))\n                    #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))+L).join((signal &gt;&gt; D*(nn+1))-L)\n                # (4)\n                smooth_signal = utils.gaussian_filter(expanded, **kwargs)\n                smooth_data.append(\n                    smooth_signal._data[\n                        :, n_reps * tmp.n_samples : (n_reps + 1) * (tmp.n_samples)\n                    ].squeeze()\n                )\n            # (5)\n            out._data = np.array(smooth_data)\n            out.__renew__()\n\n            if self._ordinate.is_wrapping:\n                if ord_is_wrapped:\n                    out = out.wrap()\n\n            return out\n\n        # case 7: abs.wrapping=True, ord.linking=True, ord.wrapping=False\n        elif (\n            self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 8: abs.wrapping=True, ord.linking=True, ord.wrapping=True\n        elif (\n            self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        out = utils.gaussian_filter(out, **kwargs)\n        out.__renew__()\n\n        if self._ordinate.is_wrapping:\n            if ord_is_wrapped:\n                out = out.wrap()\n\n        return out\n\n    @property\n    def lengths(self):\n        \"\"\"(list) The number of samples in each interval.\"\"\"\n        indices = []\n        for interval in self.support.data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            frm, to = np.searchsorted(self._abscissa_vals, (a_start, a_stop))\n            indices.append((frm, to))\n        indices = np.array(indices, ndmin=2)\n        lengths = np.atleast_1d(np.diff(indices).squeeze())\n        return lengths\n\n    @property\n    def labels(self):\n        \"\"\"(list) The labels corresponding to each signal.\"\"\"\n        # TODO: make this faster and better!\n        return self._labels\n\n    @property\n    def n_signals(self):\n        \"\"\"(int) The number of signals.\"\"\"\n        try:\n            return utils.PrettyInt(self.data.shape[0])\n        except AttributeError:\n            return 0\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self.n_intervals)\n        else:\n            epstr = \"\"\n        try:\n            if self.n_signals &gt; 0:\n                nstr = \" %s signals%s\" % (self.n_signals, epstr)\n        except IndexError:\n            nstr = \" 1 signal%s\" % epstr\n        dstr = \" for a total of {}\".format(\n            self._abscissa.formatter(self.support.length)\n        )\n        return \"&lt;%s%s:%s&gt;%s\" % (self.type_name, address_str, nstr, dstr)\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns an RegularlySampledAnalogSignalArray whose support has been\n        partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_samples : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            RegularlySampledAnalogSignalArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_samples or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        out._abscissa.support = out.support.partition(ds=ds, n_intervals=n_intervals)\n        return out\n\n    # @property\n    # def ydata(self):\n    #     \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n    #     # LEGACY\n    #     return self.data\n\n    @property\n    def data(self):\n        \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, val):\n        \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n        self._data = val\n        # print('data was modified, so clearing interp, etc.')\n        self.__renew__()\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval_array_fast(intervalarray=self._abscissa.support)\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval_array_fast(intervalarray=self._abscissa.support)\n\n    @property\n    def range(self):\n        \"\"\"(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._ordinate.range\n\n    @range.setter\n    def range(self, val):\n        \"\"\"(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify range\n        self._ordinate.range = val\n\n    @property\n    def step(self):\n        \"\"\"steps per sample\n        Example 1: sample_numbers = np.array([1,2,3,4,5,6]) #aka time\n        Steps per sample in the above case would be 1\n\n        Example 2: sample_numbers = np.array([1,3,5,7,9]) #aka time\n        Steps per sample in Example 2 would be 2\n        \"\"\"\n        return self._step\n\n    @property\n    def abscissa_vals(self):\n        \"\"\"(np.array 1D) Time in seconds.\"\"\"\n        return self._abscissa_vals\n\n    @abscissa_vals.setter\n    def abscissa_vals(self, vals):\n        \"\"\"(np.array 1D) Time in seconds.\"\"\"\n        self._abscissa_vals = vals\n\n    @property\n    def fs(self):\n        \"\"\"(float) Sampling frequency.\"\"\"\n        if self._fs is None:\n            logging.warning(\"No sampling frequency has been specified!\")\n        return self._fs\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) checks length of data input\"\"\"\n        try:\n            return self.data.shape[1] == 0\n        except IndexError:  # IndexError should happen if _data = []\n            return True\n\n    @property\n    def n_bytes(self):\n        \"\"\"Approximate number of bytes taken up by object.\"\"\"\n        return utils.PrettyBytes(self.data.nbytes + self._abscissa_vals.nbytes)\n\n    @property\n    def n_intervals(self):\n        \"\"\"(int) number of intervals in RegularlySampledAnalogSignalArray\"\"\"\n        return self._abscissa.support.n_intervals\n\n    @property\n    def n_samples(self):\n        \"\"\"(int) number of abscissa samples where signal is defined.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self._abscissa_vals))\n\n    def __iter__(self):\n        \"\"\"AnalogSignal iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"AnalogSignal iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_intervals - 1:\n            raise StopIteration\n        logging.disable(logging.CRITICAL)\n        intervalarray = type(self.support)(empty=True)\n        exclude = [\"_abscissa_vals\"]\n        attrs = (x for x in self._abscissa.support.__attributes__ if x not in exclude)\n\n        for attr in attrs:\n            exec(\"intervalarray.\" + attr + \" = self._abscissa.support.\" + attr)\n        try:\n            intervalarray._data = self._abscissa.support.data[\n                tuple([index]), :\n            ]  # use np integer indexing! Cool!\n        except IndexError:\n            # index is out of bounds, so return an empty IntervalArray\n            pass\n        logging.disable(0)\n\n        self._index += 1\n\n        asa = type(self)([], empty=True)\n        exclude = [\"_interp\", \"_support\"]\n        attrs = (x for x in self.__attributes__ if x not in exclude)\n        logging.disable(logging.CRITICAL)\n        for attr in attrs:\n            exec(\"asa.\" + attr + \" = self.\" + attr)\n        logging.disable(0)\n        asa._restrict_to_interval_array_fast(intervalarray=intervalarray)\n        if asa.support.isempty:\n            logging.warning(\n                \"Support is empty. Empty RegularlySampledAnalogSignalArray returned\"\n            )\n            asa = type(self)([], empty=True)\n\n        asa.__renew__()\n        return asa\n\n    def empty(self, inplace=True):\n        \"\"\"Remove data (but not metadata) from RegularlySampledAnalogSignalArray.\n\n        Attributes 'data', 'abscissa_vals', and 'support' are all emptied.\n\n        Note: n_signals is preserved.\n        \"\"\"\n        n_signals = self.n_signals\n        if not inplace:\n            out = self._copy_without_data()\n        else:\n            out = self\n            out._data = np.zeros((n_signals, 0))\n        out._abscissa.support = type(self.support)(empty=True)\n        out._abscissa_vals = []\n        out.__renew__()\n        return out\n\n    def __getitem__(self, idx):\n        \"\"\"RegularlySampledAnalogSignalArray index access.\n\n        Parameters\n        ----------\n        idx : IntervalArray, int, slice\n            intersect passed intervalarray with support,\n            index particular a singular interval or multiple intervals with slice\n        \"\"\"\n        intervalslice, signalslice = self._intervalsignalslicer[idx]\n\n        asa = self._subset(signalslice)\n\n        if asa.isempty:\n            asa.__renew__()\n            return asa\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                asa.__renew__()\n                return asa\n\n        newintervals = self._abscissa.support[intervalslice]\n        # TODO: this needs to change so that n_signals etc. are preserved\n        ################################################################\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=False)\n        ################################################################\n\n        asa._restrict_to_interval_array_fast(intervalarray=newintervals)\n        asa.__renew__()\n        return asa\n\n    def _subset(self, idx):\n        asa = self.copy()\n        try:\n            asa._data = np.atleast_2d(self.data[idx, :])\n        except IndexError:\n            raise IndexError(\n                \"index {} is out of bounds for n_signals with size {}\".format(\n                    idx, self.n_signals\n                )\n            )\n        asa.__renew__()\n        return asa\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without data and abscissa_vals.\n\n        Note: the support is left unchanged.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._abscissa_vals = None\n        out._data = np.zeros((self.n_signals, 0))\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Return a copy of the current object.\"\"\"\n        out = copy.deepcopy(self)\n        out.__renew__()\n        return out\n\n    def median(self, *, axis=1):\n        \"\"\"Returns the median of each signal in RegularlySampledAnalogSignalArray.\"\"\"\n        try:\n            medians = np.nanmedian(self.data, axis=axis).squeeze()\n            if medians.size == 1:\n                return medians.item()\n            return medians\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate median\"\n            )\n\n    def mean(self, *, axis=1):\n        \"\"\"\n        Compute the mean of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the mean (default is 1, i.e., across samples).\n\n        Returns\n        -------\n        mean : np.ndarray\n            Mean values along the specified axis.\n        \"\"\"\n        try:\n            means = np.nanmean(self.data, axis=axis).squeeze()\n            if means.size == 1:\n                return means.item()\n            return means\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate mean\"\n            )\n\n    def std(self, *, axis=1):\n        \"\"\"\n        Compute the standard deviation of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the standard deviation (default is 1).\n\n        Returns\n        -------\n        std : np.ndarray\n            Standard deviation values along the specified axis.\n        \"\"\"\n        try:\n            stds = np.nanstd(self.data, axis=axis).squeeze()\n            if stds.size == 1:\n                return stds.item()\n            return stds\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate standard deviation\"\n            )\n\n    def max(self, *, axis=1):\n        \"\"\"\n        Compute the maximum value of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the maximum (default is 1).\n\n        Returns\n        -------\n        max : np.ndarray\n            Maximum values along the specified axis.\n        \"\"\"\n        try:\n            maxes = np.amax(self.data, axis=axis).squeeze()\n            if maxes.size == 1:\n                return maxes.item()\n            return maxes\n        except ValueError:\n            raise ValueError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate maximum\"\n            )\n\n    def min(self, *, axis=1):\n        \"\"\"\n        Compute the minimum value of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the minimum (default is 1).\n\n        Returns\n        -------\n        min : np.ndarray\n            Minimum values along the specified axis.\n        \"\"\"\n        try:\n            mins = np.amin(self.data, axis=axis).squeeze()\n            if mins.size == 1:\n                return mins.item()\n            return mins\n        except ValueError:\n            raise ValueError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate minimum\"\n            )\n\n    def clip(self, min, max):\n        \"\"\"\n        Clip (limit) the values in the data to the interval [min, max].\n\n        Parameters\n        ----------\n        min : float\n            Minimum value.\n        max : float\n            Maximum value.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            New object with clipped data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; clipped = asa.clip(-1, 1)\n        \"\"\"\n        out = self.copy()\n        out._data = np.clip(self.data, min, max)\n        return out\n\n    def trim(self, start, stop=None, *, fs=None):\n        \"\"\"\n        Trim the signal to the specified start and stop times.\n\n        Parameters\n        ----------\n        start : float\n            Start time.\n        stop : float, optional\n            Stop time. If None, trims to the end.\n        fs : float, optional\n            Sampling frequency. If None, uses self.fs.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Trimmed signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; trimmed = asa.trim(0, 10)\n        \"\"\"\n        logging.warning(\"RegularlySampledAnalogSignalArray: Trim may not work!\")\n        # TODO: do comprehensive input validation\n        if stop is not None:\n            try:\n                start = np.array(start, ndmin=1)\n                if len(start) != 1:\n                    raise TypeError(\"start must be a scalar float\")\n            except TypeError:\n                raise TypeError(\"start must be a scalar float\")\n            try:\n                stop = np.array(stop, ndmin=1)\n                if len(stop) != 1:\n                    raise TypeError(\"stop must be a scalar float\")\n            except TypeError:\n                raise TypeError(\"stop must be a scalar float\")\n        else:  # start must have two elements\n            try:\n                if len(np.array(start, ndmin=1)) &gt; 2:\n                    raise TypeError(\n                        \"unsupported input to RegularlySampledAnalogSignalArray.trim()\"\n                    )\n                stop = np.array(start[1], ndmin=1)\n                start = np.array(start[0], ndmin=1)\n                if len(start) != 1 or len(stop) != 1:\n                    raise TypeError(\"start and stop must be scalar floats\")\n            except TypeError:\n                raise TypeError(\"start and stop must be scalar floats\")\n\n        logging.disable(logging.CRITICAL)\n        interval = self._abscissa.support.intersect(\n            type(self.support)([start, stop], fs=fs)\n        )\n        if not interval.isempty:\n            analogsignalarray = self[interval]\n        else:\n            analogsignalarray = type(self)([], empty=True)\n        logging.disable(0)\n        analogsignalarray.__renew__()\n        return analogsignalarray\n\n    @property\n    def _ydata_rowsig(self):\n        \"\"\"returns wide-format data s.t. each row is a signal.\"\"\"\n        # LEGACY\n        return self.data\n\n    @property\n    def _ydata_colsig(self):\n        # LEGACY\n        \"\"\"returns skinny-format data s.t. each column is a signal.\"\"\"\n        return self.data.T\n\n    @property\n    def _data_rowsig(self):\n        \"\"\"returns wide-format data s.t. each row is a signal.\"\"\"\n        return self.data\n\n    @property\n    def _data_colsig(self):\n        \"\"\"returns skinny-format data s.t. each column is a signal.\"\"\"\n        return self.data.T\n\n    def _get_interp1d(\n        self,\n        *,\n        kind=\"linear\",\n        copy=True,\n        bounds_error=False,\n        fill_value=np.nan,\n        assume_sorted=None,\n    ):\n        \"\"\"returns a scipy interp1d object, extended to have values at all interval\n        boundaries!\n        \"\"\"\n\n        if assume_sorted is None:\n            assume_sorted = utils.is_sorted(self._abscissa_vals)\n\n        if self.n_signals &gt; 1:\n            axis = 1\n        else:\n            axis = -1\n\n        abscissa_vals = self._abscissa_vals\n\n        if self._ordinate.is_wrapping:\n            yvals = self._unwrap(\n                self._data_rowsig, self._ordinate.range.min, self._ordinate.range.max\n            )  # always interpolate on the unwrapped data!\n        else:\n            yvals = self._data_rowsig\n\n        lengths = self.lengths\n        empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n        first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n        first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n        last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        first_abscissavals_per_interval = self._abscissa_vals[\n            first_abscissavals_per_interval_idx\n        ]\n        last_abscissavals_per_interval = self._abscissa_vals[\n            last_abscissavals_per_interval_idx\n        ]\n\n        boundary_abscissa_vals = []\n        boundary_vals = []\n        for ii, (start, stop) in enumerate(self.support.data):\n            if lengths[ii] == 0:\n                continue\n            if first_abscissavals_per_interval[ii] &gt; start:\n                boundary_abscissa_vals.append(start)\n                boundary_vals.append(yvals[:, first_abscissavals_per_interval_idx[ii]])\n                # print('adding {} at abscissa_vals {}'.format(yvals[:,first_abscissavals_per_interval_idx[ii]], start))\n            if last_abscissavals_per_interval[ii] &lt; stop:\n                boundary_abscissa_vals.append(stop)\n                boundary_vals.append(yvals[:, last_abscissavals_per_interval_idx[ii]])\n\n        if boundary_abscissa_vals:\n            insert_locs = np.searchsorted(abscissa_vals, boundary_abscissa_vals)\n            abscissa_vals = np.insert(\n                abscissa_vals, insert_locs, boundary_abscissa_vals\n            )\n            yvals = np.insert(yvals, insert_locs, np.array(boundary_vals).T, axis=1)\n\n            abscissa_vals, unique_idx = np.unique(abscissa_vals, return_index=True)\n            yvals = yvals[:, unique_idx]\n\n        f = interpolate.interp1d(\n            x=abscissa_vals,\n            y=yvals,\n            kind=kind,\n            axis=axis,\n            copy=copy,\n            bounds_error=bounds_error,\n            fill_value=fill_value,\n            assume_sorted=assume_sorted,\n        )\n        return f\n\n    def asarray(\n        self,\n        *,\n        where=None,\n        at=None,\n        kind=\"linear\",\n        copy=True,\n        bounds_error=False,\n        fill_value=np.nan,\n        assume_sorted=None,\n        recalculate=False,\n        store_interp=True,\n        n_samples=None,\n        split_by_interval=False,\n    ):\n        \"\"\"\n        Return a data-like array at requested points, with optional interpolation.\n\n        Parameters\n        ----------\n        where : array_like or tuple, optional\n            Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).\n        at : array_like, optional\n            Array of points to evaluate array at. If None, uses self._abscissa_vals.\n        n_samples : int, optional\n            Number of points to interpolate at, distributed uniformly from support start to stop.\n        split_by_interval : bool, optional\n            If True, separate arrays by intervals and return in a list.\n        kind : str, optional\n            Interpolation method. Default is 'linear'.\n        copy : bool, optional\n            If True, returns a copy. Default is True.\n        bounds_error : bool, optional\n            If True, raises an error for out-of-bounds interpolation. Default is False.\n        fill_value : float, optional\n            Value to use for out-of-bounds points. Default is np.nan.\n        assume_sorted : bool, optional\n            If True, assumes input is sorted. Default is None.\n        recalculate : bool, optional\n            If True, recalculates the interpolation. Default is False.\n        store_interp : bool, optional\n            If True, stores the interpolation object. Default is True.\n\n        Returns\n        -------\n        out : namedtuple (xvals, yvals)\n            xvals: array of abscissa values for which data are returned.\n            yvals: array of shape (n_signals, n_samples) with interpolated data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n        \"\"\"\n\n        # TODO: implement splitting by interval\n\n        if split_by_interval:\n            raise NotImplementedError(\"split_by_interval not yet implemented...\")\n\n        XYArray = namedtuple(\"XYArray\", [\"xvals\", \"yvals\"])\n\n        if (\n            at is None\n            and where is None\n            and split_by_interval is False\n            and n_samples is None\n        ):\n            xyarray = XYArray(self._abscissa_vals, self._data_rowsig.squeeze())\n            return xyarray\n\n        if where is not None:\n            assert at is None and n_samples is None, (\n                \"'where', 'at', and 'n_samples' cannot be used at the same time\"\n            )\n            if isinstance(where, tuple):\n                y = np.array(where[1]).squeeze()\n                x = where[0]\n                assert len(x) == len(y), (\n                    \"'where' condition and array must have same number of elements\"\n                )\n                at = y[x]\n            else:\n                x = np.asanyarray(where).squeeze()\n                assert len(x) == len(self._abscissa_vals), (\n                    \"'where' condition must have same number of elements as self._abscissa_vals\"\n                )\n                at = self._abscissa_vals[x]\n        elif at is not None:\n            assert n_samples is None, (\n                \"'at' and 'n_samples' cannot be used at the same time\"\n            )\n        else:\n            at = np.linspace(self.support.start, self.support.stop, n_samples)\n\n        at = np.atleast_1d(at)\n        if at.ndim &gt; 1:\n            raise ValueError(\"Requested points must be one-dimensional!\")\n        if at.shape[0] == 0:\n            raise ValueError(\"No points were requested to interpolate\")\n\n        # if we made it this far, either at or where has been specified, and at is now well defined.\n\n        kwargs = {\n            \"kind\": kind,\n            \"copy\": copy,\n            \"bounds_error\": bounds_error,\n            \"fill_value\": fill_value,\n            \"assume_sorted\": assume_sorted,\n        }\n\n        # retrieve an existing, or construct a new interpolation object\n        if recalculate:\n            interpobj = self._get_interp1d(**kwargs)\n        else:\n            try:\n                interpobj = self._interp\n                if interpobj is None:\n                    interpobj = self._get_interp1d(**kwargs)\n            except AttributeError:  # does not exist yet\n                interpobj = self._get_interp1d(**kwargs)\n\n        # store interpolation object, if desired\n        if store_interp:\n            self._interp = interpobj\n\n        # do not interpolate points that lie outside the support\n        interval_data = self.support.data[:, :, None]\n        # use broadcasting to check in a vectorized manner if\n        # each sample falls within the support, haha aren't we clever?\n        # (n_intervals, n_requested_samples)\n        valid = np.logical_and(\n            at &gt;= interval_data[:, 0, :], at &lt;= interval_data[:, 1, :]\n        )\n        valid_mask = np.any(valid, axis=0)\n        n_invalid = at.size - np.sum(valid_mask)\n        if n_invalid &gt; 0:\n            logging.warning(\n                \"{} values outside the support were removed\".format(n_invalid)\n            )\n        at = at[valid_mask]\n\n        # do the actual interpolation\n        if self._ordinate.is_wrapping:\n            try:\n                if self.is_wrapped:\n                    out = self._wrap(\n                        interpobj(at),\n                        self._ordinate.range.min,\n                        self._ordinate.range.max,\n                    )\n                else:\n                    out = interpobj(at)\n            except SystemError:\n                interpobj = self._get_interp1d(**kwargs)\n                if store_interp:\n                    self._interp = interpobj\n                if self.is_wrapped:\n                    out = self._wrap(\n                        interpobj(at),\n                        self._ordinate.range.min,\n                        self._ordinate.range.max,\n                    )\n                else:\n                    out = interpobj(at)\n        else:\n            try:\n                out = interpobj(at)\n            except SystemError:\n                interpobj = self._get_interp1d(**kwargs)\n                if store_interp:\n                    self._interp = interpobj\n                out = interpobj(at)\n\n        xyarray = XYArray(xvals=np.asanyarray(at), yvals=np.asanyarray(out))\n        return xyarray\n\n    def subsample(self, *, fs):\n        \"\"\"Subsamples a RegularlySampledAnalogSignalArray\n\n        WARNING! Aliasing can occur! It is better to use downsample when\n        lowering the sampling rate substantially.\n\n        Parameters\n        ----------\n        fs : float, optional\n            Desired output sampling rate, in Hz\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n            new subset of points.\n        \"\"\"\n\n        return self.simplify(ds=1 / fs)\n\n    def simplify(self, *, ds=None, n_samples=None, **kwargs):\n        \"\"\"Returns an RegularlySampledAnalogSignalArray where the data has been\n        simplified / subsampled.\n\n        This function is primarily intended to be used for plotting and\n        saving vector graphics without having too large file sizes as\n        a result of too many points.\n\n        Irrespective of whether 'ds' or 'n_samples' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_samples or ds to be violated.\n\n        WARNING! Simplify can create nan samples, when requesting a timestamp\n        within an interval, but outside of the (first, last) abscissa_vals within that\n        interval, since we don't extrapolate, but only interpolate. # TODO: fix\n\n        Parameters\n        ----------\n        ds : float, optional\n            Time (in seconds), in which to step points.\n        n_samples : int, optional\n            Number of points at which to intepolate data. If ds is None\n            and n_samples is None, then default is to use n_samples=5,000\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n            new subset of points.\n        \"\"\"\n\n        if self.isempty:\n            return self\n\n        # legacy kwarg support:\n        n_points = kwargs.pop(\"n_points\", False)\n        if n_points:\n            n_samples = n_points\n\n        if ds is not None and n_samples is not None:\n            raise ValueError(\"ds and n_samples cannot be used together\")\n\n        if n_samples is not None:\n            assert float(n_samples).is_integer(), (\n                \"n_samples must be a positive integer!\"\n            )\n            assert n_samples &gt; 1, \"n_samples must be a positive integer &gt; 1\"\n            # determine ds from number of desired points:\n            ds = self.support.length / (n_samples - 1)\n\n        if ds is None:\n            # neither n_samples nor ds was specified, so assume defaults:\n            n_samples = np.min((5000, 250 + self.n_samples // 2, self.n_samples))\n            ds = self.support.length / (n_samples - 1)\n\n        # build list of points at which to evaluate the RegularlySampledAnalogSignalArray\n\n        # we exclude all empty intervals:\n        at = []\n        lengths = self.lengths\n        empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n        first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n        first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n        last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        first_abscissavals_per_interval = self._abscissa_vals[\n            first_abscissavals_per_interval_idx\n        ]\n        last_abscissavals_per_interval = self._abscissa_vals[\n            last_abscissavals_per_interval_idx\n        ]\n\n        for ii, (start, stop) in enumerate(self.support.data):\n            if lengths[ii] == 0:\n                continue\n            newxvals = utils.frange(\n                first_abscissavals_per_interval[ii],\n                last_abscissavals_per_interval[ii],\n                step=ds,\n            ).tolist()\n            at.extend(newxvals)\n            try:\n                if newxvals[-1] &lt; last_abscissavals_per_interval[ii]:\n                    at.append(last_abscissavals_per_interval[ii])\n            except IndexError:\n                at.append(first_abscissavals_per_interval[ii])\n                at.append(last_abscissavals_per_interval[ii])\n\n        _, yvals = self.asarray(at=at, recalculate=True, store_interp=False)\n        yvals = np.array(yvals, ndmin=2)\n\n        asa = self.copy()\n        asa._abscissa_vals = np.asanyarray(at)\n        asa._data = yvals\n        asa._fs = 1 / ds\n\n        return asa\n\n    def join(self, other, *, mode=None, inplace=False):\n        \"\"\"Join another RegularlySampledAnalogSignalArray to this one.\n\n        WARNING! Numerical precision might cause some epochs to be considered\n        non-disjoint even when they really are, so a better check than ep1[ep2].isempty\n        is to check for samples contained in the intersection of ep1 and ep2.\n\n        Parameters\n        ----------\n        other : RegularlySampledAnalogSignalArray\n            RegularlySampledAnalogSignalArray (or derived type) to join to the current\n            RegularlySampledAnalogSignalArray. Other must have the same number of signals as\n            the current RegularlySampledAnalogSignalArray.\n        mode : string, optional\n            One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the\n            signals are merged inside overlapping intervals. Default is 'left'.\n        inplace : boolean, optional\n            If True, then current RegularlySampledAnalogSignalArray is modified. If False, then\n            a copy with the joined result is returned. Default is False.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been\n            joined to the current RegularlySampledAnalogSignalArray.\n        \"\"\"\n\n        if mode is None:\n            mode = \"left\"\n\n        asa = self.copy()  # copy without data since we change data at the end?\n\n        times = np.zeros((1, 0))\n        data = np.zeros((asa.n_signals, 0))\n\n        # if ASAs are disjoint:\n        if not self.support[other.support].length &gt; 50 * float_info.epsilon:\n            # do a simple-as-butter join (concat) and sort\n            times = np.append(times, self._abscissa_vals)\n            data = np.hstack((data, self.data))\n            times = np.append(times, other._abscissa_vals)\n            data = np.hstack((data, other.data))\n        else:  # not disjoint\n            both_eps = self.support[other.support]\n            self_eps = self.support - both_eps - other.support\n            other_eps = other.support - both_eps - self.support\n\n            if mode == \"left\":\n                self_eps += both_eps\n                # print(self_eps)\n\n                tmp = self[self_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n\n                if not other_eps.isempty:\n                    tmp = other[other_eps]\n                    times = np.append(times, tmp._abscissa_vals)\n                    data = np.hstack((data, tmp.data))\n            elif mode == \"right\":\n                other_eps += both_eps\n\n                tmp = other[other_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n\n                if not self_eps.isempty:\n                    tmp = self[self_eps]\n                    times = np.append(times, tmp._abscissa_vals)\n                    data = np.hstack((data, tmp.data))\n            else:\n                raise NotImplementedError(\n                    \"asa.join() has not yet been implemented for mode '{}'!\".format(\n                        mode\n                    )\n                )\n\n        sample_order = np.argsort(times)\n        times = times[sample_order]\n        data = data[:, sample_order]\n\n        asa._data = data\n        asa._abscissa_vals = times\n        dom1 = self.domain\n        dom2 = other.domain\n        asa._abscissa.support = (self.support + other.support).merge()\n        asa._abscissa.support.domain = (dom1 + dom2).merge()\n        return asa\n\n    def _pdf(self, bins=None, n_samples=None):\n        \"\"\"Return the probability distribution function for each signal.\"\"\"\n        from scipy import integrate\n\n        if bins is None:\n            bins = 100\n\n        if n_samples is None:\n            n_samples = 100\n\n        if self.n_signals &gt; 1:\n            raise NotImplementedError(\"multiple signals not supported yet!\")\n\n        # fx, bins = np.histogram(self.data.squeeze(), bins=bins, normed=True)\n        fx, bins = np.histogram(self.data.squeeze(), bins=bins)\n        bin_centers = (bins + (bins[1] - bins[0]) / 2)[:-1]\n\n        Ifx = integrate.simps(fx, bin_centers)\n\n        pdf = type(self)(\n            abscissa_vals=bin_centers,\n            data=fx / Ifx,\n            fs=1 / (bin_centers[1] - bin_centers[0]),\n            support=type(self.support)(self.data.min(), self.data.max()),\n        ).simplify(n_samples=n_samples)\n\n        return pdf\n\n        # data = []\n        # for signal in self.data:\n        #     fx, bins = np.histogram(signal, bins=bins)\n        #     bin_centers = (bins + (bins[1]-bins[0])/2)[:-1]\n\n    def _cdf(self, n_samples=None):\n        \"\"\"Return the probability distribution function for each signal.\"\"\"\n\n        if n_samples is None:\n            n_samples = 100\n\n        if self.n_signals &gt; 1:\n            raise NotImplementedError(\"multiple signals not supported yet!\")\n\n        X = np.sort(self.data.squeeze())\n        F = np.array(range(self.n_samples)) / float(self.n_samples)\n\n        logging.disable(logging.CRITICAL)\n        cdf = type(self)(\n            abscissa_vals=X,\n            data=F,\n            support=type(self.support)(self.data.min(), self.data.max()),\n        ).simplify(n_samples=n_samples)\n        logging.disable(0)\n\n        return cdf\n\n    def _eegplot(self, ax=None, normalize=False, pad=None, fill=True, color=None):\n        \"\"\"custom_func docstring goes here.\"\"\"\n\n        import matplotlib.pyplot as plt\n\n        from ..plotting import utils as plotutils\n\n        if ax is None:\n            ax = plt.gca()\n\n        xmin = self.support.min\n        xmax = self.support.max\n        xvals = self._abscissa_vals\n\n        if pad is None:\n            pad = np.mean(self.data) / 2\n\n        data = self.data.copy()\n\n        if normalize:\n            peak_vals = self.max()\n            data = (data.T / peak_vals).T\n\n        n_traces = self.n_signals\n\n        for tt, trace in enumerate(data):\n            if color is None:\n                line = ax.plot(\n                    xvals, tt * pad + trace, zorder=int(10 + 2 * n_traces - 2 * tt)\n                )\n            else:\n                line = ax.plot(\n                    xvals,\n                    tt * pad + trace,\n                    zorder=int(10 + 2 * n_traces - 2 * tt),\n                    color=color,\n                )\n            if fill:\n                # Get the color from the current curve\n                fillcolor = line[0].get_color()\n                ax.fill_between(\n                    xvals,\n                    tt * pad,\n                    tt * pad + trace,\n                    alpha=0.3,\n                    color=fillcolor,\n                    zorder=int(10 + 2 * n_traces - 2 * tt - 1),\n                )\n\n        ax.set_xlim(xmin, xmax)\n        if pad != 0:\n            # yticks = np.arange(n_traces)*pad + 0.5*pad\n            yticks = []\n            ax.set_yticks(yticks)\n            ax.set_xlabel(self._abscissa.label)\n            ax.set_ylabel(self._ordinate.label)\n            plotutils.no_yticks(ax)\n            plotutils.clear_left(ax)\n\n        plotutils.clear_top(ax)\n        plotutils.clear_right(ax)\n\n        return ax\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.abs","title":"<code>abs</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with absolute value of (potentially complex) data.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.abscissa_vals","title":"<code>abscissa_vals</code>  <code>property</code> <code>writable</code>","text":"<p>(np.array 1D) Time in seconds.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.angle","title":"<code>angle</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only phase angle (in radians) of data.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.base_unit","title":"<code>base_unit</code>  <code>property</code>","text":"<p>Base unit of the abscissa.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>(np.array N-Dimensional) data with shape (n_signals, n_samples).</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.fs","title":"<code>fs</code>  <code>property</code>","text":"<p>(float) Sampling frequency.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.imag","title":"<code>imag</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only imaginary part of data.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.iscomplex","title":"<code>iscomplex</code>  <code>property</code>","text":"<p>Returns True if any part of the signal is complex.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) checks length of data input</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.isreal","title":"<code>isreal</code>  <code>property</code>","text":"<p>Returns True if entire signal is real.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.labels","title":"<code>labels</code>  <code>property</code>","text":"<p>(list) The labels corresponding to each signal.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>(list) The number of samples in each interval.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.n_bytes","title":"<code>n_bytes</code>  <code>property</code>","text":"<p>Approximate number of bytes taken up by object.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.n_intervals","title":"<code>n_intervals</code>  <code>property</code>","text":"<p>(int) number of intervals in RegularlySampledAnalogSignalArray</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.n_samples","title":"<code>n_samples</code>  <code>property</code>","text":"<p>(int) number of abscissa samples where signal is defined.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.n_signals","title":"<code>n_signals</code>  <code>property</code>","text":"<p>(int) The number of signals.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.range","title":"<code>range</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.real","title":"<code>real</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only real part of data.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.signals","title":"<code>signals</code>  <code>property</code>","text":"<p>Returns a list of RegularlySampledAnalogSignalArrays, each array containing a single signal (channel).</p> <p>WARNING: this method creates a copy of each signal, so is not particularly efficient at this time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for channel in lfp.signals:\n    print(channel)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.step","title":"<code>step</code>  <code>property</code>","text":"<p>steps per sample Example 1: sample_numbers = np.array([1,2,3,4,5,6]) #aka time Steps per sample in the above case would be 1</p> <p>Example 2: sample_numbers = np.array([1,3,5,7,9]) #aka time Steps per sample in Example 2 would be 2</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.add_signal","title":"<code>add_signal(signal, label=None)</code>","text":"<p>Docstring goes here. Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def add_signal(self, signal, label=None):\n    \"\"\"Docstring goes here.\n    Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?\n    \"\"\"\n    # TODO: add functionality to check that supports are the same, etc.\n    if isinstance(signal, RegularlySampledAnalogSignalArray):\n        signal = signal.data\n\n    signal = np.squeeze(signal)\n    if signal.ndim &gt; 1:\n        raise TypeError(\"Can only add one signal at a time!\")\n    if self.data.ndim == 1:\n        self._data = np.vstack(\n            [np.array(self.data, ndmin=2), np.array(signal, ndmin=2)]\n        )\n    else:\n        self._data = np.vstack([self.data, np.array(signal, ndmin=2)])\n    if label is None:\n        logging.warning(\"None label appended\")\n    self._labels = np.append(self._labels, label)\n    return self\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.asarray","title":"<code>asarray(*, where=None, at=None, kind='linear', copy=True, bounds_error=False, fill_value=np.nan, assume_sorted=None, recalculate=False, store_interp=True, n_samples=None, split_by_interval=False)</code>","text":"<p>Return a data-like array at requested points, with optional interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>array_like or tuple</code> <p>Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).</p> <code>None</code> <code>at</code> <code>array_like</code> <p>Array of points to evaluate array at. If None, uses self._abscissa_vals.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of points to interpolate at, distributed uniformly from support start to stop.</p> <code>None</code> <code>split_by_interval</code> <code>bool</code> <p>If True, separate arrays by intervals and return in a list.</p> <code>False</code> <code>kind</code> <code>str</code> <p>Interpolation method. Default is 'linear'.</p> <code>'linear'</code> <code>copy</code> <code>bool</code> <p>If True, returns a copy. Default is True.</p> <code>True</code> <code>bounds_error</code> <code>bool</code> <p>If True, raises an error for out-of-bounds interpolation. Default is False.</p> <code>False</code> <code>fill_value</code> <code>float</code> <p>Value to use for out-of-bounds points. Default is np.nan.</p> <code>nan</code> <code>assume_sorted</code> <code>bool</code> <p>If True, assumes input is sorted. Default is None.</p> <code>None</code> <code>recalculate</code> <code>bool</code> <p>If True, recalculates the interpolation. Default is False.</p> <code>False</code> <code>store_interp</code> <code>bool</code> <p>If True, stores the interpolation object. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>out</code> <code>namedtuple(xvals, yvals)</code> <p>xvals: array of abscissa values for which data are returned. yvals: array of shape (n_signals, n_samples) with interpolated data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def asarray(\n    self,\n    *,\n    where=None,\n    at=None,\n    kind=\"linear\",\n    copy=True,\n    bounds_error=False,\n    fill_value=np.nan,\n    assume_sorted=None,\n    recalculate=False,\n    store_interp=True,\n    n_samples=None,\n    split_by_interval=False,\n):\n    \"\"\"\n    Return a data-like array at requested points, with optional interpolation.\n\n    Parameters\n    ----------\n    where : array_like or tuple, optional\n        Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).\n    at : array_like, optional\n        Array of points to evaluate array at. If None, uses self._abscissa_vals.\n    n_samples : int, optional\n        Number of points to interpolate at, distributed uniformly from support start to stop.\n    split_by_interval : bool, optional\n        If True, separate arrays by intervals and return in a list.\n    kind : str, optional\n        Interpolation method. Default is 'linear'.\n    copy : bool, optional\n        If True, returns a copy. Default is True.\n    bounds_error : bool, optional\n        If True, raises an error for out-of-bounds interpolation. Default is False.\n    fill_value : float, optional\n        Value to use for out-of-bounds points. Default is np.nan.\n    assume_sorted : bool, optional\n        If True, assumes input is sorted. Default is None.\n    recalculate : bool, optional\n        If True, recalculates the interpolation. Default is False.\n    store_interp : bool, optional\n        If True, stores the interpolation object. Default is True.\n\n    Returns\n    -------\n    out : namedtuple (xvals, yvals)\n        xvals: array of abscissa values for which data are returned.\n        yvals: array of shape (n_signals, n_samples) with interpolated data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n    \"\"\"\n\n    # TODO: implement splitting by interval\n\n    if split_by_interval:\n        raise NotImplementedError(\"split_by_interval not yet implemented...\")\n\n    XYArray = namedtuple(\"XYArray\", [\"xvals\", \"yvals\"])\n\n    if (\n        at is None\n        and where is None\n        and split_by_interval is False\n        and n_samples is None\n    ):\n        xyarray = XYArray(self._abscissa_vals, self._data_rowsig.squeeze())\n        return xyarray\n\n    if where is not None:\n        assert at is None and n_samples is None, (\n            \"'where', 'at', and 'n_samples' cannot be used at the same time\"\n        )\n        if isinstance(where, tuple):\n            y = np.array(where[1]).squeeze()\n            x = where[0]\n            assert len(x) == len(y), (\n                \"'where' condition and array must have same number of elements\"\n            )\n            at = y[x]\n        else:\n            x = np.asanyarray(where).squeeze()\n            assert len(x) == len(self._abscissa_vals), (\n                \"'where' condition must have same number of elements as self._abscissa_vals\"\n            )\n            at = self._abscissa_vals[x]\n    elif at is not None:\n        assert n_samples is None, (\n            \"'at' and 'n_samples' cannot be used at the same time\"\n        )\n    else:\n        at = np.linspace(self.support.start, self.support.stop, n_samples)\n\n    at = np.atleast_1d(at)\n    if at.ndim &gt; 1:\n        raise ValueError(\"Requested points must be one-dimensional!\")\n    if at.shape[0] == 0:\n        raise ValueError(\"No points were requested to interpolate\")\n\n    # if we made it this far, either at or where has been specified, and at is now well defined.\n\n    kwargs = {\n        \"kind\": kind,\n        \"copy\": copy,\n        \"bounds_error\": bounds_error,\n        \"fill_value\": fill_value,\n        \"assume_sorted\": assume_sorted,\n    }\n\n    # retrieve an existing, or construct a new interpolation object\n    if recalculate:\n        interpobj = self._get_interp1d(**kwargs)\n    else:\n        try:\n            interpobj = self._interp\n            if interpobj is None:\n                interpobj = self._get_interp1d(**kwargs)\n        except AttributeError:  # does not exist yet\n            interpobj = self._get_interp1d(**kwargs)\n\n    # store interpolation object, if desired\n    if store_interp:\n        self._interp = interpobj\n\n    # do not interpolate points that lie outside the support\n    interval_data = self.support.data[:, :, None]\n    # use broadcasting to check in a vectorized manner if\n    # each sample falls within the support, haha aren't we clever?\n    # (n_intervals, n_requested_samples)\n    valid = np.logical_and(\n        at &gt;= interval_data[:, 0, :], at &lt;= interval_data[:, 1, :]\n    )\n    valid_mask = np.any(valid, axis=0)\n    n_invalid = at.size - np.sum(valid_mask)\n    if n_invalid &gt; 0:\n        logging.warning(\n            \"{} values outside the support were removed\".format(n_invalid)\n        )\n    at = at[valid_mask]\n\n    # do the actual interpolation\n    if self._ordinate.is_wrapping:\n        try:\n            if self.is_wrapped:\n                out = self._wrap(\n                    interpobj(at),\n                    self._ordinate.range.min,\n                    self._ordinate.range.max,\n                )\n            else:\n                out = interpobj(at)\n        except SystemError:\n            interpobj = self._get_interp1d(**kwargs)\n            if store_interp:\n                self._interp = interpobj\n            if self.is_wrapped:\n                out = self._wrap(\n                    interpobj(at),\n                    self._ordinate.range.min,\n                    self._ordinate.range.max,\n                )\n            else:\n                out = interpobj(at)\n    else:\n        try:\n            out = interpobj(at)\n        except SystemError:\n            interpobj = self._get_interp1d(**kwargs)\n            if store_interp:\n                self._interp = interpobj\n            out = interpobj(at)\n\n    xyarray = XYArray(xvals=np.asanyarray(at), yvals=np.asanyarray(out))\n    return xyarray\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.center","title":"<code>center(inplace=False)</code>","text":"<p>Center the data to have zero mean along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The centered signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; centered = asa.center()\n&gt;&gt;&gt; centered.mean()\n0.0\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def center(self, inplace=False):\n    \"\"\"\n    Center the data to have zero mean along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The centered signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; centered = asa.center()\n    &gt;&gt;&gt; centered.mean()\n    0.0\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.clip","title":"<code>clip(min, max)</code>","text":"<p>Clip (limit) the values in the data to the interval [min, max].</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float</code> <p>Minimum value.</p> required <code>max</code> <code>float</code> <p>Maximum value.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>New object with clipped data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clipped = asa.clip(-1, 1)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def clip(self, min, max):\n    \"\"\"\n    Clip (limit) the values in the data to the interval [min, max].\n\n    Parameters\n    ----------\n    min : float\n        Minimum value.\n    max : float\n        Maximum value.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        New object with clipped data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; clipped = asa.clip(-1, 1)\n    \"\"\"\n    out = self.copy()\n    out._data = np.clip(self.data, min, max)\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the current object.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def copy(self):\n    \"\"\"Return a copy of the current object.\"\"\"\n    out = copy.deepcopy(self)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.ddt","title":"<code>ddt(rectify=False)</code>","text":"<p>Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.</p> <p>asa.data = f(t) asa.ddt = d/dt (asa.data)</p> <p>Parameters:</p> Name Type Description Default <code>rectify</code> <code>boolean</code> <p>If True, the absolute value of the derivative will be returned. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ddt</code> <code>RegularlySampledAnalogSignalArray</code> <p>Time derivative of each signal in the RegularlySampledAnalogSignalArray.</p> Note <p>Second order central differences are used here, and it is assumed that the signals are sampled uniformly. If the signals are not uniformly sampled, it is recommended to resample the signal before computing the derivative.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def ddt(self, rectify=False):\n    \"\"\"Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n    asa.data = f(t)\n    asa.ddt = d/dt (asa.data)\n\n    Parameters\n    ----------\n    rectify : boolean, optional\n        If True, the absolute value of the derivative will be returned.\n        Default is False.\n\n    Returns\n    -------\n    ddt : RegularlySampledAnalogSignalArray\n        Time derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n    Note\n    ----\n    Second order central differences are used here, and it is assumed that\n    the signals are sampled uniformly. If the signals are not uniformly\n    sampled, it is recommended to resample the signal before computing the\n    derivative.\n    \"\"\"\n    ddt = utils.ddt_asa(self, rectify=rectify)\n    return ddt\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.downsample","title":"<code>downsample(*, fs_out, aafilter=True, inplace=False, **kwargs)</code>","text":"<p>Downsamples the RegularlySampledAnalogSignalArray</p> <p>Parameters:</p> Name Type Description Default <code>fs_out</code> <code>float</code> <p>Desired output sampling rate in Hz</p> required <code>aafilter</code> <code>boolean</code> <p>Whether to apply an anti-aliasing filter before performing the actual downsampling. Default is True</p> <code>True</code> <code>inplace</code> <code>boolean</code> <p>If True, the output ASA will replace the input ASA. Default is False</p> <code>False</code> <code>kwargs</code> <p>Other keyword arguments are passed to sosfiltfilt() in the <code>filtering</code> module</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The downsampled RegularlySampledAnalogSignalArray</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def downsample(self, *, fs_out, aafilter=True, inplace=False, **kwargs):\n    \"\"\"Downsamples the RegularlySampledAnalogSignalArray\n\n    Parameters\n    ----------\n    fs_out : float, optional\n        Desired output sampling rate in Hz\n    aafilter : boolean, optional\n        Whether to apply an anti-aliasing filter before performing the actual\n        downsampling. Default is True\n    inplace : boolean, optional\n        If True, the output ASA will replace the input ASA. Default is False\n    kwargs :\n        Other keyword arguments are passed to sosfiltfilt() in the `filtering`\n        module\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The downsampled RegularlySampledAnalogSignalArray\n    \"\"\"\n\n    if not fs_out &lt; self._fs:\n        raise ValueError(\"fs_out must be less than current sampling rate!\")\n\n    if aafilter:\n        fh = fs_out / 2.0\n        out = filtering.sosfiltfilt(self, fl=None, fh=fh, inplace=inplace, **kwargs)\n\n    downsampled = out.simplify(ds=1 / fs_out)\n    out._data = downsampled._data\n    out._abscissa_vals = downsampled._abscissa_vals\n    out._fs = fs_out\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.empty","title":"<code>empty(inplace=True)</code>","text":"<p>Remove data (but not metadata) from RegularlySampledAnalogSignalArray.</p> <p>Attributes 'data', 'abscissa_vals', and 'support' are all emptied.</p> <p>Note: n_signals is preserved.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def empty(self, inplace=True):\n    \"\"\"Remove data (but not metadata) from RegularlySampledAnalogSignalArray.\n\n    Attributes 'data', 'abscissa_vals', and 'support' are all emptied.\n\n    Note: n_signals is preserved.\n    \"\"\"\n    n_signals = self.n_signals\n    if not inplace:\n        out = self._copy_without_data()\n    else:\n        out = self\n        out._data = np.zeros((n_signals, 0))\n    out._abscissa.support = type(self.support)(empty=True)\n    out._abscissa_vals = []\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.join","title":"<code>join(other, *, mode=None, inplace=False)</code>","text":"<p>Join another RegularlySampledAnalogSignalArray to this one.</p> <p>WARNING! Numerical precision might cause some epochs to be considered non-disjoint even when they really are, so a better check than ep1[ep2].isempty is to check for samples contained in the intersection of ep1 and ep2.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RegularlySampledAnalogSignalArray</code> <p>RegularlySampledAnalogSignalArray (or derived type) to join to the current RegularlySampledAnalogSignalArray. Other must have the same number of signals as the current RegularlySampledAnalogSignalArray.</p> required <code>mode</code> <code>string</code> <p>One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the signals are merged inside overlapping intervals. Default is 'left'.</p> <code>None</code> <code>inplace</code> <code>boolean</code> <p>If True, then current RegularlySampledAnalogSignalArray is modified. If False, then a copy with the joined result is returned. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been joined to the current RegularlySampledAnalogSignalArray.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def join(self, other, *, mode=None, inplace=False):\n    \"\"\"Join another RegularlySampledAnalogSignalArray to this one.\n\n    WARNING! Numerical precision might cause some epochs to be considered\n    non-disjoint even when they really are, so a better check than ep1[ep2].isempty\n    is to check for samples contained in the intersection of ep1 and ep2.\n\n    Parameters\n    ----------\n    other : RegularlySampledAnalogSignalArray\n        RegularlySampledAnalogSignalArray (or derived type) to join to the current\n        RegularlySampledAnalogSignalArray. Other must have the same number of signals as\n        the current RegularlySampledAnalogSignalArray.\n    mode : string, optional\n        One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the\n        signals are merged inside overlapping intervals. Default is 'left'.\n    inplace : boolean, optional\n        If True, then current RegularlySampledAnalogSignalArray is modified. If False, then\n        a copy with the joined result is returned. Default is False.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been\n        joined to the current RegularlySampledAnalogSignalArray.\n    \"\"\"\n\n    if mode is None:\n        mode = \"left\"\n\n    asa = self.copy()  # copy without data since we change data at the end?\n\n    times = np.zeros((1, 0))\n    data = np.zeros((asa.n_signals, 0))\n\n    # if ASAs are disjoint:\n    if not self.support[other.support].length &gt; 50 * float_info.epsilon:\n        # do a simple-as-butter join (concat) and sort\n        times = np.append(times, self._abscissa_vals)\n        data = np.hstack((data, self.data))\n        times = np.append(times, other._abscissa_vals)\n        data = np.hstack((data, other.data))\n    else:  # not disjoint\n        both_eps = self.support[other.support]\n        self_eps = self.support - both_eps - other.support\n        other_eps = other.support - both_eps - self.support\n\n        if mode == \"left\":\n            self_eps += both_eps\n            # print(self_eps)\n\n            tmp = self[self_eps]\n            times = np.append(times, tmp._abscissa_vals)\n            data = np.hstack((data, tmp.data))\n\n            if not other_eps.isempty:\n                tmp = other[other_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n        elif mode == \"right\":\n            other_eps += both_eps\n\n            tmp = other[other_eps]\n            times = np.append(times, tmp._abscissa_vals)\n            data = np.hstack((data, tmp.data))\n\n            if not self_eps.isempty:\n                tmp = self[self_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n        else:\n            raise NotImplementedError(\n                \"asa.join() has not yet been implemented for mode '{}'!\".format(\n                    mode\n                )\n            )\n\n    sample_order = np.argsort(times)\n    times = times[sample_order]\n    data = data[:, sample_order]\n\n    asa._data = data\n    asa._abscissa_vals = times\n    dom1 = self.domain\n    dom2 = other.domain\n    asa._abscissa.support = (self.support + other.support).merge()\n    asa._abscissa.support.domain = (dom1 + dom2).merge()\n    return asa\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.max","title":"<code>max(*, axis=1)</code>","text":"<p>Compute the maximum value of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the maximum (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>max</code> <code>ndarray</code> <p>Maximum values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def max(self, *, axis=1):\n    \"\"\"\n    Compute the maximum value of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the maximum (default is 1).\n\n    Returns\n    -------\n    max : np.ndarray\n        Maximum values along the specified axis.\n    \"\"\"\n    try:\n        maxes = np.amax(self.data, axis=axis).squeeze()\n        if maxes.size == 1:\n            return maxes.item()\n        return maxes\n    except ValueError:\n        raise ValueError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate maximum\"\n        )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.mean","title":"<code>mean(*, axis=1)</code>","text":"<p>Compute the mean of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the mean (default is 1, i.e., across samples).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>mean</code> <code>ndarray</code> <p>Mean values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def mean(self, *, axis=1):\n    \"\"\"\n    Compute the mean of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the mean (default is 1, i.e., across samples).\n\n    Returns\n    -------\n    mean : np.ndarray\n        Mean values along the specified axis.\n    \"\"\"\n    try:\n        means = np.nanmean(self.data, axis=axis).squeeze()\n        if means.size == 1:\n            return means.item()\n        return means\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate mean\"\n        )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.median","title":"<code>median(*, axis=1)</code>","text":"<p>Returns the median of each signal in RegularlySampledAnalogSignalArray.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def median(self, *, axis=1):\n    \"\"\"Returns the median of each signal in RegularlySampledAnalogSignalArray.\"\"\"\n    try:\n        medians = np.nanmedian(self.data, axis=axis).squeeze()\n        if medians.size == 1:\n            return medians.item()\n        return medians\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate median\"\n        )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.min","title":"<code>min(*, axis=1)</code>","text":"<p>Compute the minimum value of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the minimum (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>min</code> <code>ndarray</code> <p>Minimum values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def min(self, *, axis=1):\n    \"\"\"\n    Compute the minimum value of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the minimum (default is 1).\n\n    Returns\n    -------\n    min : np.ndarray\n        Minimum values along the specified axis.\n    \"\"\"\n    try:\n        mins = np.amin(self.data, axis=axis).squeeze()\n        if mins.size == 1:\n            return mins.item()\n        return mins\n    except ValueError:\n        raise ValueError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate minimum\"\n        )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.normalize","title":"<code>normalize(inplace=False)</code>","text":"<p>Normalize the data to have unit standard deviation along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The normalized signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalized = asa.normalize()\n&gt;&gt;&gt; normalized.std()\n1.0\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def normalize(self, inplace=False):\n    \"\"\"\n    Normalize the data to have unit standard deviation along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The normalized signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; normalized = asa.normalize()\n    &gt;&gt;&gt; normalized.std()\n    1.0\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    std = np.atleast_1d(out.std())\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns an RegularlySampledAnalogSignalArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>RegularlySampledAnalogSignalArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_samples or ds to be violated.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns an RegularlySampledAnalogSignalArray whose support has been\n    partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_samples : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        RegularlySampledAnalogSignalArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_samples or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    out._abscissa.support = out.support.partition(ds=ds, n_intervals=n_intervals)\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.simplify","title":"<code>simplify(*, ds=None, n_samples=None, **kwargs)</code>","text":"<p>Returns an RegularlySampledAnalogSignalArray where the data has been simplified / subsampled.</p> <p>This function is primarily intended to be used for plotting and saving vector graphics without having too large file sizes as a result of too many points.</p> <p>Irrespective of whether 'ds' or 'n_samples' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_samples or ds to be violated.</p> <p>WARNING! Simplify can create nan samples, when requesting a timestamp within an interval, but outside of the (first, last) abscissa_vals within that interval, since we don't extrapolate, but only interpolate. # TODO: fix</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Time (in seconds), in which to step points.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of points at which to intepolate data. If ds is None and n_samples is None, then default is to use n_samples=5,000</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where data is only stored at the new subset of points.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def simplify(self, *, ds=None, n_samples=None, **kwargs):\n    \"\"\"Returns an RegularlySampledAnalogSignalArray where the data has been\n    simplified / subsampled.\n\n    This function is primarily intended to be used for plotting and\n    saving vector graphics without having too large file sizes as\n    a result of too many points.\n\n    Irrespective of whether 'ds' or 'n_samples' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_samples or ds to be violated.\n\n    WARNING! Simplify can create nan samples, when requesting a timestamp\n    within an interval, but outside of the (first, last) abscissa_vals within that\n    interval, since we don't extrapolate, but only interpolate. # TODO: fix\n\n    Parameters\n    ----------\n    ds : float, optional\n        Time (in seconds), in which to step points.\n    n_samples : int, optional\n        Number of points at which to intepolate data. If ds is None\n        and n_samples is None, then default is to use n_samples=5,000\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n        new subset of points.\n    \"\"\"\n\n    if self.isempty:\n        return self\n\n    # legacy kwarg support:\n    n_points = kwargs.pop(\"n_points\", False)\n    if n_points:\n        n_samples = n_points\n\n    if ds is not None and n_samples is not None:\n        raise ValueError(\"ds and n_samples cannot be used together\")\n\n    if n_samples is not None:\n        assert float(n_samples).is_integer(), (\n            \"n_samples must be a positive integer!\"\n        )\n        assert n_samples &gt; 1, \"n_samples must be a positive integer &gt; 1\"\n        # determine ds from number of desired points:\n        ds = self.support.length / (n_samples - 1)\n\n    if ds is None:\n        # neither n_samples nor ds was specified, so assume defaults:\n        n_samples = np.min((5000, 250 + self.n_samples // 2, self.n_samples))\n        ds = self.support.length / (n_samples - 1)\n\n    # build list of points at which to evaluate the RegularlySampledAnalogSignalArray\n\n    # we exclude all empty intervals:\n    at = []\n    lengths = self.lengths\n    empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n    first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n    first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n    last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n    last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n    first_abscissavals_per_interval = self._abscissa_vals[\n        first_abscissavals_per_interval_idx\n    ]\n    last_abscissavals_per_interval = self._abscissa_vals[\n        last_abscissavals_per_interval_idx\n    ]\n\n    for ii, (start, stop) in enumerate(self.support.data):\n        if lengths[ii] == 0:\n            continue\n        newxvals = utils.frange(\n            first_abscissavals_per_interval[ii],\n            last_abscissavals_per_interval[ii],\n            step=ds,\n        ).tolist()\n        at.extend(newxvals)\n        try:\n            if newxvals[-1] &lt; last_abscissavals_per_interval[ii]:\n                at.append(last_abscissavals_per_interval[ii])\n        except IndexError:\n            at.append(first_abscissavals_per_interval[ii])\n            at.append(last_abscissavals_per_interval[ii])\n\n    _, yvals = self.asarray(at=at, recalculate=True, store_interp=False)\n    yvals = np.array(yvals, ndmin=2)\n\n    asa = self.copy()\n    asa._abscissa_vals = np.asanyarray(at)\n    asa._data = yvals\n    asa._fs = 1 / ds\n\n    return asa\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.smooth","title":"<code>smooth(*, fs=None, sigma=None, truncate=None, inplace=False, mode=None, cval=None, within_intervals=False)</code>","text":"<p>Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.</p> <p>Smoothing is applied along the abscissa, and the same smoothing is applied to each signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.</p> <p>Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.</code> required <code>fs</code> <code>float</code> <p>Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will be inferred.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05 (50 ms if base_unit=seconds).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <code>mode</code> <code>(reflect, constant, nearest, mirror, wrap)</code> <p>The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to 'constant'. Default is 'reflect'.</p> <code>'reflect'</code> <code>cval</code> <code>scalar</code> <p>Value to fill past edges of input if mode is 'constant'. Default is 0.0.</p> <code>None</code> <code>within_intervals</code> <code>boolean</code> <p>If True, then smooth within each epoch. Otherwise smooth across epochs. Default is False. Note that when mode = 'wrap', then smoothing within epochs aren't affected by wrapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as obj</code> <p>An object with smoothed data is returned.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(\n    self,\n    *,\n    fs=None,\n    sigma=None,\n    truncate=None,\n    inplace=False,\n    mode=None,\n    cval=None,\n    within_intervals=False,\n):\n    \"\"\"Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.\n\n    Smoothing is applied along the abscissa, and the same smoothing is applied to each\n    signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n    Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n    Parameters\n    ----------\n    obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n    fs : float, optional\n        Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n        be inferred.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n        (50 ms if base_unit=seconds).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0.\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        'reflect'.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n    within_intervals : boolean, optional\n        If True, then smooth within each epoch. Otherwise smooth across epochs.\n        Default is False.\n        Note that when mode = 'wrap', then smoothing within epochs aren't affected\n        by wrapping.\n\n    Returns\n    -------\n    out : same type as obj\n        An object with smoothed data is returned.\n\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.05\n    if truncate is None:\n        truncate = 4\n\n    kwargs = {\n        \"inplace\": inplace,\n        \"fs\": fs,\n        \"sigma\": sigma,\n        \"truncate\": truncate,\n        \"mode\": mode,\n        \"cval\": cval,\n        \"within_intervals\": within_intervals,\n    }\n\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    if self._ordinate.is_wrapping:\n        ord_is_wrapped = self.is_wrapped\n\n        if ord_is_wrapped:\n            out = out.unwrap()\n\n    # case 1: abs.wrapping=False, ord.linking=False, ord.wrapping=False\n    if (\n        not self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        pass\n\n    # case 2: abs.wrapping=False, ord.linking=False, ord.wrapping=True\n    elif (\n        not self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        pass\n\n    # case 3: abs.wrapping=False, ord.linking=True, ord.wrapping=False\n    elif (\n        not self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 4: abs.wrapping=False, ord.linking=True, ord.wrapping=True\n    elif (\n        not self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 5: abs.wrapping=True, ord.linking=False, ord.wrapping=False\n    elif (\n        self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        if mode is None:\n            kwargs[\"mode\"] = \"wrap\"\n\n    # case 6: abs.wrapping=True, ord.linking=False, ord.wrapping=True\n    elif (\n        self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        # (1) unwrap ordinate (abscissa wrap=False)\n        # (2) smooth unwrapped ordinate (absissa wrap=False)\n        # (3) repeat unwrapped signal based on conditions from (2):\n        # if smoothed wrapped ordinate samples\n        # HH ==&gt; SSS (this must be done on a per-signal basis!!!) H = high; L = low; S = same\n        # LL ==&gt; SSS (the vertical offset must be such that neighbors have smallest displacement)\n        # LH ==&gt; LSH\n        # HL ==&gt; HSL\n        # (4) smooth expanded and unwrapped ordinate (abscissa wrap=False)\n        # (5) cut out orignal signal\n\n        # (1)\n        kwargs[\"mode\"] = \"reflect\"\n        L = out._ordinate.range.max - out._ordinate.range.min\n        D = out.domain.length\n\n        tmp = utils.gaussian_filter(out.unwrap(), **kwargs)\n        # (2) (3)\n        n_reps = int(np.ceil((sigma * truncate) / float(D)))\n\n        smooth_data = []\n        for ss, signal in enumerate(tmp.signals):\n            # signal = signal.wrap()\n            offset = (\n                float((signal._data[:, -1] - signal._data[:, 0]) // (L / 2)) * L\n            )\n            # print(offset)\n            # left_high = signal._data[:,0] &gt;= out._ordinate.range.min + L/2\n            # right_high = signal._data[:,-1] &gt;= out._ordinate.range.min + L/2\n            # signal = signal.unwrap()\n\n            expanded = signal.copy()\n            for nn in range(n_reps):\n                expanded = expanded.join((signal &lt;&lt; D * (nn + 1)) - offset).join(\n                    (signal &gt;&gt; D * (nn + 1)) + offset\n                )\n                # print(expanded)\n                # if left_high == right_high:\n                #     print('extending flat! signal {}'.format(ss))\n                #     expanded = expanded.join(signal &lt;&lt; D*(nn+1)).join(signal &gt;&gt; D*(nn+1))\n                # elif left_high &lt; right_high:\n                #     print('extending LSH! signal {}'.format(ss))\n                #     # LSH\n                #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))-L).join((signal &gt;&gt; D*(nn+1))+L)\n                # else:\n                #     # HSL\n                #     print('extending HSL! signal {}'.format(ss))\n                #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))+L).join((signal &gt;&gt; D*(nn+1))-L)\n            # (4)\n            smooth_signal = utils.gaussian_filter(expanded, **kwargs)\n            smooth_data.append(\n                smooth_signal._data[\n                    :, n_reps * tmp.n_samples : (n_reps + 1) * (tmp.n_samples)\n                ].squeeze()\n            )\n        # (5)\n        out._data = np.array(smooth_data)\n        out.__renew__()\n\n        if self._ordinate.is_wrapping:\n            if ord_is_wrapped:\n                out = out.wrap()\n\n        return out\n\n    # case 7: abs.wrapping=True, ord.linking=True, ord.wrapping=False\n    elif (\n        self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 8: abs.wrapping=True, ord.linking=True, ord.wrapping=True\n    elif (\n        self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    out = utils.gaussian_filter(out, **kwargs)\n    out.__renew__()\n\n    if self._ordinate.is_wrapping:\n        if ord_is_wrapped:\n            out = out.wrap()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.standardize","title":"<code>standardize(inplace=False)</code>","text":"<p>Standardize the data to zero mean and unit standard deviation along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The standardized signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; standardized = asa.standardize()\n&gt;&gt;&gt; standardized.mean(), standardized.std()\n(0.0, 1.0)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def standardize(self, inplace=False):\n    \"\"\"\n    Standardize the data to zero mean and unit standard deviation along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The standardized signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; standardized = asa.standardize()\n    &gt;&gt;&gt; standardized.mean(), standardized.std()\n    (0.0, 1.0)\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    std = np.atleast_1d(out.std())\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.std","title":"<code>std(*, axis=1)</code>","text":"<p>Compute the standard deviation of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the standard deviation (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>std</code> <code>ndarray</code> <p>Standard deviation values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def std(self, *, axis=1):\n    \"\"\"\n    Compute the standard deviation of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the standard deviation (default is 1).\n\n    Returns\n    -------\n    std : np.ndarray\n        Standard deviation values along the specified axis.\n    \"\"\"\n    try:\n        stds = np.nanstd(self.data, axis=axis).squeeze()\n        if stds.size == 1:\n            return stds.item()\n        return stds\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate standard deviation\"\n        )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.subsample","title":"<code>subsample(*, fs)</code>","text":"<p>Subsamples a RegularlySampledAnalogSignalArray</p> <p>WARNING! Aliasing can occur! It is better to use downsample when lowering the sampling rate substantially.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>Desired output sampling rate, in Hz</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where data is only stored at the new subset of points.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def subsample(self, *, fs):\n    \"\"\"Subsamples a RegularlySampledAnalogSignalArray\n\n    WARNING! Aliasing can occur! It is better to use downsample when\n    lowering the sampling rate substantially.\n\n    Parameters\n    ----------\n    fs : float, optional\n        Desired output sampling rate, in Hz\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n        new subset of points.\n    \"\"\"\n\n    return self.simplify(ds=1 / fs)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.trim","title":"<code>trim(start, stop=None, *, fs=None)</code>","text":"<p>Trim the signal to the specified start and stop times.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>Start time.</p> required <code>stop</code> <code>float</code> <p>Stop time. If None, trims to the end.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling frequency. If None, uses self.fs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Trimmed signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trimmed = asa.trim(0, 10)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def trim(self, start, stop=None, *, fs=None):\n    \"\"\"\n    Trim the signal to the specified start and stop times.\n\n    Parameters\n    ----------\n    start : float\n        Start time.\n    stop : float, optional\n        Stop time. If None, trims to the end.\n    fs : float, optional\n        Sampling frequency. If None, uses self.fs.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Trimmed signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; trimmed = asa.trim(0, 10)\n    \"\"\"\n    logging.warning(\"RegularlySampledAnalogSignalArray: Trim may not work!\")\n    # TODO: do comprehensive input validation\n    if stop is not None:\n        try:\n            start = np.array(start, ndmin=1)\n            if len(start) != 1:\n                raise TypeError(\"start must be a scalar float\")\n        except TypeError:\n            raise TypeError(\"start must be a scalar float\")\n        try:\n            stop = np.array(stop, ndmin=1)\n            if len(stop) != 1:\n                raise TypeError(\"stop must be a scalar float\")\n        except TypeError:\n            raise TypeError(\"stop must be a scalar float\")\n    else:  # start must have two elements\n        try:\n            if len(np.array(start, ndmin=1)) &gt; 2:\n                raise TypeError(\n                    \"unsupported input to RegularlySampledAnalogSignalArray.trim()\"\n                )\n            stop = np.array(start[1], ndmin=1)\n            start = np.array(start[0], ndmin=1)\n            if len(start) != 1 or len(stop) != 1:\n                raise TypeError(\"start and stop must be scalar floats\")\n        except TypeError:\n            raise TypeError(\"start and stop must be scalar floats\")\n\n    logging.disable(logging.CRITICAL)\n    interval = self._abscissa.support.intersect(\n        type(self.support)([start, stop], fs=fs)\n    )\n    if not interval.isempty:\n        analogsignalarray = self[interval]\n    else:\n        analogsignalarray = type(self)([], empty=True)\n    logging.disable(0)\n    analogsignalarray.__renew__()\n    return analogsignalarray\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.unwrap","title":"<code>unwrap(inplace=False)</code>","text":"<p>Unwrap the ordinate values by minimizing total displacement, useful for phase data.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The unwrapped signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unwrapped = asa.unwrap()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def unwrap(self, inplace=False):\n    \"\"\"\n    Unwrap the ordinate values by minimizing total displacement, useful for phase data.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The unwrapped signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; unwrapped = asa.unwrap()\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    out.data = np.atleast_2d(\n        out._unwrap(out._data, out._ordinate.range.min, out._ordinate.range.max)\n    )\n    # out._is_wrapped = False\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.wrap","title":"<code>wrap(inplace=False)</code>","text":"<p>Wrap the ordinate values within the finite range defined by the ordinate's range.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The wrapped signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; wrapped = asa.wrap()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def wrap(self, inplace=False):\n    \"\"\"\n    Wrap the ordinate values within the finite range defined by the ordinate's range.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The wrapped signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; wrapped = asa.wrap()\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    out.data = np.atleast_2d(\n        out._wrap(out.data, out._ordinate.range.min, out._ordinate.range.max)\n    )\n    # out._is_wrapped = True\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.RegularlySampledAnalogSignalArray.zscore","title":"<code>zscore()</code>","text":"<p>Normalize each signal in the array using z-scores (zero mean, unit variance).</p> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>New object with z-scored data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zscored = asa.zscore()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def zscore(self):\n    \"\"\"\n    Normalize each signal in the array using z-scores (zero mean, unit variance).\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        New object with z-scored data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; zscored = asa.zscore()\n    \"\"\"\n    out = self.copy()\n    out._data = zscore(out._data, axis=1)\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.SpaceArray","title":"<code>SpaceArray</code>","text":"<p>               Bases: <code>IntervalArray</code></p> <p>IntervalArray containing spatial intervals (in centimeters).</p> <p>This class extends <code>IntervalArray</code> to specifically handle space-based intervals, such as linear or 2D spatial regions. It provides a formatter for displaying spatial lengths and can be used for spatial segmentation in behavioral or neural data analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_intervals, 1) or (n_intervals,), the start position for each interval (which then requires a <code>length</code> to be specified). If shape (n_intervals, 2), the start and stop positions for each interval. Defaults to None, creating an empty <code>SpaceArray</code>.</p> required <code>length</code> <code>np.array, float, or None</code> <p>The length of the interval (in base units, centimeters). If a float, the same length is assumed for every interval. Only used if <code>data</code> is a 1D array of start positions.</p> required <code>meta</code> <code>dict</code> <p>Metadata associated with the spatial intervals.</p> required <code>empty</code> <code>bool</code> <p>If True, an empty <code>SpaceArray</code> is returned, ignoring <code>data</code> and <code>length</code>. Defaults to False.</p> required <code>domain</code> <code>IntervalArray</code> <p>The domain within which the spatial intervals are defined. If None, it defaults to an infinite domain.</p> required <code>label</code> <code>str</code> <p>A descriptive label for the space array.</p> required <p>Attributes:</p> Name Type Description <code>data</code> <code>array</code> <p>The start and stop positions for each interval, with shape (n_intervals, 2).</p> <code>n_intervals</code> <code>int</code> <p>The number of spatial intervals in the array.</p> <code>lengths</code> <code>array</code> <p>The length of each spatial interval (in centimeters).</p> <code>formatter</code> <code>PrettySpace</code> <p>The formatter used for displaying spatial lengths.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the intervals, which is 'cm' for SpaceArray.</p> Notes <p>This class inherits all methods and properties from <code>IntervalArray</code>. It is intended for use with spatial data, such as segmenting a linear track or defining regions of interest in a behavioral arena.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy.core import SpaceArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a SpaceArray from start and stop positions\n&gt;&gt;&gt; regions = SpaceArray(data=np.array([[0, 50], [100, 150]]))\n&gt;&gt;&gt; print(regions)\n&lt;SpaceArray at 0x...: 2 intervals&gt; of length 100 cm\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a SpaceArray from start positions and a common length\n&gt;&gt;&gt; starts = np.array([0, 100])\n&gt;&gt;&gt; length = 25.0\n&gt;&gt;&gt; regions_with_length = SpaceArray(data=starts, length=length)\n&gt;&gt;&gt; print(regions_with_length)\n&lt;SpaceArray at 0x...: 2 intervals&gt; of length 50 cm\n</code></pre> <pre><code>&gt;&gt;&gt; # Accessing attributes\n&gt;&gt;&gt; print(f\"Number of regions: {regions.n_intervals}\")\nNumber of regions: 2\n&gt;&gt;&gt; print(f\"Lengths: {regions.lengths}\")\nLengths: [50 50]\n</code></pre> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class SpaceArray(IntervalArray):\n    \"\"\"\n    IntervalArray containing spatial intervals (in centimeters).\n\n    This class extends `IntervalArray` to specifically handle space-based\n    intervals, such as linear or 2D spatial regions. It provides a formatter\n    for displaying spatial lengths and can be used for spatial segmentation\n    in behavioral or neural data analysis.\n\n    Parameters\n    ----------\n    data : np.array, optional\n        If shape (n_intervals, 1) or (n_intervals,), the start position for each\n        interval (which then requires a `length` to be specified).\n        If shape (n_intervals, 2), the start and stop positions for each interval.\n        Defaults to None, creating an empty `SpaceArray`.\n    length : np.array, float, or None, optional\n        The length of the interval (in base units, centimeters). If a float,\n        the same length is assumed for every interval. Only used if `data`\n        is a 1D array of start positions.\n    meta : dict, optional\n        Metadata associated with the spatial intervals.\n    empty : bool, optional\n        If True, an empty `SpaceArray` is returned, ignoring `data` and `length`.\n        Defaults to False.\n    domain : IntervalArray, optional\n        The domain within which the spatial intervals are defined. If None, it defaults\n        to an infinite domain.\n    label : str, optional\n        A descriptive label for the space array.\n\n    Attributes\n    ----------\n    data : np.array\n        The start and stop positions for each interval, with shape (n_intervals, 2).\n    n_intervals : int\n        The number of spatial intervals in the array.\n    lengths : np.array\n        The length of each spatial interval (in centimeters).\n    formatter : formatters.PrettySpace\n        The formatter used for displaying spatial lengths.\n    base_unit : str\n        The base unit of the intervals, which is 'cm' for SpaceArray.\n\n    Notes\n    -----\n    This class inherits all methods and properties from `IntervalArray`.\n    It is intended for use with spatial data, such as segmenting a linear track\n    or defining regions of interest in a behavioral arena.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy.core import SpaceArray\n\n    &gt;&gt;&gt; # Create a SpaceArray from start and stop positions\n    &gt;&gt;&gt; regions = SpaceArray(data=np.array([[0, 50], [100, 150]]))\n    &gt;&gt;&gt; print(regions)\n    &lt;SpaceArray at 0x...: 2 intervals&gt; of length 100 cm\n\n    &gt;&gt;&gt; # Create a SpaceArray from start positions and a common length\n    &gt;&gt;&gt; starts = np.array([0, 100])\n    &gt;&gt;&gt; length = 25.0\n    &gt;&gt;&gt; regions_with_length = SpaceArray(data=starts, length=length)\n    &gt;&gt;&gt; print(regions_with_length)\n    &lt;SpaceArray at 0x...: 2 intervals&gt; of length 50 cm\n\n    &gt;&gt;&gt; # Accessing attributes\n    &gt;&gt;&gt; print(f\"Number of regions: {regions.n_intervals}\")\n    Number of regions: 2\n    &gt;&gt;&gt; print(f\"Lengths: {regions.lengths}\")\n    Lengths: [50 50]\n    \"\"\"\n\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n        self.formatter = formatters.PrettySpace\n        self.base_unit = self.formatter.base_unit\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.SpikeTrainArray","title":"<code>SpikeTrainArray</code>","text":"<p>               Bases: <code>EventArray</code></p> <p>A multiseries spike train array with shared support.</p> <p>SpikeTrainArray is a specialized EventArray for handling neural spike train data. It provides unit-specific aliases and methods for analyzing spike timing data across multiple recording units with a common temporal support.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000.</p> required <code>support</code> <code>IntervalArray</code> <p>IntervalArray on which spike trains are defined. Default is [0, last spike] inclusive.</p> required <code>unit_ids</code> <code>list of int</code> <p>Unit IDs. Alias for series_ids.</p> required <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Default casts unit_ids to str. Alias for series_labels.</p> required <code>unit_tags</code> <code>optional</code> <p>Tags corresponding to units. Alias for series_tags. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> required <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the spike train array.</p> required <code>empty</code> <code>bool</code> <p>Whether an empty SpikeTrainArray should be constructed (no data).</p> required <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments forwarded to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray and EventArray superclasses</code> <code>for additional attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the SpikeTrainArray is empty (no data).</p> <code>n_units</code> <code>int</code> <p>The number of units. Alias for n_series.</p> <code>n_active</code> <code>int</code> <p>The number of active units. A unit is considered active if it fired at least one spike.</p> <code>time</code> <code>array of np.array(dtype=np.float64)</code> <p>Spike time data in seconds. Array of length n_units, each entry with shape (n_spikes,). Alias for data.</p> <code>n_spikes</code> <code>ndarray</code> <p>The number of spikes in each unit. Alias for n_events.</p> <code>issorted</code> <code>bool</code> <p>Whether the spike times are sorted.</p> <code>first_spike</code> <code>float</code> <p>The time of the very first spike, across all units.</p> <code>last_spike</code> <code>float</code> <p>The time of the very last spike, across all units.</p> <code>unit_ids</code> <code>list of int</code> <p>Unit IDs. Alias for series_ids.</p> <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Alias for series_labels.</p> <code>unit_tags</code> <p>Tags corresponding to units. Alias for series_tags.</p> <code>n_epochs</code> <code>int</code> <p>The number of epochs/intervals. Alias for n_intervals.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the SpikeTrainArray.</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz).</p> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the spike train array.</p> <p>Methods:</p> Name Description <code>bin</code> <p>Return a BinnedSpikeTrainArray.</p> <code>get_spike_firing_order</code> <p>Returns unit_ids ordered by when they first fire.</p> <code>reorder_units_by_ids</code> <p>Reorder units according to specified unit_ids.</p> <code>flatten</code> <p>Collapse spikes across units into a single unit.</p> <code>partition</code> <p>Returns a SpikeTrainArray whose support has been partitioned.</p> <code>copy</code> <p>Returns a copy of the SpikeTrainArray.</p> <code>empty</code> <p>Remove data (but not metadata) from SpikeTrainArray.</p> <p>Examples:</p> <p>Create a SpikeTrainArray with two units:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n&gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n&gt;&gt;&gt; print(sta.n_units)\n2\n&gt;&gt;&gt; print(sta.n_spikes)\n[3 3]\n</code></pre> <p>Access spike times using the time alias:</p> <pre><code>&gt;&gt;&gt; print(sta.time[0])  # First unit's spike times\n[0.1 0.3 0.7]\n</code></pre> <p>Create a binned version:</p> <pre><code>&gt;&gt;&gt; binned = sta.bin(ds=0.1)  # 100ms bins\n</code></pre> Notes <p>SpikeTrainArray provides neuroscience-specific aliases for EventArray functionality. For example, 'units' instead of 'series', 'spikes' instead of 'events', and 'time' instead of 'data'. This makes the API more intuitive for neuroscience applications while maintaining full compatibility with the underlying EventArray infrastructure.</p> <p>The class automatically handles spike time sorting and provides efficient methods for common spike train analyses. All spike times are stored in seconds and should be non-negative values within the support interval.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class SpikeTrainArray(EventArray):\n    \"\"\"A multiseries spike train array with shared support.\n\n    SpikeTrainArray is a specialized EventArray for handling neural spike train data.\n    It provides unit-specific aliases and methods for analyzing spike timing data\n    across multiple recording units with a common temporal support.\n\n    Parameters\n    ----------\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000.\n    support : IntervalArray, optional\n        IntervalArray on which spike trains are defined.\n        Default is [0, last spike] inclusive.\n    unit_ids : list of int, optional\n        Unit IDs. Alias for series_ids.\n    unit_labels : list of str, optional\n        Labels corresponding to units. Default casts unit_ids to str.\n        Alias for series_labels.\n    unit_tags : optional\n        Tags corresponding to units. Alias for series_tags.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    label : str or None, optional\n        Information pertaining to the source of the spike train array.\n    empty : bool, optional\n        Whether an empty SpikeTrainArray should be constructed (no data).\n    **kwargs : optional\n        Additional keyword arguments forwarded to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray and EventArray superclasses\n    for additional attributes that are defined there.\n\n    isempty : bool\n        Whether the SpikeTrainArray is empty (no data).\n    n_units : int\n        The number of units. Alias for n_series.\n    n_active : int\n        The number of active units. A unit is considered active if\n        it fired at least one spike.\n    time : array of np.array(dtype=np.float64)\n        Spike time data in seconds. Array of length n_units, each entry with\n        shape (n_spikes,). Alias for data.\n    n_spikes : np.ndarray\n        The number of spikes in each unit. Alias for n_events.\n    issorted : bool\n        Whether the spike times are sorted.\n    first_spike : float\n        The time of the very first spike, across all units.\n    last_spike : float\n        The time of the very last spike, across all units.\n    unit_ids : list of int\n        Unit IDs. Alias for series_ids.\n    unit_labels : list of str\n        Labels corresponding to units. Alias for series_labels.\n    unit_tags :\n        Tags corresponding to units. Alias for series_tags.\n    n_epochs : int\n        The number of epochs/intervals. Alias for n_intervals.\n    support : IntervalArray\n        The support of the SpikeTrainArray.\n    fs : float\n        Sampling frequency (Hz).\n    label : str or None\n        Information pertaining to the source of the spike train array.\n\n    Methods\n    -------\n    bin(ds=None)\n        Return a BinnedSpikeTrainArray.\n    get_spike_firing_order()\n        Returns unit_ids ordered by when they first fire.\n    reorder_units_by_ids(neworder, inplace=False)\n        Reorder units according to specified unit_ids.\n    flatten(unit_id=None, unit_label=None)\n        Collapse spikes across units into a single unit.\n    partition(ds=None, n_epochs=None)\n        Returns a SpikeTrainArray whose support has been partitioned.\n    copy()\n        Returns a copy of the SpikeTrainArray.\n    empty(inplace=False)\n        Remove data (but not metadata) from SpikeTrainArray.\n\n    Examples\n    --------\n    Create a SpikeTrainArray with two units:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n    &gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n    &gt;&gt;&gt; print(sta.n_units)\n    2\n    &gt;&gt;&gt; print(sta.n_spikes)\n    [3 3]\n\n    Access spike times using the time alias:\n\n    &gt;&gt;&gt; print(sta.time[0])  # First unit's spike times\n    [0.1 0.3 0.7]\n\n    Create a binned version:\n\n    &gt;&gt;&gt; binned = sta.bin(ds=0.1)  # 100ms bins\n\n    Notes\n    -----\n    SpikeTrainArray provides neuroscience-specific aliases for EventArray\n    functionality. For example, 'units' instead of 'series', 'spikes' instead\n    of 'events', and 'time' instead of 'data'. This makes the API more intuitive\n    for neuroscience applications while maintaining full compatibility with the\n    underlying EventArray infrastructure.\n\n    The class automatically handles spike time sorting and provides efficient\n    methods for common spike train analyses. All spike times are stored in\n    seconds and should be non-negative values within the support interval.\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"units\"\n        kwargs[\"series_label\"] = series_label\n\n        # legacy STA constructor support for backward compatibility\n        kwargs = legacySTAkwargs(**kwargs)\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n\n    # @keyword_equivalence(this_or_that={'n_intervals':'n_epochs'})\n    # def partition(self, ds=None, n_intervals=None, n_epochs=None):\n    #     if n_intervals is None:\n    #         n_intervals = n_epochs\n    #     kwargs = {'ds':ds, 'n_intervals': n_intervals}\n    #     return super().partition(**kwargs)\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n        return BinnedSpikeTrainArray(self, ds=ds)  # TODO #FIXME\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.SpikeTrainArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedSpikeTrainArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n    return BinnedSpikeTrainArray(self, ds=ds)  # TODO #FIXME\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray","title":"<code>StatefulValueEventArray</code>","text":"<p>               Bases: <code>BaseValueEventArray</code></p> <p>StatefulValueEventArray for storing events with associated values and states.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>array - like</code> <p>Event times for each series. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>values</code> <code>array - like</code> <p>Values associated with each event. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>states</code> <code>array - like</code> <p>States associated with each event. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the events. If None, inferred from events.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Default is 30000.</p> <code>None</code> <code>series_ids</code> <code>list</code> <p>List of series IDs. If None, defaults to [1, ..., n_series].</p> <code>None</code> <code>empty</code> <code>bool</code> <p>If True, create an empty object.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>events</code> <code>ndarray</code> <p>Event times for each series. Ragged array, shape (n_series, n_events_i).</p> <code>values</code> <code>ndarray</code> <p>Values for each event. Ragged array, shape (n_series, n_events_i).</p> <code>states</code> <code>ndarray</code> <p>States for each event. Ragged array, shape (n_series, n_events_i).</p> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the events.</p> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> <code>n_series</code> <code>int</code> <p>Number of series.</p> <code>n_events</code> <code>ndarray</code> <p>Number of events in each series.</p> <code>series_ids</code> <code>list</code> <p>List of series IDs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n&gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; states = [[10, 20, 30], [40, 50, 60]]\n&gt;&gt;&gt; sveva = nel.StatefulValueEventArray(\n...     events=events, values=values, states=states, fs=10\n... )\n&gt;&gt;&gt; sveva.n_series\n2\n&gt;&gt;&gt; sveva.n_events\narray([3, 3])\n&gt;&gt;&gt; sveva.events[0]\narray([0.1, 0.5, 1.0])\n&gt;&gt;&gt; sveva.values[0]\narray([1, 2, 3])\n&gt;&gt;&gt; sveva.states[0]\narray([10, 20, 30])\n</code></pre> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class StatefulValueEventArray(BaseValueEventArray):\n    \"\"\"\n    StatefulValueEventArray for storing events with associated values and states.\n\n    Parameters\n    ----------\n    events : array-like\n        Event times for each series. List of arrays, shape (n_series, n_events_i).\n    values : array-like\n        Values associated with each event. List of arrays, shape (n_series, n_events_i).\n    states : array-like\n        States associated with each event. List of arrays, shape (n_series, n_events_i).\n    support : nelpy.IntervalArray, optional\n        Support intervals for the events. If None, inferred from events.\n    fs : float, optional\n        Sampling frequency in Hz. Default is 30000.\n    series_ids : list, optional\n        List of series IDs. If None, defaults to [1, ..., n_series].\n    empty : bool, optional\n        If True, create an empty object.\n    **kwargs\n        Additional keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    events : np.ndarray\n        Event times for each series. Ragged array, shape (n_series, n_events_i).\n    values : np.ndarray\n        Values for each event. Ragged array, shape (n_series, n_events_i).\n    states : np.ndarray\n        States for each event. Ragged array, shape (n_series, n_events_i).\n    support : nelpy.IntervalArray\n        Support intervals for the events.\n    fs : float\n        Sampling frequency in Hz.\n    n_series : int\n        Number of series.\n    n_events : np.ndarray\n        Number of events in each series.\n    series_ids : list\n        List of series IDs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n    &gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n    &gt;&gt;&gt; states = [[10, 20, 30], [40, 50, 60]]\n    &gt;&gt;&gt; sveva = nel.StatefulValueEventArray(\n    ...     events=events, values=values, states=states, fs=10\n    ... )\n    &gt;&gt;&gt; sveva.n_series\n    2\n    &gt;&gt;&gt; sveva.n_events\n    array([3, 3])\n    &gt;&gt;&gt; sveva.events[0]\n    array([0.1, 0.5, 1.0])\n    &gt;&gt;&gt; sveva.values[0]\n    array([1, 2, 3])\n    &gt;&gt;&gt; sveva.states[0]\n    array([10, 20, 30])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"n_marks\": \"n_values\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n        \"marks\": \"values\",\n        \"spikes\": \"events\",\n    }\n\n    def __init__(\n        self,\n        events=None,\n        values=None,\n        states=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        # add class-specific aliases to existing aliases:\n        # self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        # print('in init')\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        # print('non-stateful preprocessing')\n        self._val_init(\n            events=events,\n            values=values,\n            states=states,\n            fs=fs,\n            support=support,\n            series_ids=series_ids,\n            empty=empty,\n            **kwargs,\n        )\n\n        # print('making stateful')\n        data = self._make_stateful(data=self.data)\n        self._data = data\n\n    def _val_init(\n        self,\n        events=None,\n        values=None,\n        states=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        #############################################\n        #            standardize kwargs             #\n        #############################################\n        if events is not None:\n            kwargs[\"events\"] = events\n        if values is not None:\n            kwargs[\"values\"] = values\n        if states is not None:\n            kwargs[\"states\"] = states\n        kwargs = self._standardize_kwargs(**kwargs)\n        events = kwargs.pop(\"events\", None)\n        values = kwargs.pop(\"values\", None)\n        states = kwargs.pop(\"states\", None)\n        #############################################\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.info(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            data = np.array(data)\n            try:\n                if data.shape[-1] &lt; 2 and np.max(data.shape) &gt; 1:\n                    return True\n                if max(np.array(data).shape[:-1]) &gt; 1 and data.shape[-1] == 1:\n                    return True\n            except (IndexError, TypeError, ValueError):\n                return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            ========\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            try:\n                if isinstance(data[0][0], list) or isinstance(data[0][0], np.ndarray):\n                    logging.info(\"event datas input has too many layers!\")\n                    try:\n                        if max(np.array(data).shape[:-1]) &gt; 1:\n                            #                 singletons = True\n                            return False\n                    except ValueError:\n                        return False\n                    data = np.squeeze(data)\n            except (IndexError, TypeError):\n                pass\n            try:\n                if isinstance(data[1], list) or isinstance(data[1], np.ndarray):\n                    return False\n            except (IndexError, TypeError):\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            # Handle ragged input: list/tuple or np.ndarray of dtype=object\n            is_ragged = False\n            if isinstance(data, (list, tuple)):\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            elif isinstance(data, np.ndarray) and data.dtype == object:\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            if is_ragged:\n                return utils.ragged_array([np.array(st, ndmin=1) for st in data])\n            # Only here, if not ragged, use np.array/np.squeeze\n            return np.array(np.squeeze(data), ndmin=2)\n\n        def standardize_values_to_2d(data):\n            data = standardize_to_2d(data)\n            for ii, series in enumerate(data):\n                if len(series.shape) == 2:\n                    pass\n                else:\n                    for xx in series:\n                        if len(np.atleast_1d(xx)) &gt; 1:\n                            raise ValueError(\n                                \"each series must have a fixed number of values; mismatch in series {}\".format(\n                                    ii\n                                )\n                            )\n            return data\n\n        events = standardize_to_2d(events)\n        values = standardize_values_to_2d(values)\n        states = standardize_to_2d(states)\n\n        data = []\n        for a, v, s in zip(events, values, states):\n            data.append(np.vstack((a, v.T, s.T)).T)\n        data = np.array(data)\n\n        # sort event series, but only if necessary:\n        for ii, train in enumerate(events):\n            if not utils.is_sorted(train):\n                sortidx = np.argsort(train)\n                data[ii] = (data[ii])[sortidx, :]\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            self._abscissa.support = type(self._abscissa.support)(\n                np.array([self.first_event, self.last_event + 1 / fs])\n            )\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            self._abscissa.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        data = self._restrict_to_interval_array_fast(\n            intervalarray=self.support, data=data\n        )\n\n        self._data = data\n        return\n\n    @property\n    def data(self):\n        \"\"\"Event datas in seconds.\"\"\"\n        return self._data\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to ValueEventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_value_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_value_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    def _intervalslicer(self, idx):\n        \"\"\"Helper function to restrict object to EpochArray.\"\"\"\n        # if self.isempty:\n        #     return self\n\n        if isinstance(idx, core.IntervalArray):\n            if idx.isempty:\n                return type(self)(empty=True)\n            support = self._abscissa.support.intersect(\n                interval=idx, boundaries=True\n            )  # what if fs of slicing interval is different?\n            if support.isempty:\n                return type(self)(empty=True)\n\n            logging.disable(logging.CRITICAL)\n            data = self._restrict_to_interval_array_value_fast(\n                intervalarray=support, data=self.data, copyover=True\n            )\n            eventarray = self._copy_without_data()\n            eventarray._data = data\n            eventarray._abscissa.support = support\n            eventarray.__renew__()\n            logging.disable(0)\n            return eventarray\n        elif isinstance(idx, int):\n            eventarray = self._copy_without_data()\n            support = self._abscissa.support[idx]\n            eventarray._abscissa.support = support\n            if (idx &gt;= self._abscissa.support.n_intervals) or idx &lt; (\n                -self._abscissa.support.n_intervals\n            ):\n                eventarray.__renew__()\n                return eventarray\n            else:\n                data = self._restrict_to_interval_array_value_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                return eventarray\n        else:  # most likely slice indexing\n            try:\n                logging.disable(logging.CRITICAL)\n                support = self._abscissa.support[idx]\n                data = self._restrict_to_interval_array_value_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray = self._copy_without_data()\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                logging.disable(0)\n                return eventarray\n            except Exception:\n                raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    @staticmethod\n    def _restrict_to_interval_array_fast(intervalarray, data, copyover=True):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                frm, to = np.searchsorted(evt_data[:, 0], (t_start, t_stop))\n                indices.append((frm, to))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n        return data\n\n    def _restrict_to_interval_array_value_fast(\n        self, intervalarray, data, copyover=True\n    ):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        # plan of action\n        # create pseudo events supporting each interval\n        # then restrict existing data (pseudo and real events)\n        # then merge in all pseudo events that don't exist yet\n        starts = intervalarray.starts\n        stops = intervalarray.stops\n\n        kinds = []\n        events = []\n        states = []\n\n        for series in data:\n            tvect = series[:, 0].astype(float)\n            statevals = series[:, 2:]\n\n            kind = []\n            state = []\n\n            for start in starts:\n                idx = np.max((np.searchsorted(tvect, start, side=\"right\") - 1, 0))\n                kind.append(0)\n                state.append(statevals[[idx]])\n\n            for stop in stops:\n                idx = np.max((np.searchsorted(tvect, stop, side=\"right\") - 1, 0))\n                kind.append(2)\n                state.append(statevals[[idx]])\n\n            states.append(np.array(state).squeeze())  ## squeeze???\n            events.append(np.hstack((starts, stops)))\n            kinds.append(np.array(kind))\n\n        pseudodata = []\n        for e, k, s in zip(events, kinds, states):\n            pseudodata.append(np.vstack((e, k, s.T)).T)\n\n        pseudodata = utils.ragged_array(pseudodata)\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                frm, to = np.searchsorted(evt_data[:, 0], (t_start, t_stop))\n                indices.append((frm, to))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n\n        # now add in all pseudo events that don't already exist in data\n\n        kinds = []\n        events = []\n        states = []\n\n        for pseries, series in zip(pseudodata, data):\n            ptvect = pseries[:, 0].astype(float)\n            pkind = pseries[:, 1].astype(int)\n            pstatevals = pseries[:, 2:]\n\n            try:\n                tvect = series[:, 0].astype(float)\n                kind = series[:, 1]\n                statevals = series[:, 2:]\n            except IndexError:\n                tvect = np.zeros((0))\n                kind = np.zeros((0))\n                statevals = np.zeros((0,))\n\n            for tt, kk, psv in zip(ptvect, pkind, pstatevals):\n                # print(tt, kk, psv)\n                idx = np.searchsorted(tvect, tt, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                try:\n                    if tt == tvect[idx2]:\n                        pass\n                        # print('pseudo event {} not necessary...'.format(tt))\n                    else:\n                        # print('pseudo event {} necessary...'.format(tt))\n                        kind = np.insert(kind, idx, kk)\n                        tvect = np.insert(tvect, idx, tt)\n                        statevals = np.insert(statevals, idx, psv, axis=0)\n                except IndexError:\n                    kind = np.insert(kind, idx, kk)\n                    tvect = np.insert(tvect, idx, tt)\n                    statevals = np.insert(statevals, idx, psv, axis=0)\n\n            states.append(np.array(statevals).squeeze())\n            events.append(tvect)\n            kinds.append(kind)\n\n        # print(states)\n        # print(tvect)\n        # print(kinds)\n\n        data = []\n        for e, k, s in zip(events, kinds, states):\n            data.append(np.vstack((e, k, s.T)).T)\n\n        data = utils.ragged_array(data)\n\n        return data\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedValueEventArray.\"\"\"\n        raise NotImplementedError\n        return BinnedValueEventArray(self, ds=ds)\n\n    def __call__(self, *args):\n        \"\"\"StatefulValueEventArray callable method; by default returns state values\"\"\"\n        values = []\n        for events, vals in zip(self.state_events, self.state_values):\n            idx = np.searchsorted(events, args, side=\"right\") - 1\n            idx[idx &lt; 0] = 0\n            values.append(vals[[idx]])\n        values = np.asarray(values)\n        return values\n\n    def _make_stateful(self, data, intervalarray=None, initial_state=np.nan):\n        \"\"\"\n        [i, e0, e1, e2, ..., f] for every epoch\n\n        matrix of size (n_values x (n_epochs*2 + n_events) )\n        matrix of size (nSeries: n_values x (n_epochs*2 + n_events) )\n\n        needs to change when calling loc, iloc, restrict, getitem, ...\n\n        TODO: initial_state is not used yet!!!\n        \"\"\"\n        kinds = []\n        events = []\n        states = []\n\n        if intervalarray is None:\n            intervalarray = self.support\n\n        for series in data:\n            starts = intervalarray.starts\n            stops = intervalarray.stops\n            tvect = series[:, 0].astype(float)\n            statevals = series[:, 1:]\n            kind = np.ones(tvect.size).astype(int)\n\n            for start in starts:\n                idx = np.searchsorted(tvect, start, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                if start == tvect[idx2]:\n                    continue\n                else:\n                    kind = np.insert(kind, idx, 0)\n                    tvect = np.insert(tvect, idx, start)\n                    statevals = np.insert(statevals, idx, statevals[idx2], axis=0)\n\n            for stop in stops:\n                idx = np.searchsorted(tvect, stop, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                if stop == tvect[idx2]:\n                    continue\n                else:\n                    kind = np.insert(kind, idx, 2)\n                    tvect = np.insert(tvect, idx, stop)\n                    statevals = np.insert(statevals, idx, statevals[idx2], axis=0)\n\n            states.append(statevals)\n            events.append(tvect)\n            kinds.append(kind)\n\n        data = []\n        for e, k, s in zip(events, kinds, states):\n            data.append(np.vstack((e, k, s.T)).T)\n        data = utils.ragged_array(data)\n\n        return data\n\n    @property\n    def n_values(self):\n        \"\"\"(int) The number of values associated with each event series.\"\"\"\n        if self.isempty:\n            return 0\n        n_values = []\n        for series in self.data:\n            n_values.append(series.squeeze().shape[1] - 2)\n        return n_values\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s\" % (self.type_name, address_str, numstr, epstr, fsstr)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.events])\n\n    @property\n    def events(self):\n        events = []\n        for series, kinds in zip(self.state_events, self.state_kinds):\n            keep_idx = np.argwhere(kinds == 1)\n            events.append(series[keep_idx].squeeze())\n        return np.asarray(events)\n\n    @property\n    def values(self):\n        values = []\n        for series, kinds in zip(self.state_values, self.state_kinds):\n            keep_idx = np.argwhere(kinds == 1)\n            values.append(series[keep_idx].squeeze())\n        return np.asarray(values)\n\n    @property\n    def state_events(self):\n        events = []\n        for series in self.data:\n            events.append(series[:, 0].squeeze())\n\n        return np.asarray(events)\n\n    @property\n    def state_values(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 2:].squeeze())\n\n        return np.asarray(values)\n\n    @property\n    def state_kinds(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 1].squeeze())\n\n        return np.asarray(values)\n\n    def _plot(self, *args, **kwargs):\n        if self.n_series &gt; 1:\n            raise NotImplementedError\n        if np.any(np.array(self.n_values) &gt; 1):\n            raise NotImplementedError\n\n        import matplotlib.pyplot as plt\n\n        events = self.state_events.squeeze()\n        values = self.state_values.squeeze()\n        kinds = self.state_kinds.squeeze()\n\n        for (a, b), val, (ka, kb) in zip(\n            utils.pairwise(events), values, utils.pairwise(kinds)\n        ):\n            if kb == 1:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"b\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n            if ka == 1:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"g\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n            if kb == 1:\n                plt.plot(b, val, \"o\", color=\"k\", markerfacecolor=\"w\", lw=1.5, mew=1.5)\n            if ka == 1:\n                plt.plot(a, val, \"o\", color=\"k\", markerfacecolor=\"k\", lw=1.5, mew=1.5)\n            if ka == 0 and kb == 2:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"r\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Event datas in seconds.</p>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the underlying EventArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray.n_values","title":"<code>n_values</code>  <code>property</code>","text":"<p>(int) The number of values associated with each event series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying EventArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedValueEventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedValueEventArray.\"\"\"\n    raise NotImplementedError\n    return BinnedValueEventArray(self, ds=ds)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.StatefulValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.TemporalAbscissa","title":"<code>TemporalAbscissa</code>","text":"<p>               Bases: <code>Abscissa</code></p> <p>Abscissa for time series data.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class TemporalAbscissa(Abscissa):\n    \"\"\"Abscissa for time series data.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        labelstring = kwargs.get(\n            \"labelstring\", \"time ({})\"\n        )  # TODO FIXME after unit inheritance; inherit from formatter?\n\n        if support is None:\n            support = core.EpochArray(empty=True)\n\n        kwargs[\"support\"] = support\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n\n        self.formatter = self.support.formatter\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray","title":"<code>ValueEventArray</code>","text":"<p>               Bases: <code>BaseValueEventArray</code></p> <p>A multiseries eventarray with shared support.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,)</p> required <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000</p> <code>None</code> <code>support</code> <code>EpochArray</code> <p>EpochArray on which eventarrays are defined. Default is [0, last event] inclusive.</p> <code>None</code> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> required <code>cell_type</code> <code>list (of length n_series) of str or other</code> <p>Identified cell type indicator, e.g., 'pyr', 'int'.</p> required <code>series_ids</code> <code>list (of length n_series) of indices corresponding to</code> <p>curated data. If no series_ids are specified, then [1,...,n_series] will be used. WARNING! The first series will have index 1, not 0!</p> <code>None</code> <code>meta</code> <code>dict</code> <p>Metadata associated with eventarray.</p> required <p>Attributes:</p> Name Type Description <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,)</p> <code>support</code> <code>EpochArray on which eventarray is defined.</code> <code>n_events</code> <code>np.array(dtype=np.int) of shape (n_series,)</code> <p>Number of events in each series.</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz).</p> <code>cell_types</code> <code>np.array of str or other</code> <p>Identified cell type for each series.</p> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> <code>meta</code> <code>dict</code> <p>Metadata associated with eventseries.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class ValueEventArray(BaseValueEventArray):\n    \"\"\"A multiseries eventarray with shared support.\n\n    Parameters\n    ----------\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,)\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000\n    support : EpochArray, optional\n        EpochArray on which eventarrays are defined.\n        Default is [0, last event] inclusive.\n    label : str or None, optional\n        Information pertaining to the source of the eventarray.\n    cell_type : list (of length n_series) of str or other, optional\n        Identified cell type indicator, e.g., 'pyr', 'int'.\n    series_ids : list (of length n_series) of indices corresponding to\n        curated data. If no series_ids are specified, then [1,...,n_series]\n        will be used. WARNING! The first series will have index 1, not 0!\n    meta : dict\n        Metadata associated with eventarray.\n\n    Attributes\n    ----------\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,)\n    support : EpochArray on which eventarray is defined.\n    n_events: np.array(dtype=np.int) of shape (n_series,)\n        Number of events in each series.\n    fs: float\n        Sampling frequency (Hz).\n    cell_types : np.array of str or other\n        Identified cell type for each series.\n    label : str or None\n        Information pertaining to the source of the eventarray.\n    meta : dict\n        Metadata associated with eventseries.\n    \"\"\"\n\n    __attributes__ = [\"_data\"]\n    __attributes__.extend(BaseValueEventArray.__attributes__)\n\n    def __init__(\n        self,\n        events=None,\n        values=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        self._val_init(\n            events=events,\n            values=values,\n            fs=fs,\n            support=support,\n            series_ids=series_ids,\n            empty=empty,\n            **kwargs,\n        )\n\n    def _val_init(\n        self,\n        events=None,\n        values=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        #############################################\n        #            standardize kwargs             #\n        #############################################\n        if events is not None:\n            kwargs[\"events\"] = events\n        if values is not None:\n            kwargs[\"values\"] = values\n        kwargs = self._standardize_kwargs(**kwargs)\n        events = kwargs.pop(\"events\", None)\n        values = kwargs.pop(\"values\", None)\n        #############################################\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.info(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            # Avoid np.array on jagged input\n            if isinstance(data, (list, tuple)) and len(data) &gt; 1:\n                # If all elements are scalars or 1-element lists/arrays\n                try:\n                    if all(\n                        (not hasattr(x, \"__len__\") or len(np.atleast_1d(x)) == 1)\n                        for x in data\n                    ):\n                        return True\n                except Exception:\n                    return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            --------\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            # Avoid np.array on jagged input\n            try:\n                # If first element is a list/array and has more than 1 element, not single series\n                if hasattr(data[0], \"__len__\") and len(data[0]) &gt; 1:\n                    # If data[0][0] is also a list/array, too many layers\n                    if hasattr(data[0][0], \"__len__\"):\n                        return False\n                # If second element exists and is a list/array, not single series\n                if len(data) &gt; 1 and hasattr(data[1], \"__len__\"):\n                    return False\n            except Exception:\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            # Handle ragged input: list/tuple or np.ndarray of dtype=object\n            is_ragged = False\n            if isinstance(data, (list, tuple)):\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            elif isinstance(data, np.ndarray) and data.dtype == object:\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            if is_ragged:\n                return utils.ragged_array([np.array(st, ndmin=1) for st in data])\n            # Only here, if not ragged, use np.array/np.squeeze\n            return np.array(np.squeeze(data), ndmin=2)\n\n        def standardize_values_to_2d(data):\n            data = standardize_to_2d(data)\n            for ii, series in enumerate(data):\n                if len(series.shape) == 2:\n                    pass\n                else:\n                    for xx in series:\n                        if len(np.atleast_1d(xx)) &gt; 1:\n                            raise ValueError(\n                                \"each series must have a fixed number of values; mismatch in series {}\".format(\n                                    ii\n                                )\n                            )\n            return data\n\n        events = standardize_to_2d(events)\n        values = standardize_values_to_2d(values)\n\n        data = []\n        for a, v in zip(events, values):\n            data.append(np.vstack((a, v.T)).T)\n        # Use ragged_array to support jagged arrays (multi-series with different numbers of events)\n        data = utils.ragged_array(data)\n\n        # sort event series, but only if necessary:\n        for ii, train in enumerate(events):\n            if not utils.is_sorted(train):\n                sortidx = np.argsort(train)\n                data[ii] = (data[ii])[sortidx, :]\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # print(self.type_name, kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            self.support = type(self._abscissa.support)(\n                np.array([self.first_event, self.last_event + 1 / fs])\n            )\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            # print('restricting, here')\n            self.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        data = self._restrict_to_interval_array_fast(\n            intervalarray=self.support, data=data\n        )\n\n        self._data = data\n        return\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def _intervalslicer(self, idx):\n        \"\"\"Helper function to restrict object to EpochArray.\"\"\"\n        # if self.isempty:\n        #     return self\n\n        if isinstance(idx, core.IntervalArray):\n            if idx.isempty:\n                return type(self)(empty=True)\n            support = self._abscissa.support.intersect(\n                interval=idx, boundaries=True\n            )  # what if fs of slicing interval is different?\n            if support.isempty:\n                return type(self)(empty=True)\n\n            logging.disable(logging.CRITICAL)\n            data = self._restrict_to_interval_array_fast(\n                intervalarray=support, data=self.data, copyover=True\n            )\n            eventarray = self._copy_without_data()\n            eventarray._data = data\n            eventarray._abscissa.support = support\n            eventarray.__renew__()\n            logging.disable(0)\n            return eventarray\n        elif isinstance(idx, int):\n            eventarray = self._copy_without_data()\n            support = self._abscissa.support[idx]\n            eventarray._abscissa.support = support\n            if (idx &gt;= self._abscissa.support.n_intervals) or idx &lt; (\n                -self._abscissa.support.n_intervals\n            ):\n                eventarray.__renew__()\n                return eventarray\n            else:\n                data = self._restrict_to_interval_array_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                return eventarray\n        else:  # most likely slice indexing\n            try:\n                logging.disable(logging.CRITICAL)\n                support = self._abscissa.support[idx]\n                data = self._restrict_to_interval_array_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray = self._copy_without_data()\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                logging.disable(0)\n                return eventarray\n            except Exception:\n                raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to ValueEventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def n_active(self):\n        \"\"\"(int) The number of active series.\n\n        A series is considered active if it fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    @property\n    def events(self):\n        events = []\n        for series in self.data:\n            events.append(series[:, 0].squeeze())\n        return utils.ragged_array(events)\n\n    @property\n    def values(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 1:].squeeze())\n        return utils.ragged_array(values)\n\n    def flatten(self, *, series_id=None):\n        \"\"\"Collapse events across series.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n\n        flattened = self._copy_without_data()\n\n        flattened._series_ids = [series_id]\n\n        raise NotImplementedError\n        alldatas = self.data[0]\n        for series in range(1, self.n_series):\n            alldatas = utils.linear_merge(alldatas, self.data[series])\n\n        flattened._data = np.array(list(alldatas), ndmin=2)\n        flattened.__renew__()\n        return flattened\n\n    @staticmethod\n    def _restrict_to_interval_array_fast(intervalarray, data, copyover=True):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            evt_data = ValueEventArray._to_2d_array(evt_data)\n            if evt_data.size == 0 or evt_data.shape[1] &lt; 1:\n                if singleseries:\n                    data = np.array([[]])\n                else:\n                    data_ = data.tolist()\n                    data_[series] = np.array([])\n                    data = utils.ragged_array(data_)\n                continue\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                # Ensure we have a proper 1D array of event times\n                if evt_data.ndim &gt; 1:\n                    event_times = evt_data[:, 0].flatten()\n                else:\n                    event_times = evt_data.flatten()\n                # Ensure event_times is a proper 1D array and not an object array\n                if event_times.dtype == object:\n                    # Handle object array by extracting the actual values\n                    event_times = np.array(\n                        [\n                            float(t) if hasattr(t, \"__float__\") else t\n                            for t in event_times\n                        ]\n                    )\n                # Ensure event_times is a proper 1D array\n                if event_times.size == 0:\n                    indices.append((0, 0))\n                else:\n                    try:\n                        frm, to = np.searchsorted(event_times, (t_start, t_stop))\n                        indices.append((frm, to))\n                    except (ValueError, TypeError):\n                        # Fallback: handle case where searchsorted fails\n                        indices.append((0, 0))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n        return data\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s\" % (self.type_name, address_str, numstr, epstr, fsstr)\n\n    def bin(self, *, ds=None, method=\"mean\", **kwargs):\n        \"\"\"Return a binned value event array.\n\n        method in [sum, mean, median, min, max] or a custom function.\n        Additional keyword arguments are passed to BinnedValueEventArray.\n        \"\"\"\n        return BinnedValueEventArray(self, ds=ds, method=method, **kwargs)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.data])\n\n    @property\n    def n_values(self):\n        \"\"\"(int) The number of values associated with each event series.\"\"\"\n        if self.isempty:\n            return 0\n        n_values = []\n        for series in self.data:\n            n_values.append(series.squeeze().shape[1] - 1)\n        return n_values\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Sorted EventArray.\"\"\"\n        if self.isempty:\n            return True\n        return np.array(\n            [utils.is_sorted(eventarray[:, 0]) for eventarray in self.data]\n        ).all()\n\n    def _reorder_series_by_idx(self, neworder, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,)\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n        out.__renew__()\n\n        return out\n\n    def reorder_series_by_ids(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        neworder = [self.series_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def make_stateful(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def _to_2d_array(arr):\n        \"\"\"Convert array to 2D numpy array, handling object arrays properly.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.dtype == object:\n            # Handle object arrays by extracting the actual data\n            if arr.size == 1:\n                # Single element object array\n                return np.atleast_2d(arr[0])\n            else:\n                # Multiple element object array - concatenate\n                flattened = []\n                for item in arr:\n                    if isinstance(item, np.ndarray):\n                        flattened.append(item)\n                    else:\n                        flattened.append(np.array(item))\n                if flattened:\n                    return np.vstack(flattened)\n                else:\n                    return np.array([]).reshape(0, 0)\n        else:\n            return np.atleast_2d(arr)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Sorted EventArray.</p>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>(int) The number of active series.</p> <p>A series is considered active if it fired at least one event.</p>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.n_values","title":"<code>n_values</code>  <code>property</code>","text":"<p>(int) The number of values associated with each event series.</p>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.bin","title":"<code>bin(*, ds=None, method='mean', **kwargs)</code>","text":"<p>Return a binned value event array.</p> <p>method in [sum, mean, median, min, max] or a custom function. Additional keyword arguments are passed to BinnedValueEventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None, method=\"mean\", **kwargs):\n    \"\"\"Return a binned value event array.\n\n    method in [sum, mean, median, min, max] or a custom function.\n    Additional keyword arguments are passed to BinnedValueEventArray.\n    \"\"\"\n    return BinnedValueEventArray(self, ds=ds, method=method, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.flatten","title":"<code>flatten(*, series_id=None)</code>","text":"<p>Collapse events across series.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def flatten(self, *, series_id=None):\n    \"\"\"Collapse events across series.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n\n    flattened = self._copy_without_data()\n\n    flattened._series_ids = [series_id]\n\n    raise NotImplementedError\n    alldatas = self.data[0]\n    for series in range(1, self.n_series):\n        alldatas = utils.linear_merge(alldatas, self.data[series])\n\n    flattened._data = np.array(list(alldatas), ndmin=2)\n    flattened.__renew__()\n    return flattened\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/all/#nelpy.all.ValueEventArray.reorder_series_by_ids","title":"<code>reorder_series_by_ids(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def reorder_series_by_ids(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    neworder = [self.series_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        utils.swap_rows(out._data, frm, to)\n        out._series_ids[frm], out._series_ids[to] = (\n            out._series_ids[to],\n            out._series_ids[frm],\n        )\n        # TODO: re-build series tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/decoding/","title":"nelpy.decoding","text":"<p>Bayesian encoding and decoding</p>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.BayesianDecoder","title":"<code>BayesianDecoder</code>","text":"<p>               Bases: <code>object</code></p> <p>Bayesian decoder for neural population activity.</p> <p>This class provides a scikit-learn-like API for Bayesian decoding using tuning curves. Supports 1D and 2D decoding, and can be extended for more complex models.</p> <p>Parameters:</p> Name Type Description Default <code>tuningcurve</code> <code>TuningCurve1D or TuningCurve2D</code> <p>Tuning curve to use for decoding.</p> <code>None</code> Source code in <code>nelpy/decoding.py</code> <pre><code>class BayesianDecoder(object):\n    \"\"\"\n    Bayesian decoder for neural population activity.\n\n    This class provides a scikit-learn-like API for Bayesian decoding using tuning curves.\n    Supports 1D and 2D decoding, and can be extended for more complex models.\n\n    Parameters\n    ----------\n    tuningcurve : TuningCurve1D or TuningCurve2D, optional\n        Tuning curve to use for decoding.\n    \"\"\"\n\n    def __init__(self, tuningcurve=None):\n        \"\"\"\n        Initialize the BayesianDecoder.\n\n        Parameters\n        ----------\n        tuningcurve : TuningCurve1D or TuningCurve2D, optional\n            Tuning curve to use for decoding.\n        \"\"\"\n        self.tuningcurve = tuningcurve\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the decoder to data X. (Stores the tuning curve if provided.)\n\n        Parameters\n        ----------\n        X : array-like or TuningCurve1D/2D\n            Training data or tuning curve.\n        y : Ignored\n        \"\"\"\n        # If X is a tuning curve, store it\n        self.tuningcurve = X\n        return self\n\n    def predict_proba(self, X, **kwargs):\n        \"\"\"\n        Predict posterior probabilities for data X.\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Data to decode.\n        Returns\n        -------\n        posterior : np.ndarray\n            Posterior probability matrix.\n        \"\"\"\n        if self.tuningcurve is None:\n            raise ValueError(\n                \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n            )\n        # Use decode1D or decode2D depending on tuning curve\n        if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n            # TuningCurve1D or TuningCurve2D object\n            ratemap = self.tuningcurve.ratemap\n        else:\n            ratemap = self.tuningcurve\n        # Try to infer 1D vs 2D\n        if ratemap.ndim == 2:\n            posterior, _, _, _ = decode1D(X, ratemap, **kwargs)\n        elif ratemap.ndim == 3:\n            posterior, _, _, _ = decode2D(X, ratemap, **kwargs)\n        else:\n            raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n        return posterior\n\n    def predict(self, X, **kwargs):\n        \"\"\"\n        Predict external variable from data X (returns mode path).\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Data to decode.\n        Returns\n        -------\n        mode_pth : np.ndarray\n            Most likely position at each time bin.\n        \"\"\"\n        if self.tuningcurve is None:\n            raise ValueError(\n                \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n            )\n        if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n            ratemap = self.tuningcurve.ratemap\n        else:\n            ratemap = self.tuningcurve\n        if ratemap.ndim == 2:\n            _, _, mode_pth, _ = decode1D(X, ratemap, **kwargs)\n        elif ratemap.ndim == 3:\n            _, _, mode_pth, _ = decode2D(X, ratemap, **kwargs)\n        else:\n            raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n        return mode_pth\n\n    def predict_asa(self, X, **kwargs):\n        \"\"\"\n        Predict analog signal array (mean path) from data X.\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Data to decode.\n        Returns\n        -------\n        asa : AnalogSignalArray or np.ndarray\n            Mean path as AnalogSignalArray if possible, else array.\n        \"\"\"\n        if self.tuningcurve is None:\n            raise ValueError(\n                \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n            )\n        if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n            ratemap = self.tuningcurve.ratemap\n        else:\n            ratemap = self.tuningcurve\n        if ratemap.ndim == 2:\n            _, _, _, mean_pth = decode1D(X, ratemap, **kwargs)\n        elif ratemap.ndim == 3:\n            _, _, _, mean_pth = decode2D(X, ratemap, **kwargs)\n        else:\n            raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n        # Try to return as AnalogSignalArray if possible, with timestamps if available\n        try:\n            from .core import AnalogSignalArray\n\n            abscissa_vals = None\n            # Try to get bin centers from X if possible\n            if hasattr(X, \"bin_centers\"):\n                abscissa_vals = X.bin_centers\n            elif hasattr(X, \"abscissa_vals\"):\n                abscissa_vals = X.abscissa_vals\n            return AnalogSignalArray(data=mean_pth, abscissa_vals=abscissa_vals)\n        except Exception:\n            return mean_pth\n\n    def __repr__(self):\n        s = \"&lt;BayesianDecoder: \"\n        if self.tuningcurve is not None:\n            s += f\"tuningcurve shape={getattr(self.tuningcurve, 'ratemap', self.tuningcurve).shape}\"\n        else:\n            s += \"no tuningcurve\"\n        s += \"&gt;\"\n        return s\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.BayesianDecoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the decoder to data X. (Stores the tuning curve if provided.)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like or TuningCurve1D/2D</code> <p>Training data or tuning curve.</p> required <code>y</code> <code>Ignored</code> <code>None</code> Source code in <code>nelpy/decoding.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    Fit the decoder to data X. (Stores the tuning curve if provided.)\n\n    Parameters\n    ----------\n    X : array-like or TuningCurve1D/2D\n        Training data or tuning curve.\n    y : Ignored\n    \"\"\"\n    # If X is a tuning curve, store it\n    self.tuningcurve = X\n    return self\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.BayesianDecoder.predict","title":"<code>predict(X, **kwargs)</code>","text":"<p>Predict external variable from data X (returns mode path).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or BinnedEventArray</code> <p>Data to decode.</p> required <p>Returns:</p> Name Type Description <code>mode_pth</code> <code>ndarray</code> <p>Most likely position at each time bin.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def predict(self, X, **kwargs):\n    \"\"\"\n    Predict external variable from data X (returns mode path).\n\n    Parameters\n    ----------\n    X : array-like or BinnedEventArray\n        Data to decode.\n    Returns\n    -------\n    mode_pth : np.ndarray\n        Most likely position at each time bin.\n    \"\"\"\n    if self.tuningcurve is None:\n        raise ValueError(\n            \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n        )\n    if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n        ratemap = self.tuningcurve.ratemap\n    else:\n        ratemap = self.tuningcurve\n    if ratemap.ndim == 2:\n        _, _, mode_pth, _ = decode1D(X, ratemap, **kwargs)\n    elif ratemap.ndim == 3:\n        _, _, mode_pth, _ = decode2D(X, ratemap, **kwargs)\n    else:\n        raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n    return mode_pth\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.BayesianDecoder.predict_asa","title":"<code>predict_asa(X, **kwargs)</code>","text":"<p>Predict analog signal array (mean path) from data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or BinnedEventArray</code> <p>Data to decode.</p> required <p>Returns:</p> Name Type Description <code>asa</code> <code>AnalogSignalArray or ndarray</code> <p>Mean path as AnalogSignalArray if possible, else array.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def predict_asa(self, X, **kwargs):\n    \"\"\"\n    Predict analog signal array (mean path) from data X.\n\n    Parameters\n    ----------\n    X : array-like or BinnedEventArray\n        Data to decode.\n    Returns\n    -------\n    asa : AnalogSignalArray or np.ndarray\n        Mean path as AnalogSignalArray if possible, else array.\n    \"\"\"\n    if self.tuningcurve is None:\n        raise ValueError(\n            \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n        )\n    if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n        ratemap = self.tuningcurve.ratemap\n    else:\n        ratemap = self.tuningcurve\n    if ratemap.ndim == 2:\n        _, _, _, mean_pth = decode1D(X, ratemap, **kwargs)\n    elif ratemap.ndim == 3:\n        _, _, _, mean_pth = decode2D(X, ratemap, **kwargs)\n    else:\n        raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n    # Try to return as AnalogSignalArray if possible, with timestamps if available\n    try:\n        from .core import AnalogSignalArray\n\n        abscissa_vals = None\n        # Try to get bin centers from X if possible\n        if hasattr(X, \"bin_centers\"):\n            abscissa_vals = X.bin_centers\n        elif hasattr(X, \"abscissa_vals\"):\n            abscissa_vals = X.abscissa_vals\n        return AnalogSignalArray(data=mean_pth, abscissa_vals=abscissa_vals)\n    except Exception:\n        return mean_pth\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.BayesianDecoder.predict_proba","title":"<code>predict_proba(X, **kwargs)</code>","text":"<p>Predict posterior probabilities for data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or BinnedEventArray</code> <p>Data to decode.</p> required <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def predict_proba(self, X, **kwargs):\n    \"\"\"\n    Predict posterior probabilities for data X.\n\n    Parameters\n    ----------\n    X : array-like or BinnedEventArray\n        Data to decode.\n    Returns\n    -------\n    posterior : np.ndarray\n        Posterior probability matrix.\n    \"\"\"\n    if self.tuningcurve is None:\n        raise ValueError(\n            \"No tuning curve set. Call fit() or provide tuningcurve in constructor.\"\n        )\n    # Use decode1D or decode2D depending on tuning curve\n    if hasattr(self.tuningcurve, \"ratemap\") and hasattr(self.tuningcurve, \"bins\"):\n        # TuningCurve1D or TuningCurve2D object\n        ratemap = self.tuningcurve.ratemap\n    else:\n        ratemap = self.tuningcurve\n    # Try to infer 1D vs 2D\n    if ratemap.ndim == 2:\n        posterior, _, _, _ = decode1D(X, ratemap, **kwargs)\n    elif ratemap.ndim == 3:\n        posterior, _, _, _ = decode2D(X, ratemap, **kwargs)\n    else:\n        raise ValueError(\"Tuning curve must be 2D or 3D array.\")\n    return posterior\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.Cumhist","title":"<code>Cumhist</code>","text":"<p>               Bases: <code>ndarray</code></p> <p>Cumulative histogram with interpolation support.</p> <p>Parameters:</p> Name Type Description Default <code>cumhist</code> <code>ndarray</code> <p>Cumulative histogram values.</p> required <code>bincenters</code> <code>ndarray</code> <p>Bin centers corresponding to the cumulative histogram.</p> required Source code in <code>nelpy/decoding.py</code> <pre><code>class Cumhist(np.ndarray):\n    \"\"\"\n    Cumulative histogram with interpolation support.\n\n    Parameters\n    ----------\n    cumhist : np.ndarray\n        Cumulative histogram values.\n    bincenters : np.ndarray\n        Bin centers corresponding to the cumulative histogram.\n    \"\"\"\n\n    def __new__(cls, cumhist, bincenters):\n        obj = np.asarray(cumhist).view(cls)\n        obj._bincenters = bincenters\n        return obj\n\n    def __call__(self, *val):\n        f = interpolate.interp1d(\n            x=self, y=self._bincenters, kind=\"linear\", fill_value=np.nan\n        )\n        try:\n            vals = f(*val).item()\n        except AttributeError:\n            vals = f(*val)\n\n        return vals\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.ItemGetter_iloc","title":"<code>ItemGetter_iloc</code>","text":"<p>               Bases: <code>object</code></p> <p>.iloc is primarily integer position based (from 0 to length-1 of the axis).</p> <p>Allows integer-based selection of intervals and series in event arrays. Raises IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (conforms with python/numpy slice semantics).</p> <p>Allowed inputs are:     - An integer e.g. 5     - A list or array of integers [4, 3, 0]     - A slice object with ints 1:7</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The parent object to slice.</p> required Source code in <code>nelpy/decoding.py</code> <pre><code>class ItemGetter_iloc(object):\n    \"\"\"\n    .iloc is primarily integer position based (from 0 to length-1 of the axis).\n\n    Allows integer-based selection of intervals and series in event arrays.\n    Raises IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (conforms with python/numpy slice semantics).\n\n    Allowed inputs are:\n        - An integer e.g. 5\n        - A list or array of integers [4, 3, 0]\n        - A slice object with ints 1:7\n\n    Parameters\n    ----------\n    obj : object\n        The parent object to slice.\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = self.obj._slicer[idx]\n        out = copy.copy(self.obj)\n        if isinstance(seriesslice, int):\n            seriesslice = [seriesslice]\n        out._data = out._data[seriesslice]\n        singleseries = len(out._data) == 1\n        if singleseries:\n            out._data = np.array(out._data[0], ndmin=2)\n        out._series_ids = list(\n            np.atleast_1d(np.atleast_1d(out._series_ids)[seriesslice])\n        )\n        out._series_labels = list(\n            np.atleast_1d(np.atleast_1d(out._series_labels)[seriesslice])\n        )\n        # TODO: update tags\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                out.loc = ItemGetter_loc(out)\n                out.iloc = ItemGetter_iloc(out)\n                return out\n        out = out._intervalslicer(intervalslice)\n        out.loc = ItemGetter_loc(out)\n        out.iloc = ItemGetter_iloc(out)\n        return out\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.ItemGetter_loc","title":"<code>ItemGetter_loc</code>","text":"<p>               Bases: <code>object</code></p> <p>.loc is primarily label based (that is, series_id based).</p> <p>Allows label-based selection of intervals and series in event arrays. Raises KeyError when the items are not found.</p> <p>Allowed inputs are:     - A single label, e.g. 5 or 'a' (interpreted as a label, not a position)     - A list or array of labels ['a', 'b', 'c']     - A slice object with labels 'a':'f' (both start and stop are included)</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The parent object to slice.</p> required Source code in <code>nelpy/decoding.py</code> <pre><code>class ItemGetter_loc(object):\n    \"\"\"\n    .loc is primarily label based (that is, series_id based).\n\n    Allows label-based selection of intervals and series in event arrays.\n    Raises KeyError when the items are not found.\n\n    Allowed inputs are:\n        - A single label, e.g. 5 or 'a' (interpreted as a label, not a position)\n        - A list or array of labels ['a', 'b', 'c']\n        - A slice object with labels 'a':'f' (both start and stop are included)\n\n    Parameters\n    ----------\n    obj : object\n        The parent object to slice.\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        intervalslice, seriesslice = self.obj._slicer[idx]\n\n        # first convert series slice into list\n        if isinstance(seriesslice, slice):\n            start = seriesslice.start\n            stop = seriesslice.stop\n            istep = seriesslice.step\n            try:\n                if start is None:\n                    istart = 0\n                else:\n                    istart = self.obj._series_ids.index(start)\n            except ValueError:\n                raise KeyError(\n                    \"series_id {} could not be found in BaseEventArray!\".format(start)\n                )\n            try:\n                if stop is None:\n                    istop = self.obj.n_series\n                else:\n                    istop = self.obj._series_ids.index(stop) + 1\n            except ValueError:\n                raise KeyError(\n                    \"series_id {} could not be found in BaseEventArray!\".format(stop)\n                )\n            if istep is None:\n                istep = 1\n            if istep &lt; 0:\n                istop -= 1\n                istart -= 1\n                istart, istop = istop, istart\n            series_idx_list = list(range(istart, istop, istep))\n        else:\n            series_idx_list = []\n            seriesslice = np.atleast_1d(seriesslice)\n            for series in seriesslice:\n                try:\n                    uidx = self.obj.series_ids.index(series)\n                except ValueError:\n                    raise KeyError(\n                        \"series_id {} could not be found in BaseEventArray!\".format(\n                            series\n                        )\n                    )\n                else:\n                    series_idx_list.append(uidx)\n\n        if not isinstance(series_idx_list, list):\n            series_idx_list = list(series_idx_list)\n        out = copy.copy(self.obj)\n        try:\n            out._data = out._data[series_idx_list]\n            singleseries = len(out._data) == 1\n        except AttributeError:\n            out._data = out._data[series_idx_list]\n            singleseries = len(out._data) == 1\n\n        if singleseries:\n            out._data = np.array(out._data[0], ndmin=2)\n        out._series_ids = list(\n            np.atleast_1d(np.atleast_1d(out._series_ids)[series_idx_list])\n        )\n        out._series_labels = list(\n            np.atleast_1d(np.atleast_1d(out._series_labels)[series_idx_list])\n        )\n        # TODO: update tags\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                out.loc = ItemGetter_loc(out)\n                out.iloc = ItemGetter_iloc(out)\n                return out\n        out = out._intervalslicer(intervalslice)\n        out.loc = ItemGetter_loc(out)\n        out.iloc = ItemGetter_iloc(out)\n        return out\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.cumulative_dist_decoding_error","title":"<code>cumulative_dist_decoding_error(bst, *, tuningcurve, extern, decodefunc=decode1D, transfunc=None, n_bins=None)</code>","text":"<p>Compute the cumulative distribution of decoding errors using a fixed tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all epochs to decode.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve to use for decoding.</p> required <code>extern</code> <code>object</code> <p>Query-able object of external correlates (e.g., position AnalogSignalArray).</p> required <code>decodefunc</code> <code>callable</code> <p>Decoding function to use (default is decode1D).</p> <code>decode1D</code> <code>transfunc</code> <code>callable</code> <p>Function to transform external variable (default is None).</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>Number of decoding error bins (default is 200).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>cumhist</code> <code>Cumhist</code> <p>Cumulative histogram of decoding errors.</p> <code>bincenters</code> <code>ndarray</code> <p>Bin centers for the cumulative histogram.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def cumulative_dist_decoding_error(\n    bst, *, tuningcurve, extern, decodefunc=decode1D, transfunc=None, n_bins=None\n):\n    \"\"\"\n    Compute the cumulative distribution of decoding errors using a fixed tuning curve.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all epochs to decode.\n    tuningcurve : TuningCurve1D\n        Tuning curve to use for decoding.\n    extern : object\n        Query-able object of external correlates (e.g., position AnalogSignalArray).\n    decodefunc : callable, optional\n        Decoding function to use (default is decode1D).\n    transfunc : callable, optional\n        Function to transform external variable (default is None).\n    n_bins : int, optional\n        Number of decoding error bins (default is 200).\n\n    Returns\n    -------\n    cumhist : Cumhist\n        Cumulative histogram of decoding errors.\n    bincenters : np.ndarray\n        Bin centers for the cumulative histogram.\n    \"\"\"\n\n    def _trans_func(extern, at):\n        \"\"\"Default transform function to map extern into numerical bins\"\"\"\n\n        _, ext = extern.asarray(at=at)\n\n        return ext\n\n    if transfunc is None:\n        transfunc = _trans_func\n    if n_bins is None:\n        n_bins = 200\n\n    # indices of training and validation epochs / events\n\n    max_error = tuningcurve.bins[-1] - tuningcurve.bins[0]\n\n    posterior, _, mode_pth, mean_pth = decodefunc(bst=bst, ratemap=tuningcurve)\n    target = transfunc(extern, at=bst.bin_centers)\n    hist, bins = np.histogram(\n        np.abs(target - mean_pth), bins=n_bins, range=(0, max_error)\n    )\n\n    # build cumulative error distribution\n    cumhist = np.cumsum(hist)\n    cumhist = cumhist / cumhist[-1]\n    bincenters = (bins + (bins[1] - bins[0]) / 2)[:-1]\n\n    # modify to start at (0,0):\n    cumhist = np.insert(cumhist, 0, 0)\n    bincenters = np.insert(bincenters, 0, 0)\n\n    # modify to end at (max_error,1):\n    cumhist = np.append(cumhist, 1)\n    bincenters = np.append(bincenters, max_error)\n\n    cumhist = Cumhist(cumhist, bincenters)\n\n    return cumhist, bincenters\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.cumulative_dist_decoding_error_using_xval","title":"<code>cumulative_dist_decoding_error_using_xval(bst, extern, *, decodefunc=decode1D, k=5, transfunc=None, n_extern=100, extmin=0, extmax=100, sigma=3, n_bins=None, randomize=False)</code>","text":"<p>Compute the cumulative distribution of decoding errors using k-fold cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all epochs to decode.</p> required <code>extern</code> <code>object</code> <p>Query-able object of external correlates (e.g., position AnalogSignalArray).</p> required <code>decodefunc</code> <code>callable</code> <p>Decoding function to use (default is decode1D).</p> <code>decode1D</code> <code>k</code> <code>int</code> <p>Number of folds for cross-validation (default is 5).</p> <code>5</code> <code>transfunc</code> <code>callable</code> <p>Function to transform external variable (default is None).</p> <code>None</code> <code>n_extern</code> <code>int</code> <p>Number of external bins (default is 100).</p> <code>100</code> <code>extmin</code> <code>float</code> <p>Minimum value of external variable (default is 0).</p> <code>0</code> <code>extmax</code> <code>float</code> <p>Maximum value of external variable (default is 100).</p> <code>100</code> <code>sigma</code> <code>float</code> <p>Smoothing parameter for tuning curve (default is 3).</p> <code>3</code> <code>n_bins</code> <code>int</code> <p>Number of decoding error bins (default is 200).</p> <code>None</code> <code>randomize</code> <code>bool</code> <p>If True, randomize the order of epochs (default is False).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>cumhist</code> <code>Cumhist</code> <p>Cumulative histogram of decoding errors.</p> <code>bincenters</code> <code>ndarray</code> <p>Bin centers for the cumulative histogram.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def cumulative_dist_decoding_error_using_xval(\n    bst,\n    extern,\n    *,\n    decodefunc=decode1D,\n    k=5,\n    transfunc=None,\n    n_extern=100,\n    extmin=0,\n    extmax=100,\n    sigma=3,\n    n_bins=None,\n    randomize=False,\n):\n    \"\"\"\n    Compute the cumulative distribution of decoding errors using k-fold cross-validation.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all epochs to decode.\n    extern : object\n        Query-able object of external correlates (e.g., position AnalogSignalArray).\n    decodefunc : callable, optional\n        Decoding function to use (default is decode1D).\n    k : int, optional\n        Number of folds for cross-validation (default is 5).\n    transfunc : callable, optional\n        Function to transform external variable (default is None).\n    n_extern : int, optional\n        Number of external bins (default is 100).\n    extmin : float, optional\n        Minimum value of external variable (default is 0).\n    extmax : float, optional\n        Maximum value of external variable (default is 100).\n    sigma : float, optional\n        Smoothing parameter for tuning curve (default is 3).\n    n_bins : int, optional\n        Number of decoding error bins (default is 200).\n    randomize : bool, optional\n        If True, randomize the order of epochs (default is False).\n\n    Returns\n    -------\n    cumhist : Cumhist\n        Cumulative histogram of decoding errors.\n    bincenters : np.ndarray\n        Bin centers for the cumulative histogram.\n    \"\"\"\n\n    def _trans_func(extern, at):\n        \"\"\"Default transform function to map extern into numerical bins\"\"\"\n\n        _, ext = extern.asarray(at=at)\n\n        return ext\n\n    if transfunc is None:\n        transfunc = _trans_func\n\n    if n_bins is None:\n        n_bins = 200\n\n    max_error = extmax - extmin\n\n    # indices of training and validation epochs / events\n\n    hist = np.zeros(n_bins)\n    for training, validation in k_fold_cross_validation(\n        bst.n_epochs, k=k, randomize=randomize\n    ):\n        # estimate place fields using bst[training]\n        tc = auxiliary.TuningCurve1D(\n            bst=bst[training],\n            extern=extern,\n            n_extern=n_extern,\n            extmin=extmin,\n            extmax=extmax,\n            sigma=sigma,\n        )\n        # decode position using bst[validation]\n        posterior, _, mode_pth, mean_pth = decodefunc(bst[validation], tc)\n        # calculate validation error (for current fold) by comapring\n        # decoded pos v target pos\n        target = transfunc(extern, at=bst[validation].bin_centers)\n\n        histnew, bins = np.histogram(\n            np.abs(target - mean_pth), bins=n_bins, range=(0, max_error)\n        )\n        hist = hist + histnew\n\n    # build cumulative error distribution\n    cumhist = np.cumsum(hist)\n    cumhist = cumhist / cumhist[-1]\n    bincenters = (bins + (bins[1] - bins[0]) / 2)[:-1]\n\n    # modify to start at (0,0):\n    cumhist = np.insert(cumhist, 0, 0)\n    bincenters = np.insert(bincenters, 0, 0)\n\n    # modify to end at (max_error,1):\n    cumhist = np.append(cumhist, 1)\n    bincenters = np.append(bincenters, max_error)\n\n    cumhist = Cumhist(cumhist, bincenters)\n    return cumhist, bincenters\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.decode1D","title":"<code>decode1D(bst, ratemap, xmin=0, xmax=100, w=1, nospk_prior=None, _skip_empty_bins=True)</code>","text":"<p>Decode binned spike trains using a 1D ratemap (Bayesian decoding).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to decode.</p> required <code>ratemap</code> <code>array_like or TuningCurve1D</code> <p>Firing rate map with shape (n_units, n_ext), where n_ext is the number of external correlates (e.g., position bins). The rate map is in spks/second.</p> required <code>xmin</code> <code>float</code> <p>Minimum value of external variable (default is 0).</p> <code>0</code> <code>xmax</code> <code>float</code> <p>Maximum value of external variable (default is 100).</p> <code>100</code> <code>w</code> <code>int</code> <p>Window size for decoding (default is 1).</p> <code>1</code> <code>nospk_prior</code> <code>array_like or float</code> <p>Prior distribution over external correlates with shape (n_ext,). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.</p> <code>None</code> <code>_skip_empty_bins</code> <code>bool</code> <p>If True, skip bins with no spikes. If False, fill with prior.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray</code> <p>Posterior distribution with shape (n_ext, n_posterior_bins).</p> <code>cum_posterior_lengths</code> <code>ndarray</code> <p>Cumulative posterior lengths for each epoch.</p> <code>mode_pth</code> <code>ndarray</code> <p>Most likely position at each time bin.</p> <code>mean_pth</code> <code>ndarray</code> <p>Expected position at each time bin.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode1D(bst, ratemap)\n</code></pre> Source code in <code>nelpy/decoding.py</code> <pre><code>def decode1D(\n    bst, ratemap, xmin=0, xmax=100, w=1, nospk_prior=None, _skip_empty_bins=True\n):\n    \"\"\"\n    Decode binned spike trains using a 1D ratemap (Bayesian decoding).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to decode.\n    ratemap : array_like or TuningCurve1D\n        Firing rate map with shape (n_units, n_ext), where n_ext is the number of external correlates (e.g., position bins). The rate map is in spks/second.\n    xmin : float, optional\n        Minimum value of external variable (default is 0).\n    xmax : float, optional\n        Maximum value of external variable (default is 100).\n    w : int, optional\n        Window size for decoding (default is 1).\n    nospk_prior : array_like or float, optional\n        Prior distribution over external correlates with shape (n_ext,). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.\n    _skip_empty_bins : bool, optional\n        If True, skip bins with no spikes. If False, fill with prior.\n\n    Returns\n    -------\n    posterior : np.ndarray\n        Posterior distribution with shape (n_ext, n_posterior_bins).\n    cum_posterior_lengths : np.ndarray\n        Cumulative posterior lengths for each epoch.\n    mode_pth : np.ndarray\n        Most likely position at each time bin.\n    mean_pth : np.ndarray\n        Expected position at each time bin.\n\n    Examples\n    --------\n    &gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode1D(bst, ratemap)\n    \"\"\"\n\n    if w is None:\n        w = 1\n    assert float(w).is_integer(), \"w must be a positive integer!\"\n    assert w &gt; 0, \"w must be a positive integer!\"\n\n    n_units, t_bins = bst.data.shape\n    _, n_xbins = ratemap.shape\n\n    # if we pass a TuningCurve1D object, extract the ratemap and re-order\n    # units if necessary\n    if isinstance(ratemap, auxiliary.TuningCurve1D) | isinstance(\n        ratemap, auxiliary._tuningcurve.TuningCurve1D\n    ):\n        # xmin = ratemap.bins[0]\n        xmax = ratemap.bins[-1]\n        bin_centers = ratemap.bin_centers\n        # re-order units if necessary\n        ratemap = ratemap.reorder_units_by_ids(bst.unit_ids)\n        ratemap = ratemap.ratemap\n    else:\n        # xmin = 0\n        xmax = n_xbins\n        bin_centers = np.arange(n_xbins)\n\n    if nospk_prior is None:\n        nospk_prior = np.full(n_xbins, np.nan)\n    elif isinstance(nospk_prior, numbers.Number):\n        nospk_prior = np.full(n_xbins, 1.0)\n\n    assert nospk_prior.shape[0] == n_xbins, \"prior must have length {}\".format(n_xbins)\n    assert nospk_prior.size == n_xbins, (\n        \"prior must be a 1D array with length {}\".format(n_xbins)\n    )\n\n    lfx = np.log(ratemap)\n\n    eterm = -ratemap.sum(axis=0) * bst.ds * w\n\n    # if we decode using multiple bins at a time (w&gt;1) then we have to decode each epoch separately:\n\n    # first, we determine the number of bins we will decode. This requires us to scan over the epochs\n    n_bins = 0\n    cumlengths = np.cumsum(bst.lengths)\n    posterior_lengths = np.zeros(bst.n_epochs, dtype=int)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        datalen = to_idx - prev_idx\n        prev_idx = to_idx\n        posterior_lengths[ii] = np.max((1, datalen - w + 1))\n\n    n_bins = posterior_lengths.sum()\n    posterior = np.zeros((n_xbins, n_bins))\n\n    # next, we decode each epoch separately, one bin at a time\n    cum_posterior_lengths = np.insert(np.cumsum(posterior_lengths), 0, 0)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        data = bst.data[:, prev_idx:to_idx]\n        prev_idx = to_idx\n        datacum = np.cumsum(\n            data, axis=1\n        )  # ii'th data segment, with column of zeros prepended\n        datacum = np.hstack((np.zeros((n_units, 1)), datacum))\n        re = w  # right edge ptr\n        # TODO: check if datalen &lt; w and act appropriately\n        if posterior_lengths[ii] &gt; 1:  # more than one full window fits into data length\n            for tt in range(posterior_lengths[ii]):\n                obs = datacum[:, re] - datacum[:, re - w]  # spikes in window of size w\n                re += 1\n                post_idx = cum_posterior_lengths[ii] + tt\n                if obs.sum() == 0 and _skip_empty_bins:\n                    # no spikes to decode in window!\n                    posterior[:, post_idx] = nospk_prior\n                else:\n                    posterior[:, post_idx] = (\n                        np.tile(np.array(obs, ndmin=2).T, n_xbins) * lfx\n                    ).sum(axis=0) + eterm\n        else:  # only one window can fit in, and perhaps only partially. We just take all the data we can get,\n            # and ignore the scaling problem where the window size is now possibly less than bst.ds*w\n            post_idx = cum_posterior_lengths[ii]\n            obs = datacum[:, -1]  # spikes in window of size at most w\n            if obs.sum() == 0 and _skip_empty_bins:\n                # no spikes to decode in window!\n                posterior[:, post_idx] = nospk_prior\n            else:\n                posterior[:, post_idx] = (\n                    np.tile(np.array(obs, ndmin=2).T, n_xbins) * lfx\n                ).sum(axis=0) + eterm\n\n    # normalize posterior:\n    posterior = np.exp(posterior - logsumexp(posterior, axis=0))\n\n    # TODO: what was my rationale behid the following? Why not use bin centers?\n    # _, bins = np.histogram([], bins=n_xbins, range=(xmin,xmax))\n    # xbins = (bins + xmax/n_xbins)[:-1]\n\n    mode_pth = np.argmax(posterior, axis=0) * xmax / n_xbins\n    mode_pth = np.where(np.isnan(posterior.sum(axis=0)), np.nan, mode_pth)\n    mean_pth = (bin_centers * posterior.T).sum(axis=1)\n    return posterior, cum_posterior_lengths, mode_pth, mean_pth\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.decode2D","title":"<code>decode2D(bst, ratemap, xmin=0, xmax=100, ymin=0, ymax=100, w=1, nospk_prior=None, _skip_empty_bins=True)</code>","text":"<p>Decode binned spike trains using a 2D ratemap (Bayesian decoding).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to decode.</p> required <code>ratemap</code> <code>array_like or TuningCurve2D</code> <p>Firing rate map with shape (n_units, ext_nx, ext_ny), where ext_nx and ext_ny are the number of external correlates (e.g., position bins). The rate map is in spks/second.</p> required <code>xmin</code> <code>float</code> <p>Minimum x value of external variable (default is 0).</p> <code>0</code> <code>xmax</code> <code>float</code> <p>Maximum x value of external variable (default is 100).</p> <code>100</code> <code>ymin</code> <code>float</code> <p>Minimum y value of external variable (default is 0).</p> <code>0</code> <code>ymax</code> <code>float</code> <p>Maximum y value of external variable (default is 100).</p> <code>100</code> <code>w</code> <code>int</code> <p>Window size for decoding (default is 1).</p> <code>1</code> <code>nospk_prior</code> <code>array_like or float</code> <p>Prior distribution over external correlates with shape (ext_nx, ext_ny). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.</p> <code>None</code> <code>_skip_empty_bins</code> <code>bool</code> <p>If True, skip bins with no spikes. If False, fill with prior.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>posterior</code> <code>ndarray</code> <p>Posterior distribution with shape (ext_nx, ext_ny, n_posterior_bins).</p> <code>cum_posterior_lengths</code> <code>ndarray</code> <p>Cumulative posterior lengths for each epoch.</p> <code>mode_pth</code> <code>ndarray</code> <p>Most likely (x, y) position at each time bin.</p> <code>mean_pth</code> <code>ndarray</code> <p>Expected (x, y) position at each time bin.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode2D(bst, ratemap)\n</code></pre> Source code in <code>nelpy/decoding.py</code> <pre><code>def decode2D(\n    bst,\n    ratemap,\n    xmin=0,\n    xmax=100,\n    ymin=0,\n    ymax=100,\n    w=1,\n    nospk_prior=None,\n    _skip_empty_bins=True,\n):\n    \"\"\"\n    Decode binned spike trains using a 2D ratemap (Bayesian decoding).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to decode.\n    ratemap : array_like or TuningCurve2D\n        Firing rate map with shape (n_units, ext_nx, ext_ny), where ext_nx and ext_ny are the number of external correlates (e.g., position bins). The rate map is in spks/second.\n    xmin : float, optional\n        Minimum x value of external variable (default is 0).\n    xmax : float, optional\n        Maximum x value of external variable (default is 100).\n    ymin : float, optional\n        Minimum y value of external variable (default is 0).\n    ymax : float, optional\n        Maximum y value of external variable (default is 100).\n    w : int, optional\n        Window size for decoding (default is 1).\n    nospk_prior : array_like or float, optional\n        Prior distribution over external correlates with shape (ext_nx, ext_ny). Used if no spikes are observed in a decoding window. If scalar, a uniform prior is assumed. Default is np.nan.\n    _skip_empty_bins : bool, optional\n        If True, skip bins with no spikes. If False, fill with prior.\n\n    Returns\n    -------\n    posterior : np.ndarray\n        Posterior distribution with shape (ext_nx, ext_ny, n_posterior_bins).\n    cum_posterior_lengths : np.ndarray\n        Cumulative posterior lengths for each epoch.\n    mode_pth : np.ndarray\n        Most likely (x, y) position at each time bin.\n    mean_pth : np.ndarray\n        Expected (x, y) position at each time bin.\n\n    Examples\n    --------\n    &gt;&gt;&gt; posterior, cum_posterior_lengths, mode_pth, mean_pth = decode2D(bst, ratemap)\n    \"\"\"\n\n    def tile_obs(obs, nx, ny):\n        n_units = len(obs)\n        out = np.zeros((n_units, nx, ny))\n        for unit in range(n_units):\n            out[unit, :, :] = obs[unit]\n        return out\n\n    if w is None:\n        w = 1\n    assert float(w).is_integer(), \"w must be a positive integer!\"\n    assert w &gt; 0, \"w must be a positive integer!\"\n\n    n_units, t_bins = bst.data.shape\n\n    xbins = None\n    ybins = None\n\n    # if we pass a TuningCurve2D object, extract the ratemap and re-order\n    # units if necessary\n    if isinstance(ratemap, auxiliary.TuningCurve2D):\n        xbins = ratemap.xbins\n        ybins = ratemap.ybins\n        xbin_centers = ratemap.xbin_centers\n        ybin_centers = ratemap.ybin_centers\n        # re-order units if necessary\n        ratemap = ratemap.reorder_units_by_ids(bst.unit_ids)\n        ratemap = ratemap.ratemap\n\n    _, n_xbins, n_ybins = ratemap.shape\n\n    if nospk_prior is None:\n        nospk_prior = np.full((n_xbins, n_ybins), np.nan)\n    elif isinstance(nospk_prior, numbers.Number):\n        nospk_prior = np.full((n_xbins, n_ybins), 1.0)\n\n    assert nospk_prior.shape == (\n        n_xbins,\n        n_ybins,\n    ), \"prior must have shape ({}, {})\".format(n_xbins, n_ybins)\n\n    lfx = np.log(ratemap)\n\n    eterm = -ratemap.sum(axis=0) * bst.ds * w\n\n    # if we decode using multiple bins at a time (w&gt;1) then we have to decode each epoch separately:\n\n    # first, we determine the number of bins we will decode. This requires us to scan over the epochs\n    n_tbins = 0\n    cumlengths = np.cumsum(bst.lengths)\n    posterior_lengths = np.zeros(bst.n_epochs, dtype=int)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        datalen = to_idx - prev_idx\n        prev_idx = to_idx\n        posterior_lengths[ii] = np.max((1, datalen - w + 1))\n\n    n_tbins = posterior_lengths.sum()\n\n    ########################################################################\n    posterior = np.zeros((n_xbins, n_ybins, n_tbins))\n\n    # next, we decode each epoch separately, one bin at a time\n    cum_posterior_lengths = np.insert(np.cumsum(posterior_lengths), 0, 0)\n    prev_idx = 0\n    for ii, to_idx in enumerate(cumlengths):\n        data = bst.data[:, prev_idx:to_idx]\n        prev_idx = to_idx\n        datacum = np.cumsum(\n            data, axis=1\n        )  # ii'th data segment, with column of zeros prepended\n        datacum = np.hstack((np.zeros((n_units, 1)), datacum))\n        re = w  # right edge ptr\n        # TODO: check if datalen &lt; w and act appropriately\n        if posterior_lengths[ii] &gt; 1:  # more than one full window fits into data length\n            for tt in range(posterior_lengths[ii]):\n                obs = datacum[:, re] - datacum[:, re - w]  # spikes in window of size w\n                re += 1\n                post_idx = cum_posterior_lengths[ii] + tt\n                if obs.sum() == 0 and not _skip_empty_bins:\n                    # no spikes to decode in window!\n                    posterior[:, :, post_idx] = nospk_prior\n                else:\n                    posterior[:, :, post_idx] = (\n                        tile_obs(obs, n_xbins, n_ybins) * lfx\n                    ).sum(axis=0) + eterm\n        else:  # only one window can fit in, and perhaps only partially. We just take all the data we can get,\n            # and ignore the scaling problem where the window size is now possibly less than bst.ds*w\n            post_idx = cum_posterior_lengths[ii]\n            obs = datacum[:, -1]  # spikes in window of size at most w\n            if obs.sum() == 0 and not _skip_empty_bins:\n                # no spikes to decode in window!\n                posterior[:, :, post_idx] = nospk_prior\n            else:\n                posterior[:, :, post_idx] = (tile_obs(obs, n_xbins, n_ybins) * lfx).sum(\n                    axis=0\n                ) + eterm\n\n    # normalize posterior:\n    # see http://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n\n    for tt in range(n_tbins):\n        posterior[:, :, tt] = posterior[:, :, tt] - posterior[:, :, tt].max()\n        posterior[:, :, tt] = np.exp(posterior[:, :, tt])\n        posterior[:, :, tt] = posterior[:, :, tt] / posterior[:, :, tt].sum()\n\n    # if xbins is None:\n    #     _, bins = np.histogram([], bins=n_xbins, range=(xmin,xmax))\n    #     xbins = (bins + xmax/n_xbins)[:-1]\n    # if ybins is None:\n    #     _, bins = np.histogram([], bins=n_ybins, range=(ymin,ymax))\n    #     ybins = (bins + ymax/n_ybins)[:-1]\n\n    mode_pth = np.zeros((2, n_tbins))\n    for tt in range(n_tbins):\n        if np.any(np.isnan(posterior[:, :, tt])):\n            mode_pth[0, tt] = np.nan\n            mode_pth[0, tt] = np.nan\n        else:\n            x_, y_ = np.unravel_index(\n                np.argmax(posterior[:, :, tt]), (n_xbins, n_ybins)\n            )\n            mode_pth[0, tt] = xbins[x_]\n            mode_pth[1, tt] = ybins[y_]\n\n    expected_x = (xbin_centers * posterior.sum(axis=1).T).sum(axis=1)\n    expected_y = (ybin_centers * posterior.sum(axis=0).T).sum(axis=1)\n    mean_pth = np.vstack((expected_x, expected_y))\n\n    posterior = np.transpose(posterior, axes=[1, 0, 2])\n\n    return posterior, cum_posterior_lengths, mode_pth, mean_pth\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.get_mean_pth_from_array","title":"<code>get_mean_pth_from_array(posterior, tuningcurve=None)</code>","text":"<p>Compute the mean path (expected position) from a posterior probability matrix.</p> <p>If a tuning curve is provided, the mean is mapped back to external coordinates/units. Otherwise, the mean is in bin space.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for mapping bins to external coordinates.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mean_pth</code> <code>ndarray</code> <p>Expected position at each time bin.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def get_mean_pth_from_array(posterior, tuningcurve=None):\n    \"\"\"\n    Compute the mean path (expected position) from a posterior probability matrix.\n\n    If a tuning curve is provided, the mean is mapped back to external coordinates/units.\n    Otherwise, the mean is in bin space.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    tuningcurve : TuningCurve1D, optional\n        Tuning curve for mapping bins to external coordinates.\n\n    Returns\n    -------\n    mean_pth : np.ndarray\n        Expected position at each time bin.\n    \"\"\"\n    n_xbins = posterior.shape[0]\n\n    if tuningcurve is None:\n        xmin = 0\n        xmax = 1\n    else:\n        # TODO: this only works for TuningCurve1D currently\n        if isinstance(tuningcurve, auxiliary.TuningCurve1D):\n            xmin = tuningcurve.bins[0]\n            xmax = tuningcurve.bins[-1]\n        else:\n            raise TypeError(\"tuningcurve type not yet supported!\")\n\n    _, bins = np.histogram([], bins=n_xbins, range=(xmin, xmax))\n    xbins = (bins + xmax / n_xbins)[:-1]\n\n    mean_pth = (xbins * posterior.T).sum(axis=1)\n\n    return mean_pth\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.get_mode_pth_from_array","title":"<code>get_mode_pth_from_array(posterior, tuningcurve=None)</code>","text":"<p>Compute the mode path (most likely position) from a posterior probability matrix.</p> <p>If a tuning curve is provided, the mode is mapped back to external coordinates/units. Otherwise, the mode is in bin space.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for mapping bins to external coordinates.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mode_pth</code> <code>ndarray</code> <p>Most likely position at each time bin.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def get_mode_pth_from_array(posterior, tuningcurve=None):\n    \"\"\"\n    Compute the mode path (most likely position) from a posterior probability matrix.\n\n    If a tuning curve is provided, the mode is mapped back to external coordinates/units.\n    Otherwise, the mode is in bin space.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    tuningcurve : TuningCurve1D, optional\n        Tuning curve for mapping bins to external coordinates.\n\n    Returns\n    -------\n    mode_pth : np.ndarray\n        Most likely position at each time bin.\n    \"\"\"\n    n_xbins = posterior.shape[0]\n\n    if tuningcurve is None:\n        xmin = 0\n        xmax = n_xbins\n    else:\n        # TODO: this only works for TuningCurve1D currently\n        if isinstance(tuningcurve, auxiliary.TuningCurve1D):\n            xmin = tuningcurve.bins[0]\n            xmax = tuningcurve.bins[-1]\n        else:\n            raise TypeError(\"tuningcurve type not yet supported!\")\n\n    _, bins = np.histogram([], bins=n_xbins, range=(xmin, xmax))\n    # xbins = (bins + xmax / n_xbins)[:-1]\n\n    mode_pth = np.argmax(posterior, axis=0) * xmax / n_xbins\n    mode_pth = np.where(np.isnan(posterior.sum(axis=0)), np.nan, mode_pth)\n\n    return mode_pth\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.k_fold_cross_validation","title":"<code>k_fold_cross_validation(X, k=None, randomize=False)</code>","text":"<p>Generate K (training, validation) pairs from the items in X for cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>list or int</code> <p>List of items, list of indices, or integer number of indices.</p> required <code>k</code> <code>int or str</code> <p>Number of folds for k-fold cross-validation. 'loo' or 'LOO' for leave-one-out. Default is 5.</p> <code>None</code> <code>randomize</code> <code>bool</code> <p>If True, shuffle X before partitioning. Default is False.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>training</code> <code>list</code> <p>Training set indices.</p> <code>validation</code> <code>list</code> <p>Validation set indices.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = [i for i in range(97)]\n&gt;&gt;&gt; for training, validation in k_fold_cross_validation(X, k=5):\n...     print(training, validation)\n</code></pre> Source code in <code>nelpy/decoding.py</code> <pre><code>def k_fold_cross_validation(X, k=None, randomize=False):\n    \"\"\"\n    Generate K (training, validation) pairs from the items in X for cross-validation.\n\n    Parameters\n    ----------\n    X : list or int\n        List of items, list of indices, or integer number of indices.\n    k : int or str, optional\n        Number of folds for k-fold cross-validation. 'loo' or 'LOO' for leave-one-out. Default is 5.\n    randomize : bool, optional\n        If True, shuffle X before partitioning. Default is False.\n\n    Yields\n    ------\n    training : list\n        Training set indices.\n    validation : list\n        Validation set indices.\n\n    Examples\n    --------\n    &gt;&gt;&gt; X = [i for i in range(97)]\n    &gt;&gt;&gt; for training, validation in k_fold_cross_validation(X, k=5):\n    ...     print(training, validation)\n    \"\"\"\n    # deal with default values:\n    if isinstance(X, int):\n        X = range(X)\n    n_samples = len(X)\n    if k is None:\n        k = 5\n    elif k == \"loo\" or k == \"LOO\":\n        k = n_samples\n\n    if randomize:\n        from random import shuffle\n\n        X = list(X)\n        shuffle(X)\n    for _k_ in range(k):\n        training = [x for i, x in enumerate(X) if i % k != _k_]\n        validation = [x for i, x in enumerate(X) if i % k == _k_]\n        try:\n            yield training, validation\n        except StopIteration:\n            return\n</code></pre>"},{"location":"reference/nelpy/decoding/#nelpy.decoding.rmse","title":"<code>rmse(predictions, targets)</code>","text":"<p>Calculate the root mean squared error (RMSE) between predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array_like</code> <p>Array of predicted values.</p> required <code>targets</code> <code>array_like</code> <p>Array of target values.</p> required <p>Returns:</p> Name Type Description <code>rmse</code> <code>float</code> <p>Root mean squared error of the predictions with respect to the targets.</p> Source code in <code>nelpy/decoding.py</code> <pre><code>def rmse(predictions, targets):\n    \"\"\"\n    Calculate the root mean squared error (RMSE) between predictions and targets.\n\n    Parameters\n    ----------\n    predictions : array_like\n        Array of predicted values.\n    targets : array_like\n        Array of target values.\n\n    Returns\n    -------\n    rmse : float\n        Root mean squared error of the predictions with respect to the targets.\n    \"\"\"\n    predictions = np.asanyarray(predictions)\n    targets = np.asanyarray(targets)\n    rmse = np.sqrt(np.nanmean((predictions - targets) ** 2))\n    return rmse\n</code></pre>"},{"location":"reference/nelpy/estimators/","title":"nelpy.estimators","text":""},{"location":"reference/nelpy/estimators/#nelpy.estimators.BayesianDecoderTemp","title":"<code>BayesianDecoderTemp</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Bayesian decoder wrapper class.</p> <p>This class implements a Bayesian decoder for neural data, supporting various estimation modes.</p> <p>Parameters:</p> Name Type Description Default <code>rate_estimator</code> <code>FiringRateEstimator</code> <p>The firing rate estimator to use.</p> <code>None</code> <code>w</code> <code>any</code> <p>Window parameter for decoding.</p> <code>None</code> <code>ratemap</code> <code>RateMap</code> <p>Precomputed rate map.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>rate_estimator</code> <code>FiringRateEstimator</code> <p>The firing rate estimator.</p> <code>ratemap</code> <code>RateMap</code> <p>The estimated or provided rate map.</p> <code>w</code> <code>any</code> <p>Window parameter.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class BayesianDecoderTemp(BaseEstimator):\n    \"\"\"\n    Bayesian decoder wrapper class.\n\n    This class implements a Bayesian decoder for neural data, supporting various estimation modes.\n\n    Parameters\n    ----------\n    rate_estimator : FiringRateEstimator, optional\n        The firing rate estimator to use.\n    w : any, optional\n        Window parameter for decoding.\n    ratemap : RateMap, optional\n        Precomputed rate map.\n\n    Attributes\n    ----------\n    rate_estimator : FiringRateEstimator\n        The firing rate estimator.\n    ratemap : RateMap\n        The estimated or provided rate map.\n    w : any\n        Window parameter.\n    \"\"\"\n\n    def __init__(self, rate_estimator=None, w=None, ratemap=None):\n        self._rate_estimator = self._validate_rate_estimator(rate_estimator)\n        self._ratemap = self._validate_ratemap(ratemap)\n        self._w = self._validate_window(w)\n\n    @property\n    def rate_estimator(self):\n        return self._rate_estimator\n\n    @property\n    def ratemap(self):\n        return self._ratemap\n\n    @property\n    def w(self):\n        return self._w\n\n    @staticmethod\n    def _validate_rate_estimator(rate_estimator):\n        if rate_estimator is None:\n            rate_estimator = FiringRateEstimator()\n        elif not isinstance(rate_estimator, FiringRateEstimator):\n            raise TypeError(\n                \"'rate_estimator' must be a nelpy FiringRateEstimator() type!\"\n            )\n        return rate_estimator\n\n    @staticmethod\n    def _validate_ratemap(ratemap):\n        if ratemap is None:\n            ratemap = NDRateMap()\n        elif not isinstance(ratemap, NDRateMap):\n            raise TypeError(\"'ratemap' must be a nelpy RateMap() type!\")\n        return ratemap\n\n    @staticmethod\n    def _validate_window(w):\n        if w is None:\n            w = DataWindow(sum=True, bin_width=1)\n        elif not isinstance(w, DataWindow):\n            raise TypeError(\"w must be a nelpy DataWindow() type!\")\n        else:\n            w = copy.copy(w)\n        if w._sum is False:\n            logging.warning(\n                \"BayesianDecoder requires DataWindow (w) to have sum=True; changing to True\"\n            )\n            w._sum = True\n        if w.bin_width is None:\n            w.bin_width = 1\n        return w\n\n    def _check_X_dt(self, X, *, lengths=None, dt=None):\n        if isinstance(X, core.BinnedEventArray):\n            if dt is not None:\n                logging.warning(\n                    \"A {} was passed in, so 'dt' will be ignored...\".format(X.type_name)\n                )\n            dt = X.ds\n            if self._w.bin_width != dt:\n                raise ValueError(\n                    \"BayesianDecoder was fit with a bin_width of {}, but is being used to predict data with a bin_width of {}\".format(\n                        self.w.bin_width, dt\n                    )\n                )\n            X, T = self.w.transform(X, lengths=lengths, sum=True)\n        else:\n            if dt is not None:\n                if self._w.bin_width != dt:\n                    raise ValueError(\n                        \"BayesianDecoder was fit with a bin_width of {}, but is being used to predict data with a bin_width of {}\".format(\n                            self.w.bin_width, dt\n                        )\n                    )\n            else:\n                dt = self._w.bin_width\n\n        return X, dt\n\n    def _check_X_y(self, X, y, *, method=\"score\", lengths=None):\n        if isinstance(X, core.BinnedEventArray):\n            if method == \"fit\":\n                self._w.bin_width = X.ds\n                logging.info(\"Updating DataWindow.bin_width from training data.\")\n            else:\n                if self._w.bin_width != X.ds:\n                    raise ValueError(\n                        \"BayesianDecoder was fit with a bin_width of {}, but is being used to predict data with a bin_width of {}\".format(\n                            self.w.bin_width, X.ds\n                        )\n                    )\n\n            X, T = self.w.transform(X, lengths=lengths, sum=True)\n\n            if isinstance(y, core.RegularlySampledAnalogSignalArray):\n                y = y(T).T\n\n        if isinstance(y, core.RegularlySampledAnalogSignalArray):\n            raise TypeError(\n                \"y can only be a RegularlySampledAnalogSignalArray if X is a BinnedEventArray.\"\n            )\n\n        assert len(X) == len(y), \"X and y must have the same number of samples!\"\n\n        return X, y\n\n    def _ratemap_permute_unit_order(self, unit_ids, inplace=False):\n        \"\"\"Permute the unit ordering.\n\n        If no order is specified, and an ordering exists from fit(), then the\n        data in X will automatically be permuted to match that registered during\n        fit().\n\n        Parameters\n        ----------\n        unit_ids : array-like, shape (n_units,)\n        \"\"\"\n        unit_ids = self._check_unit_ids(unit_ids=unit_ids)\n        if len(unit_ids) != len(self.unit_ids):\n            raise ValueError(\n                \"To re-order (permute) units, 'unit_ids' must have the same length as self._unit_ids.\"\n            )\n        self._ratemap.reorder_units_by_ids(unit_ids, inplace=inplace)\n\n    def _check_unit_ids(self, *, X=None, unit_ids=None, fit=False):\n        \"\"\"Check that unit_ids are valid (if provided), and return unit_ids.\n\n        if calling from fit(), pass in fit=True, which will skip checks against\n        self.ratemap, which doesn't exist before fitting...\n\n        \"\"\"\n\n        def a_contains_b(a, b):\n            \"\"\"Returns True iff 'b' is a subset of 'a'.\"\"\"\n            for bb in b:\n                if bb not in a:\n                    logging.warning(\"{} was not found in set\".format(bb))\n                    return False\n            return True\n\n        if isinstance(X, core.BinnedEventArray):\n            if unit_ids is not None:\n                # unit_ids were passed in, even though it's also contained in X.unit_ids\n                # 1. check that unit_ids are contained in the data:\n                if not a_contains_b(X.series_ids, unit_ids):\n                    raise ValueError(\"Some unit_ids were not contained in X!\")\n                # 2. check that unit_ids are contained in self (decoder ratemap)\n                if not fit:\n                    if not a_contains_b(self.unit_ids, unit_ids):\n                        raise ValueError(\"Some unit_ids were not contained in ratemap!\")\n            else:\n                # infer unit_ids from X\n                unit_ids = X.series_ids\n                # check that unit_ids are contained in self (decoder ratemap)\n                if not fit:\n                    if not a_contains_b(self.unit_ids, unit_ids):\n                        raise ValueError(\n                            \"Some unit_ids from X were not contained in ratemap!\"\n                        )\n        else:  # a non-nelpy X was passed, possibly X=None\n            if unit_ids is not None:\n                # 1. check that unit_ids are contained in self (decoder ratemap)\n                if not fit:\n                    if not a_contains_b(self.unit_ids, unit_ids):\n                        raise ValueError(\"Some unit_ids were not contained in ratemap!\")\n            else:  # no unit_ids were passed, only a non-nelpy X\n                if X is not None:\n                    n_samples, n_units = X.shape\n                    if not fit:\n                        if n_units &gt; self.n_units:\n                            raise ValueError(\n                                \"X contains more units than decoder! {} &gt; {}\".format(\n                                    n_units, self.n_units\n                                )\n                            )\n                        unit_ids = self.unit_ids[:n_units]\n                    else:\n                        unit_ids = np.arange(n_units)\n                else:\n                    raise NotImplementedError(\"unexpected branch reached...\")\n        return unit_ids\n\n    def _get_transformed_ratemap(self, unit_ids):\n        # first, trim ratemap to subset of units\n        ratemap = self.ratemap.loc[unit_ids]\n        # then, permute the ratemap\n        ratemap = ratemap.reorder_units_by_ids(\n            unit_ids\n        )  # maybe unneccessary, since .loc already permutes\n        return ratemap\n\n    def fit(\n        self,\n        X,\n        y,\n        *,\n        lengths=None,\n        dt=None,\n        unit_ids=None,\n        n_bins=None,\n        sample_weight=None,\n    ):\n        \"\"\"Fit Gaussian Naive Bayes according to X, y\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n                OR\n            nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n                The number of spikes in each time bin for each neuron/unit.\n        y : array-like, shape (n_samples, n_output_dims)\n            Target values.\n                OR\n            nelpy.core.RegularlySampledAnalogSignalArray\n                containing the target values corresponding to X.\n            NOTE: If X is an array-like, then y must be an array-like.\n        lengths : array-like, shape (n_epochs,), optional (default=None)\n            Lengths (in samples) of contiguous segments in (X, y).\n            .. versionadded:: x.xx\n                BayesianDecoder does not yet support *lengths*.\n        unit_ids : array-like, shape (n_units,), optional (default=None)\n            Persistent unit IDs that are used to associate units after\n            permutation. Unit IDs are inherited from nelpy.core.BinnedEventArray\n            objects, or initialized to np.arange(n_units).\n        sample_weight : array-like, shape (n_samples,), optional (default=None)\n            Weights applied to individual samples (1. for unweighted).\n            .. versionadded:: x.xx\n               BayesianDecoder does not yet support fitting with *sample_weight*.\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n\n        # TODO dt should probably come from datawindow specification, but may be overridden here!\n\n        unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids, fit=True)\n\n        # estimate the firing rate(s):\n        self.rate_estimator.fit(X=X, y=y, dt=dt, n_bins=n_bins)\n\n        # store the estimated firing rates as a rate map:\n        bin_centers = self.rate_estimator.tc_.bin_centers  # temp code FIXME\n        # bins = self.rate_estimator.tc_.bins  # temp code FIXME\n        rates = self.rate_estimator.tc_.ratemap  # temp code FIXME\n        # unit_ids = np.array(self.rate_estimator.tc_.unit_ids) #temp code FIXME\n        self.ratemap.fit(X=bin_centers, y=rates, unit_ids=unit_ids)  # temp code FIXME\n\n        X, y = self._check_X_y(\n            X, y, method=\"fit\", lengths=lengths\n        )  # can I remove this? no; it sets the bin width... but maybe we should refactor...\n        self.ratemap_ = self.ratemap.ratemap_\n\n    def predict(\n        self, X, *, output=None, mode=\"mean\", lengths=None, unit_ids=None, dt=None\n    ):\n        # if output is 'asa', then return an ASA\n        check_is_fitted(self, \"ratemap_\")\n        unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids)\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        X, dt = self._check_X_dt(X=X, lengths=lengths, dt=dt)\n\n        posterior, mean_pth = decode_bayesian_memoryless_nd(\n            X=X, ratemap=ratemap.ratemap_, dt=dt, bin_centers=ratemap.bin_centers\n        )\n\n        if output is not None:\n            raise NotImplementedError(\"output mode not implemented yet\")\n        return posterior, mean_pth\n\n    def predict_proba(self, X, *, lengths=None, unit_ids=None, dt=None):\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        return self._predict_proba_from_ratemap(X, ratemap)\n\n    def score(self, X, y, *, lengths=None, unit_ids=None, dt=None):\n        # check that unit_ids are valid\n        # THEN, transform X, y into standardized form (including trimming and permutation) and continue with scoring\n\n        check_is_fitted(self, \"ratemap_\")\n        unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids)\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        # X = self._permute_unit_order(X)\n        # X, y = self._check_X_y(X, y, method='score', unit_ids=unit_ids)\n\n        raise NotImplementedError\n        ratemap = self._get_transformed_ratemap(unit_ids)\n        return self._score_from_ratemap(X, ratemap)\n\n    def score_samples(self, X, y, *, lengths=None, unit_ids=None, dt=None):\n        # X = self._permute_unit_order(X)\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    @property\n    def unit_ids(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap.unit_ids\n\n    @property\n    def n_units(self):\n        check_is_fitted(self, \"ratemap_\")\n        return len(self.unit_ids)\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.BayesianDecoderTemp.fit","title":"<code>fit(X, y, *, lengths=None, dt=None, unit_ids=None, n_bins=None, sample_weight=None)</code>","text":"<p>Fit Gaussian Naive Bayes according to X, y</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Training vectors, where n_samples is the number of samples and n_features is the number of features.     OR nelpy.core.BinnedEventArray / BinnedSpikeTrainArray     The number of spikes in each time bin for each neuron/unit.</p> required <code>y</code> <code>(array - like, shape(n_samples, n_output_dims))</code> <p>Target values.     OR nelpy.core.RegularlySampledAnalogSignalArray     containing the target values corresponding to X. NOTE: If X is an array-like, then y must be an array-like.</p> required <code>lengths</code> <code>(array - like, shape(n_epochs), optional(default=None))</code> <p>Lengths (in samples) of contiguous segments in (X, y). .. versionadded:: x.xx     BayesianDecoder does not yet support lengths.</p> <code>None</code> <code>unit_ids</code> <code>(array - like, shape(n_units), optional(default=None))</code> <p>Persistent unit IDs that are used to associate units after permutation. Unit IDs are inherited from nelpy.core.BinnedEventArray objects, or initialized to np.arange(n_units).</p> <code>None</code> <code>sample_weight</code> <code>(array - like, shape(n_samples), optional(default=None))</code> <p>Weights applied to individual samples (1. for unweighted). .. versionadded:: x.xx    BayesianDecoder does not yet support fitting with sample_weight.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    *,\n    lengths=None,\n    dt=None,\n    unit_ids=None,\n    n_bins=None,\n    sample_weight=None,\n):\n    \"\"\"Fit Gaussian Naive Bayes according to X, y\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Training vectors, where n_samples is the number of samples\n        and n_features is the number of features.\n            OR\n        nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n            The number of spikes in each time bin for each neuron/unit.\n    y : array-like, shape (n_samples, n_output_dims)\n        Target values.\n            OR\n        nelpy.core.RegularlySampledAnalogSignalArray\n            containing the target values corresponding to X.\n        NOTE: If X is an array-like, then y must be an array-like.\n    lengths : array-like, shape (n_epochs,), optional (default=None)\n        Lengths (in samples) of contiguous segments in (X, y).\n        .. versionadded:: x.xx\n            BayesianDecoder does not yet support *lengths*.\n    unit_ids : array-like, shape (n_units,), optional (default=None)\n        Persistent unit IDs that are used to associate units after\n        permutation. Unit IDs are inherited from nelpy.core.BinnedEventArray\n        objects, or initialized to np.arange(n_units).\n    sample_weight : array-like, shape (n_samples,), optional (default=None)\n        Weights applied to individual samples (1. for unweighted).\n        .. versionadded:: x.xx\n           BayesianDecoder does not yet support fitting with *sample_weight*.\n    Returns\n    -------\n    self : object\n\n    \"\"\"\n\n    # TODO dt should probably come from datawindow specification, but may be overridden here!\n\n    unit_ids = self._check_unit_ids(X=X, unit_ids=unit_ids, fit=True)\n\n    # estimate the firing rate(s):\n    self.rate_estimator.fit(X=X, y=y, dt=dt, n_bins=n_bins)\n\n    # store the estimated firing rates as a rate map:\n    bin_centers = self.rate_estimator.tc_.bin_centers  # temp code FIXME\n    # bins = self.rate_estimator.tc_.bins  # temp code FIXME\n    rates = self.rate_estimator.tc_.ratemap  # temp code FIXME\n    # unit_ids = np.array(self.rate_estimator.tc_.unit_ids) #temp code FIXME\n    self.ratemap.fit(X=bin_centers, y=rates, unit_ids=unit_ids)  # temp code FIXME\n\n    X, y = self._check_X_y(\n        X, y, method=\"fit\", lengths=lengths\n    )  # can I remove this? no; it sets the bin width... but maybe we should refactor...\n    self.ratemap_ = self.ratemap.ratemap_\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.FiringRateEstimator","title":"<code>FiringRateEstimator</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>FiringRateEstimator Estimate the firing rate of a spike train.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>(hist, glm - poisson, glm - binomial, glm, gvm, bars, gp)</code> <p>The estimation mode. Default is 'hist'. - 'hist': Histogram-based estimation. - 'glm-poisson': Generalized linear model with Poisson distribution. - 'glm-binomial': Generalized linear model with Binomial distribution. - 'glm': Generalized linear model. - 'gvm': Generalized von Mises. - 'bars': Bayesian adaptive regression splines. - 'gp': Gaussian process.</p> <code>'hist'</code> <p>Attributes:</p> Name Type Description <code>mode</code> <code>str</code> <p>The estimation mode.</p> <code>tc_</code> <code>TuningCurve1D or TuningCurve2D</code> <p>The estimated tuning curve.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class FiringRateEstimator(BaseEstimator):\n    \"\"\"\n    FiringRateEstimator\n    Estimate the firing rate of a spike train.\n\n    Parameters\n    ----------\n    mode : {'hist', 'glm-poisson', 'glm-binomial', 'glm', 'gvm', 'bars', 'gp'}, optional\n        The estimation mode. Default is 'hist'.\n        - 'hist': Histogram-based estimation.\n        - 'glm-poisson': Generalized linear model with Poisson distribution.\n        - 'glm-binomial': Generalized linear model with Binomial distribution.\n        - 'glm': Generalized linear model.\n        - 'gvm': Generalized von Mises.\n        - 'bars': Bayesian adaptive regression splines.\n        - 'gp': Gaussian process.\n\n    Attributes\n    ----------\n    mode : str\n        The estimation mode.\n    tc_ : TuningCurve1D or TuningCurve2D\n        The estimated tuning curve.\n    \"\"\"\n\n    def __init__(self, mode=\"hist\"):\n        \"\"\"\n        Initialize a FiringRateEstimator.\n\n        Parameters\n        ----------\n        mode : str, optional\n            The estimation mode. Default is 'hist'.\n        \"\"\"\n        if mode not in [\"hist\"]:\n            raise NotImplementedError(\n                \"mode '{}' not supported / implemented yet!\".format(mode)\n            )\n        self._mode = mode\n\n    def _check_X_y_dt(self, X, y, lengths=None, dt=None, timestamps=None, n_bins=None):\n        \"\"\"\n        Validate and standardize input data for fitting or prediction.\n\n        Parameters\n        ----------\n        X : array-like or BinnedEventArray\n            Input data.\n        y : array-like or RegularlySampledAnalogSignalArray\n            Target values.\n        lengths : array-like, optional\n            Lengths of intervals.\n        dt : float, optional\n            Temporal bin size.\n        timestamps : array-like, optional\n            Timestamps for the data.\n        n_bins : int or array-like, optional\n            Number of bins for discretization.\n\n        Returns\n        -------\n        X : np.ndarray\n            Standardized input data.\n        y : np.ndarray\n            Standardized target values.\n        dt : float\n            Temporal bin size.\n        n_bins : int or array-like\n            Number of bins for discretization.\n        \"\"\"\n        if isinstance(X, core.BinnedEventArray):\n            T = X.bin_centers\n            if lengths is not None:\n                logging.warning(\n                    \"'lengths' was passed in, but will be\"\n                    \" overwritten by 'X's 'lengths' attribute\"\n                )\n            if timestamps is not None:\n                logging.warning(\n                    \"'timestamps' was passed in, but will be\"\n                    \" overwritten by 'X's 'bin_centers' attribute\"\n                )\n            if dt is not None:\n                logging.warning(\n                    \"'dt' was passed in, but will be overwritten by 'X's 'ds' attribute\"\n                )\n            if isinstance(y, core.RegularlySampledAnalogSignalArray):\n                y = y(T).T\n\n            dt = X.ds\n            lengths = X.lengths\n            X = X.data.T\n        elif isinstance(X, np.ndarray):\n            if dt is None:\n                raise ValueError(\n                    \"'dt' is a required argument when 'X' is passed in as a numpy array!\"\n                )\n            if isinstance(y, core.RegularlySampledAnalogSignalArray):\n                if timestamps is not None:\n                    y = y(timestamps).T\n                else:\n                    raise ValueError(\n                        \"'timestamps' required when passing in 'X' as a numpy array and 'y' as a nelpy RegularlySampledAnalogSignalArray!\"\n                    )\n        else:\n            raise TypeError(\n                \"'X' should be either a nelpy BinnedEventArray, or a numpy array!\"\n            )\n\n        n_samples, n_units = X.shape\n        _, n_dims = y.shape\n        print(\"{}-dimensional y passed in\".format(n_dims))\n\n        assert n_samples == len(y), (\n            \"'X' and 'y' must have the same number\"\n            \" of samples! len(X)=={} but len(y)=={}\".format(n_samples, len(y))\n        )\n        if n_bins is not None:\n            n_bins = np.atleast_1d(n_bins)\n            assert len(n_bins) == n_dims, (\n                \"'n_bins' must have one entry for each dimension in 'y'!\"\n            )\n\n        return X, y, dt, n_bins\n\n    def fit(\n        self,\n        X,\n        y,\n        lengths=None,\n        dt=None,\n        timestamps=None,\n        unit_ids=None,\n        n_bins=None,\n        sample_weight=None,\n    ):\n        \"\"\"\n        Fit the firing rate estimator to the data.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data.\n        y : array-like\n            Target values.\n        lengths : array-like, optional\n            Lengths of intervals.\n        dt : float, optional\n            Temporal bin size.\n        timestamps : array-like, optional\n            Timestamps for the data.\n        unit_ids : array-like, optional\n            Unit identifiers.\n        n_bins : int or array-like, optional\n            Number of bins for discretization.\n        sample_weight : array-like, optional\n            Weights for each sample.\n\n        Returns\n        -------\n        self : FiringRateEstimator\n            The fitted estimator.\n        \"\"\"\n        X, y, dt, n_bins = self._check_X_y_dt(\n            X=X, y=y, lengths=lengths, dt=dt, timestamps=timestamps, n_bins=n_bins\n        )\n\n        # 1. estimate mask\n        # 2. estimate occupancy\n        # 3. compute spikes histogram\n        # 4. normalize spike histogram by occupancy\n        # 5. apply mask\n\n        # if y.n_signals == 1:\n        #     self.tc_ = TuningCurve1D(bst=X, extern=y, n_extern=100, extmin=y.min(), extmax=y.max(), sigma=2.5, min_duration=0)\n        # if y.n_signals == 2:\n        #     xmin, ymin = y.min()\n        #     xmax, ymax = y.max()\n        #     self.tc_ = TuningCurve2D(bst=X, extern=y, ext_nx=50, ext_ny=50, ext_xmin=xmin, ext_xmax=xmax, ext_ymin=ymin, ext_ymax=ymax, sigma=2.5, min_duration=0)\n\n    @property\n    def mode(self):\n        return self._mode\n\n    def predict(self, X, lengths=None):\n        \"\"\"\n        Predict firing rates for the given input data.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        rates : array-like\n            Predicted firing rates.\n        \"\"\"\n        raise NotImplementedError\n\n    def predict_proba(self, X, lengths=None):\n        \"\"\"\n        Predict firing rate probabilities for the given input data.\n\n        Parameters\n        ----------\n        X : array-like\n            Input data.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        probabilities : array-like\n            Predicted probabilities.\n        \"\"\"\n        raise NotImplementedError\n\n    def score(self, X, y, lengths=None):\n        \"\"\"\n        Return the mean accuracy on the given test data and labels.\n\n        Parameters\n        ----------\n        X : array-like\n            Test samples.\n        y : array-like\n            True values for X.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        raise NotImplementedError\n\n    def score_samples(self, X, y, lengths=None):\n        \"\"\"\n        Return the per-sample accuracy on the given test data and labels.\n\n        Parameters\n        ----------\n        X : array-like\n            Test samples.\n        y : array-like\n            True values for X.\n        lengths : array-like, optional\n            Lengths of intervals.\n\n        Returns\n        -------\n        scores : array-like\n            Per-sample accuracy of self.predict(X) wrt. y.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.FiringRateEstimator.fit","title":"<code>fit(X, y, lengths=None, dt=None, timestamps=None, unit_ids=None, n_bins=None, sample_weight=None)</code>","text":"<p>Fit the firing rate estimator to the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data.</p> required <code>y</code> <code>array - like</code> <p>Target values.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <code>dt</code> <code>float</code> <p>Temporal bin size.</p> <code>None</code> <code>timestamps</code> <code>array - like</code> <p>Timestamps for the data.</p> <code>None</code> <code>unit_ids</code> <code>array - like</code> <p>Unit identifiers.</p> <code>None</code> <code>n_bins</code> <code>int or array - like</code> <p>Number of bins for discretization.</p> <code>None</code> <code>sample_weight</code> <code>array - like</code> <p>Weights for each sample.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>FiringRateEstimator</code> <p>The fitted estimator.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    lengths=None,\n    dt=None,\n    timestamps=None,\n    unit_ids=None,\n    n_bins=None,\n    sample_weight=None,\n):\n    \"\"\"\n    Fit the firing rate estimator to the data.\n\n    Parameters\n    ----------\n    X : array-like\n        Input data.\n    y : array-like\n        Target values.\n    lengths : array-like, optional\n        Lengths of intervals.\n    dt : float, optional\n        Temporal bin size.\n    timestamps : array-like, optional\n        Timestamps for the data.\n    unit_ids : array-like, optional\n        Unit identifiers.\n    n_bins : int or array-like, optional\n        Number of bins for discretization.\n    sample_weight : array-like, optional\n        Weights for each sample.\n\n    Returns\n    -------\n    self : FiringRateEstimator\n        The fitted estimator.\n    \"\"\"\n    X, y, dt, n_bins = self._check_X_y_dt(\n        X=X, y=y, lengths=lengths, dt=dt, timestamps=timestamps, n_bins=n_bins\n    )\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.FiringRateEstimator.predict","title":"<code>predict(X, lengths=None)</code>","text":"<p>Predict firing rates for the given input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rates</code> <code>array - like</code> <p>Predicted firing rates.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict(self, X, lengths=None):\n    \"\"\"\n    Predict firing rates for the given input data.\n\n    Parameters\n    ----------\n    X : array-like\n        Input data.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    rates : array-like\n        Predicted firing rates.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.FiringRateEstimator.predict_proba","title":"<code>predict_proba(X, lengths=None)</code>","text":"<p>Predict firing rate probabilities for the given input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>probabilities</code> <code>array - like</code> <p>Predicted probabilities.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict_proba(self, X, lengths=None):\n    \"\"\"\n    Predict firing rate probabilities for the given input data.\n\n    Parameters\n    ----------\n    X : array-like\n        Input data.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    probabilities : array-like\n        Predicted probabilities.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.FiringRateEstimator.score","title":"<code>score(X, y, lengths=None)</code>","text":"<p>Return the mean accuracy on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Test samples.</p> required <code>y</code> <code>array - like</code> <p>True values for X.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>Mean accuracy of self.predict(X) wrt. y.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def score(self, X, y, lengths=None):\n    \"\"\"\n    Return the mean accuracy on the given test data and labels.\n\n    Parameters\n    ----------\n    X : array-like\n        Test samples.\n    y : array-like\n        True values for X.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    score : float\n        Mean accuracy of self.predict(X) wrt. y.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.FiringRateEstimator.score_samples","title":"<code>score_samples(X, y, lengths=None)</code>","text":"<p>Return the per-sample accuracy on the given test data and labels.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Test samples.</p> required <code>y</code> <code>array - like</code> <p>True values for X.</p> required <code>lengths</code> <code>array - like</code> <p>Lengths of intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>array - like</code> <p>Per-sample accuracy of self.predict(X) wrt. y.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def score_samples(self, X, y, lengths=None):\n    \"\"\"\n    Return the per-sample accuracy on the given test data and labels.\n\n    Parameters\n    ----------\n    X : array-like\n        Test samples.\n    y : array-like\n        True values for X.\n    lengths : array-like, optional\n        Lengths of intervals.\n\n    Returns\n    -------\n    scores : array-like\n        Per-sample accuracy of self.predict(X) wrt. y.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.ItemGetter_iloc","title":"<code>ItemGetter_iloc</code>","text":"<p>               Bases: <code>object</code></p> <p>.iloc is primarily integer position based (from 0 to length-1 of the axis).</p> <p>.iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with python/numpy slice semantics).</p> <p>Allowed inputs are:     - An integer e.g. 5     - A list or array of integers [4, 3, 0]     - A slice object with ints 1:7</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class ItemGetter_iloc(object):\n    \"\"\".iloc is primarily integer position based (from 0 to length-1\n    of the axis).\n\n    .iloc will raise IndexError if a requested indexer is\n    out-of-bounds, except slice indexers which allow out-of-bounds\n    indexing. (this conforms with python/numpy slice semantics).\n\n    Allowed inputs are:\n        - An integer e.g. 5\n        - A list or array of integers [4, 3, 0]\n        - A slice object with ints 1:7\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"intervals, series\"\"\"\n        unit_idx_list = idx\n        if isinstance(idx, int):\n            unit_idx_list = [idx]\n\n        return self.obj[unit_idx_list]\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.ItemGetter_loc","title":"<code>ItemGetter_loc</code>","text":"<p>               Bases: <code>object</code></p> <p>.loc is primarily label based (that is, unit_id based)</p> <p>.loc will raise KeyError when the items are not found.</p> <p>Allowed inputs are:     - A single label, e.g. 5 or 'a', (note that 5 is interpreted         as a label of the index. This use is not an integer         position along the index)     - A list or array of labels ['a', 'b', 'c']     - A slice object with labels 'a':'f', (note that contrary to         usual python slices, both the start and the stop are         included!)</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class ItemGetter_loc(object):\n    \"\"\".loc is primarily label based (that is, unit_id based)\n\n    .loc will raise KeyError when the items are not found.\n\n    Allowed inputs are:\n        - A single label, e.g. 5 or 'a', (note that 5 is interpreted\n            as a label of the index. This use is not an integer\n            position along the index)\n        - A list or array of labels ['a', 'b', 'c']\n        - A slice object with labels 'a':'f', (note that contrary to\n            usual python slices, both the start and the stop are\n            included!)\n    \"\"\"\n\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, idx):\n        \"\"\"unit_ids\"\"\"\n        unit_idx_list = self.obj._slicer[idx]\n\n        return self.obj[unit_idx_list]\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.KeywordError","title":"<code>KeywordError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Explanation of the error.</p> required Source code in <code>nelpy/estimators.py</code> <pre><code>class KeywordError(Exception):\n    \"\"\"\n    Exception raised for errors in keyword arguments.\n\n    Parameters\n    ----------\n    message : str\n        Explanation of the error.\n    \"\"\"\n\n    def __init__(self, message):\n        \"\"\"\n        Initialize the KeywordError.\n\n        Parameters\n        ----------\n        message : str\n            Explanation of the error.\n        \"\"\"\n        self.message = message\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap","title":"<code>NDRateMap</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>NDRateMap with persistent unit_ids and firing rates in Hz for N-dimensional data.</p> <p>Parameters:</p> Name Type Description Default <code>connectivity</code> <code>(continuous, discrete, circular)</code> <p>Defines how smoothing is applied. Default is 'continuous'. - 'continuous': Continuous smoothing. - 'discrete': No smoothing is applied. - 'circular': Circular smoothing (for angular variables).</p> <code>'continuous'</code> <p>Attributes:</p> Name Type Description <code>connectivity</code> <code>str</code> <p>Smoothing mode.</p> <code>ratemap_</code> <code>ndarray</code> <p>The estimated firing rate map.</p> <code>_unit_ids</code> <code>ndarray</code> <p>Persistent unit IDs.</p> <code>_bins</code> <code>ndarray</code> <p>Bin edges for each dimension.</p> <code>_bin_centers</code> <code>ndarray</code> <p>Bin centers for each dimension.</p> <code>_mask</code> <code>ndarray</code> <p>Mask for valid regions.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class NDRateMap(BaseEstimator):\n    \"\"\"\n    NDRateMap with persistent unit_ids and firing rates in Hz for N-dimensional data.\n\n    Parameters\n    ----------\n    connectivity : {'continuous', 'discrete', 'circular'}, optional\n        Defines how smoothing is applied. Default is 'continuous'.\n        - 'continuous': Continuous smoothing.\n        - 'discrete': No smoothing is applied.\n        - 'circular': Circular smoothing (for angular variables).\n\n    Attributes\n    ----------\n    connectivity : str\n        Smoothing mode.\n    ratemap_ : np.ndarray\n        The estimated firing rate map.\n    _unit_ids : np.ndarray\n        Persistent unit IDs.\n    _bins : np.ndarray\n        Bin edges for each dimension.\n    _bin_centers : np.ndarray\n        Bin centers for each dimension.\n    _mask : np.ndarray\n        Mask for valid regions.\n    \"\"\"\n\n    def __init__(self, connectivity=\"continuous\"):\n        self.connectivity = connectivity\n\n        self._slicer = UnitSlicer(self)\n        self.loc = ItemGetter_loc(self)\n        self.iloc = ItemGetter_iloc(self)\n\n    def __repr__(self):\n        r = super().__repr__()\n        if self._is_fitted():\n            dimstr = \"\"\n            for dd in range(self.n_dims):\n                dimstr += \", n_bins_d{}={}\".format(dd + 1, self.shape[dd + 1])\n            r += \" with shape (n_units={}{})\".format(self.n_units, dimstr)\n        return r\n\n    def fit(self, X, y, dt=1, unit_ids=None):\n        \"\"\"\n        Fit firing rates to the provided data.\n\n        Parameters\n        ----------\n        X : array-like, with shape (n_dims, ), each element of which has\n            shape (n_bins_dn, ) for n=1, ..., N; N=n_dims.\n            Bin locations (centers) where ratemap is defined.\n        y : array-like, shape (n_units, n_bins_d1, ..., n_bins_dN)\n            Expected number of spikes in a temporal bin of width dt, for each of\n            the predictor bins specified in X.\n        dt : float, optional\n            Temporal bin size with which firing rate y is defined. Default is 1.\n        unit_ids : array-like, shape (n_units,), optional\n            Persistent unit IDs that are used to associate units after\n            permutation. If None, uses np.arange(n_units).\n\n        Returns\n        -------\n        self : NDRateMap\n            The fitted NDRateMap instance.\n        \"\"\"\n        n_units, n_bins, n_dims = self._check_X_y(X, y)\n\n        self.ratemap_ = y / dt\n        self._bin_centers = X\n        self._bins = np.array(n_dims * [None])\n\n        if n_dims &gt; 1:\n            for dd in range(n_dims):\n                bin_centers = np.squeeze(X[dd])\n                dx = np.median(np.diff(bin_centers))\n                bins = np.insert(\n                    bin_centers[-1] + np.diff(bin_centers) / 2,\n                    0,\n                    bin_centers[0] - dx / 2,\n                )\n                bins = np.append(bins, bins[-1] + dx)\n                self._bins[dd] = bins\n        else:\n            bin_centers = np.squeeze(X)\n            dx = np.median(np.diff(bin_centers))\n            bins = np.insert(\n                bin_centers[-1] + np.diff(bin_centers) / 2, 0, bin_centers[0] - dx / 2\n            )\n            bins = np.append(bins, bins[-1] + dx)\n            self._bins = bins\n\n        if unit_ids is not None:\n            if len(unit_ids) != n_units:\n                raise ValueError(\n                    \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                        len(unit_ids), n_units\n                    )\n                )\n            self._unit_ids = unit_ids\n        else:\n            self._unit_ids = np.arange(n_units)\n\n    def predict(self, X):\n        \"\"\"\n        Predict firing rates for the given bin locations.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to predict firing rates for.\n\n        Returns\n        -------\n        rates : array-like\n            Predicted firing rates.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def synthesize(self, X):\n        \"\"\"\n        Generate synthetic spike data based on the ratemap.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to synthesize spikes for.\n\n        Returns\n        -------\n        spikes : array-like\n            Synthetic spike data.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def __len__(self):\n        return self.n_units\n\n    def __iter__(self):\n        \"\"\"TuningCurve1D iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"TuningCurve1D iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_units - 1:\n            raise StopIteration\n        out = copy.copy(self)\n        out.ratemap_ = self.ratemap_[tuple([index])]\n        out._unit_ids = self._unit_ids[index]\n        self._index += 1\n        return out\n\n    def __getitem__(self, *idx):\n        \"\"\"\n        Access RateMap units by index.\n\n        Parameters\n        ----------\n        *idx : int, slice, or list\n            Indices of units to access.\n\n        Returns\n        -------\n        out : NDRateMap\n            Subset NDRateMap with selected units.\n        \"\"\"\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        try:\n            out = copy.copy(self)\n            out.ratemap_ = self.ratemap_[tuple([idx])]\n            out._unit_ids = list(np.array(out._unit_ids)[tuple([idx])])\n            out._slicer = UnitSlicer(out)\n            out.loc = ItemGetter_loc(out)\n            out.iloc = ItemGetter_iloc(out)\n            return out\n        except Exception:\n            raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def get_peak_firing_order_ids(self):\n        \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n        Returns\n        -------\n        unit_ids : array-like\n            The permutaiton of unit_ids such that after reordering, the peak\n            firing locations are ordered along the RateMap.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\n                \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n            )\n        peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n        return np.array(self.unit_ids)[peakorder]\n\n    def reorder_units_by_ids(self, unit_ids, inplace=False):\n        \"\"\"Permute the unit ordering.\n\n        #TODO\n        If no order is specified, and an ordering exists from fit(), then the\n        data in X will automatically be permuted to match that registered during\n        fit().\n\n        Parameters\n        ----------\n        unit_ids : array-like, shape (n_units,)\n\n        Returns\n        -------\n        out : reordered RateMap\n        \"\"\"\n\n        def swap_units(arr, frm, to):\n            \"\"\"swap 'units' of a 3D np.array\"\"\"\n            arr[(frm, to), :] = arr[(to, frm), :]\n\n        self._validate_unit_ids(unit_ids)\n        if len(unit_ids) != len(self._unit_ids):\n            raise ValueError(\n                \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n            )\n\n        if inplace:\n            out = self\n        else:\n            out = copy.deepcopy(self)\n\n        neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_units(out.ratemap_, frm, to)\n            out._unit_ids[frm], out._unit_ids[to] = (\n                out._unit_ids[to],\n                out._unit_ids[frm],\n            )\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n        return out\n\n    def _check_X_y(self, X, y):\n        y = np.atleast_2d(y)\n\n        n_units = y.shape[0]\n        n_bins = y.shape[1:]\n        n_dims = len(n_bins)\n\n        if n_dims &gt; 1:\n            n_x_bins = tuple([len(x) for x in X])\n        else:\n            n_x_bins = tuple([len(X)])\n\n        assert n_units &gt; 0, \"n_units must be a positive integer!\"\n        assert n_x_bins == n_bins, \"X and y must have the same number of bins!\"\n\n        return n_units, n_bins, n_dims\n\n    def _validate_unit_ids(self, unit_ids):\n        self._check_unit_ids_in_ratemap(unit_ids)\n\n        if len(set(unit_ids)) != len(unit_ids):\n            raise ValueError(\"Duplicate unit_ids are not allowed.\")\n\n    def _check_unit_ids_in_ratemap(self, unit_ids):\n        for unit_id in unit_ids:\n            # NOTE: the check below allows for predict() to pass on only\n            # a subset of the units that were used during fit! So we\n            # could fit on 100 units, and then predict on only 10 of\n            # them, if we wanted.\n            if unit_id not in self.unit_ids:\n                raise ValueError(\n                    \"unit_id {} was not present during fit(); aborting...\".format(\n                        unit_id\n                    )\n                )\n\n    def _is_fitted(self):\n        try:\n            check_is_fitted(self, \"ratemap_\")\n        except Exception:  # should really be except NotFitterError\n            return False\n        return True\n\n    @property\n    def connectivity(self):\n        return self._connectivity\n\n    @connectivity.setter\n    def connectivity(self, val):\n        self._connectivity = self._validate_connectivity(val)\n\n    @staticmethod\n    def _validate_connectivity(connectivity):\n        connectivity = str(connectivity).strip().lower()\n        options = [\"continuous\", \"discrete\", \"circular\"]\n        if connectivity in options:\n            return connectivity\n        raise NotImplementedError(\n            \"connectivity '{}' is not supported yet!\".format(str(connectivity))\n        )\n\n    @property\n    def shape(self):\n        \"\"\"\n        RateMap.shape = (n_units, n_features_x, n_features_y)\n            OR\n        RateMap.shape = (n_units, n_features)\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape\n\n    @property\n    def n_dims(self):\n        check_is_fitted(self, \"ratemap_\")\n        n_dims = len(self.shape) - 1\n        return n_dims\n\n    @property\n    def is_1d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 2:\n            return True\n        return False\n\n    @property\n    def is_2d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 3:\n            return True\n        return False\n\n    @property\n    def n_units(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape[0]\n\n    @property\n    def unit_ids(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self._unit_ids\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) Number of external correlates (bins) along each dimension.\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.n_dims &gt; 1:\n            n_bins = tuple([len(x) for x in self.bin_centers])\n        else:\n            n_bins = len(self.bin_centers)\n        return n_bins\n\n    def max(self, axis=None, out=None):\n        \"\"\"\n        maximum firing rate for each unit:\n            RateMap.max()\n        maximum firing rate across units:\n            RateMap.max(axis=0)\n        \"\"\"\n        raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n            else:\n                return self.ratemap_.max(axis=1, out=out)\n        return self.ratemap_.max(axis=axis, out=out)\n\n    def min(self, axis=None, out=None):\n        raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.min(axis=1, out=out).min(axis=1, out=out)\n            else:\n                return self.ratemap_.min(axis=1, out=out)\n        return self.ratemap_.min(axis=axis, out=out)\n\n    def mean(self, axis=None, dtype=None, out=None, keepdims=False):\n        raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n        check_is_fitted(self, \"ratemap_\")\n        kwargs = {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.mean(axis=1, **kwargs).mean(axis=1, **kwargs)\n            else:\n                return self.ratemap_.mean(axis=1, **kwargs)\n        return self.ratemap_.mean(axis=axis, **kwargs)\n\n    @property\n    def bins(self):\n        return self._bins\n\n    @property\n    def bin_centers(self):\n        return self._bin_centers\n\n    @property\n    def mask(self):\n        return self._mask\n\n    @mask.setter\n    def mask(self, val):\n        # TODO: mask validation\n        raise NotImplementedError\n        self._mask = val\n\n    def plot(self, **kwargs):\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\"plot() not yet implemented for 2D RateMaps.\")\n        pad = kwargs.pop(\"pad\", None)\n        _plot_ratemap(self, pad=pad, **kwargs)\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n        \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n        mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to 'constant'. Default is\n            'reflect'\n        truncate : float\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        truncate : float, deprecated\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        cval : scalar, optional\n            Value to fill past edges of input if mode is 'constant'. Default is 0.0\n        \"\"\"\n\n        raise NotImplementedError\n\n        if sigma is None:\n            sigma = 0.1  # in units of extern\n        if truncate is None:\n            truncate = 4\n        if mode is None:\n            mode = \"reflect\"\n        if cval is None:\n            cval = 0.0\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins) along each dimension.</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>RateMap.shape = (n_units, n_features_x, n_features_y)     OR RateMap.shape = (n_units, n_features)</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.fit","title":"<code>fit(X, y, dt=1, unit_ids=None)</code>","text":"<p>Fit firing rates to the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, with shape (n_dims, ), each element of which has</code> <p>shape (n_bins_dn, ) for n=1, ..., N; N=n_dims. Bin locations (centers) where ratemap is defined.</p> required <code>y</code> <code>(array - like, shape(n_units, n_bins_d1, ..., n_bins_dN))</code> <p>Expected number of spikes in a temporal bin of width dt, for each of the predictor bins specified in X.</p> required <code>dt</code> <code>float</code> <p>Temporal bin size with which firing rate y is defined. Default is 1.</p> <code>1</code> <code>unit_ids</code> <code>(array - like, shape(n_units))</code> <p>Persistent unit IDs that are used to associate units after permutation. If None, uses np.arange(n_units).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>NDRateMap</code> <p>The fitted NDRateMap instance.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(self, X, y, dt=1, unit_ids=None):\n    \"\"\"\n    Fit firing rates to the provided data.\n\n    Parameters\n    ----------\n    X : array-like, with shape (n_dims, ), each element of which has\n        shape (n_bins_dn, ) for n=1, ..., N; N=n_dims.\n        Bin locations (centers) where ratemap is defined.\n    y : array-like, shape (n_units, n_bins_d1, ..., n_bins_dN)\n        Expected number of spikes in a temporal bin of width dt, for each of\n        the predictor bins specified in X.\n    dt : float, optional\n        Temporal bin size with which firing rate y is defined. Default is 1.\n    unit_ids : array-like, shape (n_units,), optional\n        Persistent unit IDs that are used to associate units after\n        permutation. If None, uses np.arange(n_units).\n\n    Returns\n    -------\n    self : NDRateMap\n        The fitted NDRateMap instance.\n    \"\"\"\n    n_units, n_bins, n_dims = self._check_X_y(X, y)\n\n    self.ratemap_ = y / dt\n    self._bin_centers = X\n    self._bins = np.array(n_dims * [None])\n\n    if n_dims &gt; 1:\n        for dd in range(n_dims):\n            bin_centers = np.squeeze(X[dd])\n            dx = np.median(np.diff(bin_centers))\n            bins = np.insert(\n                bin_centers[-1] + np.diff(bin_centers) / 2,\n                0,\n                bin_centers[0] - dx / 2,\n            )\n            bins = np.append(bins, bins[-1] + dx)\n            self._bins[dd] = bins\n    else:\n        bin_centers = np.squeeze(X)\n        dx = np.median(np.diff(bin_centers))\n        bins = np.insert(\n            bin_centers[-1] + np.diff(bin_centers) / 2, 0, bin_centers[0] - dx / 2\n        )\n        bins = np.append(bins, bins[-1] + dx)\n        self._bins = bins\n\n    if unit_ids is not None:\n        if len(unit_ids) != n_units:\n            raise ValueError(\n                \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                    len(unit_ids), n_units\n                )\n            )\n        self._unit_ids = unit_ids\n    else:\n        self._unit_ids = np.arange(n_units)\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.get_peak_firing_order_ids","title":"<code>get_peak_firing_order_ids()</code>","text":"<p>Get the unit_ids in order of peak firing location for 1D RateMaps.</p> <p>Returns:</p> Name Type Description <code>unit_ids</code> <code>array - like</code> <p>The permutaiton of unit_ids such that after reordering, the peak firing locations are ordered along the RateMap.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def get_peak_firing_order_ids(self):\n    \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n    Returns\n    -------\n    unit_ids : array-like\n        The permutaiton of unit_ids such that after reordering, the peak\n        firing locations are ordered along the RateMap.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    if self.is_2d:\n        raise NotImplementedError(\n            \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n        )\n    peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n    return np.array(self.unit_ids)[peakorder]\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.max","title":"<code>max(axis=None, out=None)</code>","text":"<p>maximum firing rate for each unit:     RateMap.max() maximum firing rate across units:     RateMap.max(axis=0)</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def max(self, axis=None, out=None):\n    \"\"\"\n    maximum firing rate for each unit:\n        RateMap.max()\n    maximum firing rate across units:\n        RateMap.max(axis=0)\n    \"\"\"\n    raise NotImplementedError(\"the code was still for the 1D and 2D only version\")\n    check_is_fitted(self, \"ratemap_\")\n    if axis is None:\n        if self.is_2d:\n            return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n        else:\n            return self.ratemap_.max(axis=1, out=out)\n    return self.ratemap_.max(axis=axis, out=out)\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.predict","title":"<code>predict(X)</code>","text":"<p>Predict firing rates for the given bin locations.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to predict firing rates for.</p> required <p>Returns:</p> Name Type Description <code>rates</code> <code>array - like</code> <p>Predicted firing rates.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict firing rates for the given bin locations.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to predict firing rates for.\n\n    Returns\n    -------\n    rates : array-like\n        Predicted firing rates.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.reorder_units_by_ids","title":"<code>reorder_units_by_ids(unit_ids, inplace=False)</code>","text":"<p>Permute the unit ordering.</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.reorder_units_by_ids--todo","title":"TODO","text":"<p>If no order is specified, and an ordering exists from fit(), then the data in X will automatically be permuted to match that registered during fit().</p> <p>Parameters:</p> Name Type Description Default <code>unit_ids</code> <code>(array - like, shape(n_units))</code> required <p>Returns:</p> Name Type Description <code>out</code> <code>reordered RateMap</code> Source code in <code>nelpy/estimators.py</code> <pre><code>def reorder_units_by_ids(self, unit_ids, inplace=False):\n    \"\"\"Permute the unit ordering.\n\n    #TODO\n    If no order is specified, and an ordering exists from fit(), then the\n    data in X will automatically be permuted to match that registered during\n    fit().\n\n    Parameters\n    ----------\n    unit_ids : array-like, shape (n_units,)\n\n    Returns\n    -------\n    out : reordered RateMap\n    \"\"\"\n\n    def swap_units(arr, frm, to):\n        \"\"\"swap 'units' of a 3D np.array\"\"\"\n        arr[(frm, to), :] = arr[(to, frm), :]\n\n    self._validate_unit_ids(unit_ids)\n    if len(unit_ids) != len(self._unit_ids):\n        raise ValueError(\n            \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n        )\n\n    if inplace:\n        out = self\n    else:\n        out = copy.deepcopy(self)\n\n    neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        swap_units(out.ratemap_, frm, to)\n        out._unit_ids[frm], out._unit_ids[to] = (\n            out._unit_ids[to],\n            out._unit_ids[frm],\n        )\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n    return out\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.smooth","title":"<code>smooth(*, sigma=None, truncate=None, inplace=False, mode=None, cval=None)</code>","text":"<p>Smooths the tuning curve with a Gaussian kernel.</p> <p>mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional     The mode parameter determines how the array borders are handled,     where cval is the value when mode is equal to 'constant'. Default is     'reflect' truncate : float     Truncate the filter at this many standard deviations. Default is 4.0. truncate : float, deprecated     Truncate the filter at this many standard deviations. Default is 4.0. cval : scalar, optional     Value to fill past edges of input if mode is 'constant'. Default is 0.0</p> Source code in <code>nelpy/estimators.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n    \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        'reflect'\n    truncate : float\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    truncate : float, deprecated\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0\n    \"\"\"\n\n    raise NotImplementedError\n\n    if sigma is None:\n        sigma = 0.1  # in units of extern\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.NDRateMap.synthesize","title":"<code>synthesize(X)</code>","text":"<p>Generate synthetic spike data based on the ratemap.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to synthesize spikes for.</p> required <p>Returns:</p> Name Type Description <code>spikes</code> <code>array - like</code> <p>Synthetic spike data.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def synthesize(self, X):\n    \"\"\"\n    Generate synthetic spike data based on the ratemap.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to synthesize spikes for.\n\n    Returns\n    -------\n    spikes : array-like\n        Synthetic spike data.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap","title":"<code>RateMap</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>RateMap with persistent unit_ids and firing rates in Hz.</p> <p>This class estimates and stores firing rate maps for neural data, supporting both 1D and 2D spatial representations.</p> <p>Parameters:</p> Name Type Description Default <code>connectivity</code> <code>(continuous, discrete, circular)</code> <p>Defines how smoothing is applied. Default is 'continuous'. - 'continuous': Continuous smoothing. - 'discrete': No smoothing is applied. - 'circular': Circular smoothing (for angular variables).</p> <code>'continuous'</code> <p>Attributes:</p> Name Type Description <code>connectivity</code> <code>str</code> <p>Smoothing mode.</p> <code>ratemap_</code> <code>ndarray</code> <p>The estimated firing rate map.</p> <code>_unit_ids</code> <code>ndarray</code> <p>Persistent unit IDs.</p> <code>_bins_x, _bins_y</code> <code>ndarray</code> <p>Bin edges for each dimension.</p> <code>_bin_centers_x, _bin_centers_y</code> <code>ndarray</code> <p>Bin centers for each dimension.</p> <code>_mask</code> <code>ndarray</code> <p>Mask for valid regions.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>class RateMap(BaseEstimator):\n    \"\"\"\n    RateMap with persistent unit_ids and firing rates in Hz.\n\n    This class estimates and stores firing rate maps for neural data, supporting both 1D and 2D spatial representations.\n\n    Parameters\n    ----------\n    connectivity : {'continuous', 'discrete', 'circular'}, optional\n        Defines how smoothing is applied. Default is 'continuous'.\n        - 'continuous': Continuous smoothing.\n        - 'discrete': No smoothing is applied.\n        - 'circular': Circular smoothing (for angular variables).\n\n    Attributes\n    ----------\n    connectivity : str\n        Smoothing mode.\n    ratemap_ : np.ndarray\n        The estimated firing rate map.\n    _unit_ids : np.ndarray\n        Persistent unit IDs.\n    _bins_x, _bins_y : np.ndarray\n        Bin edges for each dimension.\n    _bin_centers_x, _bin_centers_y : np.ndarray\n        Bin centers for each dimension.\n    _mask : np.ndarray\n        Mask for valid regions.\n    \"\"\"\n\n    def __init__(self, connectivity=\"continuous\"):\n        \"\"\"\n        Initialize a RateMap object.\n\n        Parameters\n        ----------\n        connectivity : str, optional\n            Defines how smoothing is applied. If 'discrete', then no smoothing is\n            applied. Default is 'continuous'.\n        \"\"\"\n        self.connectivity = connectivity\n        self._slicer = UnitSlicer(self)\n        self.loc = ItemGetter_loc(self)\n        self.iloc = ItemGetter_iloc(self)\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the RateMap, including shape if fitted.\n\n        Returns\n        -------\n        r : str\n            String representation of the RateMap.\n        \"\"\"\n        r = super().__repr__()\n        if self._is_fitted():\n            if self.is_1d:\n                r += \" with shape (n_units={}, n_bins_x={})\".format(*self.shape)\n            else:\n                r += \" with shape (n_units={}, n_bins_x={}, n_bins_y={})\".format(\n                    *self.shape\n                )\n        return r\n\n    def fit(self, X, y, dt=1, unit_ids=None):\n        \"\"\"\n        Fit firing rates to the provided data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_bins,) or (n_bins_x, n_bins_y)\n            Bin locations (centers) where ratemap is defined.\n        y : array-like, shape (n_units, n_bins) or (n_units, n_bins_x, n_bins_y)\n            Expected number of spikes in a temporal bin of width dt, for each of\n            the predictor bins specified in X.\n        dt : float, optional\n            Temporal bin size with which firing rate y is defined. Default is 1.\n        unit_ids : array-like, shape (n_units,), optional\n            Persistent unit IDs that are used to associate units after\n            permutation. If None, uses np.arange(n_units).\n\n        Returns\n        -------\n        self : RateMap\n            The fitted RateMap instance.\n        \"\"\"\n        n_units, n_bins_x, n_bins_y = self._check_X_y(X, y)\n        if n_bins_y &gt; 0:\n            # self.ratemap_ = np.zeros((n_units, n_bins_x, n_bins_y)) #FIXME\n            self.ratemap_ = y / dt\n            bin_centers_x = np.squeeze(X[:, 0])\n            bin_centers_y = np.squeeze(X[:, 1])\n            bin_dx = np.median(np.diff(bin_centers_x))\n            bin_dy = np.median(np.diff(bin_centers_y))\n            bins_x = np.insert(\n                bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n                0,\n                bin_centers_x[0] - bin_dx / 2,\n            )\n            bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n            bins_y = np.insert(\n                bin_centers_y[:-1] + np.diff(bin_centers_y) / 2,\n                0,\n                bin_centers_y[0] - bin_dy / 2,\n            )\n            bins_y = np.append(bins_y, bins_y[-1] + bin_dy)\n            self._bins_x = bins_x\n            self._bins_y = bins_y\n            self._bin_centers_x = bin_centers_x\n            self._bin_centers_y = X[:, 1]\n        else:\n            # self.ratemap_ = np.zeros((n_units, n_bins_x)) #FIXME\n            self.ratemap_ = y / dt\n            bin_centers_x = np.squeeze(X)\n            bin_dx = np.median(np.diff(bin_centers_x))\n            bins_x = np.insert(\n                bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n                0,\n                bin_centers_x[0] - bin_dx / 2,\n            )\n            bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n            self._bins_x = bins_x\n            self._bin_centers_x = bin_centers_x\n\n        if unit_ids is not None:\n            if len(unit_ids) != n_units:\n                raise ValueError(\n                    \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                        len(unit_ids), n_units\n                    )\n                )\n            self._unit_ids = unit_ids\n        else:\n            self._unit_ids = np.arange(n_units)\n\n    def predict(self, X):\n        \"\"\"\n        Predict firing rates for the given bin locations.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to predict firing rates for.\n\n        Returns\n        -------\n        rates : array-like\n            Predicted firing rates.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def synthesize(self, X):\n        \"\"\"\n        Generate synthetic spike data based on the ratemap.\n\n        Parameters\n        ----------\n        X : array-like\n            Bin locations to synthesize spikes for.\n\n        Returns\n        -------\n        spikes : array-like\n            Synthetic spike data.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        raise NotImplementedError\n\n    def __len__(self):\n        return self.n_units\n\n    def __iter__(self):\n        \"\"\"TuningCurve1D iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"TuningCurve1D iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_units - 1:\n            raise StopIteration\n        out = copy.copy(self)\n        out.ratemap_ = self.ratemap_[tuple([index])]\n        out._unit_ids = self._unit_ids[index]\n        self._index += 1\n        return out\n\n    def __getitem__(self, *idx):\n        \"\"\"\n        Access RateMap units by index.\n\n        Parameters\n        ----------\n        *idx : int, slice, or list\n            Indices of units to access.\n\n        Returns\n        -------\n        out : RateMap\n            Subset RateMap with selected units.\n        \"\"\"\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        try:\n            out = copy.copy(self)\n            out.ratemap_ = self.ratemap_[tuple([idx])]\n            out._unit_ids = list(np.array(out._unit_ids)[tuple([idx])])\n            out._slicer = UnitSlicer(out)\n            out.loc = ItemGetter_loc(out)\n            out.iloc = ItemGetter_iloc(out)\n            return out\n        except Exception:\n            raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def get_peak_firing_order_ids(self):\n        \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n        Returns\n        -------\n        unit_ids : array-like\n            The permutaiton of unit_ids such that after reordering, the peak\n            firing locations are ordered along the RateMap.\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\n                \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n            )\n        peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n        return np.array(self.unit_ids)[peakorder]\n\n    def reorder_units_by_ids(self, unit_ids, inplace=False):\n        \"\"\"Permute the unit ordering.\n\n        #TODO\n        If no order is specified, and an ordering exists from fit(), then the\n        data in X will automatically be permuted to match that registered during\n        fit().\n\n        Parameters\n        ----------\n        unit_ids : array-like, shape (n_units,)\n\n        Returns\n        -------\n        out : reordered RateMap\n        \"\"\"\n\n        def swap_units(arr, frm, to):\n            \"\"\"swap 'units' of a 3D np.array\"\"\"\n            arr[(frm, to), :] = arr[(to, frm), :]\n\n        self._validate_unit_ids(unit_ids)\n        if len(unit_ids) != len(self._unit_ids):\n            raise ValueError(\n                \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n            )\n\n        if inplace:\n            out = self\n        else:\n            out = copy.deepcopy(self)\n\n        neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_units(out.ratemap_, frm, to)\n            out._unit_ids[frm], out._unit_ids[to] = (\n                out._unit_ids[to],\n                out._unit_ids[frm],\n            )\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n        return out\n\n    def _check_X_y(self, X, y):\n        X = np.atleast_1d(X)\n        y = np.atleast_2d(y)\n\n        n_units = y.shape[0]\n        n_bins_xy = y.shape[1]\n        try:\n            n_bins_yy = y.shape[2]\n        except IndexError:\n            n_bins_yy = 0\n\n        n_bins_xx = X.shape[0]\n        try:\n            n_bins_yx = X.shape[1]\n        except IndexError:\n            n_bins_yx = 0\n\n        assert n_units &gt; 0, \"n_units must be a positive integer!\"\n        assert n_bins_xx == n_bins_xy, \"X and y must have the same n_bins_x\"\n        assert n_bins_yx == n_bins_yy, \"X and y must have the same n_bins_y\"\n\n        n_bins_x = n_bins_xx\n        n_bins_y = n_bins_yy\n\n        return n_units, n_bins_x, n_bins_y\n\n    def _validate_unit_ids(self, unit_ids):\n        self._check_unit_ids_in_ratemap(unit_ids)\n\n        if len(set(unit_ids)) != len(unit_ids):\n            raise ValueError(\"Duplicate unit_ids are not allowed.\")\n\n    def _check_unit_ids_in_ratemap(self, unit_ids):\n        for unit_id in unit_ids:\n            # NOTE: the check below allows for predict() to pass on only\n            # a subset of the units that were used during fit! So we\n            # could fit on 100 units, and then predict on only 10 of\n            # them, if we wanted.\n            if unit_id not in self.unit_ids:\n                raise ValueError(\n                    \"unit_id {} was not present during fit(); aborting...\".format(\n                        unit_id\n                    )\n                )\n\n    def _is_fitted(self):\n        try:\n            check_is_fitted(self, \"ratemap_\")\n        except Exception:  # should really be except NotFitterError\n            return False\n        return True\n\n    @property\n    def connectivity(self):\n        return self._connectivity\n\n    @connectivity.setter\n    def connectivity(self, val):\n        self._connectivity = self._validate_connectivity(val)\n\n    @staticmethod\n    def _validate_connectivity(connectivity):\n        connectivity = str(connectivity).strip().lower()\n        options = [\"continuous\", \"discrete\", \"circular\"]\n        if connectivity in options:\n            return connectivity\n        raise NotImplementedError(\n            \"connectivity '{}' is not supported yet!\".format(str(connectivity))\n        )\n\n    @staticmethod\n    def _units_from_X(X):\n        \"\"\"\n        Get unit_ids from bst X, or generate them from ndarray X.\n\n        Returns\n        -------\n        n_units :\n        unit_ids :\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def T(self):\n        \"\"\"transpose the ratemap.\n        Here we transpose the x and y dims, and return a new RateMap object.\n        \"\"\"\n        if self.is_1d:\n            return self\n        out = copy.copy(self)\n        out.ratemap_ = np.transpose(out.ratemap_, axes=(0, 2, 1))\n        return out\n\n    @property\n    def shape(self):\n        \"\"\"\n        RateMap.shape = (n_units, n_features_x, n_features_y)\n            OR\n        RateMap.shape = (n_units, n_features)\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape\n\n    @property\n    def is_1d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 2:\n            return True\n        return False\n\n    @property\n    def is_2d(self):\n        check_is_fitted(self, \"ratemap_\")\n        if len(self.ratemap_.shape) == 3:\n            return True\n        return False\n\n    @property\n    def n_units(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape[0]\n\n    @property\n    def unit_ids(self):\n        check_is_fitted(self, \"ratemap_\")\n        return self._unit_ids\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            return self.n_bins_x * self.n_bins_y\n        return self.n_bins_x\n\n    @property\n    def n_bins_x(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        return self.ratemap_.shape[1]\n\n    @property\n    def n_bins_y(self):\n        \"\"\"(int) Number of external correlates (bins).\"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_1d:\n            raise ValueError(\"RateMap is 1D; no y bins are defined.\")\n        return self.ratemap_.shape[2]\n\n    def max(self, axis=None, out=None):\n        \"\"\"\n        maximum firing rate for each unit:\n            RateMap.max()\n        maximum firing rate across units:\n            RateMap.max(axis=0)\n        \"\"\"\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n            else:\n                return self.ratemap_.max(axis=1, out=out)\n        return self.ratemap_.max(axis=axis, out=out)\n\n    def min(self, axis=None, out=None):\n        check_is_fitted(self, \"ratemap_\")\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.min(axis=1, out=out).min(axis=1, out=out)\n            else:\n                return self.ratemap_.min(axis=1, out=out)\n        return self.ratemap_.min(axis=axis, out=out)\n\n    def mean(self, axis=None, dtype=None, out=None, keepdims=False):\n        check_is_fitted(self, \"ratemap_\")\n        kwargs = {\"dtype\": dtype, \"out\": out, \"keepdims\": keepdims}\n        if axis is None:\n            if self.is_2d:\n                return self.ratemap_.mean(axis=1, **kwargs).mean(axis=1, **kwargs)\n            else:\n                return self.ratemap_.mean(axis=1, **kwargs)\n        return self.ratemap_.mean(axis=axis, **kwargs)\n\n    @property\n    def bins(self):\n        if self.is_1d:\n            return self._bins_x\n        return np.vstack((self._bins_x, self._bins_y))\n\n    @property\n    def bins_x(self):\n        return self._bins_x\n\n    @property\n    def bins_y(self):\n        if self.is_2d:\n            return self._bins_y\n        else:\n            raise ValueError(\"only valid for 2D RateMap() objects.\")\n\n    @property\n    def bin_centers(self):\n        if self.is_1d:\n            return self._bin_centers_x\n        return np.vstack((self._bin_centers_x, self._bin_centers_y))\n\n    @property\n    def bin_centers_x(self):\n        return self._bin_centers_x\n\n    @property\n    def bin_centers_y(self):\n        if self.is_2d:\n            return self._bin_centers_y\n        else:\n            raise ValueError(\"only valid for 2D RateMap() objects.\")\n\n    @property\n    def mask(self):\n        return self._mask\n\n    @mask.setter\n    def mask(self, val):\n        # TODO: mask validation\n        raise NotImplementedError\n        self._mask = val\n\n    def plot(self, **kwargs):\n        check_is_fitted(self, \"ratemap_\")\n        if self.is_2d:\n            raise NotImplementedError(\"plot() not yet implemented for 2D RateMaps.\")\n        pad = kwargs.pop(\"pad\", None)\n        _plot_ratemap(self, pad=pad, **kwargs)\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n        \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n        mode : {\u2018reflect\u2019, 'constant', \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to 'constant'. Default is\n            \u2018reflect\u2019\n        truncate : float\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        truncate : float, deprecated\n            Truncate the filter at this many standard deviations. Default is 4.0.\n        cval : scalar, optional\n            Value to fill past edges of input if mode is 'constant'. Default is 0.0\n        \"\"\"\n\n        if sigma is None:\n            sigma = 0.1  # in units of extern\n        if truncate is None:\n            truncate = 4\n        if mode is None:\n            mode = \"reflect\"\n        if cval is None:\n            cval = 0.0\n\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.T","title":"<code>T</code>  <code>property</code>","text":"<p>transpose the ratemap. Here we transpose the x and y dims, and return a new RateMap object.</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.n_bins_x","title":"<code>n_bins_x</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.n_bins_y","title":"<code>n_bins_y</code>  <code>property</code>","text":"<p>(int) Number of external correlates (bins).</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>RateMap.shape = (n_units, n_features_x, n_features_y)     OR RateMap.shape = (n_units, n_features)</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.fit","title":"<code>fit(X, y, dt=1, unit_ids=None)</code>","text":"<p>Fit firing rates to the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_bins) or (n_bins_x, n_bins_y))</code> <p>Bin locations (centers) where ratemap is defined.</p> required <code>y</code> <code>(array - like, shape(n_units, n_bins) or (n_units, n_bins_x, n_bins_y))</code> <p>Expected number of spikes in a temporal bin of width dt, for each of the predictor bins specified in X.</p> required <code>dt</code> <code>float</code> <p>Temporal bin size with which firing rate y is defined. Default is 1.</p> <code>1</code> <code>unit_ids</code> <code>(array - like, shape(n_units))</code> <p>Persistent unit IDs that are used to associate units after permutation. If None, uses np.arange(n_units).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>RateMap</code> <p>The fitted RateMap instance.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def fit(self, X, y, dt=1, unit_ids=None):\n    \"\"\"\n    Fit firing rates to the provided data.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_bins,) or (n_bins_x, n_bins_y)\n        Bin locations (centers) where ratemap is defined.\n    y : array-like, shape (n_units, n_bins) or (n_units, n_bins_x, n_bins_y)\n        Expected number of spikes in a temporal bin of width dt, for each of\n        the predictor bins specified in X.\n    dt : float, optional\n        Temporal bin size with which firing rate y is defined. Default is 1.\n    unit_ids : array-like, shape (n_units,), optional\n        Persistent unit IDs that are used to associate units after\n        permutation. If None, uses np.arange(n_units).\n\n    Returns\n    -------\n    self : RateMap\n        The fitted RateMap instance.\n    \"\"\"\n    n_units, n_bins_x, n_bins_y = self._check_X_y(X, y)\n    if n_bins_y &gt; 0:\n        # self.ratemap_ = np.zeros((n_units, n_bins_x, n_bins_y)) #FIXME\n        self.ratemap_ = y / dt\n        bin_centers_x = np.squeeze(X[:, 0])\n        bin_centers_y = np.squeeze(X[:, 1])\n        bin_dx = np.median(np.diff(bin_centers_x))\n        bin_dy = np.median(np.diff(bin_centers_y))\n        bins_x = np.insert(\n            bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n            0,\n            bin_centers_x[0] - bin_dx / 2,\n        )\n        bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n        bins_y = np.insert(\n            bin_centers_y[:-1] + np.diff(bin_centers_y) / 2,\n            0,\n            bin_centers_y[0] - bin_dy / 2,\n        )\n        bins_y = np.append(bins_y, bins_y[-1] + bin_dy)\n        self._bins_x = bins_x\n        self._bins_y = bins_y\n        self._bin_centers_x = bin_centers_x\n        self._bin_centers_y = X[:, 1]\n    else:\n        # self.ratemap_ = np.zeros((n_units, n_bins_x)) #FIXME\n        self.ratemap_ = y / dt\n        bin_centers_x = np.squeeze(X)\n        bin_dx = np.median(np.diff(bin_centers_x))\n        bins_x = np.insert(\n            bin_centers_x[:-1] + np.diff(bin_centers_x) / 2,\n            0,\n            bin_centers_x[0] - bin_dx / 2,\n        )\n        bins_x = np.append(bins_x, bins_x[-1] + bin_dx)\n        self._bins_x = bins_x\n        self._bin_centers_x = bin_centers_x\n\n    if unit_ids is not None:\n        if len(unit_ids) != n_units:\n            raise ValueError(\n                \"'unit_ids' must have same number of elements as 'n_units'. {} != {}\".format(\n                    len(unit_ids), n_units\n                )\n            )\n        self._unit_ids = unit_ids\n    else:\n        self._unit_ids = np.arange(n_units)\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.get_peak_firing_order_ids","title":"<code>get_peak_firing_order_ids()</code>","text":"<p>Get the unit_ids in order of peak firing location for 1D RateMaps.</p> <p>Returns:</p> Name Type Description <code>unit_ids</code> <code>array - like</code> <p>The permutaiton of unit_ids such that after reordering, the peak firing locations are ordered along the RateMap.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def get_peak_firing_order_ids(self):\n    \"\"\"Get the unit_ids in order of peak firing location for 1D RateMaps.\n\n    Returns\n    -------\n    unit_ids : array-like\n        The permutaiton of unit_ids such that after reordering, the peak\n        firing locations are ordered along the RateMap.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    if self.is_2d:\n        raise NotImplementedError(\n            \"get_peak_firing_order_ids() only implemented for 1D RateMaps.\"\n        )\n    peakorder = np.argmax(self.ratemap_, axis=1).argsort()\n    return np.array(self.unit_ids)[peakorder]\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.max","title":"<code>max(axis=None, out=None)</code>","text":"<p>maximum firing rate for each unit:     RateMap.max() maximum firing rate across units:     RateMap.max(axis=0)</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def max(self, axis=None, out=None):\n    \"\"\"\n    maximum firing rate for each unit:\n        RateMap.max()\n    maximum firing rate across units:\n        RateMap.max(axis=0)\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    if axis is None:\n        if self.is_2d:\n            return self.ratemap_.max(axis=1, out=out).max(axis=1, out=out)\n        else:\n            return self.ratemap_.max(axis=1, out=out)\n    return self.ratemap_.max(axis=axis, out=out)\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.predict","title":"<code>predict(X)</code>","text":"<p>Predict firing rates for the given bin locations.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to predict firing rates for.</p> required <p>Returns:</p> Name Type Description <code>rates</code> <code>array - like</code> <p>Predicted firing rates.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict firing rates for the given bin locations.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to predict firing rates for.\n\n    Returns\n    -------\n    rates : array-like\n        Predicted firing rates.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.reorder_units_by_ids","title":"<code>reorder_units_by_ids(unit_ids, inplace=False)</code>","text":"<p>Permute the unit ordering.</p>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.reorder_units_by_ids--todo","title":"TODO","text":"<p>If no order is specified, and an ordering exists from fit(), then the data in X will automatically be permuted to match that registered during fit().</p> <p>Parameters:</p> Name Type Description Default <code>unit_ids</code> <code>(array - like, shape(n_units))</code> required <p>Returns:</p> Name Type Description <code>out</code> <code>reordered RateMap</code> Source code in <code>nelpy/estimators.py</code> <pre><code>def reorder_units_by_ids(self, unit_ids, inplace=False):\n    \"\"\"Permute the unit ordering.\n\n    #TODO\n    If no order is specified, and an ordering exists from fit(), then the\n    data in X will automatically be permuted to match that registered during\n    fit().\n\n    Parameters\n    ----------\n    unit_ids : array-like, shape (n_units,)\n\n    Returns\n    -------\n    out : reordered RateMap\n    \"\"\"\n\n    def swap_units(arr, frm, to):\n        \"\"\"swap 'units' of a 3D np.array\"\"\"\n        arr[(frm, to), :] = arr[(to, frm), :]\n\n    self._validate_unit_ids(unit_ids)\n    if len(unit_ids) != len(self._unit_ids):\n        raise ValueError(\n            \"unit_ids must be a permutation of self.unit_ids, not a subset thereof.\"\n        )\n\n    if inplace:\n        out = self\n    else:\n        out = copy.deepcopy(self)\n\n    neworder = [list(self.unit_ids).index(x) for x in unit_ids]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        swap_units(out.ratemap_, frm, to)\n        out._unit_ids[frm], out._unit_ids[to] = (\n            out._unit_ids[to],\n            out._unit_ids[frm],\n        )\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n    return out\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.smooth","title":"<code>smooth(*, sigma=None, truncate=None, inplace=False, mode=None, cval=None)</code>","text":"<p>Smooths the tuning curve with a Gaussian kernel.</p> <p>mode : {\u2018reflect\u2019, 'constant', \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional     The mode parameter determines how the array borders are handled,     where cval is the value when mode is equal to 'constant'. Default is     \u2018reflect\u2019 truncate : float     Truncate the filter at this many standard deviations. Default is 4.0. truncate : float, deprecated     Truncate the filter at this many standard deviations. Default is 4.0. cval : scalar, optional     Value to fill past edges of input if mode is 'constant'. Default is 0.0</p> Source code in <code>nelpy/estimators.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(self, *, sigma=None, truncate=None, inplace=False, mode=None, cval=None):\n    \"\"\"Smooths the tuning curve with a Gaussian kernel.\n\n    mode : {\u2018reflect\u2019, 'constant', \u2018nearest\u2019, \u2018mirror\u2019, \u2018wrap\u2019}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        \u2018reflect\u2019\n    truncate : float\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    truncate : float, deprecated\n        Truncate the filter at this many standard deviations. Default is 4.0.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.1  # in units of extern\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.RateMap.synthesize","title":"<code>synthesize(X)</code>","text":"<p>Generate synthetic spike data based on the ratemap.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Bin locations to synthesize spikes for.</p> required <p>Returns:</p> Name Type Description <code>spikes</code> <code>array - like</code> <p>Synthetic spike data.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def synthesize(self, X):\n    \"\"\"\n    Generate synthetic spike data based on the ratemap.\n\n    Parameters\n    ----------\n    X : array-like\n        Bin locations to synthesize spikes for.\n\n    Returns\n    -------\n    spikes : array-like\n        Synthetic spike data.\n    \"\"\"\n    check_is_fitted(self, \"ratemap_\")\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.UnitSlicer","title":"<code>UnitSlicer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>nelpy/estimators.py</code> <pre><code>class UnitSlicer(object):\n    def __init__(self, obj):\n        self.obj = obj\n\n    def __getitem__(self, *args):\n        \"\"\"units ids\"\"\"\n        # by default, keep all units\n        unitslice = slice(None, None, None)\n        if isinstance(*args, int):\n            unitslice = args[0]\n        else:\n            slices = np.s_[args]\n            slices = slices[0]\n            unitslice = slices\n\n        if isinstance(unitslice, slice):\n            start = unitslice.start\n            stop = unitslice.stop\n            istep = unitslice.step\n\n            try:\n                if start is None:\n                    istart = 0\n                else:\n                    istart = list(self.obj.unit_ids).index(start)\n            except ValueError:\n                raise KeyError(\n                    \"unit_id {} could not be found in RateMap!\".format(start)\n                )\n\n            try:\n                if stop is None:\n                    istop = self.obj.n_units\n                else:\n                    istop = list(self.obj.unit_ids).index(stop) + 1\n            except ValueError:\n                raise KeyError(\"unit_id {} could not be found in RateMap!\".format(stop))\n            if istep is None:\n                istep = 1\n            if istep &lt; 0:\n                istop -= 1\n                istart -= 1\n                istart, istop = istop, istart\n            unit_idx_list = list(range(istart, istop, istep))\n        else:\n            unit_idx_list = []\n            unitslice = np.atleast_1d(unitslice)\n            for unit in unitslice:\n                try:\n                    uidx = list(self.obj.unit_ids).index(unit)\n                except ValueError:\n                    raise KeyError(\n                        \"unit_id {} could not be found in RateMap!\".format(unit)\n                    )\n                else:\n                    unit_idx_list.append(uidx)\n\n        return unit_idx_list\n</code></pre>"},{"location":"reference/nelpy/estimators/#nelpy.estimators.decode_bayesian_memoryless_nd","title":"<code>decode_bayesian_memoryless_nd(X, *, ratemap, bin_centers, dt=1)</code>","text":"<p>Memoryless Bayesian decoding (supports multidimensional decoding).</p> <p>Decode binned spike counts (e.g. from a BinnedSpikeTrainArray) to an external correlate (e.g. position), using a memoryless Bayesian decoder and a previously estimated ratemap.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy array with shape (n_samples, n_features),</code> <p>where the features are generally putative units / cells, and where each sample represents spike counts in a singleton data window.</p> required <code>ratemap</code> <code>array-like of shape (n_units, n_bins_d1, ..., n_bins_dN)</code> <p>Expected number of spikes for each unit, within each bin, along each dimension.</p> required <code>bin_centers</code> <code>array-like with shape (n_dims, ), where each element is also</code> <p>an array-like with shape (n_bins_dn, ) containing the bin centers for the particular dimension.</p> required <code>dt</code> <code>(float, optional(default=1))</code> <p>Temporal bin width corresponding to X, in seconds.</p> <p>NOTE: generally it is assumed that ratemap will be given in Hz (that is, it has dt=1). If ratemap has a different unit, then dt might have to be adjusted to compensate for this. This can get tricky / confusing, so the recommended approach is always to construct ratemap with dt=1, and then to use the data-specific dt here when decoding.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>posterior</code> <code>numpy array of shape (n_samples, n_bins_d1, ..., n_bins_dN)</code> <p>Posterior probabilities for each voxel.</p> <code>expected_pth</code> <code>numpy array of shape (n_samples, n_dims)</code> <p>Expected (posterior-averaged) decoded trajectory.</p> Source code in <code>nelpy/estimators.py</code> <pre><code>def decode_bayesian_memoryless_nd(X, *, ratemap, bin_centers, dt=1):\n    \"\"\"Memoryless Bayesian decoding (supports multidimensional decoding).\n\n    Decode binned spike counts (e.g. from a BinnedSpikeTrainArray) to an\n    external correlate (e.g. position), using a memoryless Bayesian decoder and\n    a previously estimated ratemap.\n\n    Parameters\n    ----------\n    X : numpy array with shape (n_samples, n_features),\n        where the features are generally putative units / cells, and where\n        each sample represents spike counts in a singleton data window.\n    ratemap : array-like of shape (n_units, n_bins_d1, ..., n_bins_dN)\n        Expected number of spikes for each unit, within each bin, along each\n        dimension.\n    bin_centers : array-like with shape (n_dims, ), where each element is also\n        an array-like with shape (n_bins_dn, ) containing the bin centers for\n        the particular dimension.\n    dt : float, optional (default=1)\n        Temporal bin width corresponding to X, in seconds.\n\n        NOTE: generally it is assumed that ratemap will be given in Hz (that is,\n        it has dt=1). If ratemap has a different unit, then dt might have to be\n        adjusted to compensate for this. This can get tricky / confusing, so the\n        recommended approach is always to construct ratemap with dt=1, and then\n        to use the data-specific dt here when decoding.\n\n    Returns\n    -------\n    posterior : numpy array of shape (n_samples, n_bins_d1, ..., n_bins_dN)\n        Posterior probabilities for each voxel.\n    expected_pth : numpy array of shape (n_samples, n_dims)\n        Expected (posterior-averaged) decoded trajectory.\n    \"\"\"\n\n    def tile_obs(obs, *n_bins):\n        n_units = len(obs)\n        out = np.zeros((n_units, *n_bins))\n        for unit in range(n_units):\n            out[unit, :] = obs[unit]\n        return out\n\n    n_samples, n_features = X.shape\n    n_units = ratemap.shape[0]\n    n_bins = np.atleast_1d(ratemap.shape[1:])\n    n_dims = len(n_bins)\n\n    assert n_features == n_units, \"X has {} units, whereas ratemap has {}\".format(\n        n_features, n_units\n    )\n\n    lfx = np.log(ratemap)\n    eterm = -ratemap.sum(axis=0) * dt\n\n    posterior = np.empty((n_samples, *n_bins))\n    posterior[:] = np.nan\n\n    # decode each sample / bin separately\n    for tt in range(n_samples):\n        obs = X[tt]\n        if obs.sum() &gt; 0:\n            posterior[tt] = (tile_obs(obs, *n_bins) * lfx).sum(axis=0) + eterm\n\n    # normalize posterior:\n    posterior = np.exp(\n        posterior\n        - logsumexp(posterior, axis=tuple(np.arange(1, n_dims + 1)), keepdims=True)\n    )\n\n    if n_dims &gt; 1:\n        expected = []\n        for dd in range(1, n_dims + 1):\n            axes = tuple(set(np.arange(1, n_dims + 1)) - set([dd]))\n            expected.append(\n                (bin_centers[dd - 1] * posterior.sum(axis=axes)).sum(axis=1)\n            )\n        expected_pth = np.vstack(expected).T\n    else:\n        expected_pth = (bin_centers * posterior).sum(axis=1)\n\n    return posterior, expected_pth\n</code></pre>"},{"location":"reference/nelpy/filtering/","title":"nelpy.filtering","text":"<p>This module implements filtering functionailty for core nelpy objects.</p>"},{"location":"reference/nelpy/filtering/#nelpy.filtering.getsos","title":"<code>getsos(*, fs, fl=None, fh=None, bandstop=False, gpass=None, gstop=None, ftype='cheby2')</code>","text":"<p>Return second-order sections representation of the IIR filter.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>The sampling frequency (Hz).</p> required <code>fl</code> <code>float</code> <p>Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.</p> <code>None</code> <code>fh</code> <code>float</code> <p>Upper cut-off frequency (in Hz), 0 or None to ignore. Default is None.</p> <code>None</code> <code>bandstop</code> <code>boolean</code> <p>If False, passband is between fl and fh. If True, stopband is between fl and fh. Default is False.</p> <code>False</code> <code>gpass</code> <code>float</code> <p>The maximum loss in the passband (dB). Default is 0.1 dB.</p> <code>None</code> <code>gstop</code> <code>float</code> <p>The minimum attenuation in the stopband (dB). Default is 30 dB.</p> <code>None</code> <code>ftype</code> <code>str</code> <p>The type of IIR filter to design:     - Butterworth   : 'butter'     - Chebyshev I   : 'cheby1'     - Chebyshev II  : 'cheby2' (Default)     - Cauer/elliptic: 'ellip'     - Bessel/Thomson: 'bessel'</p> <code>'cheby2'</code> <p>Returns:</p> Name Type Description <code>sos</code> <code>ndarray</code> <p>Second-order sections representation of the IIR filter.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from scipy import signal\n&gt;&gt;&gt;\n&gt;&gt;&gt; sos = getsos(...)\n&gt;&gt;&gt; w, h = signal.sosfreqz(sos, worN=1500)\n&gt;&gt;&gt; db = 20 * np.log10(np.abs(h))\n&gt;&gt;&gt; freq = w * fs / (2 * np.pi)\n&gt;&gt;&gt; plt.subplot(2, 1, 1)\n&gt;&gt;&gt; plt.ylabel(\"Gain [dB]\")\n&gt;&gt;&gt; plt.plot(freq, db)\n&gt;&gt;&gt; plt.subplot(2, 1, 2)\n&gt;&gt;&gt; plt.plot(freq, np.angle(h))\n&gt;&gt;&gt; plt.ylabel(\"Phase [rad]\")\n</code></pre> <p>Although not currently supported, filters can be stacked as well, as follows:</p> <pre><code>&gt;&gt;&gt; sos = np.vstack((nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n             nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n             nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n             nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=1, ftype='butter')))\n</code></pre> Source code in <code>nelpy/filtering.py</code> <pre><code>def getsos(\n    *, fs, fl=None, fh=None, bandstop=False, gpass=None, gstop=None, ftype=\"cheby2\"\n):\n    \"\"\"Return second-order sections representation of the IIR filter.\n\n    Parameters\n    ----------\n    fs : float\n        The sampling frequency (Hz).\n    fl : float, optional\n        Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.\n    fh : float, optional\n        Upper cut-off frequency (in Hz), 0 or None to ignore. Default is None.\n    bandstop : boolean, optional\n        If False, passband is between fl and fh. If True, stopband is between\n        fl and fh. Default is False.\n    gpass : float, optional\n        The maximum loss in the passband (dB). Default is 0.1 dB.\n    gstop : float, optional\n        The minimum attenuation in the stopband (dB). Default is 30 dB.\n    ftype : str, optional\n        The type of IIR filter to design:\n            - Butterworth   : 'butter'\n            - Chebyshev I   : 'cheby1'\n            - Chebyshev II  : 'cheby2' (Default)\n            - Cauer/elliptic: 'ellip'\n            - Bessel/Thomson: 'bessel'\n\n    Returns\n    -------\n    sos : ndarray\n        Second-order sections representation of the IIR filter.\n\n    Examples\n    -------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from scipy import signal\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; sos = getsos(...)\n    &gt;&gt;&gt; w, h = signal.sosfreqz(sos, worN=1500)\n    &gt;&gt;&gt; db = 20 * np.log10(np.abs(h))\n    &gt;&gt;&gt; freq = w * fs / (2 * np.pi)\n    &gt;&gt;&gt; plt.subplot(2, 1, 1)\n    &gt;&gt;&gt; plt.ylabel(\"Gain [dB]\")\n    &gt;&gt;&gt; plt.plot(freq, db)\n    &gt;&gt;&gt; plt.subplot(2, 1, 2)\n    &gt;&gt;&gt; plt.plot(freq, np.angle(h))\n    &gt;&gt;&gt; plt.ylabel(\"Phase [rad]\")\n\n    Although not currently supported, filters can be stacked as well, as follows:\n    &gt;&gt;&gt; sos = np.vstack((nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n                 nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n                 nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=10, ftype='cheby2'),\n                 nel.filtering.getsos(fs=T2.fs, fl=150, fh=250, gstop=1, ftype='butter')))\n\n    \"\"\"\n\n    try:\n        assert fh &lt; fs, \"fh must be less than sampling rate!\"\n    except TypeError:\n        pass\n    try:\n        assert fl &lt; fh, \"fl must be less than fh!\"\n    except TypeError:\n        pass\n\n    if gpass is None:\n        gpass = 0.1  # max loss in passband, dB\n    if gstop is None:\n        gstop = 30  # min attenuation in stopband (dB)\n\n    try:\n        if np.isinf(fh):\n            fh = None\n    except TypeError:\n        pass\n    if fl == 0:\n        fl = None\n\n    # Handle cutoff frequencies\n    fso2 = fs / 2.0\n    if (fl is None) and (fh is None):\n        raise ValueError(\"Nonsensical all-pass filter requested...\")\n    elif fl is None:  # lowpass\n        wp = fh / fso2\n        ws = 1.4 * fh / fso2\n    elif fh is None:  # highpass\n        wp = fl / fso2\n        ws = 0.8 * fl / fso2\n    else:  # bandpass\n        wp = [fl / fso2, fh / fso2]\n        ws = [0.8 * fl / fso2, 1.4 * fh / fso2]\n    if bandstop:  # notch / bandstop filter\n        wp, ws = ws, wp\n\n    sos = sig.iirdesign(wp, ws, gpass=gpass, gstop=gstop, ftype=ftype, output=\"sos\")\n\n    return sos\n</code></pre>"},{"location":"reference/nelpy/filtering/#nelpy.filtering.sosfiltfilt","title":"<code>sosfiltfilt(timeseries, *, fl=None, fh=None, fs=None, inplace=False, bandstop=False, gpass=None, gstop=None, ftype=None, buffer_len=None, overlap_len=None, parallel=True, **kwargs)</code>","text":"<p>Apply a zero-phase digital filter using second-order sections.</p> <p>This function applies a forward and backward digital filter to a signal using second-order sections (SOS) representation. The result has zero phase distortion and the same shape as the input.</p> <p>Parameters:</p> Name Type Description Default <code>timeseries</code> <code>nelpy.RegularlySampledAnalogSignalArray (preferred), ndarray, or list</code> <p>Object or data to filter. Can be a NumPy array or a Nelpy AnalogSignalArray.</p> required <code>fs</code> <code>float, optional only if RegularlySampledAnalogSignalArray is passed</code> <p>The sampling frequency (Hz). Obtained from the input timeseries.</p> <code>None</code> <code>fl</code> <code>float</code> <p>Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.</p> <code>None</code> <code>fh</code> <code>float</code> <p>Upper cut-off frequency (in Hz), np.inf or None to ignore. Default is None.</p> <code>None</code> <code>bandstop</code> <code>boolean</code> <p>If False, passband is between fl and fh. If True, stopband is between fl and fh. Default is False.</p> <code>False</code> <code>gpass</code> <code>float</code> <p>The maximum loss in the passband (dB). Default is 0.1 dB.</p> <code>None</code> <code>gstop</code> <code>float</code> <p>The minimum attenuation in the stopband (dB). Default is 30 dB.</p> <code>None</code> <code>ftype</code> <code>str</code> <p>The type of IIR filter to design:     - Butterworth   : 'butter'     - Chebyshev I   : 'cheby1'     - Chebyshev II  : 'cheby2' (Default)     - Cauer/elliptic: 'ellip'     - Bessel/Thomson: 'bessel'</p> <code>None</code> <code>buffer_len</code> <code>int</code> <p>How much data to process at a time. Default is 2**22 = 4194304 samples.</p> <code>None</code> <code>overlap_len</code> <code>int</code> <p>How much data we add to the end of each chunk to smooth out filter transients.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True, modifies the input in place. Default is False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>If True, uses multiprocessing for parallel filtering. Default is True.</p> <code>True</code> <code>kwargs</code> <code>optional</code> <p>Other keyword arguments are passed to scipy.signal's iirdesign method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>out</code> <code>nelpy.RegularlySampledAnalogSignalArray, ndarray, or list</code> <p>Same output type as input timeseries.</p> <code>WARNING</code> <code>The data type of the output object is the same as that of the input.</code> <code>Thus it is highly recommended to have your input data be floats before calling</code> <code>this function. If the input is an RSASA, you do not need to worry because</code> <code>the underlying data are already floats.</code> See Also <p>scipy.signal.sosfilt : Apply a digital filter forward in time. scipy.signal.filtfilt : Zero-phase filtering for transfer function and FIR filters.</p> Notes <p>This function is similar to <code>scipy.signal.filtfilt</code>, but uses the second-order sections (SOS) representation for improved numerical stability, especially for high-order filters.</p> <p>Examples:</p> <p>Filter a noisy sine wave (NumPy array):</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.signal import butter\n&gt;&gt;&gt; from nelpy.filtering import sosfiltfilt\n&gt;&gt;&gt; np.random.seed(0)\n&gt;&gt;&gt; t = np.linspace(0, 1, 1000, endpoint=False)\n&gt;&gt;&gt; x = np.sin(2 * np.pi * 5 * t) + 0.5 * np.random.randn(t.size)\n&gt;&gt;&gt; sos = butter(4, 10, \"low\", fs=1000, output=\"sos\")\n&gt;&gt;&gt; y = sosfiltfilt(x, fs=1000, fl=None, fh=10)\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; plt.plot(t, x, label=\"Noisy signal\")\n&gt;&gt;&gt; plt.plot(t, y, label=\"Filtered signal\")\n&gt;&gt;&gt; plt.legend()\n&gt;&gt;&gt; plt.show()\n</code></pre> <p>Filter a Nelpy AnalogSignalArray:</p> <pre><code>&gt;&gt;&gt; from nelpy import AnalogSignalArray\n&gt;&gt;&gt; # Create a 2-channel signal\n&gt;&gt;&gt; data = np.vstack([np.sin(2 * np.pi * 5 * t), np.cos(2 * np.pi * 5 * t)])\n&gt;&gt;&gt; asa = AnalogSignalArray(data=data, abscissa_vals=t, fs=1000)\n&gt;&gt;&gt; filtered_asa = sosfiltfilt(asa, fl=None, fh=10)\n&gt;&gt;&gt; print(filtered_asa.data.shape)\n(2, 1000)\n&gt;&gt;&gt; # Plot the first channel\n&gt;&gt;&gt; plt.plot(t, asa.data[0], label=\"Original\")\n&gt;&gt;&gt; plt.plot(t, filtered_asa.data[0], label=\"Filtered\")\n&gt;&gt;&gt; plt.legend()\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/filtering.py</code> <pre><code>def sosfiltfilt(\n    timeseries,\n    *,\n    fl=None,\n    fh=None,\n    fs=None,\n    inplace=False,\n    bandstop=False,\n    gpass=None,\n    gstop=None,\n    ftype=None,\n    buffer_len=None,\n    overlap_len=None,\n    parallel=True,\n    **kwargs,\n):\n    \"\"\"\n    Apply a zero-phase digital filter using second-order sections.\n\n    This function applies a forward and backward digital filter to a signal using\n    second-order sections (SOS) representation. The result has zero phase distortion\n    and the same shape as the input.\n\n    Parameters\n    ----------\n    timeseries : nelpy.RegularlySampledAnalogSignalArray (preferred), ndarray, or list\n        Object or data to filter. Can be a NumPy array or a Nelpy AnalogSignalArray.\n    fs : float, optional only if RegularlySampledAnalogSignalArray is passed\n        The sampling frequency (Hz). Obtained from the input timeseries.\n    fl : float, optional\n        Lower cut-off frequency (in Hz), 0 or None to ignore. Default is None.\n    fh : float, optional\n        Upper cut-off frequency (in Hz), np.inf or None to ignore. Default is None.\n    bandstop : boolean, optional\n        If False, passband is between fl and fh. If True, stopband is between\n        fl and fh. Default is False.\n    gpass : float, optional\n        The maximum loss in the passband (dB). Default is 0.1 dB.\n    gstop : float, optional\n        The minimum attenuation in the stopband (dB). Default is 30 dB.\n    ftype : str, optional\n        The type of IIR filter to design:\n            - Butterworth   : 'butter'\n            - Chebyshev I   : 'cheby1'\n            - Chebyshev II  : 'cheby2' (Default)\n            - Cauer/elliptic: 'ellip'\n            - Bessel/Thomson: 'bessel'\n    buffer_len : int, optional\n        How much data to process at a time. Default is 2**22 = 4194304 samples.\n    overlap_len : int, optional\n        How much data we add to the end of each chunk to smooth out filter\n        transients.\n    inplace : bool, optional\n        If True, modifies the input in place. Default is False.\n    parallel : bool, optional\n        If True, uses multiprocessing for parallel filtering. Default is True.\n    kwargs : optional\n        Other keyword arguments are passed to scipy.signal's iirdesign method\n\n    Returns\n    -------\n    out : nelpy.RegularlySampledAnalogSignalArray, ndarray, or list\n        Same output type as input timeseries.\n\n    WARNING : The data type of the output object is the same as that of the input.\n    Thus it is highly recommended to have your input data be floats before calling\n    this function. If the input is an RSASA, you do not need to worry because\n    the underlying data are already floats.\n\n    See Also\n    --------\n    scipy.signal.sosfilt : Apply a digital filter forward in time.\n    scipy.signal.filtfilt : Zero-phase filtering for transfer function and FIR filters.\n\n    Notes\n    -----\n    This function is similar to `scipy.signal.filtfilt`, but uses the second-order sections\n    (SOS) representation for improved numerical stability, especially for high-order filters.\n\n    Examples\n    --------\n    Filter a noisy sine wave (NumPy array):\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy.signal import butter\n    &gt;&gt;&gt; from nelpy.filtering import sosfiltfilt\n    &gt;&gt;&gt; np.random.seed(0)\n    &gt;&gt;&gt; t = np.linspace(0, 1, 1000, endpoint=False)\n    &gt;&gt;&gt; x = np.sin(2 * np.pi * 5 * t) + 0.5 * np.random.randn(t.size)\n    &gt;&gt;&gt; sos = butter(4, 10, \"low\", fs=1000, output=\"sos\")\n    &gt;&gt;&gt; y = sosfiltfilt(x, fs=1000, fl=None, fh=10)\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; plt.plot(t, x, label=\"Noisy signal\")\n    &gt;&gt;&gt; plt.plot(t, y, label=\"Filtered signal\")\n    &gt;&gt;&gt; plt.legend()\n    &gt;&gt;&gt; plt.show()\n\n    Filter a Nelpy AnalogSignalArray:\n\n    &gt;&gt;&gt; from nelpy import AnalogSignalArray\n    &gt;&gt;&gt; # Create a 2-channel signal\n    &gt;&gt;&gt; data = np.vstack([np.sin(2 * np.pi * 5 * t), np.cos(2 * np.pi * 5 * t)])\n    &gt;&gt;&gt; asa = AnalogSignalArray(data=data, abscissa_vals=t, fs=1000)\n    &gt;&gt;&gt; filtered_asa = sosfiltfilt(asa, fl=None, fh=10)\n    &gt;&gt;&gt; print(filtered_asa.data.shape)\n    (2, 1000)\n    &gt;&gt;&gt; # Plot the first channel\n    &gt;&gt;&gt; plt.plot(t, asa.data[0], label=\"Original\")\n    &gt;&gt;&gt; plt.plot(t, filtered_asa.data[0], label=\"Filtered\")\n    &gt;&gt;&gt; plt.legend()\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # make sure that fs is specified, unless AnalogSignalArray is passed in\n    if isinstance(timeseries, (np.ndarray, list)):\n        if fs is None:\n            raise ValueError(\"Sampling frequency, fs, must be specified!\")\n    elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n        if fs is None:\n            fs = timeseries.fs\n    else:\n        raise TypeError(\"Unsupported input type!\")\n\n    try:\n        assert fh &lt; fs, \"fh must be less than sampling rate!\"\n    except TypeError:\n        pass\n    try:\n        assert fl &lt; fh, \"fl must be less than fh!\"\n    except TypeError:\n        pass\n\n    if inplace:\n        out = timeseries\n    else:\n        out = deepcopy(timeseries)\n    if overlap_len is None:\n        overlap_len = int(fs * 2)\n    if buffer_len is None:\n        buffer_len = 4194304\n    if gpass is None:\n        gpass = 0.1  # max loss in passband, dB\n    if gstop is None:\n        gstop = 30  # min attenuation in stopband (dB)\n    if ftype is None:\n        ftype = \"cheby2\"\n\n    try:\n        if np.isinf(fh):\n            fh = None\n    except TypeError:\n        pass\n    if fl == 0:\n        fl = None\n\n    # Handle cutoff frequencies\n    fso2 = fs / 2.0\n    if (fl is None) and (fh is None):\n        raise ValueError(\"Nonsensical all-pass filter requested...\")\n    elif fl is None:  # lowpass\n        wp = fh / fso2\n        ws = 1.4 * fh / fso2\n    elif fh is None:  # highpass\n        wp = fl / fso2\n        ws = 0.8 * fl / fso2\n    else:  # bandpass\n        wp = [fl / fso2, fh / fso2]\n        ws = [0.8 * fl / fso2, 1.4 * fh / fso2]\n    if bandstop:  # notch / bandstop filter\n        wp, ws = ws, wp\n\n    sos = sig.iirdesign(\n        wp, ws, gpass=gpass, gstop=gstop, ftype=ftype, output=\"sos\", **kwargs\n    )\n\n    # Prepare input and output data structures\n    # Output array lives in shared memory and will reduce overhead from pickling/de-pickling\n    # data if we're doing parallelized filtering\n    if isinstance(timeseries, (np.ndarray, list)):\n        temp_array = np.array(timeseries)\n        dims = temp_array.shape\n        if len(temp_array.shape) &gt; 2:\n            raise NotImplementedError(\n                \"Filtering for &gt;2D ndarray or list is not implemented\"\n            )\n        shared_array_base = Array(ctypes.c_double, temp_array.size, lock=False)\n        shared_array_out = np.ctypeslib.as_array(shared_array_base)\n        # Force input and output arrays to be 2D (N x T) where N is number of signals\n        # and T is number of time points\n        if len(temp_array.squeeze().shape) == 1:\n            shared_array_out = np.ctypeslib.as_array(shared_array_base).reshape(\n                (1, temp_array.size)\n            )\n            input_asarray = temp_array.reshape((1, temp_array.size))\n        else:\n            shared_array_out = np.ctypeslib.as_array(shared_array_base).reshape(dims)\n            input_asarray = temp_array\n    elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n        dims = timeseries._data.shape\n        shared_array_base = Array(\n            ctypes.c_double, timeseries._data_rowsig.size, lock=False\n        )\n        shared_array_out = np.ctypeslib.as_array(shared_array_base).reshape(dims)\n        input_asarray = timeseries._data\n\n    # Embedded function to avoid pickling data but need global to make this function\n    # module-visible (required by multiprocessing). I know, a bit of a hack\n    global filter_chunk\n\n    def filter_chunk(it):\n        \"\"\"The function that performs the chunked filtering\"\"\"\n\n        try:\n            start, stop, buffer_len, overlap_len, buff_st_idx = it\n            buff_nd_idx = int(min(stop, buff_st_idx + buffer_len))\n            chk_st_idx = int(max(start, buff_st_idx - overlap_len))\n            chk_nd_idx = int(min(stop, buff_nd_idx + overlap_len))\n            rel_st_idx = int(buff_st_idx - chk_st_idx)\n            rel_nd_idx = int(buff_nd_idx - chk_st_idx)\n            this_y_chk = sig.sosfiltfilt(\n                sos, input_asarray[:, chk_st_idx:chk_nd_idx], axis=1\n            )\n            shared_array_out[:, buff_st_idx:buff_nd_idx] = this_y_chk[\n                :, rel_st_idx:rel_nd_idx\n            ]\n        except ValueError:\n            raise ValueError(\n                (\n                    \"Some epochs were too short to filter. Try dropping those first,\"\n                    \" filtering, and then inserting them back in\"\n                )\n            )\n\n    # Do the actual parallellized filtering\n    if (\n        sys.platform.startswith(\"linux\") or sys.platform.startswith(\"darwin\")\n    ) and parallel:\n        pool = Pool(processes=cpu_count())\n        if isinstance(timeseries, (np.ndarray, list)):\n            # ignore epochs (information not contained in list or array) so filter directly\n            start, stop = 0, input_asarray.shape[1]\n            pool.map(\n                filter_chunk,\n                zip(\n                    repeat(start),\n                    repeat(stop),\n                    repeat(buffer_len),\n                    repeat(overlap_len),\n                    range(start, stop, buffer_len),\n                ),\n                chunksize=1,\n            )\n        elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n            fei = np.insert(\n                np.cumsum(timeseries.lengths), 0, 0\n            )  # filter epoch indices, fei\n            for ii in range(len(fei) - 1):  # filter within epochs\n                start, stop = fei[ii], fei[ii + 1]\n                pool.map(\n                    filter_chunk,\n                    zip(\n                        repeat(start),\n                        repeat(stop),\n                        repeat(buffer_len),\n                        repeat(overlap_len),\n                        range(start, stop, buffer_len),\n                    ),\n                    chunksize=1,\n                )\n        pool.close()\n        pool.join()\n    # No easy parallelized filtering for other OSes\n    else:\n        if isinstance(timeseries, (np.ndarray, list)):\n            # ignore epochs (information not contained in list or array) so filter directly\n            start, stop = 0, input_asarray.shape[1]\n            iterator = zip(\n                repeat(start),\n                repeat(stop),\n                repeat(buffer_len),\n                repeat(overlap_len),\n                range(start, stop, buffer_len),\n            )\n            for item in iterator:\n                filter_chunk(item)\n        elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n            fei = np.insert(\n                np.cumsum(timeseries.lengths), 0, 0\n            )  # filter epoch indices, fei\n            for ii in range(len(fei) - 1):  # filter within epochs\n                start, stop = fei[ii], fei[ii + 1]\n                iterator = zip(\n                    repeat(start),\n                    repeat(stop),\n                    repeat(buffer_len),\n                    repeat(overlap_len),\n                    range(start, stop, buffer_len),\n                )\n                for item in iterator:\n                    filter_chunk(item)\n\n    if isinstance(timeseries, np.ndarray):\n        out[:] = np.reshape(shared_array_out, dims)\n    elif isinstance(timeseries, list):\n        out[:] = np.reshape(shared_array_out, dims).tolist()\n    elif isinstance(timeseries, core.RegularlySampledAnalogSignalArray):\n        out._data[:] = shared_array_out\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/formatters/","title":"nelpy.formatters","text":"<p>This module contains string formatters for nelpy.</p>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.ArbitraryFormatter","title":"<code>ArbitraryFormatter</code>","text":"<p>               Bases: <code>float</code></p> <p>Formatter for arbitrary units.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>class ArbitraryFormatter(float):\n    \"\"\"Formatter for arbitrary units.\"\"\"\n\n    base_unit = \"a.u.\"\n\n    def __init__(self, val):\n        self.val = val\n        self.base_unit = type(self).base_unit\n\n    def __str__(self):\n        return \"{:g}\".format(self.val)\n\n    def __repr__(self):\n        return \"{:g}\".format(self.val)\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.BaseFormatter","title":"<code>BaseFormatter</code>","text":"<p>Base formatter for nelpy string formatting utilities.</p> <p>This class provides a base for custom formatters that convert values to human-readable strings.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>any</code> <p>The value to be formatted.</p> required <p>Attributes:</p> Name Type Description <code>base_unit</code> <code>str</code> <p>The base unit for the formatter.</p> <code>val</code> <code>any</code> <p>The value to be formatted.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>class BaseFormatter:\n    \"\"\"\n    Base formatter for nelpy string formatting utilities.\n\n    This class provides a base for custom formatters that convert values to human-readable strings.\n\n    Parameters\n    ----------\n    val : any\n        The value to be formatted.\n\n    Attributes\n    ----------\n    base_unit : str\n        The base unit for the formatter.\n    val : any\n        The value to be formatted.\n    \"\"\"\n\n    base_unit = \"base units\"\n\n    def __init__(self, val):\n        \"\"\"\n        Initialize the BaseFormatter.\n\n        Parameters\n        ----------\n        val : any\n            The value to be formatted.\n        \"\"\"\n        self.val = val\n        self.base_unit = type(self).base_unit\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettyBytes","title":"<code>PrettyBytes</code>","text":"<p>               Bases: <code>int</code></p> <p>Prints number of bytes in a more readable format</p> Source code in <code>nelpy/formatters.py</code> <pre><code>class PrettyBytes(int):\n    \"\"\"Prints number of bytes in a more readable format\"\"\"\n\n    base_unit = \"bytes\"\n\n    def __init__(self, val):\n        self.val = val\n        self.base_unit = type(self).base_unit\n\n    def __str__(self):\n        if self.val &lt; 1024:\n            return \"{} bytes\".format(self.val)\n        elif self.val &lt; 1024**2:\n            return \"{:.3f} kilobytes\".format(self.val / 1024)\n        elif self.val &lt; 1024**3:\n            return \"{:.3f} megabytes\".format(self.val / 1024**2)\n        elif self.val &lt; 1024**4:\n            return \"{:.3f} gigabytes\".format(self.val / 1024**3)\n\n    def __repr__(self):\n        return self.__str__()\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettyDuration","title":"<code>PrettyDuration</code>","text":"<p>               Bases: <code>float</code></p> <p>Time duration with pretty print.</p> <p>Behaves like a float, and can always be cast to a float.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>class PrettyDuration(float):\n    \"\"\"Time duration with pretty print.\n\n    Behaves like a float, and can always be cast to a float.\n    \"\"\"\n\n    base_unit = \"s\"\n\n    def __init__(self, seconds):\n        self.duration = seconds\n        self.base_unit = type(self).base_unit\n\n    def __str__(self):\n        return self.time_string(self.duration)\n\n    def __repr__(self):\n        return self.time_string(self.duration)\n\n    @staticmethod\n    def to_dhms(seconds):\n        \"\"\"convert seconds into hh:mm:ss:ms\"\"\"\n        pos = seconds &gt;= 0\n        if not pos:\n            seconds = -seconds\n        ms = seconds % 1\n        ms = round(ms * 10000) / 10\n        seconds = floor(seconds)\n        m, s = divmod(seconds, 60)\n        h, m = divmod(m, 60)\n        d, h = divmod(h, 24)\n        Time = namedtuple(\"Time\", \"pos dd hh mm ss ms\")\n        time = Time(pos=pos, dd=d, hh=h, mm=m, ss=s, ms=ms)\n        return time\n\n    @staticmethod\n    def time_string(seconds):\n        \"\"\"returns a formatted time string.\"\"\"\n        if np.isinf(seconds):\n            return \"inf\"\n        pos, dd, hh, mm, ss, s = PrettyDuration.to_dhms(seconds)\n        if s &gt; 0:\n            if mm == 0:\n                # in this case, represent milliseconds in terms of\n                # seconds (i.e. a decimal)\n                sstr = str(s / 1000).lstrip(\"0\")\n                if s &gt;= 999.5:\n                    ss += 1\n                    s = 0\n                    sstr = \"\"\n                    # now propagate the carry:\n                    if ss == 60:\n                        mm += 1\n                        ss = 0\n                    if mm == 60:\n                        hh += 1\n                        mm = 0\n                    if hh == 24:\n                        dd += 1\n                        hh = 0\n            else:\n                # for all other cases, milliseconds will be represented\n                # as an integer\n                if s &gt;= 999.5:\n                    ss += 1\n                    s = 0\n                    sstr = \"\"\n                    # now propagate the carry:\n                    if ss == 60:\n                        mm += 1\n                        ss = 0\n                    if mm == 60:\n                        hh += 1\n                        mm = 0\n                    if hh == 24:\n                        dd += 1\n                        hh = 0\n                else:\n                    sstr = \":{:03d}\".format(int(s))\n        else:\n            sstr = \"\"\n        if dd &gt; 0:\n            daystr = \"{:01d} days \".format(dd)\n        else:\n            daystr = \"\"\n        if hh &gt; 0:\n            timestr = daystr + \"{:01d}:{:02d}:{:02d}{} hours\".format(hh, mm, ss, sstr)\n        elif mm &gt; 0:\n            timestr = daystr + \"{:01d}:{:02d}{} minutes\".format(mm, ss, sstr)\n        elif ss &gt; 0:\n            timestr = daystr + \"{:01d}{} seconds\".format(ss, sstr)\n        else:\n            timestr = daystr + \"{} milliseconds\".format(s)\n        if not pos:\n            timestr = \"-\" + timestr\n        return timestr\n\n    def __add__(self, other):\n        \"\"\"a + b\"\"\"\n        return PrettyDuration(self.duration + other)\n\n    def __radd__(self, other):\n        \"\"\"b + a\"\"\"\n        return self.__add__(other)\n\n    def __sub__(self, other):\n        \"\"\"a - b\"\"\"\n        return PrettyDuration(self.duration - other)\n\n    def __rsub__(self, other):\n        \"\"\"b - a\"\"\"\n        return other - self.duration\n\n    def __mul__(self, other):\n        \"\"\"a * b\"\"\"\n        return PrettyDuration(self.duration * other)\n\n    def __rmul__(self, other):\n        \"\"\"b * a\"\"\"\n        return self.__mul__(other)\n\n    def __truediv__(self, other):\n        \"\"\"a / b\"\"\"\n        return PrettyDuration(self.duration / other)\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettyDuration.time_string","title":"<code>time_string(seconds)</code>  <code>staticmethod</code>","text":"<p>returns a formatted time string.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>@staticmethod\ndef time_string(seconds):\n    \"\"\"returns a formatted time string.\"\"\"\n    if np.isinf(seconds):\n        return \"inf\"\n    pos, dd, hh, mm, ss, s = PrettyDuration.to_dhms(seconds)\n    if s &gt; 0:\n        if mm == 0:\n            # in this case, represent milliseconds in terms of\n            # seconds (i.e. a decimal)\n            sstr = str(s / 1000).lstrip(\"0\")\n            if s &gt;= 999.5:\n                ss += 1\n                s = 0\n                sstr = \"\"\n                # now propagate the carry:\n                if ss == 60:\n                    mm += 1\n                    ss = 0\n                if mm == 60:\n                    hh += 1\n                    mm = 0\n                if hh == 24:\n                    dd += 1\n                    hh = 0\n        else:\n            # for all other cases, milliseconds will be represented\n            # as an integer\n            if s &gt;= 999.5:\n                ss += 1\n                s = 0\n                sstr = \"\"\n                # now propagate the carry:\n                if ss == 60:\n                    mm += 1\n                    ss = 0\n                if mm == 60:\n                    hh += 1\n                    mm = 0\n                if hh == 24:\n                    dd += 1\n                    hh = 0\n            else:\n                sstr = \":{:03d}\".format(int(s))\n    else:\n        sstr = \"\"\n    if dd &gt; 0:\n        daystr = \"{:01d} days \".format(dd)\n    else:\n        daystr = \"\"\n    if hh &gt; 0:\n        timestr = daystr + \"{:01d}:{:02d}:{:02d}{} hours\".format(hh, mm, ss, sstr)\n    elif mm &gt; 0:\n        timestr = daystr + \"{:01d}:{:02d}{} minutes\".format(mm, ss, sstr)\n    elif ss &gt; 0:\n        timestr = daystr + \"{:01d}{} seconds\".format(ss, sstr)\n    else:\n        timestr = daystr + \"{} milliseconds\".format(s)\n    if not pos:\n        timestr = \"-\" + timestr\n    return timestr\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettyDuration.to_dhms","title":"<code>to_dhms(seconds)</code>  <code>staticmethod</code>","text":"<p>convert seconds into hh:mm:ss:ms</p> Source code in <code>nelpy/formatters.py</code> <pre><code>@staticmethod\ndef to_dhms(seconds):\n    \"\"\"convert seconds into hh:mm:ss:ms\"\"\"\n    pos = seconds &gt;= 0\n    if not pos:\n        seconds = -seconds\n    ms = seconds % 1\n    ms = round(ms * 10000) / 10\n    seconds = floor(seconds)\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    d, h = divmod(h, 24)\n    Time = namedtuple(\"Time\", \"pos dd hh mm ss ms\")\n    time = Time(pos=pos, dd=d, hh=h, mm=m, ss=s, ms=ms)\n    return time\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettyInt","title":"<code>PrettyInt</code>","text":"<p>               Bases: <code>int</code></p> <p>Prints integers in a more readable format</p> Source code in <code>nelpy/formatters.py</code> <pre><code>class PrettyInt(int):\n    \"\"\"Prints integers in a more readable format\"\"\"\n\n    base_unit = \"int\"\n\n    def __init__(self, val):\n        self.val = val\n        self.base_unit = type(self).base_unit\n\n    def __str__(self):\n        return \"{:,}\".format(self.val)\n\n    def __repr__(self):\n        return \"{:,}\".format(self.val)\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettySpace","title":"<code>PrettySpace</code>","text":"<p>               Bases: <code>float</code></p> <p>Spatial distance/position with pretty print.</p> <p>Behaves like a float, and can always be cast to a float.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>class PrettySpace(float):\n    \"\"\"Spatial distance/position with pretty print.\n\n    Behaves like a float, and can always be cast to a float.\n    \"\"\"\n\n    base_unit = \"cm\"\n\n    def __init__(self, centimeters):\n        self.value = centimeters\n        self.base_unit = type(self).base_unit\n\n    def __str__(self):\n        return self.space_string(self.value)\n\n    def __repr__(self):\n        return self.space_string(self.value)\n\n    @staticmethod\n    def decompose_cm(centimeters):\n        \"\"\"decompose space into (km, m, cm, mm, um).\n        This function is needlessly complicated, but the template\n        can be useful if we wanted to work in inches, etc.\n        \"\"\"\n        pos = centimeters &gt;= 0\n        if not pos:\n            centimeters = -centimeters\n        um = round(10000 * ((centimeters * 1000) % 1)) / 10\n        mm = floor(centimeters % 1 * 1000)\n        cm = floor(centimeters)\n        m, cm = divmod(cm, 100)\n        km, m = divmod(m, 1000)\n        Space = namedtuple(\"Space\", \"pos km m cm mm um\")\n        space = Space(pos=pos, km=km, m=m, cm=cm, mm=mm, um=um)\n        return space\n\n    @staticmethod\n    def decompose_cm2(centimeters):\n        \"\"\"decompose space into (km, m, cm, mm, um).\n        This function is needlessly complicated, but the template\n        can be useful if we wanted to work in inches, etc.\n        \"\"\"\n        pos = centimeters &gt;= 0\n        if not pos:\n            centimeters = -centimeters\n        um = round(10000 * ((centimeters * 1000) % 1)) / 10\n        mm = floor(centimeters % 1 * 1000)\n        cm = floor(centimeters)\n        m, cm = divmod(cm, 100)\n        km, m = divmod(m, 1000)\n        Space = namedtuple(\"Space\", \"pos km m cm mm um\")\n        space = Space(pos=pos, km=km, m=m, cm=cm, mm=mm, um=um)\n        return space\n\n    @staticmethod\n    def space_string(centimeters):\n        \"\"\"returns a formatted space string.\"\"\"\n        sstr = str(centimeters)\n        if np.isinf(centimeters):\n            return \"inf\"\n        #         pos, km, m, cm, mm, um = PrettySpace.decompose(centimeters)\n        pos = centimeters &gt;= 0\n        if not pos:\n            centimeters = -centimeters\n        if centimeters &gt; 100000:\n            sstr = \"{:g} km\".format(centimeters / 100000)\n        elif centimeters &gt; 100:\n            sstr = \"{:g} m\".format(centimeters / 100)\n        elif centimeters &gt; 1:\n            sstr = \"{:g} cm\".format(centimeters)\n        elif centimeters &lt; 0.001:\n            sstr = \"{:g} um\".format(centimeters * 1000000)\n        else:\n            sstr = \"{:g} mm\".format(centimeters * 1000)\n        if not pos:\n            sstr = \"-\" + sstr\n        return sstr\n\n    def __add__(self, other):\n        \"\"\"a + b\"\"\"\n        return PrettySpace(self.value + other)\n\n    def __radd__(self, other):\n        \"\"\"b + a\"\"\"\n        return self.__add__(other)\n\n    def __sub__(self, other):\n        \"\"\"a - b\"\"\"\n        return PrettySpace(self.value - other)\n\n    def __rsub__(self, other):\n        \"\"\"b - a\"\"\"\n        return other - self.value\n\n    def __mul__(self, other):\n        \"\"\"a * b\"\"\"\n        return PrettySpace(self.value * other)\n\n    def __rmul__(self, other):\n        \"\"\"b * a\"\"\"\n        return self.__mul__(other)\n\n    def __truediv__(self, other):\n        \"\"\"a / b\"\"\"\n        return PrettySpace(self.value / other)\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettySpace.decompose_cm","title":"<code>decompose_cm(centimeters)</code>  <code>staticmethod</code>","text":"<p>decompose space into (km, m, cm, mm, um). This function is needlessly complicated, but the template can be useful if we wanted to work in inches, etc.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>@staticmethod\ndef decompose_cm(centimeters):\n    \"\"\"decompose space into (km, m, cm, mm, um).\n    This function is needlessly complicated, but the template\n    can be useful if we wanted to work in inches, etc.\n    \"\"\"\n    pos = centimeters &gt;= 0\n    if not pos:\n        centimeters = -centimeters\n    um = round(10000 * ((centimeters * 1000) % 1)) / 10\n    mm = floor(centimeters % 1 * 1000)\n    cm = floor(centimeters)\n    m, cm = divmod(cm, 100)\n    km, m = divmod(m, 1000)\n    Space = namedtuple(\"Space\", \"pos km m cm mm um\")\n    space = Space(pos=pos, km=km, m=m, cm=cm, mm=mm, um=um)\n    return space\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettySpace.decompose_cm2","title":"<code>decompose_cm2(centimeters)</code>  <code>staticmethod</code>","text":"<p>decompose space into (km, m, cm, mm, um). This function is needlessly complicated, but the template can be useful if we wanted to work in inches, etc.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>@staticmethod\ndef decompose_cm2(centimeters):\n    \"\"\"decompose space into (km, m, cm, mm, um).\n    This function is needlessly complicated, but the template\n    can be useful if we wanted to work in inches, etc.\n    \"\"\"\n    pos = centimeters &gt;= 0\n    if not pos:\n        centimeters = -centimeters\n    um = round(10000 * ((centimeters * 1000) % 1)) / 10\n    mm = floor(centimeters % 1 * 1000)\n    cm = floor(centimeters)\n    m, cm = divmod(cm, 100)\n    km, m = divmod(m, 1000)\n    Space = namedtuple(\"Space\", \"pos km m cm mm um\")\n    space = Space(pos=pos, km=km, m=m, cm=cm, mm=mm, um=um)\n    return space\n</code></pre>"},{"location":"reference/nelpy/formatters/#nelpy.formatters.PrettySpace.space_string","title":"<code>space_string(centimeters)</code>  <code>staticmethod</code>","text":"<p>returns a formatted space string.</p> Source code in <code>nelpy/formatters.py</code> <pre><code>@staticmethod\ndef space_string(centimeters):\n    \"\"\"returns a formatted space string.\"\"\"\n    sstr = str(centimeters)\n    if np.isinf(centimeters):\n        return \"inf\"\n    #         pos, km, m, cm, mm, um = PrettySpace.decompose(centimeters)\n    pos = centimeters &gt;= 0\n    if not pos:\n        centimeters = -centimeters\n    if centimeters &gt; 100000:\n        sstr = \"{:g} km\".format(centimeters / 100000)\n    elif centimeters &gt; 100:\n        sstr = \"{:g} m\".format(centimeters / 100)\n    elif centimeters &gt; 1:\n        sstr = \"{:g} cm\".format(centimeters)\n    elif centimeters &lt; 0.001:\n        sstr = \"{:g} um\".format(centimeters * 1000000)\n    else:\n        sstr = \"{:g} mm\".format(centimeters * 1000)\n    if not pos:\n        sstr = \"-\" + sstr\n    return sstr\n</code></pre>"},{"location":"reference/nelpy/hmmutils/","title":"nelpy.hmmutils","text":"<p>nelpy.hmmutils contains helper functions and wrappers for working with hmmlearn.</p>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM","title":"<code>PoissonHMM</code>","text":"<p>               Bases: <code>PoissonHMM</code></p> <p>Nelpy extension of PoissonHMM: Hidden Markov Model with independent Poisson emissions.</p> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of states.</p> required <code>startprob_prior</code> <code>(array, shape(n_components))</code> <p>Initial state occupation prior distribution.</p> required <code>transmat_prior</code> <code>(array, shape(n_components, n_components))</code> <p>Matrix of prior transition probabilities between states.</p> required <code>algorithm</code> <code>string, one of the :data:`base.DECODER_ALGORITHMS`</code> <p>Decoder algorithm.</p> required <code>random_state</code> <p>A random number generator instance.</p> <code>None</code> <code>n_iter</code> <code>int</code> <p>Maximum number of iterations to perform.</p> <code>None</code> <code>tol</code> <code>float</code> <p>Convergence threshold. EM will stop if the gain in log-likelihood is below this value.</p> required <code>verbose</code> <code>bool</code> <p>When <code>True</code> per-iteration convergence reports are printed to :data:<code>sys.stderr</code>. You can diagnose convergence via the :attr:<code>monitor_</code> attribute.</p> <code>False</code> <code>params</code> <code>string</code> <p>Controls which parameters are updated in the training process.  Can contain any combination of 's' for startprob, 't' for transmat, 'm' for means and 'c' for covars. Defaults to all parameters.</p> <code>None</code> <code>init_params</code> <code>string</code> <p>Controls which parameters are initialized prior to training.  Can contain any combination of 's' for startprob, 't' for transmat, 'm' for means and 'c' for covars. Defaults to all parameters.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>n_features</code> <code>int</code> <p>Dimensionality of the (independent) Poisson emissions.</p> <code>monitor_</code> <code>ConvergenceMonitor</code> <p>Monitor object used to check the convergence of EM.</p> <code>transmat_</code> <code>(array, shape(n_components, n_components))</code> <p>Matrix of transition probabilities between states.</p> <code>startprob_</code> <code>(array, shape(n_components))</code> <p>Initial state occupation distribution.</p> <code>means_</code> <code>(array, shape(n_components, n_features))</code> <p>Mean parameters for each state.</p> <code>extern_</code> <code>(array, shape(n_components, n_extern))</code> <p>Augmented mapping from state space to external variables.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.hmmutils import PoissonHMM\n&gt;&gt;&gt; PoissonHMM(n_components=2)...\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>class PoissonHMM(PHMM):\n    \"\"\"Nelpy extension of PoissonHMM: Hidden Markov Model with\n    independent Poisson emissions.\n\n    Parameters\n    ----------\n    n_components : int\n        Number of states.\n\n    startprob_prior : array, shape (n_components, )\n        Initial state occupation prior distribution.\n\n    transmat_prior : array, shape (n_components, n_components)\n        Matrix of prior transition probabilities between states.\n\n    algorithm : string, one of the :data:`base.DECODER_ALGORITHMS`\n        Decoder algorithm.\n\n    random_state: RandomState or an int seed (0 by default)\n        A random number generator instance.\n\n    n_iter : int, optional\n        Maximum number of iterations to perform.\n\n    tol : float, optional\n        Convergence threshold. EM will stop if the gain in log-likelihood\n        is below this value.\n\n    verbose : bool, optional\n        When ``True`` per-iteration convergence reports are printed\n        to :data:`sys.stderr`. You can diagnose convergence via the\n        :attr:`monitor_` attribute.\n\n    params : string, optional\n        Controls which parameters are updated in the training\n        process.  Can contain any combination of 's' for startprob,\n        't' for transmat, 'm' for means and 'c' for covars. Defaults\n        to all parameters.\n\n    init_params : string, optional\n        Controls which parameters are initialized prior to\n        training.  Can contain any combination of 's' for\n        startprob, 't' for transmat, 'm' for means and 'c' for covars.\n        Defaults to all parameters.\n\n    Attributes\n    ----------\n    n_features : int\n        Dimensionality of the (independent) Poisson emissions.\n\n    monitor_ : ConvergenceMonitor\n        Monitor object used to check the convergence of EM.\n\n    transmat_ : array, shape (n_components, n_components)\n        Matrix of transition probabilities between states.\n\n    startprob_ : array, shape (n_components, )\n        Initial state occupation distribution.\n\n    means_ : array, shape (n_components, n_features)\n        Mean parameters for each state.\n\n    extern_ : array, shape (n_components, n_extern)\n        Augmented mapping from state space to external variables.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.hmmutils import PoissonHMM\n    &gt;&gt;&gt; PoissonHMM(n_components=2)...\n\n    \"\"\"\n\n    __attributes__ = [\"_fs\", \"_ds\", \"_unit_ids\", \"_unit_labels\", \"_unit_tags\"]\n\n    def __init__(\n        self,\n        *,\n        n_components,\n        n_iter=None,\n        init_params=None,\n        params=None,\n        random_state=None,\n        verbose=False,\n    ):\n        # assign default parameter values\n        if n_iter is None:\n            n_iter = 50\n        if init_params is None:\n            init_params = \"stm\"\n        if params is None:\n            params = \"stm\"\n\n        # TODO: I don't understand why super().__init__ does not work?\n        PHMM.__init__(\n            self,\n            n_components=n_components,\n            n_iter=n_iter,\n            init_params=init_params,\n            params=params,\n            random_state=random_state,\n            verbose=verbose,\n        )\n\n        # initialize BinnedSpikeTrain attributes\n        for attrib in self.__attributes__:\n            exec(\"self.\" + attrib + \" = None\")\n\n        self._extern_ = None\n        self._ds = None\n        # self._extern_map = None\n\n        # create shortcuts to super() methods that are overridden in\n        # this class\n        self._fit = PHMM.fit\n        self._score = PHMM.score\n        self._score_samples = PHMM.score_samples\n        self._predict = PHMM.predict\n        self._predict_proba = PHMM.predict_proba\n        self._decode = PHMM.decode\n\n        self._sample = PHMM.sample\n\n    def __repr__(self):\n        try:\n            rep = super().__repr__()\n        except Exception:\n            warn(\n                \"couldn't access super().__repr__;\"\n                \" upgrade dependencies to resolve this issue.\"\n            )\n            rep = \"PoissonHMM\"\n        if self._extern_ is not None:\n            fit_ext = \"True\"\n        else:\n            fit_ext = \"False\"\n        try:\n            fit = \"False\"\n            if self.means_ is not None:\n                fit = \"True\"\n        except AttributeError:\n            fit = \"False\"\n        fitstr = \"; fit=\" + fit + \", fit_ext=\" + fit_ext\n        return \"nelpy.\" + rep + fitstr\n\n    @property\n    def extern_(self):\n        \"\"\"\n        Mapping from states to external variables (e.g., position).\n\n        Returns\n        -------\n        np.ndarray or None\n            Array of shape (n_components, n_extern) containing the mapping\n            from states to external variables. Returns None if no mapping\n            has been learned yet.\n\n        Examples\n        --------\n        &gt;&gt;&gt; hmm.fit_ext(bst, position_data)\n        &gt;&gt;&gt; extern_map = hmm.extern_\n        &gt;&gt;&gt; print(f\"State 0 maps to position bin {np.argmax(extern_map[0])}\")\n        \"\"\"\n        if self._extern_ is not None:\n            return self._extern_\n        else:\n            warn(\"no state &lt;--&gt; external mapping has been learnt yet!\")\n            return None\n\n    def _get_order_from_transmat(self, start_state=None):\n        \"\"\"Determine a state ordering based on the transition matrix.\n\n        This is a greedy approach, starting at the a priori most probable\n        state, and moving to the next most probable state according to\n        the transition matrix, and so on.\n\n        Parameters\n        ----------\n        start_state : int, optional\n            Initial state to begin from. Defaults to the most probable\n            a priori state.\n\n        Returns\n        -------\n        new_order : list\n            List of states in transmat order.\n        \"\"\"\n\n        # unless specified, start in the a priori most probable state\n        if start_state is None:\n            start_state = np.argmax(self.startprob_)\n\n        new_order = [start_state]\n        num_states = self.transmat_.shape[0]\n        rem_states = np.arange(0, start_state).tolist()\n        rem_states.extend(np.arange(start_state + 1, num_states).tolist())\n        cs = start_state  # current state\n\n        for ii in np.arange(0, num_states - 1):\n            # find largest transition to set of remaining states\n            nstilde = np.argmax(self.transmat_[cs, rem_states])\n            ns = rem_states[nstilde]\n            # remove selected state from list of remaining states\n            rem_states.remove(ns)\n            cs = ns\n            new_order.append(cs)\n\n        return new_order\n\n    @property\n    def unit_ids(self):\n        \"\"\"\n        List of unit IDs associated with the model.\n\n        Returns\n        -------\n        list\n            List of unit IDs.\n        \"\"\"\n        return self._unit_ids\n\n    @property\n    def unit_labels(self):\n        \"\"\"\n        List of unit labels associated with the model.\n\n        Returns\n        -------\n        list\n            List of unit labels.\n        \"\"\"\n        return self._unit_labels\n\n    @property\n    def means(self):\n        \"\"\"\n        Observation matrix (mean firing rates for each state and unit).\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (n_components, n_units) containing the mean parameters for each state.\n        \"\"\"\n        return self.means_\n\n    @property\n    def transmat(self):\n        \"\"\"\n        Transition probability matrix.\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (n_components, n_components) where A[i, j] = Pr(S_{t+1}=j | S_t=i).\n        \"\"\"\n        return self.transmat_\n\n    @property\n    def startprob(self):\n        \"\"\"\n        Prior distribution over states.\n\n        Returns\n        -------\n        np.ndarray\n            Array of shape (n_components,) representing the initial state probabilities.\n        \"\"\"\n        return self.startprob_\n\n    def get_state_order(self, method=None, start_state=None):\n        \"\"\"\n        Return a state ordering, optionally using augmented data.\n\n        Parameters\n        ----------\n        method : {'transmat', 'mode', 'mean'}, optional\n            Method to use for ordering states. 'transmat' (default) uses the transition matrix.\n            'mode' or 'mean' use the external mapping (requires self._extern_).\n        start_state : int, optional\n            Initial state to begin from (used only if method is 'transmat').\n\n        Returns\n        -------\n        neworder : list\n            List of state indices in the new order.\n\n        Notes\n        -----\n        Both 'mode' and 'mean' assume that _extern_ is in sorted order; this is not verified explicitly.\n\n        Examples\n        --------\n        &gt;&gt;&gt; order = hmm.get_state_order(method=\"transmat\")\n        &gt;&gt;&gt; order = hmm.get_state_order(method=\"mode\")\n        \"\"\"\n        if method is None:\n            method = \"transmat\"\n\n        neworder = []\n\n        if method == \"transmat\":\n            return self._get_order_from_transmat(start_state=start_state)\n        elif method == \"mode\":\n            if self._extern_ is not None:\n                neworder = self._extern_.argmax(axis=1).argsort()\n            else:\n                raise Exception(\n                    \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n                )\n        elif method == \"mean\":\n            if self._extern_ is not None:\n                (\n                    np.tile(np.arange(self._extern_.shape[1]), (self.n_components, 1))\n                    * self._extern_\n                ).sum(axis=1).argsort()\n                neworder = self._extern_.argmax(axis=1).argsort()\n            else:\n                raise Exception(\n                    \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n                )\n        else:\n            raise NotImplementedError(\n                \"ordering method '\" + str(method) + \"' not supported!\"\n            )\n        return neworder\n\n    def _reorder_units_by_ids(self, neworder):\n        \"\"\"\n        Reorder unit_ids to match that of a BinnedSpikeTrain.\n\n        WARNING! Modifies self.means_ in-place.\n\n        Parameters\n        ----------\n        neworder : list or array-like\n            List of unit IDs specifying the new order. Must be of size (n_units,).\n\n        Returns\n        -------\n        self : PoissonHMM\n            The reordered PoissonHMM instance.\n\n        Examples\n        --------\n        &gt;&gt;&gt; hmm._reorder_units_by_ids([3, 1, 2, 0])\n        \"\"\"\n        neworder = [self.unit_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_cols(self.means_, frm, to)\n            self._unit_ids[frm], self._unit_ids[to] = (\n                self._unit_ids[to],\n                self._unit_ids[frm],\n            )\n            self._unit_labels[frm], self._unit_labels[to] = (\n                self._unit_labels[to],\n                self._unit_labels[frm],\n            )\n            # TODO: re-build unit tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        return self\n\n    def reorder_states(self, neworder):\n        \"\"\"\n        Reorder internal HMM states according to a specified order.\n\n        Parameters\n        ----------\n        neworder : list or array-like\n            List of state indices specifying the new order. Must be of size (n_components,).\n\n        Examples\n        --------\n        &gt;&gt;&gt; hmm.reorder_states([2, 0, 1])\n        \"\"\"\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            swap_cols(self.transmat_, frm, to)\n            swap_rows(self.transmat_, frm, to)\n            swap_rows(self.means_, frm, to)\n            if self._extern_ is not None:\n                swap_rows(self._extern_, frm, to)\n            self.startprob_[frm], self.startprob_[to] = (\n                self.startprob_[to],\n                self.startprob_[frm],\n            )\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    def assume_attributes(self, binnedSpikeTrainArray):\n        \"\"\"\n        Assume subset of attributes from a BinnedSpikeTrainArray.\n\n        This is used primarily to enable the sampling of sequences after a model has been fit.\n\n        Parameters\n        ----------\n        binnedSpikeTrainArray : BinnedSpikeTrainArray\n            The BinnedSpikeTrainArray instance from which to copy attributes.\n        \"\"\"\n        if self._ds is not None:\n            warn(\"PoissonHMM(BinnedSpikeTrain) attributes already exist.\")\n        for attrib in self.__attributes__:\n            exec(\"self.\" + attrib + \" = binnedSpikeTrainArray.\" + attrib)\n        self._unit_ids = copy.copy(binnedSpikeTrainArray.unit_ids)\n        self._unit_labels = copy.copy(binnedSpikeTrainArray.unit_labels)\n        self._unit_tags = copy.copy(binnedSpikeTrainArray.unit_tags)\n\n    def _has_same_unit_id_order(self, unit_ids):\n        \"\"\"\n        Check if the provided unit_ids are in the same order as the model's unit_ids.\n\n        Parameters\n        ----------\n        unit_ids : list or array-like\n            List of unit IDs to compare.\n\n        Returns\n        -------\n        bool\n            True if the unit_ids are in the same order, False otherwise.\n\n        Raises\n        ------\n        TypeError\n            If the number of unit_ids does not match.\n        \"\"\"\n        if self._unit_ids is None:\n            return True\n        if len(unit_ids) != len(self.unit_ids):\n            raise TypeError(\"Incorrect number of unit_ids encountered!\")\n        for ii, unit_id in enumerate(unit_ids):\n            if unit_id != self.unit_ids[ii]:\n                return False\n        return True\n\n    def _sliding_window_array(self, bst, w=1):\n        \"\"\"\n        Returns an unwrapped data array by sliding w bins one bin at a time.\n\n        If w==1, then bins are non-overlapping.\n\n        Parameters\n        ----------\n        bst : BinnedSpikeTrainArray\n            Input with data array of shape (n_units, n_bins).\n        w : int, optional\n            Window size (number of bins). Default is 1.\n\n        Returns\n        -------\n        unwrapped : np.ndarray\n            New data array of shape (n_sliding_bins, n_units).\n        lengths : np.ndarray\n            Array of shape (n_sliding_bins,) indicating the lengths of each window.\n\n        Raises\n        ------\n        NotImplementedError\n            If bst is not a BinnedSpikeTrainArray.\n        AssertionError\n            If w is not a positive integer.\n\n        Examples\n        --------\n        &gt;&gt;&gt; unwrapped, lengths = hmm._sliding_window_array(bst, w=3)\n        \"\"\"\n        if w is None:\n            w = 1\n        assert float(w).is_integer(), \"w must be a positive integer!\"\n        assert w &gt; 0, \"w must be a positive integer!\"\n\n        if not isinstance(bst, BinnedSpikeTrainArray):\n            raise NotImplementedError(\n                \"support for other datatypes not yet implemented!\"\n            )\n\n        # potentially re-organize internal observation matrix to be\n        # compatible with BinnedSpikeTrainArray\n        if not self._has_same_unit_id_order(bst.unit_ids):\n            self._reorder_units_by_ids(bst.unit_ids)\n\n        if w == 1:\n            return bst.data.T, bst.lengths\n\n        n_units, t_bins = bst.data.shape\n\n        # if we decode using multiple bins at a time (w&gt;1) then we have to decode each epoch separately:\n\n        # first, we determine the number of bins we will decode. This requires us to scan over the epochs\n        n_bins = 0\n        cumlengths = np.cumsum(bst.lengths)\n        lengths = np.zeros(bst.n_epochs, dtype=np.int)\n        prev_idx = 0\n        for ii, to_idx in enumerate(cumlengths):\n            datalen = to_idx - prev_idx\n            prev_idx = to_idx\n            lengths[ii] = np.max((1, datalen - w + 1))\n\n        n_bins = lengths.sum()\n\n        unwrapped = np.zeros((n_units, n_bins))\n\n        # next, we decode each epoch separately, one bin at a time\n        cum_lengths = np.insert(np.cumsum(lengths), 0, 0)\n\n        prev_idx = 0\n        for ii, to_idx in enumerate(cumlengths):\n            data = bst.data[:, prev_idx:to_idx]\n            prev_idx = to_idx\n            datacum = np.cumsum(\n                data, axis=1\n            )  # ii'th data segment, with column of zeros prepended\n            datacum = np.hstack((np.zeros((n_units, 1)), datacum))\n            re = w  # right edge ptr\n            # TODO: check if datalen &lt; w and act appropriately\n            if lengths[ii] &gt; 1:  # more than one full window fits into data length\n                for tt in range(lengths[ii]):\n                    obs = (\n                        datacum[:, re] - datacum[:, re - w]\n                    )  # spikes in window of size w\n                    re += 1\n                    post_idx = lengths[ii] + tt\n                    unwrapped[:, post_idx] = obs\n            else:  # only one window can fit in, and perhaps only partially. We just take all the data we can get,\n                # and ignore the scaling problem where the window size is now possibly less than bst.ds*w\n                post_idx = cum_lengths[ii]\n                obs = datacum[:, -1]  # spikes in window of size at most w\n                unwrapped[:, post_idx] = obs\n\n        return unwrapped.T, lengths\n\n    def decode(self, X, lengths=None, w=None, algorithm=None):\n        \"\"\"\n        Find the most likely state sequence corresponding to ``X``.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n            WARNING: Each decoding window is assumed to be similar in size to those used during training.\n            If not, the tuning curves have to be scaled appropriately!\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n        algorithm : str, optional\n            Decoder algorithm to be used (see DECODER_ALGORITHMS).\n\n        Returns\n        -------\n        logprob : float or list of float\n            Log probability of the produced state sequence.\n        state_sequence : np.ndarray or list of np.ndarray\n            Labels for each sample from ``X`` obtained via the given decoder algorithm.\n        centers : np.ndarray or list of np.ndarray\n            Time-centers of all bins contained in ``X``.\n\n        See Also\n        --------\n        score_samples : Compute the log probability under the model and posteriors.\n        score : Compute the log probability under the model.\n\n        Examples\n        --------\n        &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(bst)\n        &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(X, algorithm=\"viterbi\")\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            return self._decode(self, X=X, lengths=lengths, algorithm=algorithm), None\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            state_sequences = []\n            centers = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                logprob, state_sequence = self._decode(\n                    self, windowed_arr, lengths=lengths, algorithm=algorithm\n                )\n                logprobs.append(logprob)\n                state_sequences.append(state_sequence)\n                centers.append(seq.centers)\n            return logprobs, state_sequences, centers\n\n    def _decode_from_lambda_only(self, X, lengths=None):\n        \"\"\"\n        Decode using the observation (lambda) matrix only (i.e., pure memoryless decoding).\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n            WARNING: Each decoding window is assumed to be similar in size to those used during training.\n            If not, the tuning curves have to be scaled appropriately!\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n\n        Returns\n        -------\n        posteriors : list of np.ndarray\n            State-membership probabilities for each sample in ``X``; one array for each sequence in X.\n        state_sequences : list of np.ndarray\n            Labels for each sample from ``X``; one array for each sequence in X.\n\n        Examples\n        --------\n        &gt;&gt;&gt; posteriors, state_sequences = hmm._decode_from_lambda_only(bst)\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            raise NotImplementedError(\"Not yet implemented!\")\n        else:\n            # we have a BinnedSpikeTrainArray\n            ratemap = copy.deepcopy(self.means_.T)\n            # make sure X and ratemap have same unit_id ordering!\n            neworder = [self.unit_ids.index(x) for x in X.unit_ids]\n            oldorder = list(range(len(neworder)))\n            for oi, ni in enumerate(neworder):\n                frm = oldorder.index(ni)\n                to = oi\n                swap_rows(ratemap, frm, to)\n                oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n            posteriors = []\n            state_sequences = []\n            for seq in X:\n                posteriors_, cumlengths, mode_pth, mean_pth = decode1D(\n                    bst=seq, ratemap=ratemap\n                )\n                # nanlocs = np.argwhere(np.isnan(mode_pth))\n                # state_sequences_ = mode_pth.astype(int)\n                state_sequences_ = mode_pth\n                posteriors.append(posteriors_)\n                state_sequences.append(state_sequences_)\n\n            return posteriors, state_sequences\n\n    def predict_proba(self, X, lengths=None, w=None, returnLengths=False):\n        \"\"\"\n        Compute the posterior probability for each state in the model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n        returnLengths : bool, optional\n            If True, also return the lengths array.\n\n        Returns\n        -------\n        posteriors : np.ndarray\n            Array of shape (n_components, n_samples) with state-membership probabilities for each sample from ``X``.\n        lengths : np.ndarray, optional\n            Returned if returnLengths is True; array of sequence lengths.\n\n        Examples\n        --------\n        &gt;&gt;&gt; posteriors = hmm.predict_proba(bst)\n        &gt;&gt;&gt; posteriors, lengths = hmm.predict_proba(bst, returnLengths=True)\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            print(\"we have a \" + str(type(X)))\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            if returnLengths:\n                return np.transpose(\n                    self._predict_proba(self, X, lengths=lengths)\n                ), lengths\n            return np.transpose(self._predict_proba(self, X, lengths=lengths))\n        else:\n            # we have a BinnedSpikeTrainArray\n            windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n            if returnLengths:\n                return np.transpose(\n                    self._predict_proba(self, windowed_arr, lengths=lengths)\n                ), lengths\n            return np.transpose(\n                self._predict_proba(self, windowed_arr, lengths=lengths)\n            )\n\n    def predict(self, X, lengths=None, w=None):\n        \"\"\"\n        Find the most likely state sequence corresponding to ``X``.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        lengths : array-like of int, shape (n_sequences,), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n\n        Returns\n        -------\n        state_sequence : np.ndarray or list of np.ndarray\n            Labels for each sample from ``X``.\n\n        Examples\n        --------\n        &gt;&gt;&gt; state_seq = hmm.predict(bst)\n        &gt;&gt;&gt; state_seq = hmm.predict(X)\n        \"\"\"\n        _, state_sequences, centers = self.decode(X=X, lengths=lengths, w=w)\n        return state_sequences\n\n    def sample(self, n_samples=1, random_state=None):\n        \"\"\"\n        Generate random samples from the model.\n\n        Parameters\n        ----------\n        n_samples : int\n            Number of samples to generate.\n        random_state : RandomState or int, optional\n            A random number generator instance or seed. If None, the object's random_state is used.\n\n        Returns\n        -------\n        X : np.ndarray\n            Feature matrix of shape (n_samples, n_features).\n        state_sequence : np.ndarray\n            State sequence produced by the model.\n\n        Examples\n        --------\n        &gt;&gt;&gt; X, states = hmm.sample(n_samples=100)\n        \"\"\"\n        return self._sample(self, n_samples=n_samples, random_state=random_state)\n\n    def score_samples(self, X, lengths=None, w=None):\n        \"\"\"Compute the log probability under the model and compute posteriors.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Feature matrix of individual samples.\n            OR\n            nelpy.BinnedSpikeTrainArray\n        lengths : array-like of integers, shape (n_sequences, ), optional\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``. This is not used when X is\n            a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n            automatically inferred.\n\n        Returns\n        -------\n        logprob : float\n            Log likelihood of ``X``; one scalar for each sequence in X.\n\n        posteriors : array, shape (n_components, n_samples)\n            State-membership probabilities for each sample in ``X``;\n            one array for each sequence in X.\n\n        See Also\n        --------\n        score : Compute the log probability under the model.\n        decode : Find most likely state sequence corresponding to ``X``.\n        \"\"\"\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            logprobs, posteriors = self._score_samples(self, X, lengths=lengths)\n            return (\n                logprobs,\n                posteriors,\n            )  # .T why does this transpose affect hmm.predict_proba!!!????\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            posteriors = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                logprob, posterior = self._score_samples(\n                    self, X=windowed_arr, lengths=lengths\n                )\n                logprobs.append(logprob)\n                posteriors.append(posterior.T)\n            return logprobs, posteriors\n\n    def score(self, X, lengths=None, w=None):\n        \"\"\"Compute the log probability under the model.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Feature matrix of individual samples.\n            OR\n            nelpy.BinnedSpikeTrainArray\n        lengths : array-like of integers, shape (n_sequences, ), optional\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``. This is not used when X is\n            a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n            automatically inferred.\n\n        Returns\n        -------\n        logprob : float, or list of floats\n            Log likelihood of ``X``; one scalar for each sequence in X.\n\n        See Also\n        --------\n        score_samples : Compute the log probability under the model and\n            posteriors.\n        decode : Find most likely state sequence corresponding to ``X``.\n        \"\"\"\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            return self._score(self, X, lengths=lengths)\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                logprob = self._score(self, X=windowed_arr, lengths=lengths)\n                logprobs.append(logprob)\n        return logprobs\n\n    def _cum_score_per_bin(self, X, lengths=None, w=None):\n        \"\"\"Compute the log probability under the model, cumulatively for each bin per event.\"\"\"\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            return self._score(self, X, lengths=lengths)\n        else:\n            # we have a BinnedSpikeTrainArray\n            logprobs = []\n            for seq in X:\n                windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n                n_bins, _ = windowed_arr.shape\n                for ii in range(1, n_bins + 1):\n                    logprob = self._score(self, X=windowed_arr[:ii, :])\n                    logprobs.append(logprob)\n        return logprobs\n\n    def fit(self, X, lengths=None, w=None):\n        \"\"\"Estimate model parameters using nelpy objects.\n\n        An initialization step is performed before entering the\n        EM-algorithm. If you want to avoid this step for a subset of\n        the parameters, pass proper ``init_params`` keyword argument\n        to estimator's constructor.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_units)\n            Feature matrix of individual samples.\n            OR\n            nelpy.BinnedSpikeTrainArray\n        lengths : array-like of integers, shape (n_sequences, )\n            Lengths of the individual sequences in ``X``. The sum of\n            these should be ``n_samples``. This is not used when X is\n            a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n            automatically inferred.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            self._fit(self, X, lengths=lengths)\n        else:\n            # we have a BinnedSpikeTrainArray\n            windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n            self._fit(self, windowed_arr, lengths=lengths)\n            # adopt unit_ids, unit_labels, etc. from BinnedSpikeTrain\n            self.assume_attributes(X)\n        return self\n\n    def fit_ext(\n        self,\n        X,\n        ext,\n        n_extern=None,\n        lengths=None,\n        save=True,\n        w=None,\n        normalize=True,\n        normalize_by_occupancy=True,\n    ):\n        \"\"\"Learn a mapping from the internal state space, to an external\n        augmented space (e.g. position).\n\n        Returns a row-normalized version of (n_states, n_ext), that\n        is, a distribution over external bins for each state.\n\n        X : BinnedSpikeTrainArray\n\n        ext : array-like\n            array of external correlates (n_bins, )\n        n_extern : int\n            number of extern variables, with range 0,.. n_extern-1\n        save : bool\n            stores extern in PoissonHMM if true, discards it if not\n        w:\n        normalize : bool\n            If True, then normalize each state to have a distribution over ext.\n        occupancy : array of bin counts\n            Default is all ones (uniform).\n\n        self.extern_ of size (n_components, n_extern)\n        \"\"\"\n\n        if n_extern is None:\n            n_extern = len(unique(ext))\n            ext_map = np.arange(n_extern)\n            for ii, ele in enumerate(unique(ext)):\n                ext_map[ele] = ii\n        else:\n            ext_map = np.arange(n_extern)\n\n        # idea: here, ext can be anything, and n_extern should be range\n        # we can e.g., define extern correlates {leftrun, rightrun} and\n        # fit the mapping. This is not expected to be good at all for\n        # most states, but it could allow us to identify a state or two\n        # for which there *might* be a strong predictive relationship.\n        # In this way, the binning, etc. should be done external to this\n        # function, but it might still make sense to encapsulate it as\n        # a helper function inside PoissonHMM?\n\n        # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n        # x0=0; xl=100; n_extern=50\n        # xx_left = np.linspace(x0,xl,n_extern+1)\n        # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n        # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n        extern = np.zeros((self.n_components, n_extern))\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n        else:\n            # we have a BinnedSpikeTrainArray\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n\n        posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n        if len(posteriors) != len(ext):\n            raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n        for ii, posterior in enumerate(posteriors):\n            if not np.isnan(ext[ii]):\n                extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n        if normalize_by_occupancy:\n            occupancy, _ = np.histogram(ext, bins=n_extern, range=[0, n_extern])\n            occupancy[occupancy == 0] = 1\n            occupancy = np.atleast_2d(occupancy)\n        else:\n            occupancy = 1\n\n        extern = extern / occupancy\n\n        if normalize:\n            # normalize extern tuning curves:\n            rowsum = np.tile(extern.sum(axis=1), (n_extern, 1)).T\n            rowsum = np.where(np.isclose(rowsum, 0), 1, rowsum)\n            extern = extern / rowsum\n\n        if save:\n            self._extern_ = extern\n            # self._extern_map = ext_map\n\n        return extern\n\n    def fit_ext2(self, X, ext, n_extern=None, lengths=None, w=None):\n        \"\"\"Learn a mapping from the internal state space, to an external\n        augmented space (e.g. position).\n\n        Returns a column-normalized version of (n_states, n_ext), that\n        is, a distribution over states for each extern bin.\n\n        X : BinnedSpikeTrainArray\n\n        ext : array-like\n            array of external correlates (n_bins, )\n        n_extern : int\n            number of extern variables, with range 0,.. n_extern-1\n\n        save : bool\n            stores extern in PoissonHMM if true, discards it if not\n\n        self.extern_ of size (n_components, n_extern)\n        \"\"\"\n\n        ext_map = np.arange(n_extern)\n        if n_extern is None:\n            n_extern = len(unique(ext))\n            for ii, ele in enumerate(unique(ext)):\n                ext_map[ele] = ii\n\n        # idea: here, ext can be anything, and n_extern should be range\n        # we can e.g., define extern correlates {leftrun, rightrun} and\n        # fit the mapping. This is not expexted to be good at all for\n        # most states, but it could allow us to identify a state or two\n        # for which there *might* be a strong predictive relationship.\n        # In this way, the binning, etc. should be done external to this\n        # function, but it might still make sense to encapsulate it as\n        # a helper function inside PoissonHMM?\n\n        # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n        # x0=0; xl=100; n_extern=50\n        # xx_left = np.linspace(x0,xl,n_extern+1)\n        # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n        # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n        extern = np.zeros((self.n_components, n_extern))\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n        else:\n            # we have a BinnedSpikeTrainArray\n            posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n        posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n        if len(posteriors) != len(ext):\n            raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n        for ii, posterior in enumerate(posteriors):\n            if not np.isnan(ext[ii]):\n                extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n        # normalize extern tuning curves:\n        colsum = np.tile(extern.sum(axis=0), (self.n_components, 1))\n        colsum = np.where(np.isclose(colsum, 0), 1, colsum)\n        extern = extern / colsum\n\n        return extern\n\n    def decode_ext(self, X, lengths=None, w=None, ext_shape=None):\n        \"\"\"\n        Find memoryless most likely state sequence corresponding to ``X``,\n        (that is, the symbol-by-symbol MAP sequence) and then map those\n        states to an associated external representation (e.g., position).\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features) or BinnedSpikeTrainArray\n            Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        lengths : array-like of integers, shape (n_sequences, ), optional\n            Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n            Not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n        w : int, optional\n            Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n        ext_shape : tuple, optional\n            Shape of the external variables.\n\n        Returns\n        -------\n        ext_posteriors : np.ndarray\n            Array of shape (n_extern, n_samples) with state-membership probabilities for each sample in ``X``.\n        bdries : np.ndarray\n            Array of bin boundaries.\n        mode_pth : np.ndarray\n            Most likely external variable sequence (mode path).\n        mean_pth : np.ndarray\n            Mean external variable sequence (mean path).\n\n        Examples\n        --------\n        For 1D external variables:\n        &gt;&gt;&gt; posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(\n        ...     bst_no_ripple, ext_shape=(vtc.n_bins,)\n        ... )\n        &gt;&gt;&gt; mean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0])\n\n        For 2D external variables:\n        &gt;&gt;&gt; posterior_, bdries_, mode_pth_, mean_pth_ = hmm.decode_ext(\n        ...     bst, ext_shape=(ext_nx, ext_ny)\n        ... )\n        &gt;&gt;&gt; mean_pth_[0, :] = vtc2d.xbins[0] + mean_pth_[0, :] * (\n        ...     vtc2d.xbins[-1] - vtc2d.xbins[0]\n        ... )\n        &gt;&gt;&gt; mean_pth_[1, :] = vtc2d.ybins[0] + mean_pth_[1, :] * (\n        ...     vtc2d.ybins[-1] - vtc2d.ybins[0]\n        ... )\n        \"\"\"\n\n        _, n_extern = self._extern_.shape\n\n        if ext_shape is None:\n            ext_shape = n_extern\n\n        if not isinstance(X, BinnedSpikeTrainArray):\n            # assume we have a feature matrix\n            raise NotImplementedError(\"not implemented yet.\")\n            if w is not None:\n                raise NotImplementedError(\n                    \"sliding window decoding for feature matrices not yet implemented!\"\n                )\n        else:\n            # we have a BinnedSpikeTrainArray\n            pass\n        if len(ext_shape) == 1:\n            # do old style decoding\n            # TODO: this can be improved to be like the 2D case!\n            state_posteriors, lengths = self.predict_proba(\n                X=X, lengths=lengths, w=w, returnLengths=True\n            )\n            # fixy = np.mean(self._extern_ * np.arange(n_extern), axis=1)\n            # mean_pth = np.sum(state_posteriors.T*fixy, axis=1) # range 0 to 1\n            ext_posteriors = np.dot(\n                (self._extern_ * np.arange(n_extern)).T, state_posteriors\n            )\n            # normalize ext_posterior distributions:\n            ext_posteriors = ext_posteriors / ext_posteriors.sum(axis=0)\n            mean_pth = (\n                ext_posteriors.T * np.atleast_2d(np.linspace(0, 1, n_extern))\n            ).sum(axis=1)\n            mode_pth = (\n                np.argmax(ext_posteriors, axis=0) / n_extern\n            )  # range 0 to n_extern\n\n        elif len(ext_shape) == 2:\n            ext_posteriors = np.zeros((ext_shape[0], ext_shape[1], X.n_bins))\n            # get posterior distribution over states, of size (num_States, n_extern)\n            state_posteriors, lengths = self.predict_proba(\n                X=X, lengths=lengths, w=w, returnLengths=True\n            )\n            # for each bin, compute the distribution in the external domain\n            for bb in range(X.n_bins):\n                ext_posteriors[:, :, bb] = np.reshape(\n                    (self._extern_ * state_posteriors[:, [bb]]).sum(axis=0), ext_shape\n                )\n            # now compute mean and mode paths\n            expected_x = np.sum(\n                (\n                    ext_posteriors.sum(axis=1)\n                    * np.atleast_2d(np.linspace(0, 1, ext_shape[0])).T\n                ),\n                axis=0,\n            )\n            expected_y = np.sum(\n                (\n                    ext_posteriors.sum(axis=0)\n                    * np.atleast_2d(np.linspace(0, 1, ext_shape[1])).T\n                ),\n                axis=0,\n            )\n            mean_pth = np.vstack((expected_x, expected_y))\n\n            mode_pth = np.zeros((2, X.n_bins))\n            for tt in range(X.n_bins):\n                if np.any(np.isnan(ext_posteriors[:, :, tt])):\n                    mode_pth[0, tt] = np.nan\n                    mode_pth[0, tt] = np.nan\n                else:\n                    x_, y_ = np.unravel_index(\n                        np.argmax(ext_posteriors[:, :, tt]),\n                        (ext_shape[0], ext_shape[1]),\n                    )\n                    mode_pth[0, tt] = x_ / ext_shape[0]\n                    mode_pth[1, tt] = y_ / ext_shape[1]\n\n            ext_posteriors = np.transpose(ext_posteriors, axes=[1, 0, 2])\n        else:\n            raise TypeError(\"shape not currently supported!\")\n\n        bdries = np.cumsum(lengths)\n\n        return ext_posteriors, bdries, mode_pth, mean_pth\n\n    def _plot_external(\n        self,\n        *,\n        figsize=(3, 5),\n        sharey=True,\n        labelstates=None,\n        ec=None,\n        fillcolor=None,\n        lw=None,\n    ):\n        \"\"\"plot the externally associated state&lt;--&gt;extern mapping\n\n        WARNING! This function is not complete, and hence 'private',\n        and may be moved somewhere else later on.\n        \"\"\"\n\n        if labelstates is None:\n            labelstates = [1, self.n_components]\n        if ec is None:\n            ec = \"k\"\n        if fillcolor is None:\n            fillcolor = \"gray\"\n        if lw is None:\n            lw = 1.5\n\n        fig, axes = subplots(self.n_components, 1, figsize=figsize, sharey=sharey)\n\n        xvals = np.arange(len(self._extern_.T[:, 0]))\n\n        for state, ax in enumerate(axes):\n            ax.fill_between(xvals, 0, self._extern_.T[:, state], color=fillcolor)\n            ax.plot(xvals, self._extern_.T[:, state], color=ec, lw=lw)\n            if state + 1 in labelstates:\n                ax.set_ylabel(str(state + 1), rotation=0, y=-0.1)\n            ax.set_xticklabels([])\n            ax.set_yticklabels([])\n            ax.spines[\"right\"].set_visible(False)\n            ax.spines[\"top\"].set_visible(False)\n            ax.spines[\"bottom\"].set_visible(False)\n            ax.spines[\"left\"].set_visible(False)\n            plotting.utils.no_yticks(ax)\n            plotting.utils.no_xticks(ax)\n        # fig.suptitle('normalized place fields sorted by peak location (left) and mean location (right)', y=0.92, fontsize=14)\n        # ax.set_xticklabels(['0','20', '40', '60', '80', '100'])\n        ax.set_xlabel(\"external variable\")\n        fig.text(\n            0.02, 0.5, \"normalized state distribution\", va=\"center\", rotation=\"vertical\"\n        )\n\n        return fig, ax\n\n    def estimate_model_quality(self, bst, *, n_shuffles=1000, k_folds=5, verbose=False):\n        \"\"\"Estimate the HMM 'model quality' associated with the set of events in bst.\n\n        TODO: finish docstring, and do some more consistency checking...\n\n        Params\n        ======\n\n        Returns\n        =======\n\n        quality :\n        scores :\n        shuffled :\n\n        \"\"\"\n        n_states = self.n_components\n        quality, scores, shuffles = estimate_model_quality(\n            bst=bst,\n            n_states=n_states,\n            n_shuffles=n_shuffles,\n            k_folds=k_folds,\n            verbose=False,\n        )\n\n        return quality, scores, shuffles\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.extern_","title":"<code>extern_</code>  <code>property</code>","text":"<p>Mapping from states to external variables (e.g., position).</p> <p>Returns:</p> Type Description <code>ndarray or None</code> <p>Array of shape (n_components, n_extern) containing the mapping from states to external variables. Returns None if no mapping has been learned yet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hmm.fit_ext(bst, position_data)\n&gt;&gt;&gt; extern_map = hmm.extern_\n&gt;&gt;&gt; print(f\"State 0 maps to position bin {np.argmax(extern_map[0])}\")\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.means","title":"<code>means</code>  <code>property</code>","text":"<p>Observation matrix (mean firing rates for each state and unit).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_components, n_units) containing the mean parameters for each state.</p>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.startprob","title":"<code>startprob</code>  <code>property</code>","text":"<p>Prior distribution over states.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_components,) representing the initial state probabilities.</p>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.transmat","title":"<code>transmat</code>  <code>property</code>","text":"<p>Transition probability matrix.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_components, n_components) where A[i, j] = Pr(S_{t+1}=j | S_t=i).</p>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.unit_ids","title":"<code>unit_ids</code>  <code>property</code>","text":"<p>List of unit IDs associated with the model.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unit IDs.</p>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.unit_labels","title":"<code>unit_labels</code>  <code>property</code>","text":"<p>List of unit labels associated with the model.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of unit labels.</p>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.assume_attributes","title":"<code>assume_attributes(binnedSpikeTrainArray)</code>","text":"<p>Assume subset of attributes from a BinnedSpikeTrainArray.</p> <p>This is used primarily to enable the sampling of sequences after a model has been fit.</p> <p>Parameters:</p> Name Type Description Default <code>binnedSpikeTrainArray</code> <code>BinnedSpikeTrainArray</code> <p>The BinnedSpikeTrainArray instance from which to copy attributes.</p> required Source code in <code>nelpy/hmmutils.py</code> <pre><code>def assume_attributes(self, binnedSpikeTrainArray):\n    \"\"\"\n    Assume subset of attributes from a BinnedSpikeTrainArray.\n\n    This is used primarily to enable the sampling of sequences after a model has been fit.\n\n    Parameters\n    ----------\n    binnedSpikeTrainArray : BinnedSpikeTrainArray\n        The BinnedSpikeTrainArray instance from which to copy attributes.\n    \"\"\"\n    if self._ds is not None:\n        warn(\"PoissonHMM(BinnedSpikeTrain) attributes already exist.\")\n    for attrib in self.__attributes__:\n        exec(\"self.\" + attrib + \" = binnedSpikeTrainArray.\" + attrib)\n    self._unit_ids = copy.copy(binnedSpikeTrainArray.unit_ids)\n    self._unit_labels = copy.copy(binnedSpikeTrainArray.unit_labels)\n    self._unit_tags = copy.copy(binnedSpikeTrainArray.unit_tags)\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.decode","title":"<code>decode(X, lengths=None, w=None, algorithm=None)</code>","text":"<p>Find the most likely state sequence corresponding to <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray. WARNING: Each decoding window is assumed to be similar in size to those used during training. If not, the tuning curves have to be scaled appropriately!</p> required <code>lengths</code> <code>array-like of int, shape (n_sequences,)</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <code>algorithm</code> <code>str</code> <p>Decoder algorithm to be used (see DECODER_ALGORITHMS).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>float or list of float</code> <p>Log probability of the produced state sequence.</p> <code>state_sequence</code> <code>np.ndarray or list of np.ndarray</code> <p>Labels for each sample from <code>X</code> obtained via the given decoder algorithm.</p> <code>centers</code> <code>np.ndarray or list of np.ndarray</code> <p>Time-centers of all bins contained in <code>X</code>.</p> See Also <p>score_samples : Compute the log probability under the model and posteriors. score : Compute the log probability under the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(bst)\n&gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(X, algorithm=\"viterbi\")\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def decode(self, X, lengths=None, w=None, algorithm=None):\n    \"\"\"\n    Find the most likely state sequence corresponding to ``X``.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n        WARNING: Each decoding window is assumed to be similar in size to those used during training.\n        If not, the tuning curves have to be scaled appropriately!\n    lengths : array-like of int, shape (n_sequences,), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n    algorithm : str, optional\n        Decoder algorithm to be used (see DECODER_ALGORITHMS).\n\n    Returns\n    -------\n    logprob : float or list of float\n        Log probability of the produced state sequence.\n    state_sequence : np.ndarray or list of np.ndarray\n        Labels for each sample from ``X`` obtained via the given decoder algorithm.\n    centers : np.ndarray or list of np.ndarray\n        Time-centers of all bins contained in ``X``.\n\n    See Also\n    --------\n    score_samples : Compute the log probability under the model and posteriors.\n    score : Compute the log probability under the model.\n\n    Examples\n    --------\n    &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(bst)\n    &gt;&gt;&gt; logprob, state_seq, centers = hmm.decode(X, algorithm=\"viterbi\")\n    \"\"\"\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        return self._decode(self, X=X, lengths=lengths, algorithm=algorithm), None\n    else:\n        # we have a BinnedSpikeTrainArray\n        logprobs = []\n        state_sequences = []\n        centers = []\n        for seq in X:\n            windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n            logprob, state_sequence = self._decode(\n                self, windowed_arr, lengths=lengths, algorithm=algorithm\n            )\n            logprobs.append(logprob)\n            state_sequences.append(state_sequence)\n            centers.append(seq.centers)\n        return logprobs, state_sequences, centers\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.decode_ext","title":"<code>decode_ext(X, lengths=None, w=None, ext_shape=None)</code>","text":"<p>Find memoryless most likely state sequence corresponding to <code>X</code>, (that is, the symbol-by-symbol MAP sequence) and then map those states to an associated external representation (e.g., position).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features) or BinnedSpikeTrainArray)</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <code>ext_shape</code> <code>tuple</code> <p>Shape of the external variables.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ext_posteriors</code> <code>ndarray</code> <p>Array of shape (n_extern, n_samples) with state-membership probabilities for each sample in <code>X</code>.</p> <code>bdries</code> <code>ndarray</code> <p>Array of bin boundaries.</p> <code>mode_pth</code> <code>ndarray</code> <p>Most likely external variable sequence (mode path).</p> <code>mean_pth</code> <code>ndarray</code> <p>Mean external variable sequence (mean path).</p> <p>Examples:</p> <p>For 1D external variables:</p> <pre><code>&gt;&gt;&gt; posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(\n...     bst_no_ripple, ext_shape=(vtc.n_bins,)\n... )\n&gt;&gt;&gt; mean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0])\n</code></pre> <p>For 2D external variables:</p> <pre><code>&gt;&gt;&gt; posterior_, bdries_, mode_pth_, mean_pth_ = hmm.decode_ext(\n...     bst, ext_shape=(ext_nx, ext_ny)\n... )\n&gt;&gt;&gt; mean_pth_[0, :] = vtc2d.xbins[0] + mean_pth_[0, :] * (\n...     vtc2d.xbins[-1] - vtc2d.xbins[0]\n... )\n&gt;&gt;&gt; mean_pth_[1, :] = vtc2d.ybins[0] + mean_pth_[1, :] * (\n...     vtc2d.ybins[-1] - vtc2d.ybins[0]\n... )\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def decode_ext(self, X, lengths=None, w=None, ext_shape=None):\n    \"\"\"\n    Find memoryless most likely state sequence corresponding to ``X``,\n    (that is, the symbol-by-symbol MAP sequence) and then map those\n    states to an associated external representation (e.g., position).\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n    lengths : array-like of integers, shape (n_sequences, ), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n    ext_shape : tuple, optional\n        Shape of the external variables.\n\n    Returns\n    -------\n    ext_posteriors : np.ndarray\n        Array of shape (n_extern, n_samples) with state-membership probabilities for each sample in ``X``.\n    bdries : np.ndarray\n        Array of bin boundaries.\n    mode_pth : np.ndarray\n        Most likely external variable sequence (mode path).\n    mean_pth : np.ndarray\n        Mean external variable sequence (mean path).\n\n    Examples\n    --------\n    For 1D external variables:\n    &gt;&gt;&gt; posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(\n    ...     bst_no_ripple, ext_shape=(vtc.n_bins,)\n    ... )\n    &gt;&gt;&gt; mean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0])\n\n    For 2D external variables:\n    &gt;&gt;&gt; posterior_, bdries_, mode_pth_, mean_pth_ = hmm.decode_ext(\n    ...     bst, ext_shape=(ext_nx, ext_ny)\n    ... )\n    &gt;&gt;&gt; mean_pth_[0, :] = vtc2d.xbins[0] + mean_pth_[0, :] * (\n    ...     vtc2d.xbins[-1] - vtc2d.xbins[0]\n    ... )\n    &gt;&gt;&gt; mean_pth_[1, :] = vtc2d.ybins[0] + mean_pth_[1, :] * (\n    ...     vtc2d.ybins[-1] - vtc2d.ybins[0]\n    ... )\n    \"\"\"\n\n    _, n_extern = self._extern_.shape\n\n    if ext_shape is None:\n        ext_shape = n_extern\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        raise NotImplementedError(\"not implemented yet.\")\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n    else:\n        # we have a BinnedSpikeTrainArray\n        pass\n    if len(ext_shape) == 1:\n        # do old style decoding\n        # TODO: this can be improved to be like the 2D case!\n        state_posteriors, lengths = self.predict_proba(\n            X=X, lengths=lengths, w=w, returnLengths=True\n        )\n        # fixy = np.mean(self._extern_ * np.arange(n_extern), axis=1)\n        # mean_pth = np.sum(state_posteriors.T*fixy, axis=1) # range 0 to 1\n        ext_posteriors = np.dot(\n            (self._extern_ * np.arange(n_extern)).T, state_posteriors\n        )\n        # normalize ext_posterior distributions:\n        ext_posteriors = ext_posteriors / ext_posteriors.sum(axis=0)\n        mean_pth = (\n            ext_posteriors.T * np.atleast_2d(np.linspace(0, 1, n_extern))\n        ).sum(axis=1)\n        mode_pth = (\n            np.argmax(ext_posteriors, axis=0) / n_extern\n        )  # range 0 to n_extern\n\n    elif len(ext_shape) == 2:\n        ext_posteriors = np.zeros((ext_shape[0], ext_shape[1], X.n_bins))\n        # get posterior distribution over states, of size (num_States, n_extern)\n        state_posteriors, lengths = self.predict_proba(\n            X=X, lengths=lengths, w=w, returnLengths=True\n        )\n        # for each bin, compute the distribution in the external domain\n        for bb in range(X.n_bins):\n            ext_posteriors[:, :, bb] = np.reshape(\n                (self._extern_ * state_posteriors[:, [bb]]).sum(axis=0), ext_shape\n            )\n        # now compute mean and mode paths\n        expected_x = np.sum(\n            (\n                ext_posteriors.sum(axis=1)\n                * np.atleast_2d(np.linspace(0, 1, ext_shape[0])).T\n            ),\n            axis=0,\n        )\n        expected_y = np.sum(\n            (\n                ext_posteriors.sum(axis=0)\n                * np.atleast_2d(np.linspace(0, 1, ext_shape[1])).T\n            ),\n            axis=0,\n        )\n        mean_pth = np.vstack((expected_x, expected_y))\n\n        mode_pth = np.zeros((2, X.n_bins))\n        for tt in range(X.n_bins):\n            if np.any(np.isnan(ext_posteriors[:, :, tt])):\n                mode_pth[0, tt] = np.nan\n                mode_pth[0, tt] = np.nan\n            else:\n                x_, y_ = np.unravel_index(\n                    np.argmax(ext_posteriors[:, :, tt]),\n                    (ext_shape[0], ext_shape[1]),\n                )\n                mode_pth[0, tt] = x_ / ext_shape[0]\n                mode_pth[1, tt] = y_ / ext_shape[1]\n\n        ext_posteriors = np.transpose(ext_posteriors, axes=[1, 0, 2])\n    else:\n        raise TypeError(\"shape not currently supported!\")\n\n    bdries = np.cumsum(lengths)\n\n    return ext_posteriors, bdries, mode_pth, mean_pth\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.estimate_model_quality","title":"<code>estimate_model_quality(bst, *, n_shuffles=1000, k_folds=5, verbose=False)</code>","text":"<p>Estimate the HMM 'model quality' associated with the set of events in bst.</p> <p>TODO: finish docstring, and do some more consistency checking...</p>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.estimate_model_quality--params","title":"Params","text":""},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.estimate_model_quality--returns","title":"Returns","text":"<p>quality : scores : shuffled :</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def estimate_model_quality(self, bst, *, n_shuffles=1000, k_folds=5, verbose=False):\n    \"\"\"Estimate the HMM 'model quality' associated with the set of events in bst.\n\n    TODO: finish docstring, and do some more consistency checking...\n\n    Params\n    ======\n\n    Returns\n    =======\n\n    quality :\n    scores :\n    shuffled :\n\n    \"\"\"\n    n_states = self.n_components\n    quality, scores, shuffles = estimate_model_quality(\n        bst=bst,\n        n_states=n_states,\n        n_shuffles=n_shuffles,\n        k_folds=k_folds,\n        verbose=False,\n    )\n\n    return quality, scores, shuffles\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.fit","title":"<code>fit(X, lengths=None, w=None)</code>","text":"<p>Estimate model parameters using nelpy objects.</p> <p>An initialization step is performed before entering the EM-algorithm. If you want to avoid this step for a subset of the parameters, pass proper <code>init_params</code> keyword argument to estimator's constructor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_units))</code> <p>Feature matrix of individual samples. OR nelpy.BinnedSpikeTrainArray</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. This is not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lenghts are automatically inferred.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Returns self.</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def fit(self, X, lengths=None, w=None):\n    \"\"\"Estimate model parameters using nelpy objects.\n\n    An initialization step is performed before entering the\n    EM-algorithm. If you want to avoid this step for a subset of\n    the parameters, pass proper ``init_params`` keyword argument\n    to estimator's constructor.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_units)\n        Feature matrix of individual samples.\n        OR\n        nelpy.BinnedSpikeTrainArray\n    lengths : array-like of integers, shape (n_sequences, )\n        Lengths of the individual sequences in ``X``. The sum of\n        these should be ``n_samples``. This is not used when X is\n        a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n        automatically inferred.\n\n    Returns\n    -------\n    self : object\n        Returns self.\n    \"\"\"\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        self._fit(self, X, lengths=lengths)\n    else:\n        # we have a BinnedSpikeTrainArray\n        windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n        self._fit(self, windowed_arr, lengths=lengths)\n        # adopt unit_ids, unit_labels, etc. from BinnedSpikeTrain\n        self.assume_attributes(X)\n    return self\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.fit_ext","title":"<code>fit_ext(X, ext, n_extern=None, lengths=None, save=True, w=None, normalize=True, normalize_by_occupancy=True)</code>","text":"<p>Learn a mapping from the internal state space, to an external augmented space (e.g. position).</p> <p>Returns a row-normalized version of (n_states, n_ext), that is, a distribution over external bins for each state.</p> <p>X : BinnedSpikeTrainArray</p> <p>ext : array-like     array of external correlates (n_bins, ) n_extern : int     number of extern variables, with range 0,.. n_extern-1 save : bool     stores extern in PoissonHMM if true, discards it if not w: normalize : bool     If True, then normalize each state to have a distribution over ext. occupancy : array of bin counts     Default is all ones (uniform).</p> <p>self.extern_ of size (n_components, n_extern)</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def fit_ext(\n    self,\n    X,\n    ext,\n    n_extern=None,\n    lengths=None,\n    save=True,\n    w=None,\n    normalize=True,\n    normalize_by_occupancy=True,\n):\n    \"\"\"Learn a mapping from the internal state space, to an external\n    augmented space (e.g. position).\n\n    Returns a row-normalized version of (n_states, n_ext), that\n    is, a distribution over external bins for each state.\n\n    X : BinnedSpikeTrainArray\n\n    ext : array-like\n        array of external correlates (n_bins, )\n    n_extern : int\n        number of extern variables, with range 0,.. n_extern-1\n    save : bool\n        stores extern in PoissonHMM if true, discards it if not\n    w:\n    normalize : bool\n        If True, then normalize each state to have a distribution over ext.\n    occupancy : array of bin counts\n        Default is all ones (uniform).\n\n    self.extern_ of size (n_components, n_extern)\n    \"\"\"\n\n    if n_extern is None:\n        n_extern = len(unique(ext))\n        ext_map = np.arange(n_extern)\n        for ii, ele in enumerate(unique(ext)):\n            ext_map[ele] = ii\n    else:\n        ext_map = np.arange(n_extern)\n\n    # idea: here, ext can be anything, and n_extern should be range\n    # we can e.g., define extern correlates {leftrun, rightrun} and\n    # fit the mapping. This is not expected to be good at all for\n    # most states, but it could allow us to identify a state or two\n    # for which there *might* be a strong predictive relationship.\n    # In this way, the binning, etc. should be done external to this\n    # function, but it might still make sense to encapsulate it as\n    # a helper function inside PoissonHMM?\n\n    # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n    # x0=0; xl=100; n_extern=50\n    # xx_left = np.linspace(x0,xl,n_extern+1)\n    # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n    # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n    extern = np.zeros((self.n_components, n_extern))\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n    else:\n        # we have a BinnedSpikeTrainArray\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n\n    posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n    if len(posteriors) != len(ext):\n        raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n    for ii, posterior in enumerate(posteriors):\n        if not np.isnan(ext[ii]):\n            extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n    if normalize_by_occupancy:\n        occupancy, _ = np.histogram(ext, bins=n_extern, range=[0, n_extern])\n        occupancy[occupancy == 0] = 1\n        occupancy = np.atleast_2d(occupancy)\n    else:\n        occupancy = 1\n\n    extern = extern / occupancy\n\n    if normalize:\n        # normalize extern tuning curves:\n        rowsum = np.tile(extern.sum(axis=1), (n_extern, 1)).T\n        rowsum = np.where(np.isclose(rowsum, 0), 1, rowsum)\n        extern = extern / rowsum\n\n    if save:\n        self._extern_ = extern\n        # self._extern_map = ext_map\n\n    return extern\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.fit_ext2","title":"<code>fit_ext2(X, ext, n_extern=None, lengths=None, w=None)</code>","text":"<p>Learn a mapping from the internal state space, to an external augmented space (e.g. position).</p> <p>Returns a column-normalized version of (n_states, n_ext), that is, a distribution over states for each extern bin.</p> <p>X : BinnedSpikeTrainArray</p> <p>ext : array-like     array of external correlates (n_bins, ) n_extern : int     number of extern variables, with range 0,.. n_extern-1</p> <p>save : bool     stores extern in PoissonHMM if true, discards it if not</p> <p>self.extern_ of size (n_components, n_extern)</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def fit_ext2(self, X, ext, n_extern=None, lengths=None, w=None):\n    \"\"\"Learn a mapping from the internal state space, to an external\n    augmented space (e.g. position).\n\n    Returns a column-normalized version of (n_states, n_ext), that\n    is, a distribution over states for each extern bin.\n\n    X : BinnedSpikeTrainArray\n\n    ext : array-like\n        array of external correlates (n_bins, )\n    n_extern : int\n        number of extern variables, with range 0,.. n_extern-1\n\n    save : bool\n        stores extern in PoissonHMM if true, discards it if not\n\n    self.extern_ of size (n_components, n_extern)\n    \"\"\"\n\n    ext_map = np.arange(n_extern)\n    if n_extern is None:\n        n_extern = len(unique(ext))\n        for ii, ele in enumerate(unique(ext)):\n            ext_map[ele] = ii\n\n    # idea: here, ext can be anything, and n_extern should be range\n    # we can e.g., define extern correlates {leftrun, rightrun} and\n    # fit the mapping. This is not expexted to be good at all for\n    # most states, but it could allow us to identify a state or two\n    # for which there *might* be a strong predictive relationship.\n    # In this way, the binning, etc. should be done external to this\n    # function, but it might still make sense to encapsulate it as\n    # a helper function inside PoissonHMM?\n\n    # xpos, ypos = get_position(exp_data['session1']['posdf'], bst.centers)\n    # x0=0; xl=100; n_extern=50\n    # xx_left = np.linspace(x0,xl,n_extern+1)\n    # xx_mid = np.linspace(x0,xl,n_extern+1)[:-1]; xx_mid += (xx_mid[1]-xx_mid[0])/2\n    # ext = np.digitize(xpos, xx_left) - 1 # spatial bin numbers\n\n    extern = np.zeros((self.n_components, n_extern))\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n    else:\n        # we have a BinnedSpikeTrainArray\n        posteriors = self.predict_proba(X=X, lengths=lengths, w=w)\n    posteriors = np.vstack(posteriors.T)  # 1D array of states, of length n_bins\n\n    if len(posteriors) != len(ext):\n        raise ValueError(\"ext must have same length as decoded state sequence!\")\n\n    for ii, posterior in enumerate(posteriors):\n        if not np.isnan(ext[ii]):\n            extern[:, ext_map[int(ext[ii])]] += np.transpose(posterior)\n\n    # normalize extern tuning curves:\n    colsum = np.tile(extern.sum(axis=0), (self.n_components, 1))\n    colsum = np.where(np.isclose(colsum, 0), 1, colsum)\n    extern = extern / colsum\n\n    return extern\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.get_state_order","title":"<code>get_state_order(method=None, start_state=None)</code>","text":"<p>Return a state ordering, optionally using augmented data.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>(transmat, mode, mean)</code> <p>Method to use for ordering states. 'transmat' (default) uses the transition matrix. 'mode' or 'mean' use the external mapping (requires self.extern).</p> <code>'transmat'</code> <code>start_state</code> <code>int</code> <p>Initial state to begin from (used only if method is 'transmat').</p> <code>None</code> <p>Returns:</p> Name Type Description <code>neworder</code> <code>list</code> <p>List of state indices in the new order.</p> Notes <p>Both 'mode' and 'mean' assume that extern is in sorted order; this is not verified explicitly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; order = hmm.get_state_order(method=\"transmat\")\n&gt;&gt;&gt; order = hmm.get_state_order(method=\"mode\")\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def get_state_order(self, method=None, start_state=None):\n    \"\"\"\n    Return a state ordering, optionally using augmented data.\n\n    Parameters\n    ----------\n    method : {'transmat', 'mode', 'mean'}, optional\n        Method to use for ordering states. 'transmat' (default) uses the transition matrix.\n        'mode' or 'mean' use the external mapping (requires self._extern_).\n    start_state : int, optional\n        Initial state to begin from (used only if method is 'transmat').\n\n    Returns\n    -------\n    neworder : list\n        List of state indices in the new order.\n\n    Notes\n    -----\n    Both 'mode' and 'mean' assume that _extern_ is in sorted order; this is not verified explicitly.\n\n    Examples\n    --------\n    &gt;&gt;&gt; order = hmm.get_state_order(method=\"transmat\")\n    &gt;&gt;&gt; order = hmm.get_state_order(method=\"mode\")\n    \"\"\"\n    if method is None:\n        method = \"transmat\"\n\n    neworder = []\n\n    if method == \"transmat\":\n        return self._get_order_from_transmat(start_state=start_state)\n    elif method == \"mode\":\n        if self._extern_ is not None:\n            neworder = self._extern_.argmax(axis=1).argsort()\n        else:\n            raise Exception(\n                \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n            )\n    elif method == \"mean\":\n        if self._extern_ is not None:\n            (\n                np.tile(np.arange(self._extern_.shape[1]), (self.n_components, 1))\n                * self._extern_\n            ).sum(axis=1).argsort()\n            neworder = self._extern_.argmax(axis=1).argsort()\n        else:\n            raise Exception(\n                \"External mapping does not exist yet.First use PoissonHMM.fit_ext()\"\n            )\n    else:\n        raise NotImplementedError(\n            \"ordering method '\" + str(method) + \"' not supported!\"\n        )\n    return neworder\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.predict","title":"<code>predict(X, lengths=None, w=None)</code>","text":"<p>Find the most likely state sequence corresponding to <code>X</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.</p> required <code>lengths</code> <code>array-like of int, shape (n_sequences,)</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>state_sequence</code> <code>np.ndarray or list of np.ndarray</code> <p>Labels for each sample from <code>X</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; state_seq = hmm.predict(bst)\n&gt;&gt;&gt; state_seq = hmm.predict(X)\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def predict(self, X, lengths=None, w=None):\n    \"\"\"\n    Find the most likely state sequence corresponding to ``X``.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n    lengths : array-like of int, shape (n_sequences,), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n\n    Returns\n    -------\n    state_sequence : np.ndarray or list of np.ndarray\n        Labels for each sample from ``X``.\n\n    Examples\n    --------\n    &gt;&gt;&gt; state_seq = hmm.predict(bst)\n    &gt;&gt;&gt; state_seq = hmm.predict(X)\n    \"\"\"\n    _, state_sequences, centers = self.decode(X=X, lengths=lengths, w=w)\n    return state_sequences\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.predict_proba","title":"<code>predict_proba(X, lengths=None, w=None, returnLengths=False)</code>","text":"<p>Compute the posterior probability for each state in the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray</code> <p>Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.</p> required <code>lengths</code> <code>array-like of int, shape (n_sequences,)</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).</p> <code>None</code> <code>returnLengths</code> <code>bool</code> <p>If True, also return the lengths array.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>posteriors</code> <code>ndarray</code> <p>Array of shape (n_components, n_samples) with state-membership probabilities for each sample from <code>X</code>.</p> <code>lengths</code> <code>(ndarray, optional)</code> <p>Returned if returnLengths is True; array of sequence lengths.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; posteriors = hmm.predict_proba(bst)\n&gt;&gt;&gt; posteriors, lengths = hmm.predict_proba(bst, returnLengths=True)\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def predict_proba(self, X, lengths=None, w=None, returnLengths=False):\n    \"\"\"\n    Compute the posterior probability for each state in the model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features) or BinnedSpikeTrainArray\n        Feature matrix of individual samples, or a nelpy.BinnedSpikeTrainArray.\n    lengths : array-like of int, shape (n_sequences,), optional\n        Lengths of the individual sequences in ``X``. The sum of these should be ``n_samples``.\n        Not used when X is a BinnedSpikeTrainArray, in which case the lengths are automatically inferred.\n    w : int, optional\n        Window size for sliding window decoding (only used for BinnedSpikeTrainArray input).\n    returnLengths : bool, optional\n        If True, also return the lengths array.\n\n    Returns\n    -------\n    posteriors : np.ndarray\n        Array of shape (n_components, n_samples) with state-membership probabilities for each sample from ``X``.\n    lengths : np.ndarray, optional\n        Returned if returnLengths is True; array of sequence lengths.\n\n    Examples\n    --------\n    &gt;&gt;&gt; posteriors = hmm.predict_proba(bst)\n    &gt;&gt;&gt; posteriors, lengths = hmm.predict_proba(bst, returnLengths=True)\n    \"\"\"\n    if not isinstance(X, BinnedSpikeTrainArray):\n        print(\"we have a \" + str(type(X)))\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        if returnLengths:\n            return np.transpose(\n                self._predict_proba(self, X, lengths=lengths)\n            ), lengths\n        return np.transpose(self._predict_proba(self, X, lengths=lengths))\n    else:\n        # we have a BinnedSpikeTrainArray\n        windowed_arr, lengths = self._sliding_window_array(bst=X, w=w)\n        if returnLengths:\n            return np.transpose(\n                self._predict_proba(self, windowed_arr, lengths=lengths)\n            ), lengths\n        return np.transpose(\n            self._predict_proba(self, windowed_arr, lengths=lengths)\n        )\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.reorder_states","title":"<code>reorder_states(neworder)</code>","text":"<p>Reorder internal HMM states according to a specified order.</p> <p>Parameters:</p> Name Type Description Default <code>neworder</code> <code>list or array - like</code> <p>List of state indices specifying the new order. Must be of size (n_components,).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; hmm.reorder_states([2, 0, 1])\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def reorder_states(self, neworder):\n    \"\"\"\n    Reorder internal HMM states according to a specified order.\n\n    Parameters\n    ----------\n    neworder : list or array-like\n        List of state indices specifying the new order. Must be of size (n_components,).\n\n    Examples\n    --------\n    &gt;&gt;&gt; hmm.reorder_states([2, 0, 1])\n    \"\"\"\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        swap_cols(self.transmat_, frm, to)\n        swap_rows(self.transmat_, frm, to)\n        swap_rows(self.means_, frm, to)\n        if self._extern_ is not None:\n            swap_rows(self._extern_, frm, to)\n        self.startprob_[frm], self.startprob_[to] = (\n            self.startprob_[to],\n            self.startprob_[frm],\n        )\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.sample","title":"<code>sample(n_samples=1, random_state=None)</code>","text":"<p>Generate random samples from the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> <code>1</code> <code>random_state</code> <code>RandomState or int</code> <p>A random number generator instance or seed. If None, the object's random_state is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> <code>state_sequence</code> <code>ndarray</code> <p>State sequence produced by the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X, states = hmm.sample(n_samples=100)\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def sample(self, n_samples=1, random_state=None):\n    \"\"\"\n    Generate random samples from the model.\n\n    Parameters\n    ----------\n    n_samples : int\n        Number of samples to generate.\n    random_state : RandomState or int, optional\n        A random number generator instance or seed. If None, the object's random_state is used.\n\n    Returns\n    -------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    state_sequence : np.ndarray\n        State sequence produced by the model.\n\n    Examples\n    --------\n    &gt;&gt;&gt; X, states = hmm.sample(n_samples=100)\n    \"\"\"\n    return self._sample(self, n_samples=n_samples, random_state=random_state)\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.score","title":"<code>score(X, lengths=None, w=None)</code>","text":"<p>Compute the log probability under the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Feature matrix of individual samples. OR nelpy.BinnedSpikeTrainArray</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. This is not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lenghts are automatically inferred.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>float, or list of floats</code> <p>Log likelihood of <code>X</code>; one scalar for each sequence in X.</p> See Also <p>score_samples : Compute the log probability under the model and     posteriors. decode : Find most likely state sequence corresponding to <code>X</code>.</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def score(self, X, lengths=None, w=None):\n    \"\"\"Compute the log probability under the model.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix of individual samples.\n        OR\n        nelpy.BinnedSpikeTrainArray\n    lengths : array-like of integers, shape (n_sequences, ), optional\n        Lengths of the individual sequences in ``X``. The sum of\n        these should be ``n_samples``. This is not used when X is\n        a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n        automatically inferred.\n\n    Returns\n    -------\n    logprob : float, or list of floats\n        Log likelihood of ``X``; one scalar for each sequence in X.\n\n    See Also\n    --------\n    score_samples : Compute the log probability under the model and\n        posteriors.\n    decode : Find most likely state sequence corresponding to ``X``.\n    \"\"\"\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        return self._score(self, X, lengths=lengths)\n    else:\n        # we have a BinnedSpikeTrainArray\n        logprobs = []\n        for seq in X:\n            windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n            logprob = self._score(self, X=windowed_arr, lengths=lengths)\n            logprobs.append(logprob)\n    return logprobs\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.PoissonHMM.score_samples","title":"<code>score_samples(X, lengths=None, w=None)</code>","text":"<p>Compute the log probability under the model and compute posteriors.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Feature matrix of individual samples. OR nelpy.BinnedSpikeTrainArray</p> required <code>lengths</code> <code>array-like of integers, shape (n_sequences, )</code> <p>Lengths of the individual sequences in <code>X</code>. The sum of these should be <code>n_samples</code>. This is not used when X is a nelpy.BinnedSpikeTrainArray, in which case the lenghts are automatically inferred.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>float</code> <p>Log likelihood of <code>X</code>; one scalar for each sequence in X.</p> <code>posteriors</code> <code>(array, shape(n_components, n_samples))</code> <p>State-membership probabilities for each sample in <code>X</code>; one array for each sequence in X.</p> See Also <p>score : Compute the log probability under the model. decode : Find most likely state sequence corresponding to <code>X</code>.</p> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def score_samples(self, X, lengths=None, w=None):\n    \"\"\"Compute the log probability under the model and compute posteriors.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix of individual samples.\n        OR\n        nelpy.BinnedSpikeTrainArray\n    lengths : array-like of integers, shape (n_sequences, ), optional\n        Lengths of the individual sequences in ``X``. The sum of\n        these should be ``n_samples``. This is not used when X is\n        a nelpy.BinnedSpikeTrainArray, in which case the lenghts are\n        automatically inferred.\n\n    Returns\n    -------\n    logprob : float\n        Log likelihood of ``X``; one scalar for each sequence in X.\n\n    posteriors : array, shape (n_components, n_samples)\n        State-membership probabilities for each sample in ``X``;\n        one array for each sequence in X.\n\n    See Also\n    --------\n    score : Compute the log probability under the model.\n    decode : Find most likely state sequence corresponding to ``X``.\n    \"\"\"\n\n    if not isinstance(X, BinnedSpikeTrainArray):\n        # assume we have a feature matrix\n        if w is not None:\n            raise NotImplementedError(\n                \"sliding window decoding for feature matrices not yet implemented!\"\n            )\n        logprobs, posteriors = self._score_samples(self, X, lengths=lengths)\n        return (\n            logprobs,\n            posteriors,\n        )  # .T why does this transpose affect hmm.predict_proba!!!????\n    else:\n        # we have a BinnedSpikeTrainArray\n        logprobs = []\n        posteriors = []\n        for seq in X:\n            windowed_arr, lengths = self._sliding_window_array(bst=seq, w=w)\n            logprob, posterior = self._score_samples(\n                self, X=windowed_arr, lengths=lengths\n            )\n            logprobs.append(logprob)\n            posteriors.append(posterior.T)\n        return logprobs, posteriors\n</code></pre>"},{"location":"reference/nelpy/hmmutils/#nelpy.hmmutils.estimate_model_quality","title":"<code>estimate_model_quality(bst, *, hmm=None, n_states=None, n_shuffles=1000, k_folds=5, mode='timeswap-pooled', verbose=False)</code>","text":"<p>Estimate the HMM 'model quality' associated with the set of events in bst.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>The binned spike train array containing the events to evaluate.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>An existing HMM model to use. If None, a new model is fit for each fold.</p> <code>None</code> <code>n_states</code> <code>int</code> <p>Number of hidden states in the HMM. If None and hmm is provided, uses hmm.n_components.</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles to perform for the null distribution. Default is 1000.</p> <code>1000</code> <code>k_folds</code> <code>int</code> <p>Number of cross-validation folds. Default is 5.</p> <code>5</code> <code>mode</code> <code>(timeswap - pooled, timeswap - within - event, temporal - within - event)</code> <p>Shuffling mode to use for generating the null distribution. Default is 'timeswap-pooled'.</p> <code>'timeswap-pooled'</code> <code>verbose</code> <code>bool</code> <p>If True, print progress information. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>quality</code> <code>float</code> <p>Z-score of the model quality compared to the shuffled null distribution.</p> <code>scores</code> <code>ndarray</code> <p>Array of log-likelihood scores for each fold.</p> <code>shuffled</code> <code>ndarray</code> <p>Array of log-likelihood scores for each shuffle and fold.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.hmmutils import estimate_model_quality\n&gt;&gt;&gt; quality, scores, shuffled = estimate_model_quality(\n...     bst, n_states=3, n_shuffles=100, k_folds=5\n... )\n</code></pre> Source code in <code>nelpy/hmmutils.py</code> <pre><code>def estimate_model_quality(\n    bst,\n    *,\n    hmm=None,\n    n_states=None,\n    n_shuffles=1000,\n    k_folds=5,\n    mode=\"timeswap-pooled\",\n    verbose=False,\n):\n    \"\"\"\n    Estimate the HMM 'model quality' associated with the set of events in bst.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        The binned spike train array containing the events to evaluate.\n    hmm : PoissonHMM, optional\n        An existing HMM model to use. If None, a new model is fit for each fold.\n    n_states : int, optional\n        Number of hidden states in the HMM. If None and hmm is provided, uses hmm.n_components.\n    n_shuffles : int, optional\n        Number of shuffles to perform for the null distribution. Default is 1000.\n    k_folds : int, optional\n        Number of cross-validation folds. Default is 5.\n    mode : {'timeswap-pooled', 'timeswap-within-event', 'temporal-within-event'}, optional\n        Shuffling mode to use for generating the null distribution. Default is 'timeswap-pooled'.\n    verbose : bool, optional\n        If True, print progress information. Default is False.\n\n    Returns\n    -------\n    quality : float\n        Z-score of the model quality compared to the shuffled null distribution.\n    scores : np.ndarray\n        Array of log-likelihood scores for each fold.\n    shuffled : np.ndarray\n        Array of log-likelihood scores for each shuffle and fold.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.hmmutils import estimate_model_quality\n    &gt;&gt;&gt; quality, scores, shuffled = estimate_model_quality(\n    ...     bst, n_states=3, n_shuffles=100, k_folds=5\n    ... )\n    \"\"\"\n    from scipy.stats import zmap\n\n    from .decoding import k_fold_cross_validation\n\n    if hmm:\n        if not n_states:\n            n_states = hmm.n_components\n\n    X = [ii for ii in range(bst.n_epochs)]\n\n    scores = np.zeros(bst.n_epochs)\n    shuffled = np.zeros((bst.n_epochs, n_shuffles))\n\n    if mode == \"timeswap-pooled\":\n        # shuffle data coherently, pooled over all events:\n        shuffle_func = replay.pooled_time_swap_bst\n    elif mode == \"timeswap-within-event\":\n        # shuffle data coherently within events:\n        shuffle_func = replay.time_swap_bst\n    elif mode == \"temporal-within-event\":\n        shuffle_func = replay.incoherent_shuffle_bst\n    else:\n        raise NotImplementedError\n\n    for kk, (training, validation) in enumerate(k_fold_cross_validation(X, k=k_folds)):\n        if verbose:\n            print(\"  fold {}/{}\".format(kk + 1, k_folds))\n\n        PBEs_train = bst[training]\n        PBEs_test = bst[validation]\n\n        # train HMM on all training PBEs\n        hmm = PoissonHMM(n_components=n_states, verbose=False)\n        hmm.fit(PBEs_train)\n\n        # compute scores_hmm (log likelihoods) of validation set:\n        scores[validation] = hmm.score(PBEs_test)\n\n        for nn in range(n_shuffles):\n            # shuffle data:\n            bst_test_shuffled = shuffle_func(PBEs_test)\n\n            # score validation set with shuffled-data HMM\n            shuffled[validation, nn] = hmm.score(bst_test_shuffled)\n\n    quality = zmap(scores.mean(), shuffled.mean(axis=0))\n\n    return quality, scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/ipynb/","title":"nelpy.ipynb","text":"<p>ipynb.py -- helper functions for working with the IPython Notebook</p> <p>This software is licensed under the terms of the MIT License as follows:</p> <p>Copyright (c) 2013 Jessica B. Hamrick</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"reference/nelpy/ipynb/#nelpy.ipynb.clear_output","title":"<code>clear_output(delay=0.0)</code>","text":"<p>Clears the output of a cell. Useful for animating things like progress counters.</p> <p>Parameters:</p> Name Type Description Default <code>delay</code> <code>number</code> <p>Seconds to delay before clearing output</p> <code>0.0</code> Source code in <code>nelpy/ipynb.py</code> <pre><code>def clear_output(delay=0.0):\n    \"\"\"Clears the output of a cell. Useful for animating things like progress\n    counters.\n\n    Parameters\n    ----------\n    delay : number\n        Seconds to delay before clearing output\n\n    \"\"\"\n\n    sys.stdout.flush()\n    if delay:\n        time.sleep(delay)\n    IPython.core.display.clear_output()\n</code></pre>"},{"location":"reference/nelpy/min/","title":"nelpy.min","text":""},{"location":"reference/nelpy/min/#nelpy.min--nelpy-minimal-min-api","title":"nelpy minimal (min) API","text":"<p><code>nelpy</code> is a neuroelectrophysiology object model and data analysis suite based on the python-vdmlab project (https://github.com/mvdm/vandermeerlab), and inspired by the neuralensemble.org NEO project (see http://neo.readthedocs.io/en/0.4.0/core.html).</p>"},{"location":"reference/nelpy/min/#nelpy.min.Abscissa","title":"<code>Abscissa</code>","text":"<p>An abscissa (x-axis) object for core nelpy data containers.</p> <p>Parameters:</p> Name Type Description Default <code>support</code> <code>IntervalArray</code> <p>The support associated with the abscissa. Default is an empty IntervalArray.</p> <code>None</code> <code>is_wrapping</code> <code>bool</code> <p>Whether or not the abscissa is wrapping (continuous). Default is False.</p> <code>False</code> <code>labelstring</code> <code>str</code> <p>String template for the abscissa label. Default is '{}'.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>support</code> <code>IntervalArray</code> <p>The support associated with the abscissa.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the abscissa, inherited from support.</p> <code>is_wrapping</code> <code>bool</code> <p>Whether the abscissa is wrapping.</p> <code>label</code> <code>str</code> <p>The formatted label for the abscissa.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class Abscissa:\n    \"\"\"\n    An abscissa (x-axis) object for core nelpy data containers.\n\n    Parameters\n    ----------\n    support : nelpy.IntervalArray, optional\n        The support associated with the abscissa. Default is an empty IntervalArray.\n    is_wrapping : bool, optional\n        Whether or not the abscissa is wrapping (continuous). Default is False.\n    labelstring : str, optional\n        String template for the abscissa label. Default is '{}'.\n\n    Attributes\n    ----------\n    support : nelpy.IntervalArray\n        The support associated with the abscissa.\n    base_unit : str\n        The base unit of the abscissa, inherited from support.\n    is_wrapping : bool\n        Whether the abscissa is wrapping.\n    label : str\n        The formatted label for the abscissa.\n    \"\"\"\n\n    def __init__(self, support=None, is_wrapping=False, labelstring=None):\n        # TODO: add label support\n        if support is None:\n            support = core.IntervalArray(empty=True)\n        if labelstring is None:\n            labelstring = \"{}\"\n\n        self.formatter = formatters.ArbitraryFormatter\n        self.support = support\n        self.base_unit = self.support.base_unit\n        self._labelstring = labelstring\n        self.is_wrapping = is_wrapping\n\n    @property\n    def label(self):\n        \"\"\"\n        Get the abscissa label.\n\n        Returns\n        -------\n        label : str\n            The formatted abscissa label.\n        \"\"\"\n        return self._labelstring.format(self.base_unit)\n\n    @label.setter\n    def label(self, val):\n        \"\"\"\n        Set the abscissa label string template.\n\n        Parameters\n        ----------\n        val : str\n            String template for the abscissa label.\n        \"\"\"\n        if val is None:\n            val = \"{}\"\n        try:  # cast to str:\n            labelstring = str(val)\n        except TypeError:\n            raise TypeError(\"cannot convert label to string\")\n        else:\n            labelstring = val\n        self._labelstring = labelstring\n\n    def __repr__(self):\n        return \"Abscissa(base_unit={}, is_wrapping={}) on domain [{}, {})\".format(\n            self.base_unit, self.is_wrapping, self.domain.start, self.domain.stop\n        )\n\n    @property\n    def domain(self):\n        \"\"\"Domain (in base units) on which abscissa is defined.\"\"\"\n        return self.support.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"Domain (in base units) on which abscissa is defined.\"\"\"\n        # val can be an IntervalArray type, or (start, stop)\n        self.support.domain = val\n        self.support = self.support[self.support.domain]\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.Abscissa.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>Domain (in base units) on which abscissa is defined.</p>"},{"location":"reference/nelpy/min/#nelpy.min.Abscissa.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Get the abscissa label.</p> <p>Returns:</p> Name Type Description <code>label</code> <code>str</code> <p>The formatted abscissa label.</p>"},{"location":"reference/nelpy/min/#nelpy.min.AnalogSignalArray","title":"<code>AnalogSignalArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>Array of continuous analog signals with regular sampling rates.</p> <p>This class extends RegularlySampledAnalogSignalArray with additional aliases and legacy support for backward compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of signal data with shape (n_signals, n_samples). Default is empty array.</p> required <code>abscissa_vals</code> <code>ndarray</code> <p>Time values corresponding to samples, with shape (n_samples,). Default is None.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Default is None.</p> required <code>step</code> <code>float</code> <p>Sampling interval in seconds. Default is None.</p> required <code>merge_sample_gap</code> <code>float</code> <p>Maximum gap between samples to merge intervals (seconds). Default is 0.</p> required <code>support</code> <code>IntervalArray</code> <p>Time intervals where signal is defined. Default is None.</p> required <code>in_core</code> <code>bool</code> <p>Whether to keep data in core memory. Default is True.</p> required <code>labels</code> <code>array - like</code> <p>Labels for each signal. Default is None.</p> required <code>empty</code> <code>bool</code> <p>If True, creates empty array. Default is False.</p> required <code>abscissa</code> <code>AnalogSignalArrayAbscissa</code> <p>Abscissa object. Default is created from support.</p> required <code>ordinate</code> <code>AnalogSignalArrayOrdinate</code> <p>Ordinate object. Default is empty.</p> required Aliases <p>time : abscissa_vals     Alias for time values.</p> <p>n_epochs : n_intervals     Alias for number of intervals. ydata : data     Legacy alias for data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy import AnalogSignalArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a simple sine wave signal\n&gt;&gt;&gt; time = np.linspace(0, 1, 1000)  # 1 second of data\n&gt;&gt;&gt; signal = np.sin(2 * np.pi * 5 * time)  # 5 Hz sine wave\n</code></pre> <pre><code>&gt;&gt;&gt; # Create AnalogSignalArray with default parameters\n&gt;&gt;&gt; asa = AnalogSignalArray(\n...     data=signal[np.newaxis, :], abscissa_vals=time, fs=1000\n... )  # 1 kHz sampling\n</code></pre> <pre><code>&gt;&gt;&gt; # Access data using different aliases\n&gt;&gt;&gt; print(asa.data.shape)  # (1, 1000)\n&gt;&gt;&gt; print(asa.ydata.shape)  # same as data (legacy alias)\n&gt;&gt;&gt; print(asa.time.shape)  # (1000,) alias for abscissa_vals\n</code></pre> <pre><code>&gt;&gt;&gt; # Plot the signal (requires matplotlib)\n&gt;&gt;&gt; # asa.plot()\n</code></pre> <pre><code>&gt;&gt;&gt; # Create multi-channel signal with labels\n&gt;&gt;&gt; signals = np.vstack([signal, np.cos(2 * np.pi * 5 * time)])  # add cosine wave\n&gt;&gt;&gt; asa2 = AnalogSignalArray(\n...     data=signals, abscissa_vals=time, fs=1000, labels=[\"sine\", \"cosine\"]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Access individual channels\n&gt;&gt;&gt; sine_channel = asa2[:, 0]\n&gt;&gt;&gt; cosine_channel = asa2[:, 1]\n</code></pre> Notes <ul> <li>Inherits all attributes and methods from RegularlySampledAnalogSignalArray</li> <li>Provides backward compatibility with legacy parameter names</li> <li>Automatically handles abscissa and ordinate objects if not provided</li> </ul> See Also <p>RegularlySampledAnalogSignalArray : Parent class with core functionality</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class AnalogSignalArray(RegularlySampledAnalogSignalArray):\n    \"\"\"Array of continuous analog signals with regular sampling rates.\n\n    This class extends RegularlySampledAnalogSignalArray with additional aliases\n    and legacy support for backward compatibility.\n\n    Parameters\n    ----------\n    data : np.ndarray, optional\n        Array of signal data with shape (n_signals, n_samples).\n        Default is empty array.\n    abscissa_vals : np.ndarray, optional\n        Time values corresponding to samples, with shape (n_samples,).\n        Default is None.\n    fs : float, optional\n        Sampling frequency in Hz. Default is None.\n    step : float, optional\n        Sampling interval in seconds. Default is None.\n    merge_sample_gap : float, optional\n        Maximum gap between samples to merge intervals (seconds).\n        Default is 0.\n    support : nelpy.IntervalArray, optional\n        Time intervals where signal is defined. Default is None.\n    in_core : bool, optional\n        Whether to keep data in core memory. Default is True.\n    labels : array-like, optional\n        Labels for each signal. Default is None.\n    empty : bool, optional\n        If True, creates empty array. Default is False.\n    abscissa : nelpy.core.AnalogSignalArrayAbscissa, optional\n        Abscissa object. Default is created from support.\n    ordinate : nelpy.core.AnalogSignalArrayOrdinate, optional\n        Ordinate object. Default is empty.\n\n    Aliases\n    -------\n    time : abscissa_vals\n        Alias for time values.\n\n    n_epochs : n_intervals\n        Alias for number of intervals.\n    ydata : data\n        Legacy alias for data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy import AnalogSignalArray\n\n    &gt;&gt;&gt; # Create a simple sine wave signal\n    &gt;&gt;&gt; time = np.linspace(0, 1, 1000)  # 1 second of data\n    &gt;&gt;&gt; signal = np.sin(2 * np.pi * 5 * time)  # 5 Hz sine wave\n\n    &gt;&gt;&gt; # Create AnalogSignalArray with default parameters\n    &gt;&gt;&gt; asa = AnalogSignalArray(\n    ...     data=signal[np.newaxis, :], abscissa_vals=time, fs=1000\n    ... )  # 1 kHz sampling\n\n    &gt;&gt;&gt; # Access data using different aliases\n    &gt;&gt;&gt; print(asa.data.shape)  # (1, 1000)\n    &gt;&gt;&gt; print(asa.ydata.shape)  # same as data (legacy alias)\n    &gt;&gt;&gt; print(asa.time.shape)  # (1000,) alias for abscissa_vals\n\n    &gt;&gt;&gt; # Plot the signal (requires matplotlib)\n    &gt;&gt;&gt; # asa.plot()\n\n    &gt;&gt;&gt; # Create multi-channel signal with labels\n    &gt;&gt;&gt; signals = np.vstack([signal, np.cos(2 * np.pi * 5 * time)])  # add cosine wave\n    &gt;&gt;&gt; asa2 = AnalogSignalArray(\n    ...     data=signals, abscissa_vals=time, fs=1000, labels=[\"sine\", \"cosine\"]\n    ... )\n\n    &gt;&gt;&gt; # Access individual channels\n    &gt;&gt;&gt; sine_channel = asa2[:, 0]\n    &gt;&gt;&gt; cosine_channel = asa2[:, 1]\n\n    Notes\n    -----\n    - Inherits all attributes and methods from RegularlySampledAnalogSignalArray\n    - Provides backward compatibility with legacy parameter names\n    - Automatically handles abscissa and ordinate objects if not provided\n\n    See Also\n    --------\n    RegularlySampledAnalogSignalArray : Parent class with core functionality\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"abscissa_vals\",\n        \"_time\": \"_abscissa_vals\",\n        \"n_epochs\": \"n_intervals\",\n        \"ydata\": \"data\",  # legacy support\n        \"_ydata\": \"_data\",  # legacy support\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        # legacy ASA constructor support for backward compatibility\n        kwargs = legacyASAkwargs(**kwargs)\n\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        abscissa = kwargs.get(\n            \"abscissa\", core.AnalogSignalArrayAbscissa(support=support)\n        )\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.AnalogSignalArrayAbscissa","title":"<code>AnalogSignalArrayAbscissa</code>","text":"<p>               Bases: <code>Abscissa</code></p> <p>Abscissa for AnalogSignalArray.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class AnalogSignalArrayAbscissa(Abscissa):\n    \"\"\"Abscissa for AnalogSignalArray.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        labelstring = kwargs.get(\n            \"labelstring\", \"time ({})\"\n        )  # TODO FIXME after unit inheritance; inherit from formatter?\n\n        kwargs[\"support\"] = support\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n\n        self.formatter = self.support.formatter\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.AnalogSignalArrayOrdinate","title":"<code>AnalogSignalArrayOrdinate</code>","text":"<p>               Bases: <code>Ordinate</code></p> <p>Ordinate for AnalogSignalArray.</p> <p>Examples:</p> <p>nel.AnalogSignalArrayOrdinate(base_unit='uV')</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class AnalogSignalArrayOrdinate(Ordinate):\n    \"\"\"Ordinate for AnalogSignalArray.\n\n    Examples\n    -------\n    nel.AnalogSignalArrayOrdinate(base_unit='uV')\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        base_unit = kwargs.get(\"base_unit\", \"V\")\n        labelstring = kwargs.get(\"labelstring\", \"voltage ({})\")\n\n        kwargs[\"base_unit\"] = base_unit\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray","title":"<code>BinnedEventArray</code>","text":"<p>               Bases: <code>BaseEventArray</code></p> <p>BinnedEventArray.</p> <p>Parameters:</p> Name Type Description Default <code>eventarray</code> <code>EventArray or RegularlySampledAnalogSignalArray</code> <p>Input data.</p> <code>None</code> <code>ds</code> <code>float</code> <p>The bin width, in seconds. Default is 0.0625 (62.5 ms)</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Whether an empty BinnedEventArray should be constructed (no data).</p> <code>False</code> <code>fs</code> <code>float</code> <p>Sampling rate in Hz. If fs is passed as a parameter, then data is assumed to be in sample numbers instead of actual data.</p> required <code>kwargs</code> <code>optional</code> <p>Additional keyword arguments to forward along to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray superclass for additional</code> <code>attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the BinnedEventArray is empty (no data).</p> <code>n_series</code> <code>int</code> <p>The number of series.</p> <code>bin_centers</code> <code>ndarray</code> <p>The bin centers, in seconds.</p> <code>event_centers</code> <code>ndarray</code> <p>The centers of each event, in seconds.</p> <code>data</code> <code>np.array, with shape (n_series, n_bins)</code> <p>Event counts in all bins.</p> <code>bins</code> <code>ndarray</code> <p>The bin edges, in seconds.</p> <code>binned_support</code> <code>np.ndarray, with shape (n_intervals, 2)</code> <p>The binned support of the BinnedEventArray (in bin IDs).</p> <code>lengths</code> <code>ndarray</code> <p>Lengths of contiguous segments, in number of bins.</p> <code>eventarray</code> <code>EventArray</code> <p>The original eventarray associated with the binned data.</p> <code>n_bins</code> <code>int</code> <p>The number of bins.</p> <code>ds</code> <code>float</code> <p>Bin width, in seconds.</p> <code>n_active</code> <code>int</code> <p>The number of active series. A series is considered active if it fired at least one event.</p> <code>n_active_per_bin</code> <code>np.ndarray, with shape (n_bins, )</code> <p>Number of active series per data bin.</p> <code>n_events</code> <code>ndarray</code> <p>The number of events in each series.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class BinnedEventArray(BaseEventArray):\n    \"\"\"BinnedEventArray.\n\n    Parameters\n    ----------\n    eventarray : nelpy.EventArray or nelpy.RegularlySampledAnalogSignalArray\n        Input data.\n    ds : float\n        The bin width, in seconds.\n        Default is 0.0625 (62.5 ms)\n    empty : bool, optional\n        Whether an empty BinnedEventArray should be constructed (no data).\n    fs : float, optional\n        Sampling rate in Hz. If fs is passed as a parameter, then data\n        is assumed to be in sample numbers instead of actual data.\n    kwargs : optional\n        Additional keyword arguments to forward along to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray superclass for additional\n    attributes that are defined there.\n    isempty : bool\n        Whether the BinnedEventArray is empty (no data).\n    n_series : int\n        The number of series.\n    bin_centers : np.ndarray\n        The bin centers, in seconds.\n    event_centers : np.ndarray\n        The centers of each event, in seconds.\n    data : np.array, with shape (n_series, n_bins)\n        Event counts in all bins.\n    bins : np.ndarray\n        The bin edges, in seconds.\n    binned_support : np.ndarray, with shape (n_intervals, 2)\n        The binned support of the BinnedEventArray (in\n        bin IDs).\n    lengths : np.ndarray\n        Lengths of contiguous segments, in number of bins.\n    eventarray : nelpy.EventArray\n        The original eventarray associated with the binned data.\n    n_bins : int\n        The number of bins.\n    ds : float\n        Bin width, in seconds.\n    n_active : int\n        The number of active series. A series is considered active if\n        it fired at least one event.\n    n_active_per_bin : np.ndarray, with shape (n_bins, )\n        Number of active series per data bin.\n    n_events : np.ndarray\n        The number of events in each series.\n    support : nelpy.IntervalArray\n        The support of the BinnedEventArray.\n    \"\"\"\n\n    __attributes__ = [\n        \"_ds\",\n        \"_bins\",\n        \"_data\",\n        \"_bin_centers\",\n        \"_binned_support\",\n        \"_eventarray\",\n    ]\n    __attributes__.extend(BaseEventArray.__attributes__)\n\n    def __init__(self, eventarray=None, *, ds=None, empty=False, **kwargs):\n        super().__init__(empty=True)\n\n        # if an empty object is requested, return it:\n        if empty:\n            # super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self._event_centers = None\n            return\n\n        # handle casting other nelpy objects to BinnedEventArray:\n        if isinstance(eventarray, core.RegularlySampledAnalogSignalArray):\n            if eventarray.isempty:\n                for attr in self.__attributes__:\n                    exec(\"self.\" + attr + \" = None\")\n                self._abscissa.support = type(eventarray._abscissa.support)(empty=True)\n                self._event_centers = None\n                return\n            eventarray = eventarray.copy()  # Note: this is a deep copy\n            n_empty_epochs = np.sum(eventarray.support.lengths == 0)\n            if n_empty_epochs &gt; 0:\n                logging.warning(\n                    \"Detected {} empty epochs. Removing these in the cast object\".format(\n                        n_empty_epochs\n                    )\n                )\n                eventarray.support = eventarray.support._drop_empty_intervals()\n            if not eventarray.support.ismerged:\n                logging.warning(\n                    \"Detected overlapping epochs. Merging these in the cast object\"\n                )\n                eventarray.support = eventarray.support.merge()\n\n            self._eventarray = None\n            self._ds = 1 / eventarray.fs\n            self._series_labels = eventarray._series_labels\n            self._bin_centers = eventarray.abscissa_vals\n            tmp = np.insert(np.cumsum(eventarray.lengths), 0, 0)\n            self._binned_support = np.array((tmp[:-1], tmp[1:] - 1)).T\n            self._abscissa.support = eventarray.support\n            try:\n                self._series_ids = (\n                    np.array(eventarray.series_labels).astype(int)\n                ).tolist()\n            except (ValueError, TypeError):\n                self._series_ids = (np.arange(eventarray.n_signals) + 1).tolist()\n            self._data = eventarray._ydata_rowsig\n\n            bins = []\n            for starti, stopi in self._binned_support:\n                bins_edges_in_interval = (\n                    self._bin_centers[starti : stopi + 1] - self._ds / 2\n                ).tolist()\n                bins_edges_in_interval.append(self._bin_centers[stopi] + self._ds / 2)\n                bins.extend(bins_edges_in_interval)\n            self._bins = np.array(bins)\n            return\n\n        if type(eventarray).__name__ == \"BinnedSpikeTrainArray\":\n            # old-style nelpy BinnedSpikeTrainArray object?\n            try:\n                self._eventarray = eventarray._spiketrainarray\n                self._ds = eventarray.ds\n                self._series_labels = eventarray.unit_labels\n                self._bin_centers = eventarray.bin_centers\n                self._binned_support = eventarray.binned_support\n                try:\n                    self._abscissa.support = core.EpochArray(eventarray.support.data)\n                except AttributeError:\n                    self._abscissa.support = core.EpochArray(eventarray.support.time)\n                self._series_ids = eventarray.unit_ids\n                self._data = eventarray.data\n                return\n            except Exception:\n                pass\n\n        if not isinstance(eventarray, EventArray):\n            raise TypeError(\"eventarray must be a nelpy.EventArray object.\")\n\n        self._ds = None\n        self._bin_centers = np.array([])\n        self._event_centers = None\n\n        logging.disable(logging.CRITICAL)\n        kwargs = {\n            \"fs\": eventarray.fs,\n            \"series_ids\": eventarray.series_ids,\n            \"series_labels\": eventarray.series_labels,\n            \"series_tags\": eventarray.series_tags,\n            \"label\": eventarray.label,\n        }\n        logging.disable(0)\n\n        # initialize super so that self.fs is set:\n        self._data = np.zeros((eventarray.n_series, 0))\n        # the above is necessary so that super() can determine\n        # self.n_series when initializing. self.data will\n        # be updated later in __init__ to reflect subsequent changes\n        super().__init__(**kwargs)\n\n        if ds is None:\n            logging.warning(\"no bin size was given, assuming 62.5 ms\")\n            ds = 0.0625\n\n        self._eventarray = eventarray  # TODO: remove this if we don't need it, or decide that it's too wasteful\n        self._abscissa = copy.deepcopy(eventarray._abscissa)\n        self.ds = ds\n\n        self._bin_events(eventarray=eventarray, intervalArray=eventarray.support, ds=ds)\n\n    def __mul__(self, other):\n        \"\"\"Overloaded * operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data * other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T * other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for *: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __rmul__(self, other):\n        \"\"\"Overloaded * operator\"\"\"\n        return self.__mul__(other)\n\n    def __sub__(self, other):\n        \"\"\"Overloaded - operator\"\"\"\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data - other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T - other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for -: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __add__(self, other):\n        \"\"\"Overloaded + operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data + other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T + other).T\n            return neweva\n        elif isinstance(other, type(self)):\n            # TODO: additional checks need to be done, e.g., same series ids...\n            assert self.n_series == other.n_series\n            support = self._abscissa.support + other.support\n\n            newdata = []\n            for series in range(self.n_series):\n                newdata.append(np.append(self.data[series], other.data[series]))\n\n            fs = self.fs\n            if self.fs != other.fs:\n                fs = None\n            return type(self)(newdata, support=support, fs=fs)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __truediv__(self, other):\n        \"\"\"Overloaded / operator\"\"\"\n\n        if isinstance(other, numbers.Number):\n            neweva = self.copy()\n            neweva._data = self.data / other\n            return neweva\n        elif isinstance(other, np.ndarray):\n            neweva = self.copy()\n            neweva._data = (self.data.T / other).T\n            return neweva\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for /: '{}' and '{}'\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def median(self, *, axis=1):\n        \"\"\"Returns the median of each series in BinnedEventArray.\"\"\"\n        try:\n            medians = np.nanmedian(self.data, axis=axis).squeeze()\n            if medians.size == 1:\n                return medians.item()\n            return medians\n        except IndexError:\n            raise IndexError(\"Empty BinnedEventArray; cannot calculate median.\")\n\n    def mean(self, *, axis=1):\n        \"\"\"Returns the mean of each series in BinnedEventArray.\"\"\"\n        try:\n            means = np.nanmean(self.data, axis=axis).squeeze()\n            if means.size == 1:\n                return means.item()\n            return means\n        except IndexError:\n            raise IndexError(\"Empty BinnedEventArray; cannot calculate mean.\")\n\n    def std(self, *, axis=1):\n        \"\"\"Returns the standard deviation of each series in BinnedEventArray.\"\"\"\n        try:\n            stds = np.nanstd(self.data, axis=axis).squeeze()\n            if stds.size == 1:\n                return stds.item()\n            return stds\n        except IndexError:\n            raise IndexError(\n                \"Empty BinnedEventArray; cannot calculate standard deviation\"\n            )\n\n    def center(self, inplace=False):\n        \"\"\"Center data (zero mean).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        return out\n\n    def normalize(self, inplace=False):\n        \"\"\"Normalize data (unit standard deviation).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        std = out.std()\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    def standardize(self, inplace=False):\n        \"\"\"Standardize data (zero mean and unit std deviation).\"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        std = out.std()\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n\n        return out\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        partitioned = type(self)(\n            core.RegularlySampledAnalogSignalArray(self).partition(\n                ds=ds, n_intervals=n_intervals\n            )\n        )\n        # partitioned.loc = ItemGetter_loc(partitioned)\n        # partitioned.iloc = ItemGetter_iloc(partitioned)\n        return partitioned\n\n        # raise NotImplementedError('workaround: cast to AnalogSignalArray, partition, and cast back to BinnedEventArray')\n\n    def _copy_without_data(self):\n        \"\"\"Returns a copy of the BinnedEventArray, without data.\n        Note: the support is left unchanged, but the binned_support is removed.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._bin_centers = None\n        out._binned_support = None\n        out._bins = None\n        out._data = np.zeros((self.n_series, 0))\n        out._eventarray = out._eventarray._copy_without_data()\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Returns a copy of the BinnedEventArray.\"\"\"\n        newcopy = copy.deepcopy(self)\n        newcopy.__renew__()\n        return newcopy\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        # Summarize labels if too many\n        max_display = 5\n        labels = self._series_labels\n        n = len(labels)\n        if n &lt;= max_display:\n            label_str = str(labels)\n        else:\n            label_str = f\"[{', '.join(map(str, labels[:3]))}, ..., {', '.join(map(str, labels[-2:]))}]\"\n        ustr = f\" {self.n_series} {label_str}\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = f\" ({self._abscissa.support.n_intervals} segments) in\"\n        else:\n            epstr = \" in\"\n        if self.n_bins == 1:\n            bstr = f\" {self.n_bins} bin of width {utils.PrettyDuration(self.ds)}\"\n            dstr = \"\"\n        else:\n            bstr = f\" {self.n_bins} bins of width {utils.PrettyDuration(self.ds)}\"\n            dstr = f\" for a total of {utils.PrettyDuration(self.n_bins * self.ds)}\"\n        return f\"&lt;{self.type_name}{address_str}:{ustr}{epstr}{bstr}&gt;{dstr}\"\n\n    def __iter__(self):\n        \"\"\"BinnedEventArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"BinnedEventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        # TODO: return self.loc[index], and make sure that __getitem__ is updated\n        logging.disable(logging.CRITICAL)\n        support = self._abscissa.support[index]\n        bsupport = self.binned_support[[index], :]\n\n        binnedeventarray = type(self)(empty=True)\n        exclude = [\"_bins\", \"_data\", \"_support\", \"_bin_centers\", \"_binned_support\"]\n        attrs = (x for x in self.__attributes__ if x not in exclude)\n        for attr in attrs:\n            exec(\"binnedeventarray.\" + attr + \" = self.\" + attr)\n        binindices = np.insert(0, 1, np.cumsum(self.lengths + 1))  # indices of bins\n        binstart = binindices[index]\n        binstop = binindices[index + 1]\n        binnedeventarray._bins = self._bins[binstart:binstop]\n        binnedeventarray._data = self._data[:, bsupport[0][0] : bsupport[0][1] + 1]\n        binnedeventarray._abscissa.support = support\n        binnedeventarray._bin_centers = self._bin_centers[\n            bsupport[0][0] : bsupport[0][1] + 1\n        ]\n        binnedeventarray._binned_support = bsupport - bsupport[0, 0]\n        logging.disable(0)\n        self._index += 1\n        binnedeventarray.__renew__()\n        return binnedeventarray\n\n    def empty(self, *, inplace=False):\n        \"\"\"Remove data (but not metadata) from BinnedEventArray.\n\n        Attributes 'data', and 'support' 'binned_support' are all emptied.\n\n        Note: n_series, series_ids, etc. are all preserved.\n        \"\"\"\n        if not inplace:\n            out = self._copy_without_data()\n            out._abscissa.support = type(self._abscissa.support)(empty=True)\n            return out\n        out = self\n        out._data = np.zeros((self.n_series, 0))\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        out._binned_support = None\n        out._bin_centers = None\n        out._bins = None\n        out._eventarray.empty(inplace=True)\n        out.__renew__()\n        return out\n\n    def __getitem__(self, idx):\n        \"\"\"BinnedEventArray index access.\n\n        By default, this method is bound to .loc\n        \"\"\"\n        return self.loc[idx]\n\n    def _restrict(self, intervalslice, seriesslice):\n        # This function should be called only by an itemgetter\n        # because it mutates data.\n        # The itemgetter is responsible for creating copies\n        # of objects\n\n        self._restrict_to_series_subset(seriesslice)\n        self._eventarray._restrict_to_series_subset(seriesslice)\n\n        self._restrict_to_interval(intervalslice)\n        self._eventarray._restrict_to_interval(intervalslice)\n        return self\n\n    def _restrict_to_series_subset(self, idx):\n        # Warning: This function can mutate data\n\n        if isinstance(idx, core.IntervalArray):\n            raise IndexError(\n                \"Slicing is [intervals, signal]; perhaps you have the order reversed?\"\n            )\n\n        # TODO: update tags\n        try:\n            self._data = np.atleast_2d(self.data[idx, :])\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except IndexError:\n            raise IndexError(\n                \"One of more indices were out of bounds for n_series with size {}\".format(\n                    self.n_series\n                )\n            )\n        except Exception:\n            raise TypeError(\"Unsupported indexing type {}\".format(type(idx)))\n\n    def _restrict_to_interval(self, intervalslice):\n        # Warning: This function can mutate data. It should only be called from\n        # _restrict\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                # no restriction on interval\n                return self\n\n        newintervals = self._abscissa.support[intervalslice].merge()\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=True)\n\n        bcenter_inds = []\n        bin_inds = []\n        start = 0\n        bsupport = np.zeros((newintervals.n_intervals, 2), dtype=int)\n        support_intervals = np.zeros((newintervals.n_intervals, 2))\n\n        if not self.isempty:\n            for ii, interval in enumerate(newintervals.data):\n                a_start = interval[0]\n                a_stop = interval[1]\n                frm, to = np.searchsorted(self._bins, (a_start, a_stop))\n                # If bin edges equal a_stop, they should still be included\n                if self._bins[to] &lt;= a_stop:\n                    bin_inds.extend(np.arange(frm, to + 1, step=1))\n                else:\n                    bin_inds.extend(np.arange(frm, to, step=1))\n                    to -= 1\n                support_intervals[ii] = [self._bins[frm], self._bins[to]]\n\n                lind, rind = np.searchsorted(\n                    self._bin_centers, (self._bins[frm], self._bins[to])\n                )\n                # We don't have to worry about an if-else block here unlike\n                # for the bin_inds because the bin_centers can NEVER equal\n                # the bins. Therefore we know every interval looks like\n                # the following:\n                #  first desired bin         last desired bin\n                # |------------------|......|-------------------|\n                #          ^                                         ^\n                #          |                                         |\n                #        lind                                      rind\n                # Since arange is half-open, the indices we actually take\n                # will be such that all bin centers fall within the desired\n                # bin edges.\n                bcenter_inds.extend(np.arange(lind, rind, step=1))\n\n                bsupport[ii] = [start, start + (to - frm - 1)]\n                start += to - frm\n\n            self._bins = self._bins[bin_inds]\n            self._bin_centers = self._bin_centers[bcenter_inds]\n            self._data = np.atleast_2d(self._data[:, bcenter_inds])\n            self._binned_support = bsupport\n\n        self._abscissa.support = type(self._abscissa.support)(support_intervals)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty BinnedEventArray.\"\"\"\n        try:\n            return len(self.bin_centers) == 0\n        except TypeError:\n            return True  # this happens when self.bin_centers is None\n\n    @property\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        try:\n            return utils.PrettyInt(self.data.shape[0])\n        except AttributeError:\n            return 0\n\n    @property\n    def centers(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        logging.warning(\"centers is deprecated. Use bin_centers instead.\")\n        return self.bin_centers\n\n    @property\n    def _abscissa_vals(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        return self._bin_centers\n\n    @property\n    def bin_centers(self):\n        \"\"\"(np.array) The bin centers (in seconds).\"\"\"\n        return self._bin_centers\n\n    @property\n    def event_centers(self):\n        \"\"\"(np.array) The centers (in seconds) of each event.\"\"\"\n        if self._event_centers is None:\n            raise NotImplementedError(\"event_centers not yet implemented\")\n            # self._event_centers = midpoints\n        return self._event_centers\n\n    @property\n    def _midpoints(self):\n        \"\"\"(np.array) The centers (in index space) of all events.\n\n        Examples\n        -------\n        ax, img = npl.imagesc(bst.data) # data is of shape (n_series, n_bins)\n        # then _midpoints correspond to the xvals at the center of\n        # each event.\n        ax.plot(bst.event_centers, np.repeat(1, self.n_intervals), marker='o', color='w')\n\n        \"\"\"\n        if self._event_centers is None:\n            midpoints = np.zeros(len(self.lengths))\n            for idx, length in enumerate(self.lengths):\n                midpoints[idx] = np.sum(self.lengths[:idx]) + length / 2\n            self._event_centers = midpoints\n        return self._event_centers\n\n    @property\n    def data(self):\n        \"\"\"(np.array) Event counts in all bins, with shape (n_series, n_bins).\"\"\"\n        return self._data\n\n    @property\n    def bins(self):\n        \"\"\"(np.array) The bin edges (in seconds).\"\"\"\n        return self._bins\n\n    @property\n    def binnedSupport(self):\n        \"\"\"(np.array) The binned support of the BinnedEventArray (in\n        bin IDs) of shape (n_intervals, 2).\n        \"\"\"\n        logging.warning(\"binnedSupport is deprecated. Use bined_support instead.\")\n        return self._binned_support\n\n    @property\n    def binned_support(self):\n        \"\"\"(np.array) The binned support of the BinnedEventArray (in\n        bin IDs) of shape (n_intervals, 2).\n        \"\"\"\n        return self._binned_support\n\n    @property\n    def lengths(self):\n        \"\"\"Lengths of contiguous segments, in number of bins.\"\"\"\n        if self.isempty:\n            return 0\n        return np.atleast_1d(\n            (self.binned_support[:, 1] - self.binned_support[:, 0] + 1).squeeze()\n        )\n\n    @property\n    def eventarray(self):\n        \"\"\"(nelpy.EventArray) The original EventArray associated with\n        the binned data.\n        \"\"\"\n        return self._eventarray\n\n    @property\n    def n_bins(self):\n        \"\"\"(int) The number of bins.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self.bin_centers))\n\n    @property\n    def ds(self):\n        \"\"\"(float) Bin width in seconds.\"\"\"\n        return self._ds\n\n    @ds.setter\n    def ds(self, val):\n        if self._ds is not None:\n            raise AttributeError(\"can't set attribute\")\n        else:\n            try:\n                if val &lt;= 0:\n                    pass\n            except ValueError:\n                raise TypeError(\"bin width must be a scalar\")\n            if val &lt;= 0:\n                raise ValueError(\"bin width must be positive\")\n            self._ds = val\n\n    @staticmethod\n    def _get_bins_inside_interval(interval, ds):\n        \"\"\"(np.array) Return bin edges entirely contained inside an interval.\n\n        Bin edges always start at interval.start, and continue for as many\n        bins as would fit entirely inside the interval.\n\n        NOTE 1: there are (n+1) bin edges associated with n bins.\n\n        WARNING: if an interval is smaller than ds, then no bin will be\n                associated with the particular interval.\n\n        NOTE 2: nelpy uses half-open intervals [a,b), but if the bin\n                width divides b-a, then the bins will cover the entire\n                range. For example, if interval = [0,2) and ds = 1, then\n                bins = [0,1,2], even though [0,2] is not contained in\n                [0,2).\n\n        Parameters\n        ----------\n        interval : IntervalArray\n            IntervalArray containing a single interval with a start, and stop\n        ds : float\n            Time bin width, in seconds.\n\n        Returns\n        -------\n        bins : array\n            Bin edges in an array of shape (n+1,) where n is the number\n            of bins\n        centers : array\n            Bin centers in an array of shape (n,) where n is the number\n            of bins\n        \"\"\"\n\n        if interval.length &lt; ds:\n            logging.warning(\"interval duration is less than bin size: ignoring...\")\n            return None, None\n\n        n = int(np.floor(interval.length / ds))  # number of bins\n\n        # linspace is better than arange for non-integral steps\n        bins = np.linspace(interval.start, interval.start + n * ds, n + 1)\n        centers = bins[:-1] + (ds / 2)\n        return bins, centers\n\n    def _bin_events(self, eventarray, intervalArray, ds):\n        \"\"\"\n        Docstring goes here. TBD. For use with bins that are contained\n        wholly inside the intervals.\n\n        \"\"\"\n        b = []  # bin list\n        c = []  # centers list\n        s = []  # data list\n        for nn in range(eventarray.n_series):\n            s.append([])\n        left_edges = []\n        right_edges = []\n        counter = 0\n        for interval in intervalArray:\n            bins, centers = self._get_bins_inside_interval(interval, ds)\n            if bins is not None:\n                for uu, eventarraydatas in enumerate(eventarray.data):\n                    event_counts, _ = np.histogram(\n                        eventarraydatas,\n                        bins=bins,\n                        density=False,\n                        range=(interval.start, interval.stop),\n                    )  # TODO: is it faster to limit range, or to cut out events?\n                    s[uu].extend(event_counts.tolist())\n                left_edges.append(counter)\n                counter += len(centers) - 1\n                right_edges.append(counter)\n                counter += 1\n                b.extend(bins.tolist())\n                c.extend(centers.tolist())\n        self._bins = np.array(b)\n        self._bin_centers = np.array(c)\n        self._data = np.array(s)\n        le = np.array(left_edges)\n        le = le[:, np.newaxis]\n        re = np.array(right_edges)\n        re = re[:, np.newaxis]\n        self._binned_support = np.hstack((le, re))\n        support_starts = self.bins[np.insert(np.cumsum(self.lengths + 1), 0, 0)[:-1]]\n        support_stops = self.bins[np.insert(np.cumsum(self.lengths + 1) - 1, 0, 0)[1:]]\n        supportdata = np.vstack([support_starts, support_stops]).T\n        self._abscissa.support = type(self._abscissa.support)(\n            supportdata\n        )  # set support to TRUE bin support\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(\n        self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n    ):\n        \"\"\"Smooth BinnedEventArray by convolving with a Gaussian kernel.\n\n        Smoothing is applied in data, and the same smoothing is applied\n        to each series in a BinnedEventArray.\n\n        Smoothing is applied within each interval.\n\n        Parameters\n        ----------\n        sigma : float, optional\n            Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n        truncate : float, optional\n            Bandwidth outside of which the filter value will be zero. Default is 4.0\n        inplace : bool\n            If True the data will be replaced with the smoothed data.\n            Default is False.\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with smoothed data.\n        \"\"\"\n\n        if truncate is None:\n            truncate = 4\n        if sigma is None:\n            sigma = 0.01  # 10 ms default\n\n        fs = 1 / self.ds\n\n        return utils.gaussian_filter(\n            self,\n            fs=fs,\n            sigma=sigma,\n            truncate=truncate,\n            inplace=inplace,\n            within_intervals=within_intervals,\n        )\n\n    @staticmethod\n    def _smooth_array(arr, w=None):\n        \"\"\"Smooth an array by convolving a boxcar, row-wise.\n\n        Parameters\n        ----------\n        w : int, optional\n            Number of bins to include in boxcar window. Default is 10.\n\n        Returns\n        -------\n        smoothed: array\n            Smoothed array with same shape as arr.\n        \"\"\"\n\n        if w is None:\n            w = 10\n\n        if w == 1:  # perform no smoothing\n            return arr\n\n        w = np.min((w, arr.shape[1]))\n\n        smoothed = arr.astype(float)  # copy array and cast to float\n        window = np.ones((w,)) / w\n\n        # smooth per row\n        for rowi, row in enumerate(smoothed):\n            smoothed[rowi, :] = np.convolve(row, window, mode=\"same\")\n\n        if arr.shape[1] != smoothed.shape[1]:\n            raise TypeError(\"Incompatible shape returned!\")\n\n        return smoothed\n\n    @staticmethod\n    def _rebin_array(arr, w):\n        \"\"\"Rebin an array of shape (n_signals, n_bins) into a\n        coarser bin size.\n\n        Parameters\n        ----------\n        arr : array\n            Array with shape (n_signals, n_bins) to re-bin. A copy\n            is returned.\n        w : int\n            Number of original bins to combine into each new bin.\n\n        Returns\n        -------\n        out : array\n            Bnned array with shape (n_signals, n_new_bins)\n        bin_idx : array\n            Array of shape (n_new_bins,) with the indices of the new\n            binned array, relative to the original array.\n        \"\"\"\n        cs = np.cumsum(arr, axis=1)\n        binidx = np.arange(start=w, stop=cs.shape[1] + 1, step=w) - 1\n\n        rebinned = np.hstack(\n            (np.array(cs[:, w - 1], ndmin=2).T, cs[:, binidx[1:]] - cs[:, binidx[:-1]])\n        )\n        # bins = bins[np.insert(binidx+1, 0, 0)]\n        return rebinned, binidx\n\n    def rebin(self, w=None):\n        \"\"\"Rebin the BinnedEventArray into a coarser bin size.\n\n        Parameters\n        ----------\n        w : int, optional\n            number of bins of width bst.ds to bin into new bin of\n            width bst.ds*w. Default is w=1 (no re-binning).\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with coarser resolution.\n        \"\"\"\n\n        if w is None:\n            w = 1\n\n        if not float(w).is_integer:\n            raise ValueError(\"w has to be an integer!\")\n\n        w = int(w)\n\n        bst = self\n        return self._rebin_binnedeventarray(bst, w=w)\n\n    @staticmethod\n    def _rebin_binnedeventarray(bst, w=None):\n        \"\"\"Rebin a BinnedEventArray into a coarser bin size.\n\n        Parameters\n        ----------\n        bst : BinnedEventArray\n            BinnedEventArray to re-bin into a coarser resolution.\n        w : int, optional\n            number of bins of width bst.ds to bin into new bin of\n            width bst.ds*w. Default is w=1 (no re-binning).\n\n        Returns\n        -------\n        out : BinnedEventArray\n            New BinnedEventArray with coarser resolution.\n\n        # FFB! TODO: if w is longer than some event size,\n        # an exception will occur. Handle it! Although I may already\n        # implicitly do that.\n        \"\"\"\n\n        if w is None:\n            w = 1\n\n        if w == 1:\n            return bst\n\n        edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n        newlengths = [0]\n        binedges = np.insert(np.cumsum(bst.lengths + 1), 0, 0)\n        n_events = bst.support.n_intervals\n        newdata = None\n\n        for ii in range(n_events):\n            data = bst.data[:, edges[ii] : edges[ii + 1]]\n            bins = bst.bins[binedges[ii] : binedges[ii + 1]]\n\n            datalen = data.shape[1]\n            if w &lt;= datalen:\n                rebinned, binidx = bst._rebin_array(data, w=w)\n                bins = bins[np.insert(binidx + 1, 0, 0)]\n\n                newlengths.append(rebinned.shape[1])\n\n                if newdata is None:\n                    newdata = rebinned\n                    newbins = bins\n                    newcenters = bins[:-1] + np.diff(bins) / 2\n                    newsupport = np.array([bins[0], bins[-1]])\n                else:\n                    newdata = np.hstack((newdata, rebinned))\n                    newbins = np.hstack((newbins, bins))\n                    newcenters = np.hstack((newcenters, bins[:-1] + np.diff(bins) / 2))\n                    newsupport = np.vstack((newsupport, np.array([bins[0], bins[-1]])))\n            else:\n                pass\n\n        # assemble new binned event series array:\n        newedges = np.cumsum(newlengths)\n        newbst = bst._copy_without_data()\n        abscissa = copy.copy(bst._abscissa)\n        if newdata is not None:\n            newbst._data = newdata\n            newbst._abscissa = abscissa\n            newbst._abscissa.support = type(bst.support)(newsupport)\n            newbst._bins = newbins\n            newbst._bin_centers = newcenters\n            newbst._ds = bst.ds * w\n            newbst._binned_support = np.array((newedges[:-1], newedges[1:] - 1)).T\n        else:\n            logging.warning(\n                \"No events are long enough to contain any bins of width {}\".format(\n                    utils.PrettyDuration(bst.ds)\n                )\n            )\n            newbst._data = None\n            newbst._abscissa = abscissa\n            newbst._abscissa.support = None\n            newbst._binned_support = None\n            newbst._bin_centers = None\n            newbst._bins = None\n\n        newbst.__renew__()\n\n        return newbst\n\n    def bst_from_indices(self, idx):\n        \"\"\"\n        Return a BinnedEventArray from a list of indices.\n\n        bst : BinnedEventArray\n        idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE\n\n        Examples\n        --------\n        idx = [[10, 20]\n            [25, 50]]\n        bst_from_indices(bst, idx=idx)\n        \"\"\"\n\n        idx = np.atleast_2d(idx)\n\n        newbst = self._copy_without_data()\n        ds = self.ds\n        bin_centers_ = []\n        bins_ = []\n        binned_support_ = []\n        support_ = []\n        all_abscissa_vals = []\n\n        n_preceding_bins = 0\n\n        for frm, to in idx:\n            idx_array = np.arange(frm, to + 1).astype(int)\n            all_abscissa_vals.append(idx_array)\n            bin_centers = self.bin_centers[idx_array]\n            bins = np.append(bin_centers - ds / 2, bin_centers[-1] + ds / 2)\n\n            binned_support = [n_preceding_bins, n_preceding_bins + len(bins) - 2]\n            n_preceding_bins += len(bins) - 1\n            support = type(self._abscissa.support)((bins[0], bins[-1]))\n\n            bin_centers_.append(bin_centers)\n            bins_.append(bins)\n            binned_support_.append(binned_support)\n            support_.append(support)\n\n        bin_centers = np.concatenate(bin_centers_)\n        bins = np.concatenate(bins_)\n        binned_support = np.array(binned_support_)\n        support = np.sum(support_)\n        all_abscissa_vals = np.concatenate(all_abscissa_vals)\n\n        newbst._bin_centers = bin_centers\n        newbst._bins = bins\n        newbst._binned_support = binned_support\n        newbst._abscissa.support = support\n        newbst._data = newbst.data[:, all_abscissa_vals]\n\n        newbst.__renew__()\n\n        return newbst\n\n    @property\n    def n_active(self):\n        \"\"\"Number of active series.\n\n        An active series is any series that fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    @property\n    def n_active_per_bin(self):\n        \"\"\"Number of active series per data bin with shape (n_bins,).\"\"\"\n        if self.isempty:\n            return 0\n        # TODO: profile several alternatves. Could use data &gt; 0, or\n        # other numpy methods to get a more efficient implementation:\n        return self.data.clip(max=1).sum(axis=0)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return self.data.sum(axis=1)\n\n    def flatten(self, *, series_id=None, series_label=None):\n        \"\"\"Collapse events across series.\n\n        WARNING! series_tags are thrown away when flattening.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        series_label (str)\n            (series) Label for event series, default is 'flattened'.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n        if series_label is None:\n            series_label = \"flattened\"\n\n        binnedeventarray = self._copy_without_data()\n\n        binnedeventarray._data = np.array(self.data.sum(axis=0), ndmin=2)\n\n        binnedeventarray._bins = self.bins\n        binnedeventarray._abscissa.support = self.support\n        binnedeventarray._bin_centers = self.bin_centers\n        binnedeventarray._binned_support = self.binned_support\n\n        binnedeventarray._series_ids = [series_id]\n        binnedeventarray._series_labels = [series_label]\n        binnedeventarray._series_tags = None\n        binnedeventarray.__renew__()\n\n        return binnedeventarray\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying BinnedEventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying BinnedEventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval(self._abscissa.support)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.bin_centers","title":"<code>bin_centers</code>  <code>property</code>","text":"<p>(np.array) The bin centers (in seconds).</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.binnedSupport","title":"<code>binnedSupport</code>  <code>property</code>","text":"<p>(np.array) The binned support of the BinnedEventArray (in bin IDs) of shape (n_intervals, 2).</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.binned_support","title":"<code>binned_support</code>  <code>property</code>","text":"<p>(np.array) The binned support of the BinnedEventArray (in bin IDs) of shape (n_intervals, 2).</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.bins","title":"<code>bins</code>  <code>property</code>","text":"<p>(np.array) The bin edges (in seconds).</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.centers","title":"<code>centers</code>  <code>property</code>","text":"<p>(np.array) The bin centers (in seconds).</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>(np.array) Event counts in all bins, with shape (n_series, n_bins).</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.ds","title":"<code>ds</code>  <code>property</code> <code>writable</code>","text":"<p>(float) Bin width in seconds.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.event_centers","title":"<code>event_centers</code>  <code>property</code>","text":"<p>(np.array) The centers (in seconds) of each event.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.eventarray","title":"<code>eventarray</code>  <code>property</code>","text":"<p>(nelpy.EventArray) The original EventArray associated with the binned data.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty BinnedEventArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>Lengths of contiguous segments, in number of bins.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>Number of active series.</p> <p>An active series is any series that fired at least one event.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.n_active_per_bin","title":"<code>n_active_per_bin</code>  <code>property</code>","text":"<p>Number of active series per data bin with shape (n_bins,).</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.n_bins","title":"<code>n_bins</code>  <code>property</code>","text":"<p>(int) The number of bins.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.n_series","title":"<code>n_series</code>  <code>property</code>","text":"<p>(int) The number of series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying BinnedEventArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.bst_from_indices","title":"<code>bst_from_indices(idx)</code>","text":"<p>Return a BinnedEventArray from a list of indices.</p> <p>bst : BinnedEventArray idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE</p> <p>Examples:</p> <p>idx = [[10, 20]     [25, 50]] bst_from_indices(bst, idx=idx)</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bst_from_indices(self, idx):\n    \"\"\"\n    Return a BinnedEventArray from a list of indices.\n\n    bst : BinnedEventArray\n    idx : list of sample (bin) numbers with shape (n_intervals, 2) INCLUSIVE\n\n    Examples\n    --------\n    idx = [[10, 20]\n        [25, 50]]\n    bst_from_indices(bst, idx=idx)\n    \"\"\"\n\n    idx = np.atleast_2d(idx)\n\n    newbst = self._copy_without_data()\n    ds = self.ds\n    bin_centers_ = []\n    bins_ = []\n    binned_support_ = []\n    support_ = []\n    all_abscissa_vals = []\n\n    n_preceding_bins = 0\n\n    for frm, to in idx:\n        idx_array = np.arange(frm, to + 1).astype(int)\n        all_abscissa_vals.append(idx_array)\n        bin_centers = self.bin_centers[idx_array]\n        bins = np.append(bin_centers - ds / 2, bin_centers[-1] + ds / 2)\n\n        binned_support = [n_preceding_bins, n_preceding_bins + len(bins) - 2]\n        n_preceding_bins += len(bins) - 1\n        support = type(self._abscissa.support)((bins[0], bins[-1]))\n\n        bin_centers_.append(bin_centers)\n        bins_.append(bins)\n        binned_support_.append(binned_support)\n        support_.append(support)\n\n    bin_centers = np.concatenate(bin_centers_)\n    bins = np.concatenate(bins_)\n    binned_support = np.array(binned_support_)\n    support = np.sum(support_)\n    all_abscissa_vals = np.concatenate(all_abscissa_vals)\n\n    newbst._bin_centers = bin_centers\n    newbst._bins = bins\n    newbst._binned_support = binned_support\n    newbst._abscissa.support = support\n    newbst._data = newbst.data[:, all_abscissa_vals]\n\n    newbst.__renew__()\n\n    return newbst\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.center","title":"<code>center(inplace=False)</code>","text":"<p>Center data (zero mean).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def center(self, inplace=False):\n    \"\"\"Center data (zero mean).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of the BinnedEventArray.\"\"\"\n    newcopy = copy.deepcopy(self)\n    newcopy.__renew__()\n    return newcopy\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.empty","title":"<code>empty(*, inplace=False)</code>","text":"<p>Remove data (but not metadata) from BinnedEventArray.</p> <p>Attributes 'data', and 'support' 'binned_support' are all emptied.</p> <p>Note: n_series, series_ids, etc. are all preserved.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def empty(self, *, inplace=False):\n    \"\"\"Remove data (but not metadata) from BinnedEventArray.\n\n    Attributes 'data', and 'support' 'binned_support' are all emptied.\n\n    Note: n_series, series_ids, etc. are all preserved.\n    \"\"\"\n    if not inplace:\n        out = self._copy_without_data()\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        return out\n    out = self\n    out._data = np.zeros((self.n_series, 0))\n    out._abscissa.support = type(self._abscissa.support)(empty=True)\n    out._binned_support = None\n    out._bin_centers = None\n    out._bins = None\n    out._eventarray.empty(inplace=True)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.flatten","title":"<code>flatten(*, series_id=None, series_label=None)</code>","text":"<p>Collapse events across series.</p> <p>WARNING! series_tags are thrown away when flattening.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> <code>series_label</code> <p>(series) Label for event series, default is 'flattened'.</p> <code>None</code> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def flatten(self, *, series_id=None, series_label=None):\n    \"\"\"Collapse events across series.\n\n    WARNING! series_tags are thrown away when flattening.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    series_label (str)\n        (series) Label for event series, default is 'flattened'.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n    if series_label is None:\n        series_label = \"flattened\"\n\n    binnedeventarray = self._copy_without_data()\n\n    binnedeventarray._data = np.array(self.data.sum(axis=0), ndmin=2)\n\n    binnedeventarray._bins = self.bins\n    binnedeventarray._abscissa.support = self.support\n    binnedeventarray._bin_centers = self.bin_centers\n    binnedeventarray._binned_support = self.binned_support\n\n    binnedeventarray._series_ids = [series_id]\n    binnedeventarray._series_labels = [series_label]\n    binnedeventarray._series_tags = None\n    binnedeventarray.__renew__()\n\n    return binnedeventarray\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.mean","title":"<code>mean(*, axis=1)</code>","text":"<p>Returns the mean of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def mean(self, *, axis=1):\n    \"\"\"Returns the mean of each series in BinnedEventArray.\"\"\"\n    try:\n        means = np.nanmean(self.data, axis=axis).squeeze()\n        if means.size == 1:\n            return means.item()\n        return means\n    except IndexError:\n        raise IndexError(\"Empty BinnedEventArray; cannot calculate mean.\")\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.median","title":"<code>median(*, axis=1)</code>","text":"<p>Returns the median of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def median(self, *, axis=1):\n    \"\"\"Returns the median of each series in BinnedEventArray.\"\"\"\n    try:\n        medians = np.nanmedian(self.data, axis=axis).squeeze()\n        if medians.size == 1:\n            return medians.item()\n        return medians\n    except IndexError:\n        raise IndexError(\"Empty BinnedEventArray; cannot calculate median.\")\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.normalize","title":"<code>normalize(inplace=False)</code>","text":"<p>Normalize data (unit standard deviation).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def normalize(self, inplace=False):\n    \"\"\"Normalize data (unit standard deviation).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    std = out.std()\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    partitioned = type(self)(\n        core.RegularlySampledAnalogSignalArray(self).partition(\n            ds=ds, n_intervals=n_intervals\n        )\n    )\n    # partitioned.loc = ItemGetter_loc(partitioned)\n    # partitioned.iloc = ItemGetter_iloc(partitioned)\n    return partitioned\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.rebin","title":"<code>rebin(w=None)</code>","text":"<p>Rebin the BinnedEventArray into a coarser bin size.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>int</code> <p>number of bins of width bst.ds to bin into new bin of width bst.ds*w. Default is w=1 (no re-binning).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedEventArray</code> <p>New BinnedEventArray with coarser resolution.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def rebin(self, w=None):\n    \"\"\"Rebin the BinnedEventArray into a coarser bin size.\n\n    Parameters\n    ----------\n    w : int, optional\n        number of bins of width bst.ds to bin into new bin of\n        width bst.ds*w. Default is w=1 (no re-binning).\n\n    Returns\n    -------\n    out : BinnedEventArray\n        New BinnedEventArray with coarser resolution.\n    \"\"\"\n\n    if w is None:\n        w = 1\n\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    w = int(w)\n\n    bst = self\n    return self._rebin_binnedeventarray(bst, w=w)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.smooth","title":"<code>smooth(*, sigma=None, inplace=False, truncate=None, within_intervals=False)</code>","text":"<p>Smooth BinnedEventArray by convolving with a Gaussian kernel.</p> <p>Smoothing is applied in data, and the same smoothing is applied to each series in a BinnedEventArray.</p> <p>Smoothing is applied within each interval.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedEventArray</code> <p>New BinnedEventArray with smoothed data.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(\n    self, *, sigma=None, inplace=False, truncate=None, within_intervals=False\n):\n    \"\"\"Smooth BinnedEventArray by convolving with a Gaussian kernel.\n\n    Smoothing is applied in data, and the same smoothing is applied\n    to each series in a BinnedEventArray.\n\n    Smoothing is applied within each interval.\n\n    Parameters\n    ----------\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.01 (10 ms)\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n\n    Returns\n    -------\n    out : BinnedEventArray\n        New BinnedEventArray with smoothed data.\n    \"\"\"\n\n    if truncate is None:\n        truncate = 4\n    if sigma is None:\n        sigma = 0.01  # 10 ms default\n\n    fs = 1 / self.ds\n\n    return utils.gaussian_filter(\n        self,\n        fs=fs,\n        sigma=sigma,\n        truncate=truncate,\n        inplace=inplace,\n        within_intervals=within_intervals,\n    )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.standardize","title":"<code>standardize(inplace=False)</code>","text":"<p>Standardize data (zero mean and unit std deviation).</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def standardize(self, inplace=False):\n    \"\"\"Standardize data (zero mean and unit std deviation).\"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    std = out.std()\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedEventArray.std","title":"<code>std(*, axis=1)</code>","text":"<p>Returns the standard deviation of each series in BinnedEventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def std(self, *, axis=1):\n    \"\"\"Returns the standard deviation of each series in BinnedEventArray.\"\"\"\n    try:\n        stds = np.nanstd(self.data, axis=axis).squeeze()\n        if stds.size == 1:\n            return stds.item()\n        return stds\n    except IndexError:\n        raise IndexError(\n            \"Empty BinnedEventArray; cannot calculate standard deviation\"\n        )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.BinnedSpikeTrainArray","title":"<code>BinnedSpikeTrainArray</code>","text":"<p>               Bases: <code>BinnedEventArray</code></p> <p>Binned spike train array for analyzing neural spike data.</p> <p>A specialized version of BinnedEventArray designed specifically for spike train analysis. This class bins spike events into discrete time intervals and provides spike train-specific methods and properties through aliased attribute names.</p> <p>Parameters:</p> Name Type Description Default <code>eventarray</code> <code>EventArray or RegularlySampledAnalogSignalArray</code> <p>Input spike train data to be binned.</p> required <code>ds</code> <code>float</code> <p>The bin width, in seconds. Default is 0.0625 (62.5 ms).</p> required <code>empty</code> <code>bool</code> <p>Whether an empty BinnedSpikeTrainArray should be constructed (no data). Default is False.</p> required <code>fs</code> <code>float</code> <p>Sampling rate in Hz. If fs is passed as a parameter, then data is assumed to be in sample numbers instead of actual time values.</p> required <code>support</code> <code>IntervalArray</code> <p>The support (time intervals) over which the spike trains are defined.</p> required <code>unit_ids</code> <code>list of int</code> <p>Unit IDs for each spike train. Default creates sequential IDs starting from 1.</p> required <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Default casts unit_ids to str.</p> required <code>unit_tags</code> <code>optional</code> <p>Tags corresponding to units. Currently accepts any type.</p> required <code>label</code> <code>str</code> <p>Information pertaining to the source of the spike train data. Default is None.</p> required <code>abscissa</code> <code>TemporalAbscissa</code> <p>Object for the time (x-axis) coordinate. Default creates TemporalAbscissa.</p> required <code>ordinate</code> <code>AnalogSignalArrayOrdinate</code> <p>Object for the signal (y-axis) coordinate. Default creates AnalogSignalArrayOrdinate.</p> required <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments passed to the parent BinnedEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BinnedEventArray parent class for additional</code> <code>attributes that are defined there.</code> <code>Spike train-specific attributes (aliases)</code> <code>time</code> <code>array</code> <p>Alias for data. Spike counts in all bins, with shape (n_units, n_bins).</p> <code>n_epochs</code> <code>int</code> <p>Alias for n_intervals. The number of underlying time intervals.</p> <code>n_units</code> <code>int</code> <p>Alias for n_series. The number of units (neurons).</p> <code>n_spikes</code> <code>ndarray</code> <p>Alias for n_events. The number of spikes in each unit.</p> <code>unit_ids</code> <code>list of int</code> <p>Alias for series_ids. Unit IDs contained in the spike train array.</p> <code>unit_labels</code> <code>list of str</code> <p>Alias for series_labels. Labels corresponding to units.</p> <code>unit_tags</code> <p>Alias for series_tags. Tags corresponding to units.</p> <code>Inherited attributes</code> <code>isempty</code> <code>bool</code> <p>Whether the BinnedSpikeTrainArray is empty (no data).</p> <code>bin_centers</code> <code>ndarray</code> <p>The bin centers, in seconds.</p> <code>data</code> <code>np.array, with shape (n_units, n_bins)</code> <p>Spike counts in all bins.</p> <code>bins</code> <code>ndarray</code> <p>The bin edges, in seconds.</p> <code>binned_support</code> <code>np.ndarray, with shape (n_intervals, 2)</code> <p>The binned support of the array (in bin IDs).</p> <code>lengths</code> <code>ndarray</code> <p>Lengths of contiguous segments, in number of bins.</p> <code>eventarray</code> <code>EventArray</code> <p>The original EventArray associated with the binned spike data.</p> <code>n_bins</code> <code>int</code> <p>The number of bins.</p> <code>ds</code> <code>float</code> <p>Bin width, in seconds.</p> <code>n_active</code> <code>int</code> <p>The number of active units. A unit is considered active if it fired at least one spike.</p> <code>n_active_per_bin</code> <code>np.ndarray, with shape (n_bins, )</code> <p>Number of active units per data bin.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the BinnedSpikeTrainArray.</p> <p>Methods:</p> Name Description <code>All methods from BinnedEventArray are available, plus spike train-specific</code> <code>aliases for method names:</code> <code>reorder_units_by_ids</code> <p>Alias for reorder_series_by_ids. Reorder units by their IDs.</p> <code>reorder_units</code> <p>Alias for reorder_series. Reorder units.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; # Create a BinnedSpikeTrainArray from spike times\n&gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n&gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n&gt;&gt;&gt; bst = nel.BinnedSpikeTrainArray(sta, ds=0.1)\n&gt;&gt;&gt; print(bst.n_units)\n2\n&gt;&gt;&gt; print(bst.n_bins)\n7\n&gt;&gt;&gt; print(bst.time.shape)  # alias for data\n(2, 7)\n</code></pre> See Also <p>BinnedEventArray : Parent class for general event arrays EventArray : Unbinned event array class SpikeTrainArray : Unbinned spike train array class</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class BinnedSpikeTrainArray(BinnedEventArray):\n    \"\"\"Binned spike train array for analyzing neural spike data.\n\n    A specialized version of BinnedEventArray designed specifically for spike train\n    analysis. This class bins spike events into discrete time intervals and provides\n    spike train-specific methods and properties through aliased attribute names.\n\n    Parameters\n    ----------\n    eventarray : nelpy.EventArray or nelpy.RegularlySampledAnalogSignalArray, optional\n        Input spike train data to be binned.\n    ds : float, optional\n        The bin width, in seconds. Default is 0.0625 (62.5 ms).\n    empty : bool, optional\n        Whether an empty BinnedSpikeTrainArray should be constructed (no data).\n        Default is False.\n    fs : float, optional\n        Sampling rate in Hz. If fs is passed as a parameter, then data\n        is assumed to be in sample numbers instead of actual time values.\n    support : nelpy.IntervalArray, optional\n        The support (time intervals) over which the spike trains are defined.\n    unit_ids : list of int, optional\n        Unit IDs for each spike train. Default creates sequential IDs starting from 1.\n    unit_labels : list of str, optional\n        Labels corresponding to units. Default casts unit_ids to str.\n    unit_tags : optional\n        Tags corresponding to units. Currently accepts any type.\n    label : str, optional\n        Information pertaining to the source of the spike train data.\n        Default is None.\n    abscissa : nelpy.TemporalAbscissa, optional\n        Object for the time (x-axis) coordinate. Default creates TemporalAbscissa.\n    ordinate : nelpy.AnalogSignalArrayOrdinate, optional\n        Object for the signal (y-axis) coordinate. Default creates AnalogSignalArrayOrdinate.\n    **kwargs : optional\n        Additional keyword arguments passed to the parent BinnedEventArray constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BinnedEventArray parent class for additional\n    attributes that are defined there.\n\n    Spike train-specific attributes (aliases):\n    time : np.array\n        Alias for data. Spike counts in all bins, with shape (n_units, n_bins).\n    n_epochs : int\n        Alias for n_intervals. The number of underlying time intervals.\n    n_units : int\n        Alias for n_series. The number of units (neurons).\n    n_spikes : np.ndarray\n        Alias for n_events. The number of spikes in each unit.\n    unit_ids : list of int\n        Alias for series_ids. Unit IDs contained in the spike train array.\n    unit_labels : list of str\n        Alias for series_labels. Labels corresponding to units.\n    unit_tags :\n        Alias for series_tags. Tags corresponding to units.\n\n    Inherited attributes:\n    isempty : bool\n        Whether the BinnedSpikeTrainArray is empty (no data).\n    bin_centers : np.ndarray\n        The bin centers, in seconds.\n    data : np.array, with shape (n_units, n_bins)\n        Spike counts in all bins.\n    bins : np.ndarray\n        The bin edges, in seconds.\n    binned_support : np.ndarray, with shape (n_intervals, 2)\n        The binned support of the array (in bin IDs).\n    lengths : np.ndarray\n        Lengths of contiguous segments, in number of bins.\n    eventarray : nelpy.EventArray\n        The original EventArray associated with the binned spike data.\n    n_bins : int\n        The number of bins.\n    ds : float\n        Bin width, in seconds.\n    n_active : int\n        The number of active units. A unit is considered active if\n        it fired at least one spike.\n    n_active_per_bin : np.ndarray, with shape (n_bins, )\n        Number of active units per data bin.\n    support : nelpy.IntervalArray\n        The support of the BinnedSpikeTrainArray.\n\n    Methods\n    -------\n    All methods from BinnedEventArray are available, plus spike train-specific\n    aliases for method names:\n\n    reorder_units_by_ids(*args, **kwargs)\n        Alias for reorder_series_by_ids. Reorder units by their IDs.\n    reorder_units(*args, **kwargs)\n        Alias for reorder_series. Reorder units.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; # Create a BinnedSpikeTrainArray from spike times\n    &gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n    &gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n    &gt;&gt;&gt; bst = nel.BinnedSpikeTrainArray(sta, ds=0.1)\n    &gt;&gt;&gt; print(bst.n_units)\n    2\n    &gt;&gt;&gt; print(bst.n_bins)\n    7\n    &gt;&gt;&gt; print(bst.time.shape)  # alias for data\n    (2, 7)\n\n    See Also\n    --------\n    BinnedEventArray : Parent class for general event arrays\n    EventArray : Unbinned event array class\n    SpikeTrainArray : Unbinned spike train array class\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        # 'get_event_firing_order' : 'get_spike_firing_order'\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EpochArray","title":"<code>EpochArray</code>","text":"<p>               Bases: <code>IntervalArray</code></p> <p>IntervalArray containing temporal intervals (epochs, in seconds).</p> <p>This class extends <code>IntervalArray</code> to specifically handle time-based intervals, referred to as epochs. It provides aliases for common time-related attributes and uses a <code>PrettyDuration</code> formatter for displaying lengths.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_epochs, 1) or (n_epochs,), the start time for each epoch (which then requires a <code>length</code> to be specified). If shape (n_epochs, 2), the start and stop times for each epoch. Defaults to None, creating an empty <code>EpochArray</code>.</p> required <code>length</code> <code>np.array, float, or None</code> <p>The duration of the epoch (in base units, seconds). If a float, the same duration is assumed for every epoch. Only used if <code>data</code> is a 1D array of start times.</p> required <code>meta</code> <code>dict</code> <p>Metadata associated with the epoch array.</p> required <code>empty</code> <code>bool</code> <p>If True, an empty <code>EpochArray</code> is returned, ignoring <code>data</code> and <code>length</code>. Defaults to False.</p> required <code>domain</code> <code>IntervalArray</code> <p>The domain within which the epochs are defined. If None, it defaults to an infinite domain.</p> required <code>label</code> <code>str</code> <p>A descriptive label for the epoch array.</p> required <p>Attributes:</p> Name Type Description <code>time</code> <code>array</code> <p>Alias for <code>data</code>. The start and stop times for each epoch, with shape (n_epochs, 2).</p> <code>n_epochs</code> <code>int</code> <p>Alias for <code>n_intervals</code>. The number of epochs in the array.</p> <code>duration</code> <code>float</code> <p>Alias for <code>length</code>. The total duration of the [merged] epoch array.</p> <code>durations</code> <code>array</code> <p>Alias for <code>lengths</code>. The duration of each individual epoch.</p> <code>formatter</code> <code>PrettyDuration</code> <p>The formatter used for displaying time durations.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the intervals, which is 's' (seconds) for EpochArray.</p> Notes <p>This class inherits all methods and properties from <code>IntervalArray</code>. Aliases are provided for convenience to make the API more intuitive for temporal data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy.core import EpochArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create an EpochArray from start and stop times\n&gt;&gt;&gt; epochs = EpochArray(data=np.array([[0, 10], [20, 30], [40, 50]]))\n&gt;&gt;&gt; print(epochs)\n&lt;EpochArray at 0x21b641f0950: 3 epochs&gt; of length 30 seconds\n</code></pre> <pre><code>&gt;&gt;&gt; # Create an EpochArray from start times and a common length\n&gt;&gt;&gt; starts = np.array([0, 20, 40])\n&gt;&gt;&gt; length = 5.0\n&gt;&gt;&gt; epochs_with_length = EpochArray(data=starts, length=length)\n&gt;&gt;&gt; print(epochs_with_length)\n&lt;EpochArray at 0x21b631c6050: 3 epochs&gt; of length 15 seconds\n</code></pre> <pre><code>&gt;&gt;&gt; # Accessing aliased attributes\n&gt;&gt;&gt; print(f\"Number of epochs: {epochs.n_epochs}\")\nNumber of epochs: 3\n&gt;&gt;&gt; print(f\"Total duration: {epochs.duration}\")\nTotal duration: 30 seconds\n&gt;&gt;&gt; print(f\"Individual durations: {epochs.durations}\")\nIndividual durations: [10 10 10]\n</code></pre> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class EpochArray(IntervalArray):\n    \"\"\"IntervalArray containing temporal intervals (epochs, in seconds).\n\n    This class extends `IntervalArray` to specifically handle time-based\n    intervals, referred to as epochs. It provides aliases for common\n    time-related attributes and uses a `PrettyDuration` formatter for\n    displaying lengths.\n\n    Parameters\n    ----------\n    data : np.array, optional\n        If shape (n_epochs, 1) or (n_epochs,), the start time for each\n        epoch (which then requires a `length` to be specified).\n        If shape (n_epochs, 2), the start and stop times for each epoch.\n        Defaults to None, creating an empty `EpochArray`.\n    length : np.array, float, or None, optional\n        The duration of the epoch (in base units, seconds). If a float,\n        the same duration is assumed for every epoch. Only used if `data`\n        is a 1D array of start times.\n    meta : dict, optional\n        Metadata associated with the epoch array.\n    empty : bool, optional\n        If True, an empty `EpochArray` is returned, ignoring `data` and `length`.\n        Defaults to False.\n    domain : IntervalArray, optional\n        The domain within which the epochs are defined. If None, it defaults\n        to an infinite domain.\n    label : str, optional\n        A descriptive label for the epoch array.\n\n    Attributes\n    ----------\n    time : np.array\n        Alias for `data`. The start and stop times for each epoch, with shape\n        (n_epochs, 2).\n    n_epochs : int\n        Alias for `n_intervals`. The number of epochs in the array.\n    duration : float\n        Alias for `length`. The total duration of the [merged] epoch array.\n    durations : np.array\n        Alias for `lengths`. The duration of each individual epoch.\n    formatter : formatters.PrettyDuration\n        The formatter used for displaying time durations.\n    base_unit : str\n        The base unit of the intervals, which is 's' (seconds) for EpochArray.\n\n    Notes\n    -----\n    This class inherits all methods and properties from `IntervalArray`.\n    Aliases are provided for convenience to make the API more intuitive\n    for temporal data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy.core import EpochArray\n\n    &gt;&gt;&gt; # Create an EpochArray from start and stop times\n    &gt;&gt;&gt; epochs = EpochArray(data=np.array([[0, 10], [20, 30], [40, 50]]))\n    &gt;&gt;&gt; print(epochs)\n    &lt;EpochArray at 0x21b641f0950: 3 epochs&gt; of length 30 seconds\n\n    &gt;&gt;&gt; # Create an EpochArray from start times and a common length\n    &gt;&gt;&gt; starts = np.array([0, 20, 40])\n    &gt;&gt;&gt; length = 5.0\n    &gt;&gt;&gt; epochs_with_length = EpochArray(data=starts, length=length)\n    &gt;&gt;&gt; print(epochs_with_length)\n    &lt;EpochArray at 0x21b631c6050: 3 epochs&gt; of length 15 seconds\n\n    &gt;&gt;&gt; # Accessing aliased attributes\n    &gt;&gt;&gt; print(f\"Number of epochs: {epochs.n_epochs}\")\n    Number of epochs: 3\n    &gt;&gt;&gt; print(f\"Total duration: {epochs.duration}\")\n    Total duration: 30 seconds\n    &gt;&gt;&gt; print(f\"Individual durations: {epochs.durations}\")\n    Individual durations: [10 10 10]\n    \"\"\"\n\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"duration\": \"length\",\n        \"durations\": \"lengths\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n        self._interval_label = \"epoch\"\n        self.formatter = formatters.PrettyDuration\n        self.base_unit = self.formatter.base_unit\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray","title":"<code>EventArray</code>","text":"<p>               Bases: <code>BaseEventArray</code></p> <p>A multiseries eventarray with shared support.</p> <p>Parameters:</p> Name Type Description Default <code>abscissa_vals</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,).</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000.</p> <code>None</code> <code>support</code> <code>IntervalArray</code> <p>IntervalArray on which eventarrays are defined. Default is [0, last event] inclusive.</p> <code>None</code> <code>series_ids</code> <code>list of int</code> <p>Unit IDs.</p> <code>None</code> <code>series_labels</code> <code>list of str</code> <p>Labels corresponding to series. Default casts series_ids to str.</p> <code>None</code> <code>series_tags</code> <code>optional</code> <p>Tags correponding to series. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> <code>None</code> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Whether an empty EventArray should be constructed (no data).</p> <code>False</code> <code>assume_sorted</code> <code>boolean</code> <p>Whether the abscissa values should be treated as sorted (non-decreasing) or not. Significant overhead during RSASA object creation can be removed if this is True, but note that unsorted abscissa values will mess everything up. Default is False</p> <code>None</code> <code>kwargs</code> <code>optional</code> <p>Additional keyword arguments to forward along to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray superclass for additional</code> <code>attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the EventArray is empty (no data).</p> <code>n_series</code> <code>int</code> <p>The number of series.</p> <code>n_active</code> <code>int</code> <p>The number of active series. A series is considered active if it fired at least one event.</p> <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,).</p> <code>n_events</code> <code>ndarray</code> <p>The number of events in each series.</p> <code>issorted</code> <code>bool</code> <p>Whether the data are sorted.</p> <code>first_event</code> <code>float</code> <p>The time of the very first event, across all series.</p> <code>last_event</code> <code>float</code> <p>The time of the very last event, across all series.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class EventArray(BaseEventArray):\n    \"\"\"A multiseries eventarray with shared support.\n\n    Parameters\n    ----------\n    abscissa_vals : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,).\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000.\n    support : IntervalArray, optional\n        IntervalArray on which eventarrays are defined.\n        Default is [0, last event] inclusive.\n    series_ids : list of int, optional\n        Unit IDs.\n    series_labels : list of str, optional\n        Labels corresponding to series. Default casts series_ids to str.\n    series_tags : optional\n        Tags correponding to series.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    label : str or None, optional\n        Information pertaining to the source of the eventarray.\n    empty : bool, optional\n        Whether an empty EventArray should be constructed (no data).\n    assume_sorted : boolean, optional\n        Whether the abscissa values should be treated as sorted (non-decreasing)\n        or not. Significant overhead during RSASA object creation can be removed\n        if this is True, but note that unsorted abscissa values will mess\n        everything up.\n        Default is False\n    kwargs : optional\n        Additional keyword arguments to forward along to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray superclass for additional\n    attributes that are defined there.\n    isempty : bool\n        Whether the EventArray is empty (no data).\n    n_series : int\n        The number of series.\n    n_active : int\n        The number of active series. A series is considered active if\n        it fired at least one event.\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,).\n    n_events : np.ndarray\n        The number of events in each series.\n    issorted : bool\n        Whether the data are sorted.\n    first_event : np.float\n        The time of the very first event, across all series.\n    last_event : np.float\n        The time of the very last event, across all series.\n    \"\"\"\n\n    __attributes__ = [\"_data\"]\n    __attributes__.extend(BaseEventArray.__attributes__)\n\n    def __init__(\n        self,\n        abscissa_vals=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        series_labels=None,\n        series_tags=None,\n        label=None,\n        empty=False,\n        assume_sorted=None,\n        **kwargs,\n    ):\n        if assume_sorted is None:\n            assume_sorted = False\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.warning(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            data = np.array(data, dtype=object)\n            try:\n                if data.shape[-1] &lt; 2 and np.max(data.shape) &gt; 1:\n                    return True\n                if max(np.array(data).shape[:-1]) &gt; 1 and data.shape[-1] == 1:\n                    return True\n            except (IndexError, TypeError, ValueError):\n                return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            --------\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            try:\n                if isinstance(data[0][0], list) or isinstance(data[0][0], np.ndarray):\n                    logging.info(\"event datas input has too many layers!\")\n                    try:\n                        if max(np.array(data).shape[:-1]) &gt; 1:\n                            #                 singletons = True\n                            return False\n                    except ValueError:\n                        return False\n                    data = np.squeeze(data)\n            except (IndexError, TypeError):\n                pass\n            try:\n                if isinstance(data[1], list) or isinstance(data[1], np.ndarray):\n                    return False\n            except (IndexError, TypeError):\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            if is_single_series(data):\n                return np.array(np.squeeze(data), ndmin=2)\n            if is_singletons(data):\n                data = np.squeeze(data)\n                n = np.max(data.shape)\n                if len(data.shape) == 1:\n                    m = 1\n                else:\n                    m = np.min(data.shape)\n                data = np.reshape(data, (n, m))\n            else:\n                data = np.squeeze(data)\n                if data.dtype == np.dtype(\"O\"):\n                    jagged = True\n                else:\n                    jagged = False\n                if jagged:  # jagged array\n                    # standardize input so that a list of lists is converted\n                    # to an array of arrays:\n                    data = np.array([np.asarray(st) for st in data], dtype=object)\n                else:\n                    data = np.array(data, ndmin=2)\n            return data\n\n        # standardize input data to 2D array\n        data = standardize_to_2d(np.array(abscissa_vals, dtype=object))\n\n        # If user said to assume the absicssa vals are sorted but they actually\n        # aren't, then the mistake will get propagated down. The responsibility\n        # therefore lies on the user whenever he/she uses assume_sorted=True\n        # as a constructor argument\n        for ii, train in enumerate(data):\n            if not assume_sorted:\n                # sort event series, but only if necessary\n                if not utils.is_sorted(train):\n                    data[ii] = np.sort(train)\n            else:\n                data[ii] = np.sort(train)\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n        kwargs[\"series_labels\"] = series_labels\n        kwargs[\"series_tags\"] = series_tags\n        kwargs[\"label\"] = label\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # print(self.type_name, kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            first_spk = np.nanmin(\n                np.array([series[0] for series in data if len(series) != 0])\n            )\n            # BUG: if eventseries is empty np.array([]) then series[-1]\n            # raises an error in the following:\n            # FIX: list[-1] raises an IndexError for an empty list,\n            # whereas list[-1:] returns an empty list.\n            last_spk = np.nanmax(\n                np.array([series[-1:] for series in data if len(series) != 0])\n            )\n            self.support = type(self._abscissa.support)(\n                np.array([first_spk, last_spk + 1 / fs])\n            )\n            # in the above, there's no reason to restrict to support\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            self.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        self._restrict_to_interval(self._abscissa.support, data=data)\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without event datas.\n        Note: the support is left unchanged.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._data = np.array(self.n_series * [None])\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Returns a copy of the EventArray.\"\"\"\n        newcopy = copy.deepcopy(self)\n        newcopy.__renew__()\n        return newcopy\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to EventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty EventArray.\"\"\"\n        try:\n            return np.sum([len(st) for st in self.data]) == 0\n        except TypeError:\n            return True  # this happens when self.data is None\n\n    @property\n    def n_series(self):\n        \"\"\"(int) The number of series.\"\"\"\n        try:\n            return utils.PrettyInt(len(self.data))\n        except TypeError:\n            return 0\n\n    @property\n    def n_active(self):\n        \"\"\"(int) The number of active series.\n\n        A series is considered active if it fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    def flatten(self, *, series_id=None, series_label=None):\n        \"\"\"Collapse events across series.\n\n        WARNING! series_tags are thrown away when flattening.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        series_label (str)\n            (series) Label for event series, default is 'flattened'.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n        if series_label is None:\n            series_label = \"flattened\"\n\n        flattened = self._copy_without_data()\n        flattened._series_ids = [series_id]\n        flattened._series_labels = [series_label]\n        flattened._series_tags = None\n\n        # Efficient: concatenate all events, sort once\n        all_events = np.concatenate(self.data)\n        all_events.sort()\n        flattened._data = np.array([all_events], ndmin=2)\n        flattened.__renew__()\n        return flattened\n\n    def _restrict(self, intervalslice, seriesslice, *, subseriesslice=None):\n        self._restrict_to_series_subset(seriesslice)\n        self._restrict_to_interval(intervalslice)\n        return self\n\n    def _restrict_to_series_subset(self, idx):\n        # Warning: This function can mutate data\n\n        # TODO: Update tags\n        try:\n            self._data = self._data[idx]\n            singleseries = len(self._data) == 1\n            if singleseries:\n                self._data = np.array(self._data[0], ndmin=2)\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except AttributeError:\n            self._data = self._data[idx]\n            singleseries = len(self._data) == 1\n            if singleseries:\n                self._data = np.array(self._data[0], ndmin=2)\n            self._series_ids = list(np.atleast_1d(np.atleast_1d(self._series_ids)[idx]))\n            self._series_labels = list(\n                np.atleast_1d(np.atleast_1d(self._series_labels)[idx])\n            )\n        except IndexError:\n            raise IndexError(\n                \"One of more indices were out of bounds for n_series with size {}\".format(\n                    self.n_series\n                )\n            )\n        except Exception:\n            raise TypeError(\"Unsupported indexing type {}\".format(type(idx)))\n\n        return self\n\n    def _restrict_to_interval(self, intervalslice, *, data=None):\n        \"\"\"Return data restricted to an intervalarray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : nelpy.IntervalArray\n        \"\"\"\n\n        # Warning: this function can mutate data\n        # This should be called from _restrict only. That's where\n        # intervalarray is first checked against the support.\n        # This function assumes that has happened already, so\n        # every point in intervalarray is also in the support\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n\n        if data is None:\n            data = self._data\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                # no restriction on interval\n                return self\n\n        newintervals = self._abscissa.support[intervalslice].merge()\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=True)\n\n        issue_warning = False\n        if not self.isempty:\n            for series, evt_data in enumerate(data):\n                indices = []\n                for epdata in newintervals.data:\n                    t_start = epdata[0]\n                    t_stop = epdata[1]\n                    frm, to = np.searchsorted(evt_data, (t_start, t_stop))\n                    indices.append((frm, to))\n                indices = np.array(indices, ndmin=2)\n                if np.diff(indices).sum() &lt; len(evt_data):\n                    issue_warning = True\n                singleseries = len(self._data) == 1\n                if singleseries:\n                    data_list = []\n                    for start, stop in indices:\n                        data_list.extend(evt_data[start:stop])\n                    data = np.array(data_list, ndmin=2)\n                else:\n                    # here we have to do some annoying conversion between\n                    # arrays and lists to fully support jagged array\n                    # mutation\n                    data_list = []\n                    for start, stop in indices:\n                        data_list.extend(evt_data[start:stop])\n                    data_ = data.tolist()  # this creates copy\n                    data_[series] = np.array(data_list)\n                    data = utils.ragged_array(data_)\n            self._data = data\n            if issue_warning:\n                logging.warning(\"ignoring events outside of eventarray support\")\n\n        self._abscissa.support = newintervals\n        return self\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        if self.label is not None:\n            labelstr = \" from %s\" % self.label\n        else:\n            labelstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s%s\" % (\n            self.type_name,\n            address_str,\n            numstr,\n            epstr,\n            fsstr,\n            labelstr,\n        )\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a binned eventarray.\"\"\"\n        return BinnedEventArray(self, ds=ds)\n\n    @property\n    def data(self):\n        \"\"\"Event datas in seconds.\"\"\"\n        return self._data\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.data])\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Sorted EventArray.\"\"\"\n        if self.isempty:\n            return True\n        return np.array([utils.is_sorted(eventarray) for eventarray in self.data]).all()\n\n    def _reorder_series_by_idx(self, neworder, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,)\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            out._series_labels[frm], out._series_labels[to] = (\n                out._series_labels[to],\n                out._series_labels[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def reorder_series(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        raise DeprecationWarning(\n            \"reorder_series has been deprecated. Use reorder_series_by_id(x/s) instead!\"\n        )\n\n    def reorder_series_by_ids(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        neworder = [self.series_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            out._series_labels[frm], out._series_labels[to] = (\n                out._series_labels[to],\n                out._series_labels[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def get_event_firing_order(self):\n        \"\"\"Returns a list of series_ids such that the series are ordered\n        by when they first fire in the EventArray.\n\n        Return\n        ------\n        firing_order : list of series_ids\n        \"\"\"\n\n        first_events = [\n            (ii, series[0]) for (ii, series) in enumerate(self.data) if len(series) != 0\n        ]\n        first_events_series_ids = np.array(self.series_ids)[\n            [fs[0] for fs in first_events]\n        ]\n        first_events_datas = np.array([fs[1] for fs in first_events])\n        sortorder = np.argsort(first_events_datas)\n        first_events_series_ids = first_events_series_ids[sortorder]\n        remaining_ids = list(set(self.series_ids) - set(first_events_series_ids))\n        firing_order = list(first_events_series_ids)\n        firing_order.extend(remaining_ids)\n\n        return firing_order\n\n    @property\n    def first_event(self):\n        \"\"\"Returns the [time of the] first event across all series.\"\"\"\n        first = np.inf\n        for series in self.data:\n            if series[0] &lt; first:\n                first = series[0]\n        return first\n\n    @property\n    def last_event(self):\n        \"\"\"Returns the [time of the] last event across all series.\"\"\"\n        last = -np.inf\n        for series in self.data:\n            if series[-1] &gt; last:\n                last = series[-1]\n        return last\n\n    def empty(self, *, inplace=False):\n        \"\"\"Remove data (but not metadata) from EventArray.\n\n        Attributes 'data', and 'support' are both emptied.\n\n        Note: n_series, series_ids, etc. are all preserved.\n        \"\"\"\n        if not inplace:\n            out = self._copy_without_data()\n            out._abscissa.support = type(self._abscissa.support)(empty=True)\n            return out\n        out = self\n        out._data = np.array(self.n_series * [None])\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        out.__renew__()\n        return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Event datas in seconds.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.first_event","title":"<code>first_event</code>  <code>property</code>","text":"<p>Returns the [time of the] first event across all series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty EventArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Sorted EventArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.last_event","title":"<code>last_event</code>  <code>property</code>","text":"<p>Returns the [time of the] last event across all series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>(int) The number of active series.</p> <p>A series is considered active if it fired at least one event.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.n_series","title":"<code>n_series</code>  <code>property</code>","text":"<p>(int) The number of series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a binned eventarray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a binned eventarray.\"\"\"\n    return BinnedEventArray(self, ds=ds)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the EventArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of the EventArray.\"\"\"\n    newcopy = copy.deepcopy(self)\n    newcopy.__renew__()\n    return newcopy\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.empty","title":"<code>empty(*, inplace=False)</code>","text":"<p>Remove data (but not metadata) from EventArray.</p> <p>Attributes 'data', and 'support' are both emptied.</p> <p>Note: n_series, series_ids, etc. are all preserved.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def empty(self, *, inplace=False):\n    \"\"\"Remove data (but not metadata) from EventArray.\n\n    Attributes 'data', and 'support' are both emptied.\n\n    Note: n_series, series_ids, etc. are all preserved.\n    \"\"\"\n    if not inplace:\n        out = self._copy_without_data()\n        out._abscissa.support = type(self._abscissa.support)(empty=True)\n        return out\n    out = self\n    out._data = np.array(self.n_series * [None])\n    out._abscissa.support = type(self._abscissa.support)(empty=True)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.flatten","title":"<code>flatten(*, series_id=None, series_label=None)</code>","text":"<p>Collapse events across series.</p> <p>WARNING! series_tags are thrown away when flattening.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> <code>series_label</code> <p>(series) Label for event series, default is 'flattened'.</p> <code>None</code> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def flatten(self, *, series_id=None, series_label=None):\n    \"\"\"Collapse events across series.\n\n    WARNING! series_tags are thrown away when flattening.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    series_label (str)\n        (series) Label for event series, default is 'flattened'.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n    if series_label is None:\n        series_label = \"flattened\"\n\n    flattened = self._copy_without_data()\n    flattened._series_ids = [series_id]\n    flattened._series_labels = [series_label]\n    flattened._series_tags = None\n\n    # Efficient: concatenate all events, sort once\n    all_events = np.concatenate(self.data)\n    all_events.sort()\n    flattened._data = np.array([all_events], ndmin=2)\n    flattened.__renew__()\n    return flattened\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.get_event_firing_order","title":"<code>get_event_firing_order()</code>","text":"<p>Returns a list of series_ids such that the series are ordered by when they first fire in the EventArray.</p> Return <p>firing_order : list of series_ids</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def get_event_firing_order(self):\n    \"\"\"Returns a list of series_ids such that the series are ordered\n    by when they first fire in the EventArray.\n\n    Return\n    ------\n    firing_order : list of series_ids\n    \"\"\"\n\n    first_events = [\n        (ii, series[0]) for (ii, series) in enumerate(self.data) if len(series) != 0\n    ]\n    first_events_series_ids = np.array(self.series_ids)[\n        [fs[0] for fs in first_events]\n    ]\n    first_events_datas = np.array([fs[1] for fs in first_events])\n    sortorder = np.argsort(first_events_datas)\n    first_events_series_ids = first_events_series_ids[sortorder]\n    remaining_ids = list(set(self.series_ids) - set(first_events_series_ids))\n    firing_order = list(first_events_series_ids)\n    firing_order.extend(remaining_ids)\n\n    return firing_order\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.reorder_series","title":"<code>reorder_series(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def reorder_series(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    raise DeprecationWarning(\n        \"reorder_series has been deprecated. Use reorder_series_by_id(x/s) instead!\"\n    )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.EventArray.reorder_series_by_ids","title":"<code>reorder_series_by_ids(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def reorder_series_by_ids(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    neworder = [self.series_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        utils.swap_rows(out._data, frm, to)\n        out._series_ids[frm], out._series_ids[to] = (\n            out._series_ids[to],\n            out._series_ids[frm],\n        )\n        out._series_labels[frm], out._series_labels[to] = (\n            out._series_labels[to],\n            out._series_labels[frm],\n        )\n        # TODO: re-build series tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IMUSensorArray","title":"<code>IMUSensorArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>Array for storing IMU (Inertial Measurement Unit) sensor data with regular sampling rates.</p> <p>This class extends RegularlySampledAnalogSignalArray for IMU-specific data, such as accelerometer, gyroscope, and magnetometer signals.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>__aliases__</code> <code>dict</code> <p>Dictionary of class-specific aliases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; imu = IMUSensorArray(data=[[0, 1, 2], [3, 4, 5]], fs=100)\n&gt;&gt;&gt; imu.data\narray([[0, 1, 2],\n       [3, 4, 5]])\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class IMUSensorArray(RegularlySampledAnalogSignalArray):\n    \"\"\"\n    Array for storing IMU (Inertial Measurement Unit) sensor data with regular sampling rates.\n\n    This class extends RegularlySampledAnalogSignalArray for IMU-specific data, such as accelerometer, gyroscope, and magnetometer signals.\n\n    Parameters\n    ----------\n    *args :\n        Positional arguments passed to the parent class.\n    **kwargs :\n        Keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    __aliases__ : dict\n        Dictionary of class-specific aliases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; imu = IMUSensorArray(data=[[0, 1, 2], [3, 4, 5]], fs=100)\n    &gt;&gt;&gt; imu.data\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray","title":"<code>IntervalArray</code>","text":"<p>An array of intervals, where each interval has a start and stop.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_intervals, 1) or (n_intervals,), the start value for each interval (which then requires a length to be specified). If shape (n_intervals, 2), the start and stop values for each interval.</p> <code>None</code> <code>length</code> <code>np.array, float, or None</code> <p>The length of the interval (in base units). If (float) then the same length is assumed for every interval.</p> <code>None</code> <code>meta</code> <code>dict</code> <p>Metadata associated with spiketrain.</p> <code>None</code> <code>domain</code> <code>IntervalArray ??? This is pretty meta @-@</code> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>array</code> <p>The start and stop values for each interval. With shape (n_intervals, 2).</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class IntervalArray:\n    \"\"\"An array of intervals, where each interval has a start and stop.\n\n    Parameters\n    ----------\n    data : np.array\n        If shape (n_intervals, 1) or (n_intervals,), the start value for each\n        interval (which then requires a length to be specified).\n        If shape (n_intervals, 2), the start and stop values for each interval.\n    length : np.array, float, or None, optional\n        The length of the interval (in base units). If (float) then the same\n        length is assumed for every interval.\n    meta : dict, optional\n        Metadata associated with spiketrain.\n    domain : IntervalArray ??? This is pretty meta @-@\n\n    Attributes\n    ----------\n    data : np.array\n        The start and stop values for each interval. With shape (n_intervals, 2).\n    \"\"\"\n\n    __aliases__ = {}\n    __attributes__ = [\"_data\", \"_meta\", \"_domain\"]\n\n    def __init__(\n        self,\n        data=None,\n        *args,\n        length=None,\n        meta=None,\n        empty=False,\n        domain=None,\n        label=None,\n    ):\n        self.__version__ = version.__version__\n\n        self.type_name = self.__class__.__name__\n        self._interval_label = \"interval\"\n        self.formatter = formatters.ArbitraryFormatter\n        self.base_unit = self.formatter.base_unit\n\n        if len(args) &gt; 1:\n            raise TypeError(\n                \"__init__() takes from 1 to 3 positional arguments but 4 were given\"\n            )\n        elif len(args) == 1:\n            data = [data, args[0]]\n\n        # if an empty object is requested, return it:\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            return\n\n        data = np.squeeze(data)  # coerce data into np.array\n\n        # all possible inputs:\n        # 1. single interval, no length    --- OK\n        # 2. single interval and length    --- ERR\n        # 3. multiple intervals, no length --- OK\n        # 4. multiple intervals and length --- ERR\n        # 5. single scalar and length   --- OK\n        # 6. scalar list and duratin list --- OK\n        #\n        # Q. won't np.squeeze make our life difficult?\n        #\n        # Strategy: determine if length was passed. If so, try to see\n        # if data can be coerced into right shape. If not, raise\n        # error.\n        # If length was NOT passed, then do usual checks for intervals.\n\n        if length is not None:  # assume we received scalar starts\n            data = np.array(data, ndmin=1)\n            length = np.squeeze(length).astype(float)\n            if length.ndim == 0:\n                length = length[..., np.newaxis]\n\n            if data.ndim == 2 and length.ndim == 1:\n                raise ValueError(\"length not allowed when using start and stop values\")\n\n            if len(length) &gt; 1:\n                if data.ndim == 1 and data.shape[0] != length.shape[0]:\n                    raise ValueError(\"must have same number of data and length data\")\n            if data.ndim == 1 and length.ndim == 1:\n                stop_interval = data + length\n                data = np.hstack(\n                    (data[..., np.newaxis], stop_interval[..., np.newaxis])\n                )\n        else:  # length was not specified, so assume we recived intervals\n            # Note: if we have an empty array of data with no\n            # dimension, then calling len(data) will return a\n            # TypeError.\n            try:\n                # if no data were received, return an empty IntervalArray:\n                if len(data) == 0:\n                    self.__init__(empty=True)\n                    return\n            except TypeError:\n                logging.warning(\n                    \"unsupported type (\"\n                    + str(type(data))\n                    + \"); creating empty {}\".format(self.type_name)\n                )\n                self.__init__(empty=True)\n                return\n\n            # Only one interval is given eg IntervalArray([3,5,6,10]) with no\n            # length and more than two values:\n            if data.ndim == 1 and len(data) &gt; 2:  # we already know length is None\n                raise TypeError(\n                    \"data of size (n_intervals, ) has to be accompanied by a length\"\n                )\n\n            if data.ndim == 1:  # and length is None:\n                data = np.array([data])\n\n        if data.ndim &gt; 2:\n            raise ValueError(\"data must be a 1D or a 2D vector\")\n\n        try:\n            if data[:, 0].shape[0] != data[:, 1].shape[0]:\n                raise ValueError(\"must have the same number of start and stop values\")\n        except Exception:\n            raise Exception(\"Unhandled {}.__init__ case.\".format(self.type_name))\n\n        # TODO: what if start == stop? what will this break? This situation\n        # can arise automatically when slicing a spike train with one or no\n        # spikes, for example in which case the automatically inferred support\n        # is a delta dirac\n\n        if data.ndim == 2 and np.any(data[:, 1] - data[:, 0] &lt; 0):\n            raise ValueError(\"start must be less than or equal to stop\")\n\n        # potentially assign domain\n        self._domain = domain\n\n        self._data = data\n        self._meta = meta\n        self.label = label\n\n        if not self.issorted:\n            self._sort()\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self.n_intervals &gt; 1:\n            nstr = \"%s %ss\" % (self.n_intervals, self._interval_label)\n        else:\n            nstr = \"1 %s\" % self._interval_label\n        dstr = \"of length {}\".format(self.formatter(self.length))\n        return \"&lt;%s%s: %s&gt; %s\" % (self.type_name, address_str, nstr, dstr)\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without data.\"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._data = np.zeros((self.n_intervals, 2))\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        return out\n\n    def __iter__(self):\n        \"\"\"IntervalArray iterator initialization.\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"IntervalArray iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_intervals - 1:\n            raise StopIteration\n\n        intervalarray = self._copy_without_data()\n        intervalarray._data = np.array([self.data[index, :]])\n        self._index += 1\n        return intervalarray\n\n    def __getitem__(self, *idx):\n        \"\"\"IntervalArray index access.\n\n        Accepts integers, slices, and IntervalArrays.\n        \"\"\"\n        if self.isempty:\n            return self\n\n        idx = [ii for ii in idx]\n        if len(idx) == 1 and not isinstance(idx[0], int):\n            idx = idx[0]\n        if isinstance(idx, tuple):\n            idx = [ii for ii in idx]\n\n        if isinstance(idx, type(self)):\n            if idx.isempty:  # case 0:\n                return type(self)(empty=True)\n            return self.intersect(interval=idx)\n        elif isinstance(idx, IntervalArray):\n            raise TypeError(\n                \"Error taking intersection. {} expected, but got {}\".format(\n                    self.type_name, idx.type_name\n                )\n            )\n        else:\n            try:  # works for ints, lists, and slices\n                out = self.copy()\n                out._data = self.data[idx, :]\n            except IndexError:\n                raise IndexError(\"{} index out of range\".format(self.type_name))\n            except Exception:\n                raise TypeError(\"unsupported subscripting type {}\".format(type(idx)))\n        return out\n\n    def __add__(self, other):\n        \"\"\"add length to start and stop of each interval, or join two interval arrays without merging\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            return new.expand(other, direction=\"both\")\n        elif isinstance(other, type(self)):\n            return self.join(other)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __sub__(self, other):\n        \"\"\"subtract length from start and stop of each interval\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            return new.shrink(other, direction=\"both\")\n        elif isinstance(other, type(self)):\n            # A - B = A intersect ~B\n            return self.intersect(~other)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __mul__(self, other):\n        \"\"\"expand (&gt;1) or shrink (&lt;1) interval lengths\"\"\"\n        raise NotImplementedError(\"operator * not yet implemented\")\n\n    def __truediv__(self, other):\n        \"\"\"expand (&gt;1) or shrink (&gt;1) interval lengths\"\"\"\n        raise NotImplementedError(\"operator / not yet implemented\")\n\n    def __lshift__(self, other):\n        \"\"\"shift data to left (&lt;&lt;)\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            new._data = new._data - other\n            if new.domain.is_finite:\n                new.domain._data = new.domain._data - other\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &lt;&lt;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __rshift__(self, other):\n        \"\"\"shift data to right (&gt;&gt;)\"\"\"\n        if isinstance(other, numbers.Number):\n            new = copy.copy(self)\n            new._data = new._data + other\n            if new.domain.is_finite:\n                new.domain._data = new.domain._data + other\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &gt;&gt;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __and__(self, other):\n        \"\"\"intersection of interval arrays\"\"\"\n        if isinstance(other, type(self)):\n            new = copy.copy(self)\n            return new.intersect(other, boundaries=True)\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &amp;: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __or__(self, other):\n        \"\"\"join and merge interval array; set union\"\"\"\n        if isinstance(other, type(self)):\n            new = copy.copy(self)\n            joined = new.join(other)\n            union = joined.merge()\n            return union\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for |: {} and {}\".format(\n                    str(type(self)), str(type(other))\n                )\n            )\n\n    def __invert__(self):\n        \"\"\"complement within self.domain\"\"\"\n        return self.complement()\n\n    def __bool__(self):\n        \"\"\"(bool) Empty IntervalArray\"\"\"\n        return not self.isempty\n\n    def remove_duplicates(self, inplace=False):\n        \"\"\"Remove duplicate intervals.\"\"\"\n        raise NotImplementedError\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, *, ds=None, n_intervals=None):\n        \"\"\"Returns an IntervalArray that has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum length, for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : IntervalArray\n            IntervalArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        if self.isempty:\n            raise ValueError(\"cannot parition an empty object in a meaningful way!\")\n\n        if ds is not None and n_intervals is not None:\n            raise ValueError(\"ds and n_intervals cannot be used together\")\n\n        if n_intervals is not None:\n            assert float(n_intervals).is_integer(), (\n                \"n_intervals must be a positive integer!\"\n            )\n            assert n_intervals &gt; 1, \"n_intervals must be a positive integer &gt; 1\"\n            # determine ds from number of desired points:\n            ds = self.length / n_intervals\n\n        if ds is None:\n            # neither n_intervals nor ds was specified, so assume defaults:\n            n_intervals = 100\n            ds = self.length / n_intervals\n\n        # build list of points at which to esplit the IntervalArray\n        new_starts = []\n        new_stops = []\n        for start, stop in self.data:\n            newxvals = utils.frange(start, stop, step=ds).tolist()\n            # newxvals = np.arange(start, stop, step=ds).tolist()\n            if newxvals[-1] + float_info.epsilon &lt; stop:\n                newxvals.append(stop)\n            newxvals = np.asanyarray(newxvals)\n            new_starts.extend(newxvals[:-1])\n            new_stops.extend(newxvals[1:])\n\n        # now make a new interval array:\n        out = copy.copy(self)\n        out._data = np.hstack(\n            [\n                np.array(new_starts)[..., np.newaxis],\n                np.array(new_stops)[..., np.newaxis],\n            ]\n        )\n        return out\n\n    @property\n    def label(self):\n        \"\"\"Label describing the interval array.\"\"\"\n        if self._label is None:\n            logging.warning(\"label has not yet been specified\")\n        return self._label\n\n    @label.setter\n    def label(self, val):\n        if val is not None:\n            try:  # cast to str:\n                label = str(val)\n            except TypeError:\n                raise TypeError(\"cannot convert label to string\")\n        else:\n            label = val\n        self._label = label\n\n    def complement(self, domain=None):\n        \"\"\"Complement within domain.\n\n        Parameters\n        ----------\n        domain : IntervalArray, optional\n            IntervalArray specifying entire domain. Default is self.domain.\n\n        Returns\n        -------\n        complement : IntervalArray\n            IntervalArray containing all the nonzero intervals in the\n            complement set.\n        \"\"\"\n\n        if domain is None:\n            domain = self.domain\n\n        # make sure IntervalArray is sorted:\n        if not self.issorted:\n            self._sort()\n        # check that IntervalArray is entirely contained within domain\n        # if (self.start &lt; domain.start) or (self.stop &gt; domain.stop):\n        #     raise ValueError(\"IntervalArray must be entirely contained within domain\")\n\n        # check that IntervalArray is fully merged, or merge it if necessary\n        merged = self.merge()\n        # build complement intervals\n        starts = np.insert(merged.stops, 0, domain.start)\n        stops = np.append(merged.starts, domain.stop)\n        newvalues = np.vstack([starts, stops]).T\n        # remove intervals with zero length\n        lengths = newvalues[:, 1] - newvalues[:, 0]\n        newvalues = newvalues[lengths &gt; 0]\n        complement = copy.copy(self)\n        complement._data = newvalues\n\n        if domain.n_intervals &gt; 1:\n            return complement[domain]\n        try:\n            complement._data[0, 0] = np.max((complement._data[0, 0], domain.start))\n            complement._data[-1, -1] = np.min((complement._data[-1, -1], domain.stop))\n        except IndexError:  # complement is empty\n            return type(self)(empty=True)\n        return complement\n\n    @property\n    def domain(self):\n        \"\"\"domain (in base units) within which support is defined\"\"\"\n        if self._domain is None:\n            self._domain = type(self)([-np.inf, np.inf])\n        return self._domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"domain (in base units) within which support is defined\"\"\"\n        # TODO: add  input validation\n        if isinstance(val, type(self)):\n            self._domain = val\n        elif isinstance(val, (tuple, list)):\n            self._domain = type(self)([val[0], val[1]])\n\n    @property\n    def meta(self):\n        \"\"\"Meta data associated with IntervalArray.\"\"\"\n        if self._meta is None:\n            logging.warning(\"meta data is not available\")\n        return self._meta\n\n    @meta.setter\n    def meta(self, val):\n        self._meta = val\n\n    @property\n    def min(self):\n        \"\"\"Minimum bound of all intervals in IntervalArray.\"\"\"\n        return self.merge().start\n\n    @property\n    def max(self):\n        \"\"\"Maximum bound of all intervals in IntervalArray.\"\"\"\n        return self.merge().stop\n\n    @property\n    def data(self):\n        \"\"\"Interval values [start, stop) in base units.\"\"\"\n        return self._data\n\n    @property\n    def is_finite(self):\n        \"\"\"Is the interval [start, stop) finite.\"\"\"\n        return not (np.isinf(self.start) | np.isinf(self.stop))\n\n    # @property\n    # def _human_readable_posix_intervals(self):\n    #     \"\"\"Interval start and stop values in human readable POSIX time.\n\n    #     This property is left private, because it has not been carefully\n    #     vetted for public API release yet.\n    #     \"\"\"\n    #     import datetime\n    #     n_intervals_zfill = len(str(self.n_intervals))\n    #     for ii, (start, stop) in enumerate(self.time):\n    #         print('[ep ' + str(ii).zfill(n_intervals_zfill) + ']\\t' +\n    #               datetime.datetime.fromtimestamp(\n    #                 int(start)).strftime('%Y-%m-%d %H:%M:%S') + ' -- ' +\n    #               datetime.datetime.fromtimestamp(\n    #                 int(stop)).strftime('%Y-%m-%d %H:%M:%S') + '\\t(' +\n    #               str(utils.PrettyDuration(stop-start)) + ')')\n\n    @property\n    def centers(self):\n        \"\"\"(np.array) The center of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return np.mean(self.data, axis=1)\n\n    @property\n    def lengths(self):\n        \"\"\"(np.array) The length of each interval.\"\"\"\n        if self.isempty:\n            return 0\n        return self.data[:, 1] - self.data[:, 0]\n\n    @property\n    def range(self):\n        \"\"\"return IntervalArray containing range of current IntervalArray.\"\"\"\n        return type(self)([self.start, self.stop])\n\n    @property\n    def length(self):\n        \"\"\"(float) The total length of the [merged] interval array.\"\"\"\n        if self.isempty:\n            return self.formatter(0)\n        merged = self.merge()\n        return self.formatter(np.array(merged.data[:, 1] - merged.data[:, 0]).sum())\n\n    @property\n    def starts(self):\n        \"\"\"(np.array) The start of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 0]\n\n    @property\n    def start(self):\n        \"\"\"(np.array) The start of the first interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 0][0]\n\n    @property\n    def stops(self):\n        \"\"\"(np.array) The stop of each interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 1]\n\n    @property\n    def stop(self):\n        \"\"\"(np.array) The stop of the last interval.\"\"\"\n        if self.isempty:\n            return []\n        return self.data[:, 1][-1]\n\n    @property\n    def n_intervals(self):\n        \"\"\"(int) The number of intervals.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self.data[:, 0]))\n\n    def __len__(self):\n        \"\"\"(int) The number of intervals.\"\"\"\n        return self.n_intervals\n\n    @property\n    def ismerged(self):\n        \"\"\"(bool) No overlapping intervals exist.\"\"\"\n        if self.isempty:\n            return True\n        if self.n_intervals == 1:\n            return True\n        if not self.issorted:\n            self._sort()\n        if not utils.is_sorted(self.stops):\n            return False\n\n        return np.all(self.data[1:, 0] - self.data[:-1, 1] &gt; 0)\n\n    def _ismerged(self, overlap=0.0):\n        \"\"\"(bool) No overlapping intervals with overlap &gt;= overlap exist.\"\"\"\n        if self.isempty:\n            return True\n        if self.n_intervals == 1:\n            return True\n        if not self.issorted:\n            self._sort()\n        if not utils.is_sorted(self.stops):\n            return False\n\n        return np.all(self.data[1:, 0] - self.data[:-1, 1] &gt; -overlap)\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Left edges of intervals are sorted in ascending order.\"\"\"\n        if self.isempty:\n            return True\n        return utils.is_sorted(self.starts)\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) Empty IntervalArray.\"\"\"\n        try:\n            return len(self.data) == 0\n        except TypeError:\n            return True  # this happens when self.data is None\n\n    def copy(self):\n        \"\"\"(IntervalArray) Returns a copy of the current interval array.\"\"\"\n        newcopy = copy.deepcopy(self)\n        return newcopy\n\n    def _drop_empty_intervals(self):\n        \"\"\"Drops empty intervals. Not in-place, i.e. returns a copy.\"\"\"\n        keep_interval_ids = np.argwhere(self.lengths).squeeze().tolist()\n        return self[keep_interval_ids]\n\n    def intersect(self, interval, *, boundaries=True):\n        \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n        other interval array ('interval').\n        \"\"\"\n\n        if self.isempty or interval.isempty:\n            logging.warning(\"interval intersection is empty\")\n            return type(self)(empty=True)\n\n        new_intervals = []\n\n        # Extract starts and stops and convert to np.array of float64 (for numba)\n        interval_starts_a = np.array(self.starts, dtype=np.float64)\n        interval_stops_a = np.array(self.stops, dtype=np.float64)\n        if interval.data.ndim == 1:\n            interval_starts_b = np.array([interval.data[0]], dtype=np.float64)\n            interval_stops_b = np.array([interval.data[1]], dtype=np.float64)\n        else:\n            interval_starts_b = np.array(interval.data[:, 0], dtype=np.float64)\n            interval_stops_b = np.array(interval.data[:, 1], dtype=np.float64)\n\n        new_starts, new_stops = interval_intersect(\n            interval_starts_a,\n            interval_stops_a,\n            interval_starts_b,\n            interval_stops_b,\n            boundaries,\n        )\n\n        for start, stop in zip(new_starts, new_stops):\n            new_intervals.append([start, stop])\n\n        # convert to np.array of float64\n        new_intervals = np.array(new_intervals, dtype=np.float64)\n\n        out = type(self)(new_intervals)\n        out._domain = self.domain\n        return out\n\n    # def intersect(self, interval, *, boundaries=True):\n    #     \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n    #        other interval array ('interval').\n    #     \"\"\"\n\n    #     this = copy.deepcopy(self)\n    #     new_intervals = []\n    #     for epa in this:\n    #         cand_ep_idx = np.argwhere((interval.starts &lt; epa.stop) &amp; (interval.stops &gt; epa.start)).squeeze()\n    #         if np.size(cand_ep_idx) &gt; 0:\n    #             for epb in interval[cand_ep_idx.tolist()]:\n    #                 new_interval = self._intersect(epa, epb, boundaries=boundaries)\n    #                 if not new_interval.isempty:\n    #                     new_intervals.append([new_interval.start, new_interval.stop])\n    #     out = type(self)(new_intervals)\n    #     out._domain = self.domain\n    #     return out\n\n    # def _intersect(self, intervala, intervalb, *, boundaries=True, meta=None):\n    #     \"\"\"Finds intersection (overlap) between two sets of interval arrays.\n\n    #     TODO: verify if this requires a merged IntervalArray to work properly?\n    #     ISSUE_261: not fixed yet\n\n    #     TODO: domains are not preserved yet! careful consideration is necessary.\n\n    #     Parameters\n    #     ----------\n    #     interval : nelpy.IntervalArray\n    #     boundaries : bool\n    #         If True, limits start, stop to interval start and stop.\n    #     meta : dict, optional\n    #         New dictionary of meta data for interval ontersection.\n\n    #     Returns\n    #     -------\n    #     intersect_intervals : nelpy.IntervalArray\n    #     \"\"\"\n    #     if intervala.isempty or intervalb.isempty:\n    #         logging.warning('interval intersection is empty')\n    #         return type(self)(empty=True)\n\n    #     new_starts = []\n    #     new_stops = []\n    #     interval_a = intervala.merge().copy()\n    #     interval_b = intervalb.merge().copy()\n\n    #     for aa in interval_a.data:\n    #         for bb in interval_b.data:\n    #             if (aa[0] &lt;= bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &lt;= aa[1]):\n    #                 new_starts.append(bb[0])\n    #                 new_stops.append(bb[1])\n    #             elif (aa[0] &lt; bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &gt; aa[1]):\n    #                 new_starts.append(bb[0])\n    #                 if boundaries:\n    #                     new_stops.append(aa[1])\n    #                 else:\n    #                     new_stops.append(bb[1])\n    #             elif (aa[0] &gt; bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &lt; aa[1]):\n    #                 if boundaries:\n    #                     new_starts.append(aa[0])\n    #                 else:\n    #                     new_starts.append(bb[0])\n    #                 new_stops.append(bb[1])\n    #             elif (aa[0] &gt;= bb[0] &lt; aa[1]) and (aa[0] &lt; bb[1] &gt;= aa[1]):\n    #                 if boundaries:\n    #                     new_starts.append(aa[0])\n    #                     new_stops.append(aa[1])\n    #                 else:\n    #                     new_starts.append(bb[0])\n    #                     new_stops.append(bb[1])\n\n    #     if not boundaries:\n    #         new_starts = np.unique(new_starts)\n    #         new_stops = np.unique(new_stops)\n\n    #     interval_a._data = np.hstack(\n    #         [np.array(new_starts)[..., np.newaxis],\n    #             np.array(new_stops)[..., np.newaxis]])\n\n    #     return interval_a\n\n    def merge(self, *, gap=0.0, overlap=0.0):\n        \"\"\"Merge intervals that are close or overlapping.\n\n        if gap == 0 and overlap == 0:\n            [a, b) U [b, c) = [a, c)\n        if gap == None and overlap &gt; 0:\n            [a, b) U [b, c) = [a, b) U [b, c)\n            [a, b + overlap) U [b, c) = [a, c)\n            [a, b) U [b - overlap, c) = [a, c)\n        if gap &gt; 0 and overlap == None:\n            [a, b) U [b, c) = [a, c)\n            [a, b) U [b + gap, c) = [a, c)\n            [a, b - gap) U [b, c) = [a, c)\n\n        WARNING! Algorithm only works on SORTED intervals.\n\n        Parameters\n        ----------\n        gap : float, optional\n            Amount (in base units) to consider intervals close enough to merge.\n            Defaults to 0.0 (no gap).\n        Returns\n        -------\n        merged_intervals : nelpy.IntervalArray\n        \"\"\"\n\n        if gap &lt; 0:\n            raise ValueError(\"gap cannot be negative\")\n        if overlap &lt; 0:\n            raise ValueError(\"overlap cannot be negative\")\n\n        if self.isempty:\n            return self\n\n        if (self.ismerged) and (gap == 0.0):\n            # already merged\n            return self\n\n        newintervalarray = copy.copy(self)\n\n        if not newintervalarray.issorted:\n            newintervalarray._sort()\n\n        overlap_ = overlap\n\n        while not newintervalarray._ismerged(overlap=overlap) or gap &gt; 0:\n            stops = newintervalarray.stops[:-1] + gap\n            starts = newintervalarray.starts[1:] + overlap_\n            to_merge = (stops - starts) &gt;= 0\n\n            new_starts = [newintervalarray.starts[0]]\n            new_stops = []\n\n            next_stop = newintervalarray.stops[0]\n            for i in range(newintervalarray.data.shape[0] - 1):\n                this_stop = newintervalarray.stops[i]\n                next_stop = max(next_stop, this_stop)\n                if not to_merge[i]:\n                    new_stops.append(next_stop)\n                    new_starts.append(newintervalarray.starts[i + 1])\n\n            new_stops.append(max(newintervalarray.stops[-1], next_stop))\n\n            new_starts = np.array(new_starts)\n            new_stops = np.array(new_stops)\n\n            newintervalarray._data = np.vstack([new_starts, new_stops]).T\n\n            # after one pass, all the gap offsets have been added, and\n            # then we just need to keep merging...\n            gap = 0.0\n            overlap_ = 0.0\n\n        return newintervalarray\n\n    def expand(self, amount, direction=\"both\"):\n        \"\"\"Expands interval by the given amount.\n        Parameters\n        ----------\n        amount : float\n            Amount (in base units) to expand each interval.\n        direction : str\n            Can be 'both', 'start', or 'stop'. This specifies\n            which direction to resize interval.\n        Returns\n        -------\n        expanded_intervals : nelpy.IntervalArray\n        \"\"\"\n        if direction == \"both\":\n            resize_starts = self.data[:, 0] - amount\n            resize_stops = self.data[:, 1] + amount\n        elif direction == \"start\":\n            resize_starts = self.data[:, 0] - amount\n            resize_stops = self.data[:, 1]\n        elif direction == \"stop\":\n            resize_starts = self.data[:, 0]\n            resize_stops = self.data[:, 1] + amount\n        else:\n            raise ValueError(\"direction must be 'both', 'start', or 'stop'\")\n\n        newintervalarray = copy.copy(self)\n\n        newintervalarray._data = np.hstack(\n            (resize_starts[..., np.newaxis], resize_stops[..., np.newaxis])\n        )\n\n        return newintervalarray\n\n    def shrink(self, amount, direction=\"both\"):\n        \"\"\"Shrinks interval by the given amount.\n        Parameters\n        ----------\n        amount : float\n            Amount (in base units) to shrink each interval.\n        direction : str\n            Can be 'both', 'start', or 'stop'. This specifies\n            which direction to resize interval.\n        Returns\n        -------\n        shrinked_intervals : nelpy.IntervalArray\n        \"\"\"\n        both_limit = min(self.lengths / 2)\n        if amount &gt; both_limit and direction == \"both\":\n            raise ValueError(\"shrink amount too large\")\n\n        single_limit = min(self.lengths)\n        if amount &gt; single_limit and direction != \"both\":\n            raise ValueError(\"shrink amount too large\")\n\n        return self.expand(-amount, direction)\n\n    def join(self, interval, meta=None):\n        \"\"\"Combines [and merges] two sets of intervals. Intervals can have\n        different sampling rates.\n\n        Parameters\n        ----------\n        interval : nelpy.IntervalArray\n        meta : dict, optional\n            New meta data dictionary describing the joined intervals.\n\n        Returns\n        -------\n        joined_intervals : nelpy.IntervalArray\n        \"\"\"\n\n        if self.isempty:\n            return interval\n        if interval.isempty:\n            return self\n\n        newintervalarray = copy.copy(self)\n\n        join_starts = np.concatenate((self.data[:, 0], interval.data[:, 0]))\n        join_stops = np.concatenate((self.data[:, 1], interval.data[:, 1]))\n\n        newintervalarray._data = np.hstack(\n            (join_starts[..., np.newaxis], join_stops[..., np.newaxis])\n        )\n        if not newintervalarray.issorted:\n            newintervalarray._sort()\n        # if not newintervalarray.ismerged:\n        #     newintervalarray = newintervalarray.merge()\n        return newintervalarray\n\n    def __contains__(self, value):\n        \"\"\"Checks whether value is in any interval.\n\n        #TODO: add support for when value is an IntervalArray (could be easy with intersection)\n\n        Parameters\n        ----------\n        intervals: nelpy.IntervalArray\n        value: float or int\n\n        Returns\n        -------\n        boolean\n\n        \"\"\"\n        # TODO: consider vectorizing this loop, which should increase\n        # speed, but also greatly increase memory? Alternatively, if we\n        # could assume something about intervals being sorted, this can\n        # also be made much faster than the current O(N)\n        for start, stop in zip(self.starts, self.stops):\n            if start &lt;= value &lt;= stop:\n                return True\n        return False\n\n    def _sort(self):\n        \"\"\"Sort intervals by interval starts\"\"\"\n        sort_idx = np.argsort(self.data[:, 0])\n        self._data = self._data[sort_idx]\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.centers","title":"<code>centers</code>  <code>property</code>","text":"<p>(np.array) The center of each interval.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Interval values [start, stop) in base units.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>domain (in base units) within which support is defined</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.is_finite","title":"<code>is_finite</code>  <code>property</code>","text":"<p>Is the interval [start, stop) finite.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) Empty IntervalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.ismerged","title":"<code>ismerged</code>  <code>property</code>","text":"<p>(bool) No overlapping intervals exist.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Left edges of intervals are sorted in ascending order.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Label describing the interval array.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.length","title":"<code>length</code>  <code>property</code>","text":"<p>(float) The total length of the [merged] interval array.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>(np.array) The length of each interval.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.max","title":"<code>max</code>  <code>property</code>","text":"<p>Maximum bound of all intervals in IntervalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.meta","title":"<code>meta</code>  <code>property</code> <code>writable</code>","text":"<p>Meta data associated with IntervalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.min","title":"<code>min</code>  <code>property</code>","text":"<p>Minimum bound of all intervals in IntervalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.n_intervals","title":"<code>n_intervals</code>  <code>property</code>","text":"<p>(int) The number of intervals.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.range","title":"<code>range</code>  <code>property</code>","text":"<p>return IntervalArray containing range of current IntervalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.start","title":"<code>start</code>  <code>property</code>","text":"<p>(np.array) The start of the first interval.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>(np.array) The start of each interval.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.stop","title":"<code>stop</code>  <code>property</code>","text":"<p>(np.array) The stop of the last interval.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.stops","title":"<code>stops</code>  <code>property</code>","text":"<p>(np.array) The stop of each interval.</p>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.complement","title":"<code>complement(domain=None)</code>","text":"<p>Complement within domain.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>IntervalArray</code> <p>IntervalArray specifying entire domain. Default is self.domain.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>complement</code> <code>IntervalArray</code> <p>IntervalArray containing all the nonzero intervals in the complement set.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def complement(self, domain=None):\n    \"\"\"Complement within domain.\n\n    Parameters\n    ----------\n    domain : IntervalArray, optional\n        IntervalArray specifying entire domain. Default is self.domain.\n\n    Returns\n    -------\n    complement : IntervalArray\n        IntervalArray containing all the nonzero intervals in the\n        complement set.\n    \"\"\"\n\n    if domain is None:\n        domain = self.domain\n\n    # make sure IntervalArray is sorted:\n    if not self.issorted:\n        self._sort()\n    # check that IntervalArray is entirely contained within domain\n    # if (self.start &lt; domain.start) or (self.stop &gt; domain.stop):\n    #     raise ValueError(\"IntervalArray must be entirely contained within domain\")\n\n    # check that IntervalArray is fully merged, or merge it if necessary\n    merged = self.merge()\n    # build complement intervals\n    starts = np.insert(merged.stops, 0, domain.start)\n    stops = np.append(merged.starts, domain.stop)\n    newvalues = np.vstack([starts, stops]).T\n    # remove intervals with zero length\n    lengths = newvalues[:, 1] - newvalues[:, 0]\n    newvalues = newvalues[lengths &gt; 0]\n    complement = copy.copy(self)\n    complement._data = newvalues\n\n    if domain.n_intervals &gt; 1:\n        return complement[domain]\n    try:\n        complement._data[0, 0] = np.max((complement._data[0, 0], domain.start))\n        complement._data[-1, -1] = np.min((complement._data[-1, -1], domain.stop))\n    except IndexError:  # complement is empty\n        return type(self)(empty=True)\n    return complement\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.copy","title":"<code>copy()</code>","text":"<p>(IntervalArray) Returns a copy of the current interval array.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def copy(self):\n    \"\"\"(IntervalArray) Returns a copy of the current interval array.\"\"\"\n    newcopy = copy.deepcopy(self)\n    return newcopy\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.expand","title":"<code>expand(amount, direction='both')</code>","text":"<p>Expands interval by the given amount.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>float</code> <p>Amount (in base units) to expand each interval.</p> required <code>direction</code> <code>str</code> <p>Can be 'both', 'start', or 'stop'. This specifies which direction to resize interval.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>expanded_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def expand(self, amount, direction=\"both\"):\n    \"\"\"Expands interval by the given amount.\n    Parameters\n    ----------\n    amount : float\n        Amount (in base units) to expand each interval.\n    direction : str\n        Can be 'both', 'start', or 'stop'. This specifies\n        which direction to resize interval.\n    Returns\n    -------\n    expanded_intervals : nelpy.IntervalArray\n    \"\"\"\n    if direction == \"both\":\n        resize_starts = self.data[:, 0] - amount\n        resize_stops = self.data[:, 1] + amount\n    elif direction == \"start\":\n        resize_starts = self.data[:, 0] - amount\n        resize_stops = self.data[:, 1]\n    elif direction == \"stop\":\n        resize_starts = self.data[:, 0]\n        resize_stops = self.data[:, 1] + amount\n    else:\n        raise ValueError(\"direction must be 'both', 'start', or 'stop'\")\n\n    newintervalarray = copy.copy(self)\n\n    newintervalarray._data = np.hstack(\n        (resize_starts[..., np.newaxis], resize_stops[..., np.newaxis])\n    )\n\n    return newintervalarray\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.intersect","title":"<code>intersect(interval, *, boundaries=True)</code>","text":"<p>Returns intersection (overlap) between current IntervalArray (self) and other interval array ('interval').</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def intersect(self, interval, *, boundaries=True):\n    \"\"\"Returns intersection (overlap) between current IntervalArray (self) and\n    other interval array ('interval').\n    \"\"\"\n\n    if self.isempty or interval.isempty:\n        logging.warning(\"interval intersection is empty\")\n        return type(self)(empty=True)\n\n    new_intervals = []\n\n    # Extract starts and stops and convert to np.array of float64 (for numba)\n    interval_starts_a = np.array(self.starts, dtype=np.float64)\n    interval_stops_a = np.array(self.stops, dtype=np.float64)\n    if interval.data.ndim == 1:\n        interval_starts_b = np.array([interval.data[0]], dtype=np.float64)\n        interval_stops_b = np.array([interval.data[1]], dtype=np.float64)\n    else:\n        interval_starts_b = np.array(interval.data[:, 0], dtype=np.float64)\n        interval_stops_b = np.array(interval.data[:, 1], dtype=np.float64)\n\n    new_starts, new_stops = interval_intersect(\n        interval_starts_a,\n        interval_stops_a,\n        interval_starts_b,\n        interval_stops_b,\n        boundaries,\n    )\n\n    for start, stop in zip(new_starts, new_stops):\n        new_intervals.append([start, stop])\n\n    # convert to np.array of float64\n    new_intervals = np.array(new_intervals, dtype=np.float64)\n\n    out = type(self)(new_intervals)\n    out._domain = self.domain\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.join","title":"<code>join(interval, meta=None)</code>","text":"<p>Combines [and merges] two sets of intervals. Intervals can have different sampling rates.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>IntervalArray</code> required <code>meta</code> <code>dict</code> <p>New meta data dictionary describing the joined intervals.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>joined_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def join(self, interval, meta=None):\n    \"\"\"Combines [and merges] two sets of intervals. Intervals can have\n    different sampling rates.\n\n    Parameters\n    ----------\n    interval : nelpy.IntervalArray\n    meta : dict, optional\n        New meta data dictionary describing the joined intervals.\n\n    Returns\n    -------\n    joined_intervals : nelpy.IntervalArray\n    \"\"\"\n\n    if self.isempty:\n        return interval\n    if interval.isempty:\n        return self\n\n    newintervalarray = copy.copy(self)\n\n    join_starts = np.concatenate((self.data[:, 0], interval.data[:, 0]))\n    join_stops = np.concatenate((self.data[:, 1], interval.data[:, 1]))\n\n    newintervalarray._data = np.hstack(\n        (join_starts[..., np.newaxis], join_stops[..., np.newaxis])\n    )\n    if not newintervalarray.issorted:\n        newintervalarray._sort()\n    # if not newintervalarray.ismerged:\n    #     newintervalarray = newintervalarray.merge()\n    return newintervalarray\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.merge","title":"<code>merge(*, gap=0.0, overlap=0.0)</code>","text":"<p>Merge intervals that are close or overlapping.</p> <p>if gap == 0 and overlap == 0:     [a, b) U [b, c) = [a, c) if gap == None and overlap &gt; 0:     [a, b) U [b, c) = [a, b) U [b, c)     [a, b + overlap) U [b, c) = [a, c)     [a, b) U [b - overlap, c) = [a, c) if gap &gt; 0 and overlap == None:     [a, b) U [b, c) = [a, c)     [a, b) U [b + gap, c) = [a, c)     [a, b - gap) U [b, c) = [a, c)</p> <p>WARNING! Algorithm only works on SORTED intervals.</p> <p>Parameters:</p> Name Type Description Default <code>gap</code> <code>float</code> <p>Amount (in base units) to consider intervals close enough to merge. Defaults to 0.0 (no gap).</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>merged_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def merge(self, *, gap=0.0, overlap=0.0):\n    \"\"\"Merge intervals that are close or overlapping.\n\n    if gap == 0 and overlap == 0:\n        [a, b) U [b, c) = [a, c)\n    if gap == None and overlap &gt; 0:\n        [a, b) U [b, c) = [a, b) U [b, c)\n        [a, b + overlap) U [b, c) = [a, c)\n        [a, b) U [b - overlap, c) = [a, c)\n    if gap &gt; 0 and overlap == None:\n        [a, b) U [b, c) = [a, c)\n        [a, b) U [b + gap, c) = [a, c)\n        [a, b - gap) U [b, c) = [a, c)\n\n    WARNING! Algorithm only works on SORTED intervals.\n\n    Parameters\n    ----------\n    gap : float, optional\n        Amount (in base units) to consider intervals close enough to merge.\n        Defaults to 0.0 (no gap).\n    Returns\n    -------\n    merged_intervals : nelpy.IntervalArray\n    \"\"\"\n\n    if gap &lt; 0:\n        raise ValueError(\"gap cannot be negative\")\n    if overlap &lt; 0:\n        raise ValueError(\"overlap cannot be negative\")\n\n    if self.isempty:\n        return self\n\n    if (self.ismerged) and (gap == 0.0):\n        # already merged\n        return self\n\n    newintervalarray = copy.copy(self)\n\n    if not newintervalarray.issorted:\n        newintervalarray._sort()\n\n    overlap_ = overlap\n\n    while not newintervalarray._ismerged(overlap=overlap) or gap &gt; 0:\n        stops = newintervalarray.stops[:-1] + gap\n        starts = newintervalarray.starts[1:] + overlap_\n        to_merge = (stops - starts) &gt;= 0\n\n        new_starts = [newintervalarray.starts[0]]\n        new_stops = []\n\n        next_stop = newintervalarray.stops[0]\n        for i in range(newintervalarray.data.shape[0] - 1):\n            this_stop = newintervalarray.stops[i]\n            next_stop = max(next_stop, this_stop)\n            if not to_merge[i]:\n                new_stops.append(next_stop)\n                new_starts.append(newintervalarray.starts[i + 1])\n\n        new_stops.append(max(newintervalarray.stops[-1], next_stop))\n\n        new_starts = np.array(new_starts)\n        new_stops = np.array(new_stops)\n\n        newintervalarray._data = np.vstack([new_starts, new_stops]).T\n\n        # after one pass, all the gap offsets have been added, and\n        # then we just need to keep merging...\n        gap = 0.0\n        overlap_ = 0.0\n\n    return newintervalarray\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.partition","title":"<code>partition(*, ds=None, n_intervals=None)</code>","text":"<p>Returns an IntervalArray that has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum length, for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalArray</code> <p>IntervalArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, *, ds=None, n_intervals=None):\n    \"\"\"Returns an IntervalArray that has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum length, for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : IntervalArray\n        IntervalArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    if self.isempty:\n        raise ValueError(\"cannot parition an empty object in a meaningful way!\")\n\n    if ds is not None and n_intervals is not None:\n        raise ValueError(\"ds and n_intervals cannot be used together\")\n\n    if n_intervals is not None:\n        assert float(n_intervals).is_integer(), (\n            \"n_intervals must be a positive integer!\"\n        )\n        assert n_intervals &gt; 1, \"n_intervals must be a positive integer &gt; 1\"\n        # determine ds from number of desired points:\n        ds = self.length / n_intervals\n\n    if ds is None:\n        # neither n_intervals nor ds was specified, so assume defaults:\n        n_intervals = 100\n        ds = self.length / n_intervals\n\n    # build list of points at which to esplit the IntervalArray\n    new_starts = []\n    new_stops = []\n    for start, stop in self.data:\n        newxvals = utils.frange(start, stop, step=ds).tolist()\n        # newxvals = np.arange(start, stop, step=ds).tolist()\n        if newxvals[-1] + float_info.epsilon &lt; stop:\n            newxvals.append(stop)\n        newxvals = np.asanyarray(newxvals)\n        new_starts.extend(newxvals[:-1])\n        new_stops.extend(newxvals[1:])\n\n    # now make a new interval array:\n    out = copy.copy(self)\n    out._data = np.hstack(\n        [\n            np.array(new_starts)[..., np.newaxis],\n            np.array(new_stops)[..., np.newaxis],\n        ]\n    )\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.remove_duplicates","title":"<code>remove_duplicates(inplace=False)</code>","text":"<p>Remove duplicate intervals.</p> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def remove_duplicates(self, inplace=False):\n    \"\"\"Remove duplicate intervals.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.IntervalArray.shrink","title":"<code>shrink(amount, direction='both')</code>","text":"<p>Shrinks interval by the given amount.</p> <p>Parameters:</p> Name Type Description Default <code>amount</code> <code>float</code> <p>Amount (in base units) to shrink each interval.</p> required <code>direction</code> <code>str</code> <p>Can be 'both', 'start', or 'stop'. This specifies which direction to resize interval.</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>shrinked_intervals</code> <code>IntervalArray</code> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>def shrink(self, amount, direction=\"both\"):\n    \"\"\"Shrinks interval by the given amount.\n    Parameters\n    ----------\n    amount : float\n        Amount (in base units) to shrink each interval.\n    direction : str\n        Can be 'both', 'start', or 'stop'. This specifies\n        which direction to resize interval.\n    Returns\n    -------\n    shrinked_intervals : nelpy.IntervalArray\n    \"\"\"\n    both_limit = min(self.lengths / 2)\n    if amount &gt; both_limit and direction == \"both\":\n        raise ValueError(\"shrink amount too large\")\n\n    single_limit = min(self.lengths)\n    if amount &gt; single_limit and direction != \"both\":\n        raise ValueError(\"shrink amount too large\")\n\n    return self.expand(-amount, direction)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.MarkedSpikeTrainArray","title":"<code>MarkedSpikeTrainArray</code>","text":"<p>               Bases: <code>ValueEventArray</code></p> <p>MarkedSpikeTrainArray for storing spike times with associated marks (e.g., waveform features).</p> <p>This class extends ValueEventArray to support marks for each spike event, such as tetrode features or other metadata.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>array - like</code> <p>Spike times or event times.</p> required <code>marks</code> <code>array - like</code> <p>Associated marks/features for each event.</p> required <code>support</code> <code>IntervalArray</code> <p>Support intervals for the spike train.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> required <code>series_label</code> <code>str</code> <p>Label for the series (e.g., 'tetrodes').</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>events</code> <code>array - like</code> <p>Spike times or event times.</p> <code>marks</code> <code>array - like</code> <p>Associated marks/features for each event.</p> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the spike train.</p> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> <code>series_label</code> <code>str</code> <p>Label for the series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; msta = MarkedSpikeTrainArray(events=spike_times, marks=features, fs=30000)\n&gt;&gt;&gt; msta.events\narray([...])\n&gt;&gt;&gt; msta.marks\narray([...])\n</code></pre> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class MarkedSpikeTrainArray(ValueEventArray):\n    \"\"\"\n    MarkedSpikeTrainArray for storing spike times with associated marks (e.g., waveform features).\n\n    This class extends ValueEventArray to support marks for each spike event, such as tetrode features or other metadata.\n\n    Parameters\n    ----------\n    events : array-like\n        Spike times or event times.\n    marks : array-like\n        Associated marks/features for each event.\n    support : nelpy.IntervalArray, optional\n        Support intervals for the spike train.\n    fs : float, optional\n        Sampling frequency in Hz.\n    series_label : str, optional\n        Label for the series (e.g., 'tetrodes').\n    **kwargs :\n        Additional keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    events : array-like\n        Spike times or event times.\n    marks : array-like\n        Associated marks/features for each event.\n    support : nelpy.IntervalArray\n        Support intervals for the spike train.\n    fs : float\n        Sampling frequency in Hz.\n    series_label : str\n        Label for the series.\n\n    Examples\n    --------\n    &gt;&gt;&gt; msta = MarkedSpikeTrainArray(events=spike_times, marks=features, fs=30000)\n    &gt;&gt;&gt; msta.events\n    array([...])\n    &gt;&gt;&gt; msta.marks\n    array([...])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"n_marks\": \"n_values\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n        \"marks\": \"values\",\n        \"spikes\": \"events\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"tetrodes\"\n        kwargs[\"series_label\"] = series_label\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n\n    # @keyword_equivalence(this_or_that={'n_intervals':'n_epochs'})\n    # def partition(self, ds=None, n_intervals=None, n_epochs=None):\n    #     if n_intervals is None:\n    #         n_intervals = n_epochs\n    #     kwargs = {'ds':ds, 'n_intervals': n_intervals}\n    #     return super().partition(**kwargs)\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n        raise NotImplementedError\n        return BinnedMarkedSpikeTrainArray(self, ds=ds)  # noqa: F821\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.MarkedSpikeTrainArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedSpikeTrainArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n    raise NotImplementedError\n    return BinnedMarkedSpikeTrainArray(self, ds=ds)  # noqa: F821\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.MinimalExampleArray","title":"<code>MinimalExampleArray</code>","text":"<p>               Bases: <code>RegularlySampledAnalogSignalArray</code></p> <p>MinimalExampleArray is a custom example subclass of RegularlySampledAnalogSignalArray.</p> <p>This class demonstrates how to extend RegularlySampledAnalogSignalArray with custom aliases and methods.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the parent class.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>__aliases__</code> <code>dict</code> <p>Dictionary of class-specific aliases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n&gt;&gt;&gt; arr.custom_func()\nWoot! We have some special skillz!\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class MinimalExampleArray(RegularlySampledAnalogSignalArray):\n    \"\"\"\n    MinimalExampleArray is a custom example subclass of RegularlySampledAnalogSignalArray.\n\n    This class demonstrates how to extend RegularlySampledAnalogSignalArray with custom aliases and methods.\n\n    Parameters\n    ----------\n    *args :\n        Positional arguments passed to the parent class.\n    **kwargs :\n        Keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    __aliases__ : dict\n        Dictionary of class-specific aliases.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n    &gt;&gt;&gt; arr.custom_func()\n    Woot! We have some special skillz!\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n    def custom_func(self):\n        \"\"\"\n        Print a custom message demonstrating a special method.\n\n        Examples\n        --------\n        &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n        &gt;&gt;&gt; arr.custom_func()\n        Woot! We have some special skillz!\n        \"\"\"\n        print(\"Woot! We have some special skillz!\")\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.MinimalExampleArray.custom_func","title":"<code>custom_func()</code>","text":"<p>Print a custom message demonstrating a special method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n&gt;&gt;&gt; arr.custom_func()\nWoot! We have some special skillz!\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def custom_func(self):\n    \"\"\"\n    Print a custom message demonstrating a special method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = MinimalExampleArray(data=[[1, 2, 3]], fs=1000)\n    &gt;&gt;&gt; arr.custom_func()\n    Woot! We have some special skillz!\n    \"\"\"\n    print(\"Woot! We have some special skillz!\")\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.Ordinate","title":"<code>Ordinate</code>","text":"<p>An ordinate (y-axis) object for core nelpy data containers.</p> <p>Parameters:</p> Name Type Description Default <code>base_unit</code> <code>str</code> <p>The base unit for the ordinate. Default is ''.</p> <code>None</code> <code>is_linking</code> <code>bool</code> <p>Whether the ordinate is linking. Default is False.</p> <code>False</code> <code>is_wrapping</code> <code>bool</code> <p>Whether the ordinate is wrapping. Default is False.</p> <code>False</code> <code>labelstring</code> <code>str</code> <p>String template for the ordinate label. Default is '{}'.</p> <code>None</code> <code>_range</code> <code>IntervalArray</code> <p>The range of the ordinate. Default is [-inf, inf].</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>base_unit</code> <code>str</code> <p>The base unit for the ordinate.</p> <code>is_linking</code> <code>bool</code> <p>Whether the ordinate is linking.</p> <code>is_wrapping</code> <code>bool</code> <p>Whether the ordinate is wrapping.</p> <code>label</code> <code>str</code> <p>The formatted label for the ordinate.</p> <code>range</code> <code>IntervalArray</code> <p>The range of the ordinate.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class Ordinate:\n    \"\"\"\n    An ordinate (y-axis) object for core nelpy data containers.\n\n    Parameters\n    ----------\n    base_unit : str, optional\n        The base unit for the ordinate. Default is ''.\n    is_linking : bool, optional\n        Whether the ordinate is linking. Default is False.\n    is_wrapping : bool, optional\n        Whether the ordinate is wrapping. Default is False.\n    labelstring : str, optional\n        String template for the ordinate label. Default is '{}'.\n    _range : nelpy.IntervalArray, optional\n        The range of the ordinate. Default is [-inf, inf].\n\n    Attributes\n    ----------\n    base_unit : str\n        The base unit for the ordinate.\n    is_linking : bool\n        Whether the ordinate is linking.\n    is_wrapping : bool\n        Whether the ordinate is wrapping.\n    label : str\n        The formatted label for the ordinate.\n    range : nelpy.IntervalArray\n        The range of the ordinate.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_unit=None,\n        is_linking=False,\n        is_wrapping=False,\n        labelstring=None,\n        _range=None,\n    ):\n        # TODO: add label support\n\n        if base_unit is None:\n            base_unit = \"\"\n        if labelstring is None:\n            labelstring = \"{}\"\n\n        if _range is None:\n            _range = core.IntervalArray([-inf, inf])\n\n        self.base_unit = base_unit\n        self._labelstring = labelstring\n        self.is_linking = is_linking\n        self.is_wrapping = is_wrapping\n        self._is_wrapped = None  # intialize to unknown (None) state\n        self._range = _range\n\n    @property\n    def label(self):\n        \"\"\"\n        Get the ordinate label.\n\n        Returns\n        -------\n        label : str\n            The formatted ordinate label.\n        \"\"\"\n        return self._labelstring.format(self.base_unit)\n\n    @label.setter\n    def label(self, val):\n        \"\"\"\n        Set the ordinate label string template.\n\n        Parameters\n        ----------\n        val : str\n            String template for the ordinate label.\n        \"\"\"\n        if val is None:\n            val = \"{}\"\n        try:  # cast to str:\n            labelstring = str(val)\n        except TypeError:\n            raise TypeError(\"cannot convert label to string\")\n        else:\n            labelstring = val\n        self._labelstring = labelstring\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the Ordinate object.\n\n        Returns\n        -------\n        repr_str : str\n            String representation of the Ordinate.\n        \"\"\"\n        return \"Ordinate(base_unit={}, is_linking={}, is_wrapping={})\".format(\n            self.base_unit, self.is_linking, self.is_wrapping\n        )\n\n    @property\n    def range(self):\n        \"\"\"\n        Get the range (in ordinate base units) on which ordinate is defined.\n\n        Returns\n        -------\n        range : nelpy.IntervalArray\n            The range of the ordinate.\n        \"\"\"\n        return self._range\n\n    @range.setter\n    def range(self, val):\n        \"\"\"Range (in ordinate base units) on which ordinate is defined.\"\"\"\n        # val can be an IntervalArray type, or (start, stop)\n        if isinstance(val, type(self.range)):\n            self._range = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self.range.domain\n            self._range = type(self.range)([val[0], val[1]])\n            self._range.domain = prev_domain\n        else:\n            raise TypeError(\"range must be of type {}\".format(str(type(self.range))))\n\n        self._range = self.range[self.range.domain]\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.Ordinate.label","title":"<code>label</code>  <code>property</code> <code>writable</code>","text":"<p>Get the ordinate label.</p> <p>Returns:</p> Name Type Description <code>label</code> <code>str</code> <p>The formatted ordinate label.</p>"},{"location":"reference/nelpy/min/#nelpy.min.Ordinate.range","title":"<code>range</code>  <code>property</code> <code>writable</code>","text":"<p>Get the range (in ordinate base units) on which ordinate is defined.</p> <p>Returns:</p> Name Type Description <code>range</code> <code>IntervalArray</code> <p>The range of the ordinate.</p>"},{"location":"reference/nelpy/min/#nelpy.min.PositionArray","title":"<code>PositionArray</code>","text":"<p>               Bases: <code>AnalogSignalArray</code></p> <p>An array for storing position data in 1D or 2D space.</p> <p>PositionArray is a specialized subclass of AnalogSignalArray designed to handle position tracking data. It provides convenient access to x and y coordinates, supports both 1D and 2D positional data, and includes spatial boundary information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like or None</code> <p>Position data with shape (n_signals, n_samples). For 1D position data, n_signals should be 1. For 2D position data, n_signals should be 2, where the first row contains x-coordinates and the second row contains y-coordinates. Can also be specified using the alias 'posdata'.</p> required <code>timestamps</code> <code>array_like or None</code> <p>Time stamps corresponding to each sample in data. If None, timestamps are automatically generated based on fs and start time.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Used to generate timestamps if not provided.</p> required <code>support</code> <code>EpochArray or None</code> <p>EpochArray defining the time intervals over which the position data is valid.</p> required <code>label</code> <code>str</code> <p>Descriptive label for the position array.</p> required <code>xlim</code> <code>tuple or None</code> <p>Spatial boundaries for x-coordinate as (min_x, max_x).</p> required <code>ylim</code> <code>tuple or None</code> <p>Spatial boundaries for y-coordinate as (min_y, max_y).</p> required <p>Attributes:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>X-coordinates as a 1D numpy array.</p> <code>y</code> <code>ndarray</code> <p>Y-coordinates as a 1D numpy array (only available for 2D data).</p> <code>is_1d</code> <code>bool</code> <p>True if position data is 1-dimensional.</p> <code>is_2d</code> <code>bool</code> <p>True if position data is 2-dimensional.</p> <code>xlim</code> <code>tuple or None</code> <p>Spatial boundaries for x-coordinate (only for 2D data).</p> <code>ylim</code> <code>tuple or None</code> <p>Spatial boundaries for y-coordinate (only for 2D data).</p> <p>Examples:</p> <p>Create a 1D position array:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; # 1D position data (e.g., position on a linear track)\n&gt;&gt;&gt; x_pos = np.linspace(0, 100, 1000)  # 100 cm track\n&gt;&gt;&gt; timestamps = np.linspace(0, 10, 1000)  # 10 seconds\n&gt;&gt;&gt; pos_1d = nel.PositionArray(\n...     data=x_pos[np.newaxis, :],\n...     timestamps=timestamps,\n...     label=\"Linear track position\",\n... )\n&gt;&gt;&gt; print(f\"1D position: {pos_1d.is_1d}\")\n&gt;&gt;&gt; print(f\"X range: {pos_1d.x.min():.1f} to {pos_1d.x.max():.1f} cm\")\n</code></pre> <p>Create a 2D position array:</p> <pre><code>&gt;&gt;&gt; # 2D position data (e.g., open field behavior)\n&gt;&gt;&gt; t = np.linspace(0, 2 * np.pi, 1000)\n&gt;&gt;&gt; x_pos = 50 + 30 * np.cos(t)  # circular trajectory\n&gt;&gt;&gt; y_pos = 50 + 30 * np.sin(t)\n&gt;&gt;&gt; pos_data = np.vstack([x_pos, y_pos])\n&gt;&gt;&gt; pos_2d = nel.PositionArray(\n...     posdata=pos_data,\n...     fs=100,  # 100 Hz sampling\n...     xlim=(0, 100),\n...     ylim=(0, 100),\n...     label=\"Open field position\",\n... )\n&gt;&gt;&gt; print(f\"2D position: {pos_2d.is_2d}\")\n&gt;&gt;&gt; print(f\"X position shape: {pos_2d.x.shape}\")\n&gt;&gt;&gt; print(f\"Y position shape: {pos_2d.y.shape}\")\n&gt;&gt;&gt; print(f\"Spatial bounds: x={pos_2d.xlim}, y={pos_2d.ylim}\")\n</code></pre> <p>Access position data:</p> <pre><code>&gt;&gt;&gt; # Get position at specific time\n&gt;&gt;&gt; time_idx = 500\n&gt;&gt;&gt; if pos_2d.is_2d:\n...     x_at_time = pos_2d.x[time_idx]\n...     y_at_time = pos_2d.y[time_idx]\n...     print(f\"Position at sample {time_idx}: ({x_at_time:.1f}, {y_at_time:.1f})\")\n</code></pre> Notes <ul> <li>For 2D position data, the first row of data should contain x-coordinates   and the second row should contain y-coordinates.</li> <li>The xlim and ylim parameters are only meaningful for 2D position data.</li> <li>Attempting to access y-coordinates or spatial limits on 1D data will   raise a ValueError.</li> <li>The 'posdata' alias can be used interchangeably with 'data' parameter.</li> </ul> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class PositionArray(AnalogSignalArray):\n    \"\"\"An array for storing position data in 1D or 2D space.\n\n    PositionArray is a specialized subclass of AnalogSignalArray designed to\n    handle position tracking data. It provides convenient access to x and y\n    coordinates, supports both 1D and 2D positional data, and includes\n    spatial boundary information.\n\n    Parameters\n    ----------\n    data : array_like or None, optional\n        Position data with shape (n_signals, n_samples). For 1D position data,\n        n_signals should be 1. For 2D position data, n_signals should be 2,\n        where the first row contains x-coordinates and the second row contains\n        y-coordinates. Can also be specified using the alias 'posdata'.\n    timestamps : array_like or None, optional\n        Time stamps corresponding to each sample in data. If None, timestamps\n        are automatically generated based on fs and start time.\n    fs : float, optional\n        Sampling frequency in Hz. Used to generate timestamps if not provided.\n    support : EpochArray or None, optional\n        EpochArray defining the time intervals over which the position data\n        is valid.\n    label : str, optional\n        Descriptive label for the position array.\n    xlim : tuple or None, optional\n        Spatial boundaries for x-coordinate as (min_x, max_x).\n    ylim : tuple or None, optional\n        Spatial boundaries for y-coordinate as (min_y, max_y).\n\n    Attributes\n    ----------\n    x : ndarray\n        X-coordinates as a 1D numpy array.\n    y : ndarray\n        Y-coordinates as a 1D numpy array (only available for 2D data).\n    is_1d : bool\n        True if position data is 1-dimensional.\n    is_2d : bool\n        True if position data is 2-dimensional.\n    xlim : tuple or None\n        Spatial boundaries for x-coordinate (only for 2D data).\n    ylim : tuple or None\n        Spatial boundaries for y-coordinate (only for 2D data).\n\n    Examples\n    --------\n    Create a 1D position array:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; # 1D position data (e.g., position on a linear track)\n    &gt;&gt;&gt; x_pos = np.linspace(0, 100, 1000)  # 100 cm track\n    &gt;&gt;&gt; timestamps = np.linspace(0, 10, 1000)  # 10 seconds\n    &gt;&gt;&gt; pos_1d = nel.PositionArray(\n    ...     data=x_pos[np.newaxis, :],\n    ...     timestamps=timestamps,\n    ...     label=\"Linear track position\",\n    ... )\n    &gt;&gt;&gt; print(f\"1D position: {pos_1d.is_1d}\")\n    &gt;&gt;&gt; print(f\"X range: {pos_1d.x.min():.1f} to {pos_1d.x.max():.1f} cm\")\n\n    Create a 2D position array:\n\n    &gt;&gt;&gt; # 2D position data (e.g., open field behavior)\n    &gt;&gt;&gt; t = np.linspace(0, 2 * np.pi, 1000)\n    &gt;&gt;&gt; x_pos = 50 + 30 * np.cos(t)  # circular trajectory\n    &gt;&gt;&gt; y_pos = 50 + 30 * np.sin(t)\n    &gt;&gt;&gt; pos_data = np.vstack([x_pos, y_pos])\n    &gt;&gt;&gt; pos_2d = nel.PositionArray(\n    ...     posdata=pos_data,\n    ...     fs=100,  # 100 Hz sampling\n    ...     xlim=(0, 100),\n    ...     ylim=(0, 100),\n    ...     label=\"Open field position\",\n    ... )\n    &gt;&gt;&gt; print(f\"2D position: {pos_2d.is_2d}\")\n    &gt;&gt;&gt; print(f\"X position shape: {pos_2d.x.shape}\")\n    &gt;&gt;&gt; print(f\"Y position shape: {pos_2d.y.shape}\")\n    &gt;&gt;&gt; print(f\"Spatial bounds: x={pos_2d.xlim}, y={pos_2d.ylim}\")\n\n    Access position data:\n\n    &gt;&gt;&gt; # Get position at specific time\n    &gt;&gt;&gt; time_idx = 500\n    &gt;&gt;&gt; if pos_2d.is_2d:\n    ...     x_at_time = pos_2d.x[time_idx]\n    ...     y_at_time = pos_2d.y[time_idx]\n    ...     print(f\"Position at sample {time_idx}: ({x_at_time:.1f}, {y_at_time:.1f})\")\n\n    Notes\n    -----\n    - For 2D position data, the first row of data should contain x-coordinates\n      and the second row should contain y-coordinates.\n    - The xlim and ylim parameters are only meaningful for 2D position data.\n    - Attempting to access y-coordinates or spatial limits on 1D data will\n      raise a ValueError.\n    - The 'posdata' alias can be used interchangeably with 'data' parameter.\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\"posdata\": \"data\"}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        xlim = kwargs.pop(\"xlim\", None)\n        ylim = kwargs.pop(\"ylim\", None)\n        super().__init__(*args, **kwargs)\n        self._xlim = xlim\n        self._ylim = ylim\n\n    @property\n    def is_2d(self):\n        try:\n            return self.n_signals == 2\n        except IndexError:\n            return False\n\n    @property\n    def is_1d(self):\n        try:\n            return self.n_signals == 1\n        except IndexError:\n            return False\n\n    @property\n    def x(self):\n        \"\"\"return x-values, as numpy array.\"\"\"\n        return self.data[0, :]\n\n    @property\n    def y(self):\n        \"\"\"return y-values, as numpy array.\"\"\"\n        if self.is_2d:\n            return self.data[1, :]\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so y-values are undefined!\"\n        )\n\n    @property\n    def xlim(self):\n        if self.is_2d:\n            return self._xlim\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so xlim is not undefined!\"\n        )\n\n    @xlim.setter\n    def xlim(self, val):\n        if self.is_2d:\n            self._xlim = xlim  # noqa: F821\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so xlim cannot be defined!\"\n        )\n\n    @property\n    def ylim(self):\n        if self.is_2d:\n            return self._ylim\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so ylim is not undefined!\"\n        )\n\n    @ylim.setter\n    def ylim(self, val):\n        if self.is_2d:\n            self._ylim = ylim  # noqa: F821\n        raise ValueError(\n            \"PositionArray is not 2 dimensional, so ylim cannot be defined!\"\n        )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.PositionArray.x","title":"<code>x</code>  <code>property</code>","text":"<p>return x-values, as numpy array.</p>"},{"location":"reference/nelpy/min/#nelpy.min.PositionArray.y","title":"<code>y</code>  <code>property</code>","text":"<p>return y-values, as numpy array.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray","title":"<code>RegularlySampledAnalogSignalArray</code>","text":"<p>Continuous analog signal(s) with regular sampling rates (irregular sampling rates can be corrected with operations on the support) and same support. NOTE: data that is not equal dimensionality will NOT work and error/warning messages may/may not be sent out. Assumes abscissa_vals are identical for all signals passed through and are therefore expected to be 1-dimensional.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray, with shape (n_signals, n_samples).</code> <p>Data samples.</p> <code>[]</code> <code>abscissa_vals</code> <code>np.ndarray, with shape (n_samples, ).</code> <p>The abscissa coordinate values. Currently we assume that (1) these values are timestamps, and (2) the timestamps are sampled regularly (we rely on these assumptions to generate intervals). Irregular sampling rates can be corrected with operations on the support.</p> <code>None</code> <code>fs</code> <code>float</code> <p>The sampling rate. abscissa_vals are still expected to be in units of time and fs is expected to be in the corresponding sampling rate (e.g. abscissa_vals in seconds, fs in Hz). Default is 1 Hz.</p> <code>None</code> <code>step</code> <code>float</code> <p>The sampling interval of the data, in seconds. Default is None. specifies step size of samples passed as tdata if fs is given, default is None. If not passed it is inferred by the minimum difference in between samples of tdata passed in (based on if FS is passed). e.g. decimated data would have sample numbers every ten samples so step=10</p> <code>None</code> <code>merge_sample_gap</code> <code>float</code> <p>Optional merging of gaps between support intervals. If intervals are within a certain amount of time, gap, they will be merged as one interval. Example use case is when there is a dropped sample</p> <code>0</code> <code>support</code> <code>IntervalArray</code> <p>Where the data are defined. Default is [0, last abscissa value] inclusive.</p> <code>None</code> <code>in_core</code> <code>bool</code> <p>Whether the abscissa values should be treated as residing in core memory. During RSASA construction, np.diff() is called, so for large data, passing in in_core=True might help. In that case, a slower but much smaller memory footprint function is used.</p> <code>True</code> <code>labels</code> <code>np.array, dtype=np.str</code> <p>Labels for each of the signals. If fewer labels than signals are passed in, labels are padded with None's to match the number of signals. If more labels than signals are passed in, labels are truncated to match the number of signals. Default is None.</p> <code>None</code> <code>empty</code> <code>bool</code> <p>Return an empty RegularlySampledAnalogSignalArray if true else false. Default is false.</p> <code>False</code> <code>abscissa</code> <code>optional</code> <p>The object handling the abscissa values. It is recommended to leave this parameter alone and let nelpy take care of this. Default is a nelpy.core.Abscissa object.</p> <code>None</code> <code>ordinate</code> <code>optional</code> <p>The object handling the ordinate values. It is recommended to leave this parameter alone and let nelpy take care of this. Default is a nelpy.core.Ordinate object.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>np.ndarray, with shape (n_signals, n_samples)</code> <p>The underlying data.</p> <code>abscissa_vals</code> <code>np.ndarray, with shape (n_samples, )</code> <p>The values of the abscissa coordinate.</p> <code>is1d</code> <code>bool</code> <p>Whether there is only 1 signal in the RSASA</p> <code>iswrapped</code> <code>bool</code> <p>Whether the RSASA's data is wrapping.</p> <code>base_unit</code> <code>string</code> <p>Base unit of the abscissa.</p> <code>signals</code> <code>list</code> <p>A list of RegularlySampledAnalogSignalArrays, each RSASA containing a single signal (channel). WARNING: this method creates a copy of each signal, so is not particularly efficient at this time.</p> <code>isreal</code> <code>bool</code> <p>Whether ALL of the values in the RSASA's data are real.</p> <code>iscomplex</code> <code>bool</code> <p>Whether ANY values in the data are complex.</p> <code>abs</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is the absolute value of the original original RSASA's (potentially complex) data.</p> <code>phase</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the phase angle (in radians) of the original RSASA's data.</p> <code>real</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the real part of the original RSASA's data.</p> <code>imag</code> <code>RegularlySampledAnalogSignalArray</code> <p>A copy of the RSASA, whose data is just the imaginary part of the original RSASA's data.</p> <code>lengths</code> <code>list</code> <p>The number of samples in each interval.</p> <code>labels</code> <code>list</code> <p>The labels corresponding to each signal.</p> <code>n_signals</code> <code>int</code> <p>The number of signals in the RSASA.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the RSASA.</p> <code>domain</code> <code>IntervalArray</code> <p>The domain of the RSASA.</p> <code>range</code> <code>IntervalArray</code> <p>The range of the RSASA's data.</p> <code>step</code> <code>float</code> <p>The sampling interval of the RSASA. Currently the units are in seconds.</p> <code>fs</code> <code>float</code> <p>The sampling frequency of the RSASA. Currently the units are in Hz.</p> <code>isempty</code> <code>bool</code> <p>Whether the underlying data has zero length, i.e. 0 samples</p> <code>n_bytes</code> <code>int</code> <p>Approximate number of bytes taken up by the RSASA.</p> <code>n_intervals</code> <code>int</code> <p>The number of underlying intervals in the RSASA.</p> <code>n_samples</code> <code>int</code> <p>The number of abscissa values in the RSASA.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>class RegularlySampledAnalogSignalArray:\n    \"\"\"Continuous analog signal(s) with regular sampling rates (irregular\n    sampling rates can be corrected with operations on the support) and same\n    support. NOTE: data that is not equal dimensionality will NOT work\n    and error/warning messages may/may not be sent out. Assumes abscissa_vals\n    are identical for all signals passed through and are therefore expected\n    to be 1-dimensional.\n\n    Parameters\n    ----------\n    data : np.ndarray, with shape (n_signals, n_samples).\n        Data samples.\n    abscissa_vals : np.ndarray, with shape (n_samples, ).\n        The abscissa coordinate values. Currently we assume that (1) these values\n        are timestamps, and (2) the timestamps are sampled regularly (we rely on\n        these assumptions to generate intervals). Irregular sampling rates can be\n        corrected with operations on the support.\n    fs : float, optional\n        The sampling rate. abscissa_vals are still expected to be in units of\n        time and fs is expected to be in the corresponding sampling rate (e.g.\n        abscissa_vals in seconds, fs in Hz).\n        Default is 1 Hz.\n    step : float, optional\n        The sampling interval of the data, in seconds.\n        Default is None.\n        specifies step size of samples passed as tdata if fs is given,\n        default is None. If not passed it is inferred by the minimum\n        difference in between samples of tdata passed in (based on if FS\n        is passed). e.g. decimated data would have sample numbers every\n        ten samples so step=10\n    merge_sample_gap : float, optional\n        Optional merging of gaps between support intervals. If intervals are within\n        a certain amount of time, gap, they will be merged as one interval. Example\n        use case is when there is a dropped sample\n    support : nelpy.IntervalArray, optional\n        Where the data are defined. Default is [0, last abscissa value] inclusive.\n    in_core : bool, optional\n        Whether the abscissa values should be treated as residing in core memory.\n        During RSASA construction, np.diff() is called, so for large data, passing\n        in in_core=True might help. In that case, a slower but much smaller memory\n        footprint function is used.\n    labels : np.array, dtype=np.str\n        Labels for each of the signals. If fewer labels than signals are passed in,\n        labels are padded with None's to match the number of signals. If more labels\n        than signals are passed in, labels are truncated to match the number of\n        signals.\n        Default is None.\n    empty : bool, optional\n        Return an empty RegularlySampledAnalogSignalArray if true else false.\n        Default is false.\n    abscissa : optional\n        The object handling the abscissa values. It is recommended to leave\n        this parameter alone and let nelpy take care of this.\n        Default is a nelpy.core.Abscissa object.\n    ordinate : optional\n        The object handling the ordinate values. It is recommended to leave\n        this parameter alone and let nelpy take care of this.\n        Default is a nelpy.core.Ordinate object.\n\n    Attributes\n    ----------\n    data : np.ndarray, with shape (n_signals, n_samples)\n        The underlying data.\n    abscissa_vals : np.ndarray, with shape (n_samples, )\n        The values of the abscissa coordinate.\n    is1d : bool\n        Whether there is only 1 signal in the RSASA\n    iswrapped : bool\n        Whether the RSASA's data is wrapping.\n    base_unit : string\n        Base unit of the abscissa.\n    signals : list\n        A list of RegularlySampledAnalogSignalArrays, each RSASA containing\n        a single signal (channel).\n        WARNING: this method creates a copy of each signal, so is not\n        particularly efficient at this time.\n    isreal : bool\n        Whether ALL of the values in the RSASA's data are real.\n    iscomplex : bool\n        Whether ANY values in the data are complex.\n    abs : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is the absolute value of the original\n        original RSASA's (potentially complex) data.\n    phase : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the phase angle (in radians) of\n        the original RSASA's data.\n    real : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the real part of the original\n        RSASA's data.\n    imag : nelpy.RegularlySampledAnalogSignalArray\n        A copy of the RSASA, whose data is just the imaginary part of the\n        original RSASA's data.\n    lengths : list\n        The number of samples in each interval.\n    labels : list\n        The labels corresponding to each signal.\n    n_signals : int\n        The number of signals in the RSASA.\n    support : nelpy.IntervalArray\n        The support of the RSASA.\n    domain : nelpy.IntervalArray\n        The domain of the RSASA.\n    range : nelpy.IntervalArray\n        The range of the RSASA's data.\n    step : float\n        The sampling interval of the RSASA. Currently the units are\n        in seconds.\n    fs : float\n        The sampling frequency of the RSASA. Currently the units are\n        in Hz.\n    isempty : bool\n        Whether the underlying data has zero length, i.e. 0 samples\n    n_bytes : int\n        Approximate number of bytes taken up by the RSASA.\n    n_intervals : int\n        The number of underlying intervals in the RSASA.\n    n_samples : int\n        The number of abscissa values in the RSASA.\n    \"\"\"\n\n    __aliases__ = {}\n\n    __attributes__ = [\n        \"_data\",\n        \"_abscissa_vals\",\n        \"_fs\",\n        \"_support\",\n        \"_interp\",\n        \"_step\",\n        \"_labels\",\n    ]\n\n    @rsasa_init_wrapper\n    def __init__(\n        self,\n        data=[],\n        *,\n        abscissa_vals=None,\n        fs=None,\n        step=None,\n        merge_sample_gap=0,\n        support=None,\n        in_core=True,\n        labels=None,\n        empty=False,\n        abscissa=None,\n        ordinate=None,\n    ):\n        self._intervalsignalslicer = IntervalSignalSlicer(self)\n        self._intervaldata = DataSlicer(self)\n        self._intervaltime = AbscissaSlicer(self)\n\n        self.type_name = self.__class__.__name__\n        if abscissa is None:\n            abscissa = core.Abscissa()  # TODO: integrate into constructor?\n        if ordinate is None:\n            ordinate = core.Ordinate()  # TODO: integrate into constructor?\n\n        self._abscissa = abscissa\n        self._ordinate = ordinate\n\n        # TODO: #FIXME abscissa and ordinate domain, range, and supports should be integrated and/or coerced with support\n\n        self.__version__ = version.__version__\n\n        # cast derivatives of RegularlySampledAnalogSignalArray back into RegularlySampledAnalogSignalArray:\n        # if isinstance(data, auxiliary.PositionArray):\n        if isinstance(data, RegularlySampledAnalogSignalArray):\n            self.__dict__ = copy.deepcopy(data.__dict__)\n            # if self._has_changed:\n            # self.__renew__()\n            self.__renew__()\n            return\n\n        if empty:\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            self._data = np.array([])\n            self._abscissa_vals = np.array([])\n            self.__bake__()\n            return\n\n        self._step = step\n        self._fs = fs\n\n        # Note; if we have an empty array of data with no dimension,\n        # then calling len(data) will return a TypeError\n        try:\n            # if no data are given return empty RegularlySampledAnalogSignalArray\n            if data.size == 0:\n                self.__init__(empty=True)\n                return\n        except TypeError:\n            logging.warning(\n                \"unsupported type; creating empty RegularlySampledAnalogSignalArray\"\n            )\n            self.__init__(empty=True)\n            return\n\n        # Note: if both abscissa_vals and data are given and dimensionality does not\n        # match, then TypeError!\n\n        abscissa_vals = np.squeeze(abscissa_vals).astype(float)\n        if abscissa_vals.shape[0] != data.shape[1]:\n            # self.__init__([],empty=True)\n            raise TypeError(\n                \"abscissa_vals and data size mismatch! Note: data \"\n                \"is expected to have rows containing signals\"\n            )\n        # data is not sorted and user wants it to be\n        # TODO: use faster is_sort from jagular\n        if not utils.is_sorted(abscissa_vals):\n            logging.warning(\"Data is _not_ sorted! Data will be sorted automatically.\")\n            ind = np.argsort(abscissa_vals)\n            abscissa_vals = abscissa_vals[ind]\n            data = np.take(a=data, indices=ind, axis=-1)\n\n        self._data = data\n        self._abscissa_vals = abscissa_vals\n\n        # handle labels\n        if labels is not None:\n            labels = np.asarray(labels, dtype=str)\n            # label size doesn't match\n            if labels.shape[0] &gt; data.shape[0]:\n                logging.warning(\n                    \"More labels than data! Labels are truncated to size of data\"\n                )\n                labels = labels[0 : data.shape[0]]\n            elif labels.shape[0] &lt; data.shape[0]:\n                logging.warning(\n                    \"Fewer labels than abscissa_vals! Labels are filled with \"\n                    \"None to match data shape\"\n                )\n                for i in range(labels.shape[0], data.shape[0]):\n                    labels.append(None)\n        self._labels = labels\n\n        # Alright, let's handle all the possible parameter cases!\n        if support is not None:\n            self._restrict_to_interval_array_fast(intervalarray=support)\n        else:\n            logging.warning(\n                \"creating support from abscissa_vals and sampling rate, fs!\"\n            )\n            self._abscissa.support = type(self._abscissa.support)(\n                utils.get_contiguous_segments(\n                    self._abscissa_vals, step=self._step, fs=fs, in_core=in_core\n                )\n            )\n            if merge_sample_gap &gt; 0:\n                self._abscissa.support = self._abscissa.support.merge(\n                    gap=merge_sample_gap\n                )\n\n        if np.abs((self.fs - self._estimate_fs()) / self.fs) &gt; 0.01:\n            logging.warning(\"estimated fs and provided fs differ by more than 1%\")\n\n    def __bake__(self):\n        \"\"\"Fix object as-is, and bake a new hash.\n\n        For example, if a label has changed, or if an interp has been attached,\n        then the object's hash will change, and it needs to be baked\n        again for efficiency / consistency.\n        \"\"\"\n        self._stored_hash_ = self.__hash__()\n\n    # def _has_changed_data(self):\n    #     \"\"\"Compute hash on abscissa_vals and data and compare to cached hash.\"\"\"\n    #     return self.data.__hash__ elf._data_hash_\n\n    def _has_changed(self):\n        \"\"\"Compute hash on current object, and compare to previously stored hash\"\"\"\n        return self.__hash__() == self._stored_hash_\n\n    def __renew__(self):\n        \"\"\"Re-attach data slicers.\"\"\"\n        self._intervalsignalslicer = IntervalSignalSlicer(self)\n        self._intervaldata = DataSlicer(self)\n        self._intervaltime = AbscissaSlicer(self)\n        self._interp = None\n        self.__bake__()\n\n    def __call__(self, x):\n        \"\"\"RegularlySampledAnalogSignalArray callable method. Returns\n        interpolated data at requested points. Note that points falling\n        outside the support will not be interpolated.\n\n        Parameters\n        ----------\n        x : np.ndarray, list, or tuple, with length n_requested_samples\n            Points at which to interpolate the RSASA's data\n\n        Returns\n        -------\n        A np.ndarray with shape (n_signals, n_samples). If all the requested\n        points lie in the support, then n_samples = n_requested_samples.\n        Otherwise n_samples &lt; n_requested_samples.\n        \"\"\"\n\n        return self.asarray(at=x).yvals\n\n    def center(self, inplace=False):\n        \"\"\"\n        Center the data to have zero mean along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The centered signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; centered = asa.center()\n        &gt;&gt;&gt; centered.mean()\n        0.0\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        return out\n\n    def normalize(self, inplace=False):\n        \"\"\"\n        Normalize the data to have unit standard deviation along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The normalized signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; normalized = asa.normalize()\n        &gt;&gt;&gt; normalized.std()\n        1.0\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        std = np.atleast_1d(out.std())\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    def standardize(self, inplace=False):\n        \"\"\"\n        Standardize the data to zero mean and unit standard deviation along the sample axis.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The standardized signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; standardized = asa.standardize()\n        &gt;&gt;&gt; standardized.mean(), standardized.std()\n        (0.0, 1.0)\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n        out._data = (out._data.T - out.mean()).T\n        std = np.atleast_1d(out.std())\n        std[std == 0] = 1\n        out._data = (out._data.T / std).T\n        return out\n\n    @property\n    def is_1d(self):\n        try:\n            return self.n_signals == 1\n        except IndexError:\n            return False\n\n    @property\n    def is_wrapped(self):\n        if np.any(self.max() &gt; self._ordinate.range.stop) | np.any(\n            self.min() &lt; self._ordinate.range.min\n        ):\n            self._ordinate._is_wrapped = False\n        else:\n            self._ordinate._is_wrapped = True\n\n        # if self._ordinate._is_wrapped is None:\n        #     if np.any(self.max() &gt; self._ordinate.range.stop) | np.any(self.min() &lt; self._ordinate.range.min):\n        #         self._ordinate._is_wrapped = False\n        #     else:\n        #         self._ordinate._is_wrapped = True\n        return self._ordinate._is_wrapped\n\n    def _wrap(self, arr, vmin, vmax):\n        \"\"\"Wrap array within finite range.\"\"\"\n        if np.isinf(vmax - vmin):\n            raise ValueError(\"range has to be finite!\")\n        return ((arr - vmin) % (vmax - vmin)) + vmin\n\n    def wrap(self, inplace=False):\n        \"\"\"\n        Wrap the ordinate values within the finite range defined by the ordinate's range.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The wrapped signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; wrapped = asa.wrap()\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        out.data = np.atleast_2d(\n            out._wrap(out.data, out._ordinate.range.min, out._ordinate.range.max)\n        )\n        # out._is_wrapped = True\n        return out\n\n    def _unwrap(self, arr, vmin, vmax):\n        \"\"\"Unwrap 2D array (with one signal per row) by minimizing total displacement.\"\"\"\n        d = vmax - vmin\n        dh = d / 2\n\n        lin = copy.deepcopy(arr) - vmin\n        n_signals, n_samples = arr.shape\n        for ii in range(1, n_samples):\n            h1 = lin[:, ii] - lin[:, ii - 1] &gt;= dh\n            lin[h1, ii:] = lin[h1, ii:] - d\n            h2 = lin[:, ii] - lin[:, ii - 1] &lt; -dh\n            lin[h2, ii:] = lin[h2, ii:] + d\n        return np.atleast_2d(lin + vmin)\n\n    def unwrap(self, inplace=False):\n        \"\"\"\n        Unwrap the ordinate values by minimizing total displacement, useful for phase data.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            If True, modifies the data in place. If False (default), returns a new object.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The unwrapped signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; unwrapped = asa.unwrap()\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        out.data = np.atleast_2d(\n            out._unwrap(out._data, out._ordinate.range.min, out._ordinate.range.max)\n        )\n        # out._is_wrapped = False\n        return out\n\n    def _crossvals(self):\n        \"\"\"Return all abscissa values where the orinate crosses.\n\n        Note that this can return multiple values close in succession\n        if the signal oscillates around the maximum or minimum range.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def base_unit(self):\n        \"\"\"Base unit of the abscissa.\"\"\"\n        return self._abscissa.base_unit\n\n    def _data_interval_indices(self):\n        \"\"\"\n        Get the start and stop indices for each interval in the analog signal array.\n\n        Returns\n        -------\n        indices : np.ndarray\n            Array of shape (n_intervals, 2), where each row contains the start and stop indices for an interval.\n        \"\"\"\n        tmp = np.insert(np.cumsum(self.lengths), 0, 0)\n        indices = np.vstack((tmp[:-1], tmp[1:])).T\n        return indices\n\n    def ddt(self, rectify=False):\n        \"\"\"Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n        asa.data = f(t)\n        asa.ddt = d/dt (asa.data)\n\n        Parameters\n        ----------\n        rectify : boolean, optional\n            If True, the absolute value of the derivative will be returned.\n            Default is False.\n\n        Returns\n        -------\n        ddt : RegularlySampledAnalogSignalArray\n            Time derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n        Note\n        ----\n        Second order central differences are used here, and it is assumed that\n        the signals are sampled uniformly. If the signals are not uniformly\n        sampled, it is recommended to resample the signal before computing the\n        derivative.\n        \"\"\"\n        ddt = utils.ddt_asa(self, rectify=rectify)\n        return ddt\n\n    @property\n    def signals(self):\n        \"\"\"Returns a list of RegularlySampledAnalogSignalArrays, each array containing\n        a single signal (channel).\n\n        WARNING: this method creates a copy of each signal, so is not\n        particularly efficient at this time.\n\n        Examples\n        --------\n        &gt;&gt;&gt; for channel in lfp.signals:\n            print(channel)\n        \"\"\"\n        signals = []\n        for ii in range(self.n_signals):\n            signals.append(self[:, ii])\n        return signals\n        # return np.asanyarray(signals).squeeze()\n\n    @property\n    def isreal(self):\n        \"\"\"Returns True if entire signal is real.\"\"\"\n        return np.all(np.isreal(self.data))\n        # return np.isrealobj(self._data)\n\n    @property\n    def iscomplex(self):\n        \"\"\"Returns True if any part of the signal is complex.\"\"\"\n        return np.any(np.iscomplex(self.data))\n        # return np.iscomplexobj(self._data)\n\n    @property\n    def abs(self):\n        \"\"\"RegularlySampledAnalogSignalArray with absolute value of (potentially complex) data.\"\"\"\n        out = self.copy()\n        out._data = np.abs(self.data)\n        return out\n\n    @property\n    def angle(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only phase angle (in radians) of data.\"\"\"\n        out = self.copy()\n        out._data = np.angle(self.data)\n        return out\n\n    @property\n    def imag(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only imaginary part of data.\"\"\"\n        out = self.copy()\n        out._data = self.data.imag\n        return out\n\n    @property\n    def real(self):\n        \"\"\"RegularlySampledAnalogSignalArray with only real part of data.\"\"\"\n        out = self.copy()\n        out._data = self.data.real\n        return out\n\n    def __mul__(self, other):\n        \"\"\"overloaded * operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data * other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T * other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to multiply.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data * other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for *: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __add__(self, other):\n        \"\"\"overloaded + operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data + other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T + other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to add.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data + other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for +: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __sub__(self, other):\n        \"\"\"overloaded - operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data - other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T - other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to subtract.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data - other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for -: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def zscore(self):\n        \"\"\"\n        Normalize each signal in the array using z-scores (zero mean, unit variance).\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            New object with z-scored data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; zscored = asa.zscore()\n        \"\"\"\n        out = self.copy()\n        out._data = zscore(out._data, axis=1)\n        return out\n\n    def __truediv__(self, other):\n        \"\"\"overloaded / operator.\"\"\"\n        if isinstance(other, numbers.Number):\n            newasa = self.copy()\n            newasa._data = self.data / other\n            return newasa\n        elif isinstance(other, np.ndarray):\n            newasa = self.copy()\n            newasa._data = (self.data.T / other).T\n            return newasa\n        elif isinstance(other, RegularlySampledAnalogSignalArray):\n            if (\n                self.data.shape != other.data.shape\n                or not np.allclose(self.abscissa_vals, other.abscissa_vals)\n                or self.fs != other.fs\n            ):\n                raise ValueError(\n                    \"AnalogSignalArrays must have the same shape, abscissa_vals, and fs to divide.\"\n                )\n            newasa = self.copy()\n            newasa._data = self.data / other.data\n            return newasa\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for /: 'RegularlySampledAnalogSignalArray' and '{}'\".format(\n                    str(type(other))\n                )\n            )\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def __lshift__(self, val):\n        \"\"\"shift abscissa and support to left (&lt;&lt;)\"\"\"\n        if isinstance(val, numbers.Number):\n            new = self.copy()\n            new._abscissa_vals -= val\n            new._abscissa.support = new._abscissa.support &lt;&lt; val\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &lt;&lt;: {} and {}\".format(\n                    str(type(self)), str(type(val))\n                )\n            )\n\n    def __rshift__(self, val):\n        \"\"\"shift abscissa and support to right (&gt;&gt;)\"\"\"\n        if isinstance(val, numbers.Number):\n            new = self.copy()\n            new._abscissa_vals += val\n            new._abscissa.support = new._abscissa.support &gt;&gt; val\n            return new\n        else:\n            raise TypeError(\n                \"unsupported operand type(s) for &gt;&gt;: {} and {}\".format(\n                    str(type(self)), str(type(val))\n                )\n            )\n\n    def __len__(self):\n        return self.n_intervals\n\n    def _drop_empty_intervals(self):\n        \"\"\"Drops empty intervals from support. In-place.\"\"\"\n        keep_interval_ids = np.argwhere(self.lengths).squeeze().tolist()\n        self._abscissa.support = self._abscissa.support[keep_interval_ids]\n        return self\n\n    def _estimate_fs(self, abscissa_vals=None):\n        \"\"\"Estimate the sampling rate of the data.\"\"\"\n        if abscissa_vals is None:\n            abscissa_vals = self._abscissa_vals\n        return 1.0 / np.median(np.diff(abscissa_vals))\n\n    def downsample(self, *, fs_out, aafilter=True, inplace=False, **kwargs):\n        \"\"\"Downsamples the RegularlySampledAnalogSignalArray\n\n        Parameters\n        ----------\n        fs_out : float, optional\n            Desired output sampling rate in Hz\n        aafilter : boolean, optional\n            Whether to apply an anti-aliasing filter before performing the actual\n            downsampling. Default is True\n        inplace : boolean, optional\n            If True, the output ASA will replace the input ASA. Default is False\n        kwargs :\n            Other keyword arguments are passed to sosfiltfilt() in the `filtering`\n            module\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            The downsampled RegularlySampledAnalogSignalArray\n        \"\"\"\n\n        if not fs_out &lt; self._fs:\n            raise ValueError(\"fs_out must be less than current sampling rate!\")\n\n        if aafilter:\n            fh = fs_out / 2.0\n            out = filtering.sosfiltfilt(self, fl=None, fh=fh, inplace=inplace, **kwargs)\n\n        downsampled = out.simplify(ds=1 / fs_out)\n        out._data = downsampled._data\n        out._abscissa_vals = downsampled._abscissa_vals\n        out._fs = fs_out\n\n        out.__renew__()\n        return out\n\n    def add_signal(self, signal, label=None):\n        \"\"\"Docstring goes here.\n        Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?\n        \"\"\"\n        # TODO: add functionality to check that supports are the same, etc.\n        if isinstance(signal, RegularlySampledAnalogSignalArray):\n            signal = signal.data\n\n        signal = np.squeeze(signal)\n        if signal.ndim &gt; 1:\n            raise TypeError(\"Can only add one signal at a time!\")\n        if self.data.ndim == 1:\n            self._data = np.vstack(\n                [np.array(self.data, ndmin=2), np.array(signal, ndmin=2)]\n            )\n        else:\n            self._data = np.vstack([self.data, np.array(signal, ndmin=2)])\n        if label is None:\n            logging.warning(\"None label appended\")\n        self._labels = np.append(self._labels, label)\n        return self\n\n    def _restrict_to_interval_array_fast(self, *, intervalarray=None, update=True):\n        \"\"\"Restrict self._abscissa_vals and self._data to an IntervalArray. If no\n        IntervalArray is specified, self._abscissa.support is used.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray, optional\n                IntervalArray on which to restrict AnalogSignal. Default is\n                self._abscissa.support\n        update : bool, optional\n                Overwrite self._abscissa.support with intervalarray if True (default).\n        \"\"\"\n        if intervalarray is None:\n            intervalarray = self._abscissa.support\n            update = False  # support did not change; no need to update\n\n        try:\n            if intervalarray.isempty:\n                logging.warning(\"Support specified is empty\")\n                # self.__init__([],empty=True)\n                exclude = [\"_support\", \"_data\", \"_fs\", \"_step\"]\n                attrs = (x for x in self.__attributes__ if x not in exclude)\n                logging.disable(logging.CRITICAL)\n                for attr in attrs:\n                    exec(\"self.\" + attr + \" = None\")\n                logging.disable(0)\n                self._data = np.zeros([0, self.data.shape[0]])\n                self._data[:] = np.nan\n                self._abscissa.support = intervalarray\n                return\n        except AttributeError:\n            raise AttributeError(\"IntervalArray expected\")\n\n        indices = []\n        for interval in intervalarray.merge().data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            frm, to = np.searchsorted(self._abscissa_vals, (a_start, a_stop + 1e-10))\n            indices.append((frm, to))\n        indices = np.array(indices, ndmin=2)\n        if np.diff(indices).sum() &lt; len(self._abscissa_vals):\n            logging.warning(\"ignoring signal outside of support\")\n        # check if only one interval and interval is already bounds of data\n        # if so, we don't need to do anything\n        if len(indices) == 1:\n            if indices[0, 0] == 0 and indices[0, 1] == len(self._abscissa_vals):\n                if update:\n                    self._abscissa.support = intervalarray\n                    return\n        try:\n            data_list = []\n            for start, stop in indices:\n                data_list.append(self._data[:, start:stop])\n            self._data = np.hstack(data_list)\n        except IndexError:\n            self._data = np.zeros([0, self.data.shape[0]])\n            self._data[:] = np.nan\n        time_list = []\n        for start, stop in indices:\n            time_list.extend(self._abscissa_vals[start:stop])\n        self._abscissa_vals = np.array(time_list)\n        if update:\n            self._abscissa.support = intervalarray\n\n    def _restrict_to_interval_array(self, *, intervalarray=None, update=True):\n        \"\"\"Restrict self._abscissa_vals and self._data to an IntervalArray. If no\n        IntervalArray is specified, self._abscissa.support is used.\n\n        This function is quite slow, as it checks each sample for inclusion.\n        It does this in a vectorized form, which is fast for small or moderately\n        sized objects, but the memory penalty can be large, and it becomes very\n        slow for large objects. Consequently, _restrict_to_interval_array_fast\n        should be used when possible.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray, optional\n                IntervalArray on which to restrict AnalogSignal. Default is\n                self._abscissa.support\n        update : bool, optional\n                Overwrite self._abscissa.support with intervalarray if True (default).\n        \"\"\"\n        if intervalarray is None:\n            intervalarray = self._abscissa.support\n            update = False  # support did not change; no need to update\n\n        try:\n            if intervalarray.isempty:\n                logging.warning(\"Support specified is empty\")\n                # self.__init__([],empty=True)\n                exclude = [\"_support\", \"_data\", \"_fs\", \"_step\"]\n                attrs = (x for x in self.__attributes__ if x not in exclude)\n                logging.disable(logging.CRITICAL)\n                for attr in attrs:\n                    exec(\"self.\" + attr + \" = None\")\n                logging.disable(0)\n                self._data = np.zeros([0, self.data.shape[0]])\n                self._data[:] = np.nan\n                self._abscissa.support = intervalarray\n                return\n        except AttributeError:\n            raise AttributeError(\"IntervalArray expected\")\n\n        indices = []\n        for interval in intervalarray.merge().data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            indices.append(\n                (self._abscissa_vals &gt;= a_start) &amp; (self._abscissa_vals &lt; a_stop)\n            )\n        indices = np.any(np.column_stack(indices), axis=1)\n        if np.count_nonzero(indices) &lt; len(self._abscissa_vals):\n            logging.warning(\"ignoring signal outside of support\")\n        try:\n            self._data = self.data[:, indices]\n        except IndexError:\n            self._data = np.zeros([0, self.data.shape[0]])\n            self._data[:] = np.nan\n        self._abscissa_vals = self._abscissa_vals[indices]\n        if update:\n            self._abscissa.support = intervalarray\n\n    @keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\n    def smooth(\n        self,\n        *,\n        fs=None,\n        sigma=None,\n        truncate=None,\n        inplace=False,\n        mode=None,\n        cval=None,\n        within_intervals=False,\n    ):\n        \"\"\"Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.\n\n        Smoothing is applied along the abscissa, and the same smoothing is applied to each\n        signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n        Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n        Parameters\n        ----------\n        obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n        fs : float, optional\n            Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n            be inferred.\n        sigma : float, optional\n            Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n            (50 ms if base_unit=seconds).\n        truncate : float, optional\n            Bandwidth outside of which the filter value will be zero. Default is 4.0.\n        inplace : bool\n            If True the data will be replaced with the smoothed data.\n            Default is False.\n        mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n            The mode parameter determines how the array borders are handled,\n            where cval is the value when mode is equal to 'constant'. Default is\n            'reflect'.\n        cval : scalar, optional\n            Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n        within_intervals : boolean, optional\n            If True, then smooth within each epoch. Otherwise smooth across epochs.\n            Default is False.\n            Note that when mode = 'wrap', then smoothing within epochs aren't affected\n            by wrapping.\n\n        Returns\n        -------\n        out : same type as obj\n            An object with smoothed data is returned.\n\n        \"\"\"\n\n        if sigma is None:\n            sigma = 0.05\n        if truncate is None:\n            truncate = 4\n\n        kwargs = {\n            \"inplace\": inplace,\n            \"fs\": fs,\n            \"sigma\": sigma,\n            \"truncate\": truncate,\n            \"mode\": mode,\n            \"cval\": cval,\n            \"within_intervals\": within_intervals,\n        }\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        if self._ordinate.is_wrapping:\n            ord_is_wrapped = self.is_wrapped\n\n            if ord_is_wrapped:\n                out = out.unwrap()\n\n        # case 1: abs.wrapping=False, ord.linking=False, ord.wrapping=False\n        if (\n            not self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            pass\n\n        # case 2: abs.wrapping=False, ord.linking=False, ord.wrapping=True\n        elif (\n            not self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            pass\n\n        # case 3: abs.wrapping=False, ord.linking=True, ord.wrapping=False\n        elif (\n            not self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 4: abs.wrapping=False, ord.linking=True, ord.wrapping=True\n        elif (\n            not self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 5: abs.wrapping=True, ord.linking=False, ord.wrapping=False\n        elif (\n            self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            if mode is None:\n                kwargs[\"mode\"] = \"wrap\"\n\n        # case 6: abs.wrapping=True, ord.linking=False, ord.wrapping=True\n        elif (\n            self._abscissa.is_wrapping\n            and not self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            # (1) unwrap ordinate (abscissa wrap=False)\n            # (2) smooth unwrapped ordinate (absissa wrap=False)\n            # (3) repeat unwrapped signal based on conditions from (2):\n            # if smoothed wrapped ordinate samples\n            # HH ==&gt; SSS (this must be done on a per-signal basis!!!) H = high; L = low; S = same\n            # LL ==&gt; SSS (the vertical offset must be such that neighbors have smallest displacement)\n            # LH ==&gt; LSH\n            # HL ==&gt; HSL\n            # (4) smooth expanded and unwrapped ordinate (abscissa wrap=False)\n            # (5) cut out orignal signal\n\n            # (1)\n            kwargs[\"mode\"] = \"reflect\"\n            L = out._ordinate.range.max - out._ordinate.range.min\n            D = out.domain.length\n\n            tmp = utils.gaussian_filter(out.unwrap(), **kwargs)\n            # (2) (3)\n            n_reps = int(np.ceil((sigma * truncate) / float(D)))\n\n            smooth_data = []\n            for ss, signal in enumerate(tmp.signals):\n                # signal = signal.wrap()\n                offset = (\n                    float((signal._data[:, -1] - signal._data[:, 0]) // (L / 2)) * L\n                )\n                # print(offset)\n                # left_high = signal._data[:,0] &gt;= out._ordinate.range.min + L/2\n                # right_high = signal._data[:,-1] &gt;= out._ordinate.range.min + L/2\n                # signal = signal.unwrap()\n\n                expanded = signal.copy()\n                for nn in range(n_reps):\n                    expanded = expanded.join((signal &lt;&lt; D * (nn + 1)) - offset).join(\n                        (signal &gt;&gt; D * (nn + 1)) + offset\n                    )\n                    # print(expanded)\n                    # if left_high == right_high:\n                    #     print('extending flat! signal {}'.format(ss))\n                    #     expanded = expanded.join(signal &lt;&lt; D*(nn+1)).join(signal &gt;&gt; D*(nn+1))\n                    # elif left_high &lt; right_high:\n                    #     print('extending LSH! signal {}'.format(ss))\n                    #     # LSH\n                    #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))-L).join((signal &gt;&gt; D*(nn+1))+L)\n                    # else:\n                    #     # HSL\n                    #     print('extending HSL! signal {}'.format(ss))\n                    #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))+L).join((signal &gt;&gt; D*(nn+1))-L)\n                # (4)\n                smooth_signal = utils.gaussian_filter(expanded, **kwargs)\n                smooth_data.append(\n                    smooth_signal._data[\n                        :, n_reps * tmp.n_samples : (n_reps + 1) * (tmp.n_samples)\n                    ].squeeze()\n                )\n            # (5)\n            out._data = np.array(smooth_data)\n            out.__renew__()\n\n            if self._ordinate.is_wrapping:\n                if ord_is_wrapped:\n                    out = out.wrap()\n\n            return out\n\n        # case 7: abs.wrapping=True, ord.linking=True, ord.wrapping=False\n        elif (\n            self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and not self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        # case 8: abs.wrapping=True, ord.linking=True, ord.wrapping=True\n        elif (\n            self._abscissa.is_wrapping\n            and self._ordinate.is_linking\n            and self._ordinate.is_wrapping\n        ):\n            raise NotImplementedError\n\n        out = utils.gaussian_filter(out, **kwargs)\n        out.__renew__()\n\n        if self._ordinate.is_wrapping:\n            if ord_is_wrapped:\n                out = out.wrap()\n\n        return out\n\n    @property\n    def lengths(self):\n        \"\"\"(list) The number of samples in each interval.\"\"\"\n        indices = []\n        for interval in self.support.data:\n            a_start = interval[0]\n            a_stop = interval[1]\n            frm, to = np.searchsorted(self._abscissa_vals, (a_start, a_stop))\n            indices.append((frm, to))\n        indices = np.array(indices, ndmin=2)\n        lengths = np.atleast_1d(np.diff(indices).squeeze())\n        return lengths\n\n    @property\n    def labels(self):\n        \"\"\"(list) The labels corresponding to each signal.\"\"\"\n        # TODO: make this faster and better!\n        return self._labels\n\n    @property\n    def n_signals(self):\n        \"\"\"(int) The number of signals.\"\"\"\n        try:\n            return utils.PrettyInt(self.data.shape[0])\n        except AttributeError:\n            return 0\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self.n_intervals)\n        else:\n            epstr = \"\"\n        try:\n            if self.n_signals &gt; 0:\n                nstr = \" %s signals%s\" % (self.n_signals, epstr)\n        except IndexError:\n            nstr = \" 1 signal%s\" % epstr\n        dstr = \" for a total of {}\".format(\n            self._abscissa.formatter(self.support.length)\n        )\n        return \"&lt;%s%s:%s&gt;%s\" % (self.type_name, address_str, nstr, dstr)\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns an RegularlySampledAnalogSignalArray whose support has been\n        partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_samples : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            RegularlySampledAnalogSignalArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_samples or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        out._abscissa.support = out.support.partition(ds=ds, n_intervals=n_intervals)\n        return out\n\n    # @property\n    # def ydata(self):\n    #     \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n    #     # LEGACY\n    #     return self.data\n\n    @property\n    def data(self):\n        \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, val):\n        \"\"\"(np.array N-Dimensional) data with shape (n_signals, n_samples).\"\"\"\n        self._data = val\n        # print('data was modified, so clearing interp, etc.')\n        self.__renew__()\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval_array_fast(intervalarray=self._abscissa.support)\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._restrict_to_interval_array_fast(intervalarray=self._abscissa.support)\n\n    @property\n    def range(self):\n        \"\"\"(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        return self._ordinate.range\n\n    @range.setter\n    def range(self, val):\n        \"\"\"(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.\"\"\"\n        # modify range\n        self._ordinate.range = val\n\n    @property\n    def step(self):\n        \"\"\"steps per sample\n        Example 1: sample_numbers = np.array([1,2,3,4,5,6]) #aka time\n        Steps per sample in the above case would be 1\n\n        Example 2: sample_numbers = np.array([1,3,5,7,9]) #aka time\n        Steps per sample in Example 2 would be 2\n        \"\"\"\n        return self._step\n\n    @property\n    def abscissa_vals(self):\n        \"\"\"(np.array 1D) Time in seconds.\"\"\"\n        return self._abscissa_vals\n\n    @abscissa_vals.setter\n    def abscissa_vals(self, vals):\n        \"\"\"(np.array 1D) Time in seconds.\"\"\"\n        self._abscissa_vals = vals\n\n    @property\n    def fs(self):\n        \"\"\"(float) Sampling frequency.\"\"\"\n        if self._fs is None:\n            logging.warning(\"No sampling frequency has been specified!\")\n        return self._fs\n\n    @property\n    def isempty(self):\n        \"\"\"(bool) checks length of data input\"\"\"\n        try:\n            return self.data.shape[1] == 0\n        except IndexError:  # IndexError should happen if _data = []\n            return True\n\n    @property\n    def n_bytes(self):\n        \"\"\"Approximate number of bytes taken up by object.\"\"\"\n        return utils.PrettyBytes(self.data.nbytes + self._abscissa_vals.nbytes)\n\n    @property\n    def n_intervals(self):\n        \"\"\"(int) number of intervals in RegularlySampledAnalogSignalArray\"\"\"\n        return self._abscissa.support.n_intervals\n\n    @property\n    def n_samples(self):\n        \"\"\"(int) number of abscissa samples where signal is defined.\"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(len(self._abscissa_vals))\n\n    def __iter__(self):\n        \"\"\"AnalogSignal iterator initialization\"\"\"\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"AnalogSignal iterator advancer.\"\"\"\n        index = self._index\n        if index &gt; self.n_intervals - 1:\n            raise StopIteration\n        logging.disable(logging.CRITICAL)\n        intervalarray = type(self.support)(empty=True)\n        exclude = [\"_abscissa_vals\"]\n        attrs = (x for x in self._abscissa.support.__attributes__ if x not in exclude)\n\n        for attr in attrs:\n            exec(\"intervalarray.\" + attr + \" = self._abscissa.support.\" + attr)\n        try:\n            intervalarray._data = self._abscissa.support.data[\n                tuple([index]), :\n            ]  # use np integer indexing! Cool!\n        except IndexError:\n            # index is out of bounds, so return an empty IntervalArray\n            pass\n        logging.disable(0)\n\n        self._index += 1\n\n        asa = type(self)([], empty=True)\n        exclude = [\"_interp\", \"_support\"]\n        attrs = (x for x in self.__attributes__ if x not in exclude)\n        logging.disable(logging.CRITICAL)\n        for attr in attrs:\n            exec(\"asa.\" + attr + \" = self.\" + attr)\n        logging.disable(0)\n        asa._restrict_to_interval_array_fast(intervalarray=intervalarray)\n        if asa.support.isempty:\n            logging.warning(\n                \"Support is empty. Empty RegularlySampledAnalogSignalArray returned\"\n            )\n            asa = type(self)([], empty=True)\n\n        asa.__renew__()\n        return asa\n\n    def empty(self, inplace=True):\n        \"\"\"Remove data (but not metadata) from RegularlySampledAnalogSignalArray.\n\n        Attributes 'data', 'abscissa_vals', and 'support' are all emptied.\n\n        Note: n_signals is preserved.\n        \"\"\"\n        n_signals = self.n_signals\n        if not inplace:\n            out = self._copy_without_data()\n        else:\n            out = self\n            out._data = np.zeros((n_signals, 0))\n        out._abscissa.support = type(self.support)(empty=True)\n        out._abscissa_vals = []\n        out.__renew__()\n        return out\n\n    def __getitem__(self, idx):\n        \"\"\"RegularlySampledAnalogSignalArray index access.\n\n        Parameters\n        ----------\n        idx : IntervalArray, int, slice\n            intersect passed intervalarray with support,\n            index particular a singular interval or multiple intervals with slice\n        \"\"\"\n        intervalslice, signalslice = self._intervalsignalslicer[idx]\n\n        asa = self._subset(signalslice)\n\n        if asa.isempty:\n            asa.__renew__()\n            return asa\n\n        if isinstance(intervalslice, slice):\n            if (\n                intervalslice.start is None\n                and intervalslice.stop is None\n                and intervalslice.step is None\n            ):\n                asa.__renew__()\n                return asa\n\n        newintervals = self._abscissa.support[intervalslice]\n        # TODO: this needs to change so that n_signals etc. are preserved\n        ################################################################\n        if newintervals.isempty:\n            logging.warning(\"Index resulted in empty interval array\")\n            return self.empty(inplace=False)\n        ################################################################\n\n        asa._restrict_to_interval_array_fast(intervalarray=newintervals)\n        asa.__renew__()\n        return asa\n\n    def _subset(self, idx):\n        asa = self.copy()\n        try:\n            asa._data = np.atleast_2d(self.data[idx, :])\n        except IndexError:\n            raise IndexError(\n                \"index {} is out of bounds for n_signals with size {}\".format(\n                    idx, self.n_signals\n                )\n            )\n        asa.__renew__()\n        return asa\n\n    def _copy_without_data(self):\n        \"\"\"Return a copy of self, without data and abscissa_vals.\n\n        Note: the support is left unchanged.\n        \"\"\"\n        out = copy.copy(self)  # shallow copy\n        out._abscissa_vals = None\n        out._data = np.zeros((self.n_signals, 0))\n        out = copy.deepcopy(\n            out\n        )  # just to be on the safe side, but at least now we are not copying the data!\n        out.__renew__()\n        return out\n\n    def copy(self):\n        \"\"\"Return a copy of the current object.\"\"\"\n        out = copy.deepcopy(self)\n        out.__renew__()\n        return out\n\n    def median(self, *, axis=1):\n        \"\"\"Returns the median of each signal in RegularlySampledAnalogSignalArray.\"\"\"\n        try:\n            medians = np.nanmedian(self.data, axis=axis).squeeze()\n            if medians.size == 1:\n                return medians.item()\n            return medians\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate median\"\n            )\n\n    def mean(self, *, axis=1):\n        \"\"\"\n        Compute the mean of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the mean (default is 1, i.e., across samples).\n\n        Returns\n        -------\n        mean : np.ndarray\n            Mean values along the specified axis.\n        \"\"\"\n        try:\n            means = np.nanmean(self.data, axis=axis).squeeze()\n            if means.size == 1:\n                return means.item()\n            return means\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate mean\"\n            )\n\n    def std(self, *, axis=1):\n        \"\"\"\n        Compute the standard deviation of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the standard deviation (default is 1).\n\n        Returns\n        -------\n        std : np.ndarray\n            Standard deviation values along the specified axis.\n        \"\"\"\n        try:\n            stds = np.nanstd(self.data, axis=axis).squeeze()\n            if stds.size == 1:\n                return stds.item()\n            return stds\n        except IndexError:\n            raise IndexError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate standard deviation\"\n            )\n\n    def max(self, *, axis=1):\n        \"\"\"\n        Compute the maximum value of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the maximum (default is 1).\n\n        Returns\n        -------\n        max : np.ndarray\n            Maximum values along the specified axis.\n        \"\"\"\n        try:\n            maxes = np.amax(self.data, axis=axis).squeeze()\n            if maxes.size == 1:\n                return maxes.item()\n            return maxes\n        except ValueError:\n            raise ValueError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate maximum\"\n            )\n\n    def min(self, *, axis=1):\n        \"\"\"\n        Compute the minimum value of the data along the specified axis.\n\n        Parameters\n        ----------\n        axis : int, optional\n            Axis along which to compute the minimum (default is 1).\n\n        Returns\n        -------\n        min : np.ndarray\n            Minimum values along the specified axis.\n        \"\"\"\n        try:\n            mins = np.amin(self.data, axis=axis).squeeze()\n            if mins.size == 1:\n                return mins.item()\n            return mins\n        except ValueError:\n            raise ValueError(\n                \"Empty RegularlySampledAnalogSignalArray cannot calculate minimum\"\n            )\n\n    def clip(self, min, max):\n        \"\"\"\n        Clip (limit) the values in the data to the interval [min, max].\n\n        Parameters\n        ----------\n        min : float\n            Minimum value.\n        max : float\n            Maximum value.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            New object with clipped data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; clipped = asa.clip(-1, 1)\n        \"\"\"\n        out = self.copy()\n        out._data = np.clip(self.data, min, max)\n        return out\n\n    def trim(self, start, stop=None, *, fs=None):\n        \"\"\"\n        Trim the signal to the specified start and stop times.\n\n        Parameters\n        ----------\n        start : float\n            Start time.\n        stop : float, optional\n            Stop time. If None, trims to the end.\n        fs : float, optional\n            Sampling frequency. If None, uses self.fs.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Trimmed signal array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; trimmed = asa.trim(0, 10)\n        \"\"\"\n        logging.warning(\"RegularlySampledAnalogSignalArray: Trim may not work!\")\n        # TODO: do comprehensive input validation\n        if stop is not None:\n            try:\n                start = np.array(start, ndmin=1)\n                if len(start) != 1:\n                    raise TypeError(\"start must be a scalar float\")\n            except TypeError:\n                raise TypeError(\"start must be a scalar float\")\n            try:\n                stop = np.array(stop, ndmin=1)\n                if len(stop) != 1:\n                    raise TypeError(\"stop must be a scalar float\")\n            except TypeError:\n                raise TypeError(\"stop must be a scalar float\")\n        else:  # start must have two elements\n            try:\n                if len(np.array(start, ndmin=1)) &gt; 2:\n                    raise TypeError(\n                        \"unsupported input to RegularlySampledAnalogSignalArray.trim()\"\n                    )\n                stop = np.array(start[1], ndmin=1)\n                start = np.array(start[0], ndmin=1)\n                if len(start) != 1 or len(stop) != 1:\n                    raise TypeError(\"start and stop must be scalar floats\")\n            except TypeError:\n                raise TypeError(\"start and stop must be scalar floats\")\n\n        logging.disable(logging.CRITICAL)\n        interval = self._abscissa.support.intersect(\n            type(self.support)([start, stop], fs=fs)\n        )\n        if not interval.isempty:\n            analogsignalarray = self[interval]\n        else:\n            analogsignalarray = type(self)([], empty=True)\n        logging.disable(0)\n        analogsignalarray.__renew__()\n        return analogsignalarray\n\n    @property\n    def _ydata_rowsig(self):\n        \"\"\"returns wide-format data s.t. each row is a signal.\"\"\"\n        # LEGACY\n        return self.data\n\n    @property\n    def _ydata_colsig(self):\n        # LEGACY\n        \"\"\"returns skinny-format data s.t. each column is a signal.\"\"\"\n        return self.data.T\n\n    @property\n    def _data_rowsig(self):\n        \"\"\"returns wide-format data s.t. each row is a signal.\"\"\"\n        return self.data\n\n    @property\n    def _data_colsig(self):\n        \"\"\"returns skinny-format data s.t. each column is a signal.\"\"\"\n        return self.data.T\n\n    def _get_interp1d(\n        self,\n        *,\n        kind=\"linear\",\n        copy=True,\n        bounds_error=False,\n        fill_value=np.nan,\n        assume_sorted=None,\n    ):\n        \"\"\"returns a scipy interp1d object, extended to have values at all interval\n        boundaries!\n        \"\"\"\n\n        if assume_sorted is None:\n            assume_sorted = utils.is_sorted(self._abscissa_vals)\n\n        if self.n_signals &gt; 1:\n            axis = 1\n        else:\n            axis = -1\n\n        abscissa_vals = self._abscissa_vals\n\n        if self._ordinate.is_wrapping:\n            yvals = self._unwrap(\n                self._data_rowsig, self._ordinate.range.min, self._ordinate.range.max\n            )  # always interpolate on the unwrapped data!\n        else:\n            yvals = self._data_rowsig\n\n        lengths = self.lengths\n        empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n        first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n        first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n        last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        first_abscissavals_per_interval = self._abscissa_vals[\n            first_abscissavals_per_interval_idx\n        ]\n        last_abscissavals_per_interval = self._abscissa_vals[\n            last_abscissavals_per_interval_idx\n        ]\n\n        boundary_abscissa_vals = []\n        boundary_vals = []\n        for ii, (start, stop) in enumerate(self.support.data):\n            if lengths[ii] == 0:\n                continue\n            if first_abscissavals_per_interval[ii] &gt; start:\n                boundary_abscissa_vals.append(start)\n                boundary_vals.append(yvals[:, first_abscissavals_per_interval_idx[ii]])\n                # print('adding {} at abscissa_vals {}'.format(yvals[:,first_abscissavals_per_interval_idx[ii]], start))\n            if last_abscissavals_per_interval[ii] &lt; stop:\n                boundary_abscissa_vals.append(stop)\n                boundary_vals.append(yvals[:, last_abscissavals_per_interval_idx[ii]])\n\n        if boundary_abscissa_vals:\n            insert_locs = np.searchsorted(abscissa_vals, boundary_abscissa_vals)\n            abscissa_vals = np.insert(\n                abscissa_vals, insert_locs, boundary_abscissa_vals\n            )\n            yvals = np.insert(yvals, insert_locs, np.array(boundary_vals).T, axis=1)\n\n            abscissa_vals, unique_idx = np.unique(abscissa_vals, return_index=True)\n            yvals = yvals[:, unique_idx]\n\n        f = interpolate.interp1d(\n            x=abscissa_vals,\n            y=yvals,\n            kind=kind,\n            axis=axis,\n            copy=copy,\n            bounds_error=bounds_error,\n            fill_value=fill_value,\n            assume_sorted=assume_sorted,\n        )\n        return f\n\n    def asarray(\n        self,\n        *,\n        where=None,\n        at=None,\n        kind=\"linear\",\n        copy=True,\n        bounds_error=False,\n        fill_value=np.nan,\n        assume_sorted=None,\n        recalculate=False,\n        store_interp=True,\n        n_samples=None,\n        split_by_interval=False,\n    ):\n        \"\"\"\n        Return a data-like array at requested points, with optional interpolation.\n\n        Parameters\n        ----------\n        where : array_like or tuple, optional\n            Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).\n        at : array_like, optional\n            Array of points to evaluate array at. If None, uses self._abscissa_vals.\n        n_samples : int, optional\n            Number of points to interpolate at, distributed uniformly from support start to stop.\n        split_by_interval : bool, optional\n            If True, separate arrays by intervals and return in a list.\n        kind : str, optional\n            Interpolation method. Default is 'linear'.\n        copy : bool, optional\n            If True, returns a copy. Default is True.\n        bounds_error : bool, optional\n            If True, raises an error for out-of-bounds interpolation. Default is False.\n        fill_value : float, optional\n            Value to use for out-of-bounds points. Default is np.nan.\n        assume_sorted : bool, optional\n            If True, assumes input is sorted. Default is None.\n        recalculate : bool, optional\n            If True, recalculates the interpolation. Default is False.\n        store_interp : bool, optional\n            If True, stores the interpolation object. Default is True.\n\n        Returns\n        -------\n        out : namedtuple (xvals, yvals)\n            xvals: array of abscissa values for which data are returned.\n            yvals: array of shape (n_signals, n_samples) with interpolated data.\n\n        Examples\n        --------\n        &gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n        \"\"\"\n\n        # TODO: implement splitting by interval\n\n        if split_by_interval:\n            raise NotImplementedError(\"split_by_interval not yet implemented...\")\n\n        XYArray = namedtuple(\"XYArray\", [\"xvals\", \"yvals\"])\n\n        if (\n            at is None\n            and where is None\n            and split_by_interval is False\n            and n_samples is None\n        ):\n            xyarray = XYArray(self._abscissa_vals, self._data_rowsig.squeeze())\n            return xyarray\n\n        if where is not None:\n            assert at is None and n_samples is None, (\n                \"'where', 'at', and 'n_samples' cannot be used at the same time\"\n            )\n            if isinstance(where, tuple):\n                y = np.array(where[1]).squeeze()\n                x = where[0]\n                assert len(x) == len(y), (\n                    \"'where' condition and array must have same number of elements\"\n                )\n                at = y[x]\n            else:\n                x = np.asanyarray(where).squeeze()\n                assert len(x) == len(self._abscissa_vals), (\n                    \"'where' condition must have same number of elements as self._abscissa_vals\"\n                )\n                at = self._abscissa_vals[x]\n        elif at is not None:\n            assert n_samples is None, (\n                \"'at' and 'n_samples' cannot be used at the same time\"\n            )\n        else:\n            at = np.linspace(self.support.start, self.support.stop, n_samples)\n\n        at = np.atleast_1d(at)\n        if at.ndim &gt; 1:\n            raise ValueError(\"Requested points must be one-dimensional!\")\n        if at.shape[0] == 0:\n            raise ValueError(\"No points were requested to interpolate\")\n\n        # if we made it this far, either at or where has been specified, and at is now well defined.\n\n        kwargs = {\n            \"kind\": kind,\n            \"copy\": copy,\n            \"bounds_error\": bounds_error,\n            \"fill_value\": fill_value,\n            \"assume_sorted\": assume_sorted,\n        }\n\n        # retrieve an existing, or construct a new interpolation object\n        if recalculate:\n            interpobj = self._get_interp1d(**kwargs)\n        else:\n            try:\n                interpobj = self._interp\n                if interpobj is None:\n                    interpobj = self._get_interp1d(**kwargs)\n            except AttributeError:  # does not exist yet\n                interpobj = self._get_interp1d(**kwargs)\n\n        # store interpolation object, if desired\n        if store_interp:\n            self._interp = interpobj\n\n        # do not interpolate points that lie outside the support\n        interval_data = self.support.data[:, :, None]\n        # use broadcasting to check in a vectorized manner if\n        # each sample falls within the support, haha aren't we clever?\n        # (n_intervals, n_requested_samples)\n        valid = np.logical_and(\n            at &gt;= interval_data[:, 0, :], at &lt;= interval_data[:, 1, :]\n        )\n        valid_mask = np.any(valid, axis=0)\n        n_invalid = at.size - np.sum(valid_mask)\n        if n_invalid &gt; 0:\n            logging.warning(\n                \"{} values outside the support were removed\".format(n_invalid)\n            )\n        at = at[valid_mask]\n\n        # do the actual interpolation\n        if self._ordinate.is_wrapping:\n            try:\n                if self.is_wrapped:\n                    out = self._wrap(\n                        interpobj(at),\n                        self._ordinate.range.min,\n                        self._ordinate.range.max,\n                    )\n                else:\n                    out = interpobj(at)\n            except SystemError:\n                interpobj = self._get_interp1d(**kwargs)\n                if store_interp:\n                    self._interp = interpobj\n                if self.is_wrapped:\n                    out = self._wrap(\n                        interpobj(at),\n                        self._ordinate.range.min,\n                        self._ordinate.range.max,\n                    )\n                else:\n                    out = interpobj(at)\n        else:\n            try:\n                out = interpobj(at)\n            except SystemError:\n                interpobj = self._get_interp1d(**kwargs)\n                if store_interp:\n                    self._interp = interpobj\n                out = interpobj(at)\n\n        xyarray = XYArray(xvals=np.asanyarray(at), yvals=np.asanyarray(out))\n        return xyarray\n\n    def subsample(self, *, fs):\n        \"\"\"Subsamples a RegularlySampledAnalogSignalArray\n\n        WARNING! Aliasing can occur! It is better to use downsample when\n        lowering the sampling rate substantially.\n\n        Parameters\n        ----------\n        fs : float, optional\n            Desired output sampling rate, in Hz\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n            new subset of points.\n        \"\"\"\n\n        return self.simplify(ds=1 / fs)\n\n    def simplify(self, *, ds=None, n_samples=None, **kwargs):\n        \"\"\"Returns an RegularlySampledAnalogSignalArray where the data has been\n        simplified / subsampled.\n\n        This function is primarily intended to be used for plotting and\n        saving vector graphics without having too large file sizes as\n        a result of too many points.\n\n        Irrespective of whether 'ds' or 'n_samples' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_samples or ds to be violated.\n\n        WARNING! Simplify can create nan samples, when requesting a timestamp\n        within an interval, but outside of the (first, last) abscissa_vals within that\n        interval, since we don't extrapolate, but only interpolate. # TODO: fix\n\n        Parameters\n        ----------\n        ds : float, optional\n            Time (in seconds), in which to step points.\n        n_samples : int, optional\n            Number of points at which to intepolate data. If ds is None\n            and n_samples is None, then default is to use n_samples=5,000\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n            new subset of points.\n        \"\"\"\n\n        if self.isempty:\n            return self\n\n        # legacy kwarg support:\n        n_points = kwargs.pop(\"n_points\", False)\n        if n_points:\n            n_samples = n_points\n\n        if ds is not None and n_samples is not None:\n            raise ValueError(\"ds and n_samples cannot be used together\")\n\n        if n_samples is not None:\n            assert float(n_samples).is_integer(), (\n                \"n_samples must be a positive integer!\"\n            )\n            assert n_samples &gt; 1, \"n_samples must be a positive integer &gt; 1\"\n            # determine ds from number of desired points:\n            ds = self.support.length / (n_samples - 1)\n\n        if ds is None:\n            # neither n_samples nor ds was specified, so assume defaults:\n            n_samples = np.min((5000, 250 + self.n_samples // 2, self.n_samples))\n            ds = self.support.length / (n_samples - 1)\n\n        # build list of points at which to evaluate the RegularlySampledAnalogSignalArray\n\n        # we exclude all empty intervals:\n        at = []\n        lengths = self.lengths\n        empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n        first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n        first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n        last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n        first_abscissavals_per_interval = self._abscissa_vals[\n            first_abscissavals_per_interval_idx\n        ]\n        last_abscissavals_per_interval = self._abscissa_vals[\n            last_abscissavals_per_interval_idx\n        ]\n\n        for ii, (start, stop) in enumerate(self.support.data):\n            if lengths[ii] == 0:\n                continue\n            newxvals = utils.frange(\n                first_abscissavals_per_interval[ii],\n                last_abscissavals_per_interval[ii],\n                step=ds,\n            ).tolist()\n            at.extend(newxvals)\n            try:\n                if newxvals[-1] &lt; last_abscissavals_per_interval[ii]:\n                    at.append(last_abscissavals_per_interval[ii])\n            except IndexError:\n                at.append(first_abscissavals_per_interval[ii])\n                at.append(last_abscissavals_per_interval[ii])\n\n        _, yvals = self.asarray(at=at, recalculate=True, store_interp=False)\n        yvals = np.array(yvals, ndmin=2)\n\n        asa = self.copy()\n        asa._abscissa_vals = np.asanyarray(at)\n        asa._data = yvals\n        asa._fs = 1 / ds\n\n        return asa\n\n    def join(self, other, *, mode=None, inplace=False):\n        \"\"\"Join another RegularlySampledAnalogSignalArray to this one.\n\n        WARNING! Numerical precision might cause some epochs to be considered\n        non-disjoint even when they really are, so a better check than ep1[ep2].isempty\n        is to check for samples contained in the intersection of ep1 and ep2.\n\n        Parameters\n        ----------\n        other : RegularlySampledAnalogSignalArray\n            RegularlySampledAnalogSignalArray (or derived type) to join to the current\n            RegularlySampledAnalogSignalArray. Other must have the same number of signals as\n            the current RegularlySampledAnalogSignalArray.\n        mode : string, optional\n            One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the\n            signals are merged inside overlapping intervals. Default is 'left'.\n        inplace : boolean, optional\n            If True, then current RegularlySampledAnalogSignalArray is modified. If False, then\n            a copy with the joined result is returned. Default is False.\n\n        Returns\n        -------\n        out : RegularlySampledAnalogSignalArray\n            Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been\n            joined to the current RegularlySampledAnalogSignalArray.\n        \"\"\"\n\n        if mode is None:\n            mode = \"left\"\n\n        asa = self.copy()  # copy without data since we change data at the end?\n\n        times = np.zeros((1, 0))\n        data = np.zeros((asa.n_signals, 0))\n\n        # if ASAs are disjoint:\n        if not self.support[other.support].length &gt; 50 * float_info.epsilon:\n            # do a simple-as-butter join (concat) and sort\n            times = np.append(times, self._abscissa_vals)\n            data = np.hstack((data, self.data))\n            times = np.append(times, other._abscissa_vals)\n            data = np.hstack((data, other.data))\n        else:  # not disjoint\n            both_eps = self.support[other.support]\n            self_eps = self.support - both_eps - other.support\n            other_eps = other.support - both_eps - self.support\n\n            if mode == \"left\":\n                self_eps += both_eps\n                # print(self_eps)\n\n                tmp = self[self_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n\n                if not other_eps.isempty:\n                    tmp = other[other_eps]\n                    times = np.append(times, tmp._abscissa_vals)\n                    data = np.hstack((data, tmp.data))\n            elif mode == \"right\":\n                other_eps += both_eps\n\n                tmp = other[other_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n\n                if not self_eps.isempty:\n                    tmp = self[self_eps]\n                    times = np.append(times, tmp._abscissa_vals)\n                    data = np.hstack((data, tmp.data))\n            else:\n                raise NotImplementedError(\n                    \"asa.join() has not yet been implemented for mode '{}'!\".format(\n                        mode\n                    )\n                )\n\n        sample_order = np.argsort(times)\n        times = times[sample_order]\n        data = data[:, sample_order]\n\n        asa._data = data\n        asa._abscissa_vals = times\n        dom1 = self.domain\n        dom2 = other.domain\n        asa._abscissa.support = (self.support + other.support).merge()\n        asa._abscissa.support.domain = (dom1 + dom2).merge()\n        return asa\n\n    def _pdf(self, bins=None, n_samples=None):\n        \"\"\"Return the probability distribution function for each signal.\"\"\"\n        from scipy import integrate\n\n        if bins is None:\n            bins = 100\n\n        if n_samples is None:\n            n_samples = 100\n\n        if self.n_signals &gt; 1:\n            raise NotImplementedError(\"multiple signals not supported yet!\")\n\n        # fx, bins = np.histogram(self.data.squeeze(), bins=bins, normed=True)\n        fx, bins = np.histogram(self.data.squeeze(), bins=bins)\n        bin_centers = (bins + (bins[1] - bins[0]) / 2)[:-1]\n\n        Ifx = integrate.simps(fx, bin_centers)\n\n        pdf = type(self)(\n            abscissa_vals=bin_centers,\n            data=fx / Ifx,\n            fs=1 / (bin_centers[1] - bin_centers[0]),\n            support=type(self.support)(self.data.min(), self.data.max()),\n        ).simplify(n_samples=n_samples)\n\n        return pdf\n\n        # data = []\n        # for signal in self.data:\n        #     fx, bins = np.histogram(signal, bins=bins)\n        #     bin_centers = (bins + (bins[1]-bins[0])/2)[:-1]\n\n    def _cdf(self, n_samples=None):\n        \"\"\"Return the probability distribution function for each signal.\"\"\"\n\n        if n_samples is None:\n            n_samples = 100\n\n        if self.n_signals &gt; 1:\n            raise NotImplementedError(\"multiple signals not supported yet!\")\n\n        X = np.sort(self.data.squeeze())\n        F = np.array(range(self.n_samples)) / float(self.n_samples)\n\n        logging.disable(logging.CRITICAL)\n        cdf = type(self)(\n            abscissa_vals=X,\n            data=F,\n            support=type(self.support)(self.data.min(), self.data.max()),\n        ).simplify(n_samples=n_samples)\n        logging.disable(0)\n\n        return cdf\n\n    def _eegplot(self, ax=None, normalize=False, pad=None, fill=True, color=None):\n        \"\"\"custom_func docstring goes here.\"\"\"\n\n        import matplotlib.pyplot as plt\n\n        from ..plotting import utils as plotutils\n\n        if ax is None:\n            ax = plt.gca()\n\n        xmin = self.support.min\n        xmax = self.support.max\n        xvals = self._abscissa_vals\n\n        if pad is None:\n            pad = np.mean(self.data) / 2\n\n        data = self.data.copy()\n\n        if normalize:\n            peak_vals = self.max()\n            data = (data.T / peak_vals).T\n\n        n_traces = self.n_signals\n\n        for tt, trace in enumerate(data):\n            if color is None:\n                line = ax.plot(\n                    xvals, tt * pad + trace, zorder=int(10 + 2 * n_traces - 2 * tt)\n                )\n            else:\n                line = ax.plot(\n                    xvals,\n                    tt * pad + trace,\n                    zorder=int(10 + 2 * n_traces - 2 * tt),\n                    color=color,\n                )\n            if fill:\n                # Get the color from the current curve\n                fillcolor = line[0].get_color()\n                ax.fill_between(\n                    xvals,\n                    tt * pad,\n                    tt * pad + trace,\n                    alpha=0.3,\n                    color=fillcolor,\n                    zorder=int(10 + 2 * n_traces - 2 * tt - 1),\n                )\n\n        ax.set_xlim(xmin, xmax)\n        if pad != 0:\n            # yticks = np.arange(n_traces)*pad + 0.5*pad\n            yticks = []\n            ax.set_yticks(yticks)\n            ax.set_xlabel(self._abscissa.label)\n            ax.set_ylabel(self._ordinate.label)\n            plotutils.no_yticks(ax)\n            plotutils.clear_left(ax)\n\n        plotutils.clear_top(ax)\n        plotutils.clear_right(ax)\n\n        return ax\n\n    def __setattr__(self, name, value):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        name = self.__aliases__.get(name, name)\n        object.__setattr__(self, name, value)\n\n    def __getattr__(self, name):\n        # https://stackoverflow.com/questions/4017572/how-can-i-make-an-alias-to-a-non-function-member-attribute-in-a-python-class\n        if name == \"aliases\":\n            raise AttributeError  # http://nedbatchelder.com/blog/201010/surprising_getattr_recursion.html\n        name = self.__aliases__.get(name, name)\n        # return getattr(self, name) #Causes infinite recursion on non-existent attribute\n        return object.__getattribute__(self, name)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.abs","title":"<code>abs</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with absolute value of (potentially complex) data.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.abscissa_vals","title":"<code>abscissa_vals</code>  <code>property</code> <code>writable</code>","text":"<p>(np.array 1D) Time in seconds.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.angle","title":"<code>angle</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only phase angle (in radians) of data.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.base_unit","title":"<code>base_unit</code>  <code>property</code>","text":"<p>Base unit of the abscissa.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>(np.array N-Dimensional) data with shape (n_signals, n_samples).</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.fs","title":"<code>fs</code>  <code>property</code>","text":"<p>(float) Sampling frequency.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.imag","title":"<code>imag</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only imaginary part of data.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.iscomplex","title":"<code>iscomplex</code>  <code>property</code>","text":"<p>Returns True if any part of the signal is complex.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>(bool) checks length of data input</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.isreal","title":"<code>isreal</code>  <code>property</code>","text":"<p>Returns True if entire signal is real.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.labels","title":"<code>labels</code>  <code>property</code>","text":"<p>(list) The labels corresponding to each signal.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>(list) The number of samples in each interval.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.n_bytes","title":"<code>n_bytes</code>  <code>property</code>","text":"<p>Approximate number of bytes taken up by object.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.n_intervals","title":"<code>n_intervals</code>  <code>property</code>","text":"<p>(int) number of intervals in RegularlySampledAnalogSignalArray</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.n_samples","title":"<code>n_samples</code>  <code>property</code>","text":"<p>(int) number of abscissa samples where signal is defined.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.n_signals","title":"<code>n_signals</code>  <code>property</code>","text":"<p>(int) The number of signals.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.range","title":"<code>range</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The range of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.real","title":"<code>real</code>  <code>property</code>","text":"<p>RegularlySampledAnalogSignalArray with only real part of data.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.signals","title":"<code>signals</code>  <code>property</code>","text":"<p>Returns a list of RegularlySampledAnalogSignalArrays, each array containing a single signal (channel).</p> <p>WARNING: this method creates a copy of each signal, so is not particularly efficient at this time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for channel in lfp.signals:\n    print(channel)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.step","title":"<code>step</code>  <code>property</code>","text":"<p>steps per sample Example 1: sample_numbers = np.array([1,2,3,4,5,6]) #aka time Steps per sample in the above case would be 1</p> <p>Example 2: sample_numbers = np.array([1,3,5,7,9]) #aka time Steps per sample in Example 2 would be 2</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying RegularlySampledAnalogSignalArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.add_signal","title":"<code>add_signal(signal, label=None)</code>","text":"<p>Docstring goes here. Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def add_signal(self, signal, label=None):\n    \"\"\"Docstring goes here.\n    Basically we add a signal, and we add a label. THIS HAPPENS IN PLACE?\n    \"\"\"\n    # TODO: add functionality to check that supports are the same, etc.\n    if isinstance(signal, RegularlySampledAnalogSignalArray):\n        signal = signal.data\n\n    signal = np.squeeze(signal)\n    if signal.ndim &gt; 1:\n        raise TypeError(\"Can only add one signal at a time!\")\n    if self.data.ndim == 1:\n        self._data = np.vstack(\n            [np.array(self.data, ndmin=2), np.array(signal, ndmin=2)]\n        )\n    else:\n        self._data = np.vstack([self.data, np.array(signal, ndmin=2)])\n    if label is None:\n        logging.warning(\"None label appended\")\n    self._labels = np.append(self._labels, label)\n    return self\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.asarray","title":"<code>asarray(*, where=None, at=None, kind='linear', copy=True, bounds_error=False, fill_value=np.nan, assume_sorted=None, recalculate=False, store_interp=True, n_samples=None, split_by_interval=False)</code>","text":"<p>Return a data-like array at requested points, with optional interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>array_like or tuple</code> <p>Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).</p> <code>None</code> <code>at</code> <code>array_like</code> <p>Array of points to evaluate array at. If None, uses self._abscissa_vals.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of points to interpolate at, distributed uniformly from support start to stop.</p> <code>None</code> <code>split_by_interval</code> <code>bool</code> <p>If True, separate arrays by intervals and return in a list.</p> <code>False</code> <code>kind</code> <code>str</code> <p>Interpolation method. Default is 'linear'.</p> <code>'linear'</code> <code>copy</code> <code>bool</code> <p>If True, returns a copy. Default is True.</p> <code>True</code> <code>bounds_error</code> <code>bool</code> <p>If True, raises an error for out-of-bounds interpolation. Default is False.</p> <code>False</code> <code>fill_value</code> <code>float</code> <p>Value to use for out-of-bounds points. Default is np.nan.</p> <code>nan</code> <code>assume_sorted</code> <code>bool</code> <p>If True, assumes input is sorted. Default is None.</p> <code>None</code> <code>recalculate</code> <code>bool</code> <p>If True, recalculates the interpolation. Default is False.</p> <code>False</code> <code>store_interp</code> <code>bool</code> <p>If True, stores the interpolation object. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>out</code> <code>namedtuple(xvals, yvals)</code> <p>xvals: array of abscissa values for which data are returned. yvals: array of shape (n_signals, n_samples) with interpolated data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def asarray(\n    self,\n    *,\n    where=None,\n    at=None,\n    kind=\"linear\",\n    copy=True,\n    bounds_error=False,\n    fill_value=np.nan,\n    assume_sorted=None,\n    recalculate=False,\n    store_interp=True,\n    n_samples=None,\n    split_by_interval=False,\n):\n    \"\"\"\n    Return a data-like array at requested points, with optional interpolation.\n\n    Parameters\n    ----------\n    where : array_like or tuple, optional\n        Array corresponding to np where condition (e.g., where=(data[1,:]&gt;5)).\n    at : array_like, optional\n        Array of points to evaluate array at. If None, uses self._abscissa_vals.\n    n_samples : int, optional\n        Number of points to interpolate at, distributed uniformly from support start to stop.\n    split_by_interval : bool, optional\n        If True, separate arrays by intervals and return in a list.\n    kind : str, optional\n        Interpolation method. Default is 'linear'.\n    copy : bool, optional\n        If True, returns a copy. Default is True.\n    bounds_error : bool, optional\n        If True, raises an error for out-of-bounds interpolation. Default is False.\n    fill_value : float, optional\n        Value to use for out-of-bounds points. Default is np.nan.\n    assume_sorted : bool, optional\n        If True, assumes input is sorted. Default is None.\n    recalculate : bool, optional\n        If True, recalculates the interpolation. Default is False.\n    store_interp : bool, optional\n        If True, stores the interpolation object. Default is True.\n\n    Returns\n    -------\n    out : namedtuple (xvals, yvals)\n        xvals: array of abscissa values for which data are returned.\n        yvals: array of shape (n_signals, n_samples) with interpolated data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; xvals, yvals = asa.asarray(at=[0, 1, 2])\n    \"\"\"\n\n    # TODO: implement splitting by interval\n\n    if split_by_interval:\n        raise NotImplementedError(\"split_by_interval not yet implemented...\")\n\n    XYArray = namedtuple(\"XYArray\", [\"xvals\", \"yvals\"])\n\n    if (\n        at is None\n        and where is None\n        and split_by_interval is False\n        and n_samples is None\n    ):\n        xyarray = XYArray(self._abscissa_vals, self._data_rowsig.squeeze())\n        return xyarray\n\n    if where is not None:\n        assert at is None and n_samples is None, (\n            \"'where', 'at', and 'n_samples' cannot be used at the same time\"\n        )\n        if isinstance(where, tuple):\n            y = np.array(where[1]).squeeze()\n            x = where[0]\n            assert len(x) == len(y), (\n                \"'where' condition and array must have same number of elements\"\n            )\n            at = y[x]\n        else:\n            x = np.asanyarray(where).squeeze()\n            assert len(x) == len(self._abscissa_vals), (\n                \"'where' condition must have same number of elements as self._abscissa_vals\"\n            )\n            at = self._abscissa_vals[x]\n    elif at is not None:\n        assert n_samples is None, (\n            \"'at' and 'n_samples' cannot be used at the same time\"\n        )\n    else:\n        at = np.linspace(self.support.start, self.support.stop, n_samples)\n\n    at = np.atleast_1d(at)\n    if at.ndim &gt; 1:\n        raise ValueError(\"Requested points must be one-dimensional!\")\n    if at.shape[0] == 0:\n        raise ValueError(\"No points were requested to interpolate\")\n\n    # if we made it this far, either at or where has been specified, and at is now well defined.\n\n    kwargs = {\n        \"kind\": kind,\n        \"copy\": copy,\n        \"bounds_error\": bounds_error,\n        \"fill_value\": fill_value,\n        \"assume_sorted\": assume_sorted,\n    }\n\n    # retrieve an existing, or construct a new interpolation object\n    if recalculate:\n        interpobj = self._get_interp1d(**kwargs)\n    else:\n        try:\n            interpobj = self._interp\n            if interpobj is None:\n                interpobj = self._get_interp1d(**kwargs)\n        except AttributeError:  # does not exist yet\n            interpobj = self._get_interp1d(**kwargs)\n\n    # store interpolation object, if desired\n    if store_interp:\n        self._interp = interpobj\n\n    # do not interpolate points that lie outside the support\n    interval_data = self.support.data[:, :, None]\n    # use broadcasting to check in a vectorized manner if\n    # each sample falls within the support, haha aren't we clever?\n    # (n_intervals, n_requested_samples)\n    valid = np.logical_and(\n        at &gt;= interval_data[:, 0, :], at &lt;= interval_data[:, 1, :]\n    )\n    valid_mask = np.any(valid, axis=0)\n    n_invalid = at.size - np.sum(valid_mask)\n    if n_invalid &gt; 0:\n        logging.warning(\n            \"{} values outside the support were removed\".format(n_invalid)\n        )\n    at = at[valid_mask]\n\n    # do the actual interpolation\n    if self._ordinate.is_wrapping:\n        try:\n            if self.is_wrapped:\n                out = self._wrap(\n                    interpobj(at),\n                    self._ordinate.range.min,\n                    self._ordinate.range.max,\n                )\n            else:\n                out = interpobj(at)\n        except SystemError:\n            interpobj = self._get_interp1d(**kwargs)\n            if store_interp:\n                self._interp = interpobj\n            if self.is_wrapped:\n                out = self._wrap(\n                    interpobj(at),\n                    self._ordinate.range.min,\n                    self._ordinate.range.max,\n                )\n            else:\n                out = interpobj(at)\n    else:\n        try:\n            out = interpobj(at)\n        except SystemError:\n            interpobj = self._get_interp1d(**kwargs)\n            if store_interp:\n                self._interp = interpobj\n            out = interpobj(at)\n\n    xyarray = XYArray(xvals=np.asanyarray(at), yvals=np.asanyarray(out))\n    return xyarray\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.center","title":"<code>center(inplace=False)</code>","text":"<p>Center the data to have zero mean along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The centered signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; centered = asa.center()\n&gt;&gt;&gt; centered.mean()\n0.0\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def center(self, inplace=False):\n    \"\"\"\n    Center the data to have zero mean along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The centered signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; centered = asa.center()\n    &gt;&gt;&gt; centered.mean()\n    0.0\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.clip","title":"<code>clip(min, max)</code>","text":"<p>Clip (limit) the values in the data to the interval [min, max].</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float</code> <p>Minimum value.</p> required <code>max</code> <code>float</code> <p>Maximum value.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>New object with clipped data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clipped = asa.clip(-1, 1)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def clip(self, min, max):\n    \"\"\"\n    Clip (limit) the values in the data to the interval [min, max].\n\n    Parameters\n    ----------\n    min : float\n        Minimum value.\n    max : float\n        Maximum value.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        New object with clipped data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; clipped = asa.clip(-1, 1)\n    \"\"\"\n    out = self.copy()\n    out._data = np.clip(self.data, min, max)\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the current object.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def copy(self):\n    \"\"\"Return a copy of the current object.\"\"\"\n    out = copy.deepcopy(self)\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.ddt","title":"<code>ddt(rectify=False)</code>","text":"<p>Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.</p> <p>asa.data = f(t) asa.ddt = d/dt (asa.data)</p> <p>Parameters:</p> Name Type Description Default <code>rectify</code> <code>boolean</code> <p>If True, the absolute value of the derivative will be returned. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ddt</code> <code>RegularlySampledAnalogSignalArray</code> <p>Time derivative of each signal in the RegularlySampledAnalogSignalArray.</p> Note <p>Second order central differences are used here, and it is assumed that the signals are sampled uniformly. If the signals are not uniformly sampled, it is recommended to resample the signal before computing the derivative.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def ddt(self, rectify=False):\n    \"\"\"Returns the derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n    asa.data = f(t)\n    asa.ddt = d/dt (asa.data)\n\n    Parameters\n    ----------\n    rectify : boolean, optional\n        If True, the absolute value of the derivative will be returned.\n        Default is False.\n\n    Returns\n    -------\n    ddt : RegularlySampledAnalogSignalArray\n        Time derivative of each signal in the RegularlySampledAnalogSignalArray.\n\n    Note\n    ----\n    Second order central differences are used here, and it is assumed that\n    the signals are sampled uniformly. If the signals are not uniformly\n    sampled, it is recommended to resample the signal before computing the\n    derivative.\n    \"\"\"\n    ddt = utils.ddt_asa(self, rectify=rectify)\n    return ddt\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.downsample","title":"<code>downsample(*, fs_out, aafilter=True, inplace=False, **kwargs)</code>","text":"<p>Downsamples the RegularlySampledAnalogSignalArray</p> <p>Parameters:</p> Name Type Description Default <code>fs_out</code> <code>float</code> <p>Desired output sampling rate in Hz</p> required <code>aafilter</code> <code>boolean</code> <p>Whether to apply an anti-aliasing filter before performing the actual downsampling. Default is True</p> <code>True</code> <code>inplace</code> <code>boolean</code> <p>If True, the output ASA will replace the input ASA. Default is False</p> <code>False</code> <code>kwargs</code> <p>Other keyword arguments are passed to sosfiltfilt() in the <code>filtering</code> module</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The downsampled RegularlySampledAnalogSignalArray</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def downsample(self, *, fs_out, aafilter=True, inplace=False, **kwargs):\n    \"\"\"Downsamples the RegularlySampledAnalogSignalArray\n\n    Parameters\n    ----------\n    fs_out : float, optional\n        Desired output sampling rate in Hz\n    aafilter : boolean, optional\n        Whether to apply an anti-aliasing filter before performing the actual\n        downsampling. Default is True\n    inplace : boolean, optional\n        If True, the output ASA will replace the input ASA. Default is False\n    kwargs :\n        Other keyword arguments are passed to sosfiltfilt() in the `filtering`\n        module\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The downsampled RegularlySampledAnalogSignalArray\n    \"\"\"\n\n    if not fs_out &lt; self._fs:\n        raise ValueError(\"fs_out must be less than current sampling rate!\")\n\n    if aafilter:\n        fh = fs_out / 2.0\n        out = filtering.sosfiltfilt(self, fl=None, fh=fh, inplace=inplace, **kwargs)\n\n    downsampled = out.simplify(ds=1 / fs_out)\n    out._data = downsampled._data\n    out._abscissa_vals = downsampled._abscissa_vals\n    out._fs = fs_out\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.empty","title":"<code>empty(inplace=True)</code>","text":"<p>Remove data (but not metadata) from RegularlySampledAnalogSignalArray.</p> <p>Attributes 'data', 'abscissa_vals', and 'support' are all emptied.</p> <p>Note: n_signals is preserved.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def empty(self, inplace=True):\n    \"\"\"Remove data (but not metadata) from RegularlySampledAnalogSignalArray.\n\n    Attributes 'data', 'abscissa_vals', and 'support' are all emptied.\n\n    Note: n_signals is preserved.\n    \"\"\"\n    n_signals = self.n_signals\n    if not inplace:\n        out = self._copy_without_data()\n    else:\n        out = self\n        out._data = np.zeros((n_signals, 0))\n    out._abscissa.support = type(self.support)(empty=True)\n    out._abscissa_vals = []\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.join","title":"<code>join(other, *, mode=None, inplace=False)</code>","text":"<p>Join another RegularlySampledAnalogSignalArray to this one.</p> <p>WARNING! Numerical precision might cause some epochs to be considered non-disjoint even when they really are, so a better check than ep1[ep2].isempty is to check for samples contained in the intersection of ep1 and ep2.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>RegularlySampledAnalogSignalArray</code> <p>RegularlySampledAnalogSignalArray (or derived type) to join to the current RegularlySampledAnalogSignalArray. Other must have the same number of signals as the current RegularlySampledAnalogSignalArray.</p> required <code>mode</code> <code>string</code> <p>One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the signals are merged inside overlapping intervals. Default is 'left'.</p> <code>None</code> <code>inplace</code> <code>boolean</code> <p>If True, then current RegularlySampledAnalogSignalArray is modified. If False, then a copy with the joined result is returned. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been joined to the current RegularlySampledAnalogSignalArray.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def join(self, other, *, mode=None, inplace=False):\n    \"\"\"Join another RegularlySampledAnalogSignalArray to this one.\n\n    WARNING! Numerical precision might cause some epochs to be considered\n    non-disjoint even when they really are, so a better check than ep1[ep2].isempty\n    is to check for samples contained in the intersection of ep1 and ep2.\n\n    Parameters\n    ----------\n    other : RegularlySampledAnalogSignalArray\n        RegularlySampledAnalogSignalArray (or derived type) to join to the current\n        RegularlySampledAnalogSignalArray. Other must have the same number of signals as\n        the current RegularlySampledAnalogSignalArray.\n    mode : string, optional\n        One of ['max', 'min', 'left', 'right', 'mean']. Specifies how the\n        signals are merged inside overlapping intervals. Default is 'left'.\n    inplace : boolean, optional\n        If True, then current RegularlySampledAnalogSignalArray is modified. If False, then\n        a copy with the joined result is returned. Default is False.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where the new RegularlySampledAnalogSignalArray has been\n        joined to the current RegularlySampledAnalogSignalArray.\n    \"\"\"\n\n    if mode is None:\n        mode = \"left\"\n\n    asa = self.copy()  # copy without data since we change data at the end?\n\n    times = np.zeros((1, 0))\n    data = np.zeros((asa.n_signals, 0))\n\n    # if ASAs are disjoint:\n    if not self.support[other.support].length &gt; 50 * float_info.epsilon:\n        # do a simple-as-butter join (concat) and sort\n        times = np.append(times, self._abscissa_vals)\n        data = np.hstack((data, self.data))\n        times = np.append(times, other._abscissa_vals)\n        data = np.hstack((data, other.data))\n    else:  # not disjoint\n        both_eps = self.support[other.support]\n        self_eps = self.support - both_eps - other.support\n        other_eps = other.support - both_eps - self.support\n\n        if mode == \"left\":\n            self_eps += both_eps\n            # print(self_eps)\n\n            tmp = self[self_eps]\n            times = np.append(times, tmp._abscissa_vals)\n            data = np.hstack((data, tmp.data))\n\n            if not other_eps.isempty:\n                tmp = other[other_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n        elif mode == \"right\":\n            other_eps += both_eps\n\n            tmp = other[other_eps]\n            times = np.append(times, tmp._abscissa_vals)\n            data = np.hstack((data, tmp.data))\n\n            if not self_eps.isempty:\n                tmp = self[self_eps]\n                times = np.append(times, tmp._abscissa_vals)\n                data = np.hstack((data, tmp.data))\n        else:\n            raise NotImplementedError(\n                \"asa.join() has not yet been implemented for mode '{}'!\".format(\n                    mode\n                )\n            )\n\n    sample_order = np.argsort(times)\n    times = times[sample_order]\n    data = data[:, sample_order]\n\n    asa._data = data\n    asa._abscissa_vals = times\n    dom1 = self.domain\n    dom2 = other.domain\n    asa._abscissa.support = (self.support + other.support).merge()\n    asa._abscissa.support.domain = (dom1 + dom2).merge()\n    return asa\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.max","title":"<code>max(*, axis=1)</code>","text":"<p>Compute the maximum value of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the maximum (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>max</code> <code>ndarray</code> <p>Maximum values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def max(self, *, axis=1):\n    \"\"\"\n    Compute the maximum value of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the maximum (default is 1).\n\n    Returns\n    -------\n    max : np.ndarray\n        Maximum values along the specified axis.\n    \"\"\"\n    try:\n        maxes = np.amax(self.data, axis=axis).squeeze()\n        if maxes.size == 1:\n            return maxes.item()\n        return maxes\n    except ValueError:\n        raise ValueError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate maximum\"\n        )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.mean","title":"<code>mean(*, axis=1)</code>","text":"<p>Compute the mean of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the mean (default is 1, i.e., across samples).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>mean</code> <code>ndarray</code> <p>Mean values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def mean(self, *, axis=1):\n    \"\"\"\n    Compute the mean of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the mean (default is 1, i.e., across samples).\n\n    Returns\n    -------\n    mean : np.ndarray\n        Mean values along the specified axis.\n    \"\"\"\n    try:\n        means = np.nanmean(self.data, axis=axis).squeeze()\n        if means.size == 1:\n            return means.item()\n        return means\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate mean\"\n        )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.median","title":"<code>median(*, axis=1)</code>","text":"<p>Returns the median of each signal in RegularlySampledAnalogSignalArray.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def median(self, *, axis=1):\n    \"\"\"Returns the median of each signal in RegularlySampledAnalogSignalArray.\"\"\"\n    try:\n        medians = np.nanmedian(self.data, axis=axis).squeeze()\n        if medians.size == 1:\n            return medians.item()\n        return medians\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate median\"\n        )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.min","title":"<code>min(*, axis=1)</code>","text":"<p>Compute the minimum value of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the minimum (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>min</code> <code>ndarray</code> <p>Minimum values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def min(self, *, axis=1):\n    \"\"\"\n    Compute the minimum value of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the minimum (default is 1).\n\n    Returns\n    -------\n    min : np.ndarray\n        Minimum values along the specified axis.\n    \"\"\"\n    try:\n        mins = np.amin(self.data, axis=axis).squeeze()\n        if mins.size == 1:\n            return mins.item()\n        return mins\n    except ValueError:\n        raise ValueError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate minimum\"\n        )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.normalize","title":"<code>normalize(inplace=False)</code>","text":"<p>Normalize the data to have unit standard deviation along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The normalized signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normalized = asa.normalize()\n&gt;&gt;&gt; normalized.std()\n1.0\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def normalize(self, inplace=False):\n    \"\"\"\n    Normalize the data to have unit standard deviation along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The normalized signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; normalized = asa.normalize()\n    &gt;&gt;&gt; normalized.std()\n    1.0\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    std = np.atleast_1d(out.std())\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns an RegularlySampledAnalogSignalArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>RegularlySampledAnalogSignalArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_samples or ds to be violated.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns an RegularlySampledAnalogSignalArray whose support has been\n    partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_samples : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        RegularlySampledAnalogSignalArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_samples or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    out._abscissa.support = out.support.partition(ds=ds, n_intervals=n_intervals)\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.simplify","title":"<code>simplify(*, ds=None, n_samples=None, **kwargs)</code>","text":"<p>Returns an RegularlySampledAnalogSignalArray where the data has been simplified / subsampled.</p> <p>This function is primarily intended to be used for plotting and saving vector graphics without having too large file sizes as a result of too many points.</p> <p>Irrespective of whether 'ds' or 'n_samples' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_samples or ds to be violated.</p> <p>WARNING! Simplify can create nan samples, when requesting a timestamp within an interval, but outside of the (first, last) abscissa_vals within that interval, since we don't extrapolate, but only interpolate. # TODO: fix</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Time (in seconds), in which to step points.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>Number of points at which to intepolate data. If ds is None and n_samples is None, then default is to use n_samples=5,000</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where data is only stored at the new subset of points.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def simplify(self, *, ds=None, n_samples=None, **kwargs):\n    \"\"\"Returns an RegularlySampledAnalogSignalArray where the data has been\n    simplified / subsampled.\n\n    This function is primarily intended to be used for plotting and\n    saving vector graphics without having too large file sizes as\n    a result of too many points.\n\n    Irrespective of whether 'ds' or 'n_samples' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_samples or ds to be violated.\n\n    WARNING! Simplify can create nan samples, when requesting a timestamp\n    within an interval, but outside of the (first, last) abscissa_vals within that\n    interval, since we don't extrapolate, but only interpolate. # TODO: fix\n\n    Parameters\n    ----------\n    ds : float, optional\n        Time (in seconds), in which to step points.\n    n_samples : int, optional\n        Number of points at which to intepolate data. If ds is None\n        and n_samples is None, then default is to use n_samples=5,000\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n        new subset of points.\n    \"\"\"\n\n    if self.isempty:\n        return self\n\n    # legacy kwarg support:\n    n_points = kwargs.pop(\"n_points\", False)\n    if n_points:\n        n_samples = n_points\n\n    if ds is not None and n_samples is not None:\n        raise ValueError(\"ds and n_samples cannot be used together\")\n\n    if n_samples is not None:\n        assert float(n_samples).is_integer(), (\n            \"n_samples must be a positive integer!\"\n        )\n        assert n_samples &gt; 1, \"n_samples must be a positive integer &gt; 1\"\n        # determine ds from number of desired points:\n        ds = self.support.length / (n_samples - 1)\n\n    if ds is None:\n        # neither n_samples nor ds was specified, so assume defaults:\n        n_samples = np.min((5000, 250 + self.n_samples // 2, self.n_samples))\n        ds = self.support.length / (n_samples - 1)\n\n    # build list of points at which to evaluate the RegularlySampledAnalogSignalArray\n\n    # we exclude all empty intervals:\n    at = []\n    lengths = self.lengths\n    empty_interval_ids = np.argwhere(lengths == 0).squeeze().tolist()\n    first_abscissavals_per_interval_idx = np.insert(np.cumsum(lengths[:-1]), 0, 0)\n    first_abscissavals_per_interval_idx[empty_interval_ids] = 0\n    last_abscissavals_per_interval_idx = np.cumsum(lengths) - 1\n    last_abscissavals_per_interval_idx[empty_interval_ids] = 0\n    first_abscissavals_per_interval = self._abscissa_vals[\n        first_abscissavals_per_interval_idx\n    ]\n    last_abscissavals_per_interval = self._abscissa_vals[\n        last_abscissavals_per_interval_idx\n    ]\n\n    for ii, (start, stop) in enumerate(self.support.data):\n        if lengths[ii] == 0:\n            continue\n        newxvals = utils.frange(\n            first_abscissavals_per_interval[ii],\n            last_abscissavals_per_interval[ii],\n            step=ds,\n        ).tolist()\n        at.extend(newxvals)\n        try:\n            if newxvals[-1] &lt; last_abscissavals_per_interval[ii]:\n                at.append(last_abscissavals_per_interval[ii])\n        except IndexError:\n            at.append(first_abscissavals_per_interval[ii])\n            at.append(last_abscissavals_per_interval[ii])\n\n    _, yvals = self.asarray(at=at, recalculate=True, store_interp=False)\n    yvals = np.array(yvals, ndmin=2)\n\n    asa = self.copy()\n    asa._abscissa_vals = np.asanyarray(at)\n    asa._data = yvals\n    asa._fs = 1 / ds\n\n    return asa\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.smooth","title":"<code>smooth(*, fs=None, sigma=None, truncate=None, inplace=False, mode=None, cval=None, within_intervals=False)</code>","text":"<p>Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.</p> <p>Smoothing is applied along the abscissa, and the same smoothing is applied to each signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.</p> <p>Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.</code> required <code>fs</code> <code>float</code> <p>Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will be inferred.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05 (50 ms if base_unit=seconds).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <code>mode</code> <code>(reflect, constant, nearest, mirror, wrap)</code> <p>The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to 'constant'. Default is 'reflect'.</p> <code>'reflect'</code> <code>cval</code> <code>scalar</code> <p>Value to fill past edges of input if mode is 'constant'. Default is 0.0.</p> <code>None</code> <code>within_intervals</code> <code>boolean</code> <p>If True, then smooth within each epoch. Otherwise smooth across epochs. Default is False. Note that when mode = 'wrap', then smoothing within epochs aren't affected by wrapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as obj</code> <p>An object with smoothed data is returned.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef smooth(\n    self,\n    *,\n    fs=None,\n    sigma=None,\n    truncate=None,\n    inplace=False,\n    mode=None,\n    cval=None,\n    within_intervals=False,\n):\n    \"\"\"Smooths the regularly sampled RegularlySampledAnalogSignalArray with a Gaussian kernel.\n\n    Smoothing is applied along the abscissa, and the same smoothing is applied to each\n    signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n    Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n    Parameters\n    ----------\n    obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n    fs : float, optional\n        Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n        be inferred.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n        (50 ms if base_unit=seconds).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0.\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        'reflect'.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n    within_intervals : boolean, optional\n        If True, then smooth within each epoch. Otherwise smooth across epochs.\n        Default is False.\n        Note that when mode = 'wrap', then smoothing within epochs aren't affected\n        by wrapping.\n\n    Returns\n    -------\n    out : same type as obj\n        An object with smoothed data is returned.\n\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.05\n    if truncate is None:\n        truncate = 4\n\n    kwargs = {\n        \"inplace\": inplace,\n        \"fs\": fs,\n        \"sigma\": sigma,\n        \"truncate\": truncate,\n        \"mode\": mode,\n        \"cval\": cval,\n        \"within_intervals\": within_intervals,\n    }\n\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    if self._ordinate.is_wrapping:\n        ord_is_wrapped = self.is_wrapped\n\n        if ord_is_wrapped:\n            out = out.unwrap()\n\n    # case 1: abs.wrapping=False, ord.linking=False, ord.wrapping=False\n    if (\n        not self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        pass\n\n    # case 2: abs.wrapping=False, ord.linking=False, ord.wrapping=True\n    elif (\n        not self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        pass\n\n    # case 3: abs.wrapping=False, ord.linking=True, ord.wrapping=False\n    elif (\n        not self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 4: abs.wrapping=False, ord.linking=True, ord.wrapping=True\n    elif (\n        not self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 5: abs.wrapping=True, ord.linking=False, ord.wrapping=False\n    elif (\n        self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        if mode is None:\n            kwargs[\"mode\"] = \"wrap\"\n\n    # case 6: abs.wrapping=True, ord.linking=False, ord.wrapping=True\n    elif (\n        self._abscissa.is_wrapping\n        and not self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        # (1) unwrap ordinate (abscissa wrap=False)\n        # (2) smooth unwrapped ordinate (absissa wrap=False)\n        # (3) repeat unwrapped signal based on conditions from (2):\n        # if smoothed wrapped ordinate samples\n        # HH ==&gt; SSS (this must be done on a per-signal basis!!!) H = high; L = low; S = same\n        # LL ==&gt; SSS (the vertical offset must be such that neighbors have smallest displacement)\n        # LH ==&gt; LSH\n        # HL ==&gt; HSL\n        # (4) smooth expanded and unwrapped ordinate (abscissa wrap=False)\n        # (5) cut out orignal signal\n\n        # (1)\n        kwargs[\"mode\"] = \"reflect\"\n        L = out._ordinate.range.max - out._ordinate.range.min\n        D = out.domain.length\n\n        tmp = utils.gaussian_filter(out.unwrap(), **kwargs)\n        # (2) (3)\n        n_reps = int(np.ceil((sigma * truncate) / float(D)))\n\n        smooth_data = []\n        for ss, signal in enumerate(tmp.signals):\n            # signal = signal.wrap()\n            offset = (\n                float((signal._data[:, -1] - signal._data[:, 0]) // (L / 2)) * L\n            )\n            # print(offset)\n            # left_high = signal._data[:,0] &gt;= out._ordinate.range.min + L/2\n            # right_high = signal._data[:,-1] &gt;= out._ordinate.range.min + L/2\n            # signal = signal.unwrap()\n\n            expanded = signal.copy()\n            for nn in range(n_reps):\n                expanded = expanded.join((signal &lt;&lt; D * (nn + 1)) - offset).join(\n                    (signal &gt;&gt; D * (nn + 1)) + offset\n                )\n                # print(expanded)\n                # if left_high == right_high:\n                #     print('extending flat! signal {}'.format(ss))\n                #     expanded = expanded.join(signal &lt;&lt; D*(nn+1)).join(signal &gt;&gt; D*(nn+1))\n                # elif left_high &lt; right_high:\n                #     print('extending LSH! signal {}'.format(ss))\n                #     # LSH\n                #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))-L).join((signal &gt;&gt; D*(nn+1))+L)\n                # else:\n                #     # HSL\n                #     print('extending HSL! signal {}'.format(ss))\n                #     expanded = expanded.join((signal &lt;&lt; D*(nn+1))+L).join((signal &gt;&gt; D*(nn+1))-L)\n            # (4)\n            smooth_signal = utils.gaussian_filter(expanded, **kwargs)\n            smooth_data.append(\n                smooth_signal._data[\n                    :, n_reps * tmp.n_samples : (n_reps + 1) * (tmp.n_samples)\n                ].squeeze()\n            )\n        # (5)\n        out._data = np.array(smooth_data)\n        out.__renew__()\n\n        if self._ordinate.is_wrapping:\n            if ord_is_wrapped:\n                out = out.wrap()\n\n        return out\n\n    # case 7: abs.wrapping=True, ord.linking=True, ord.wrapping=False\n    elif (\n        self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and not self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    # case 8: abs.wrapping=True, ord.linking=True, ord.wrapping=True\n    elif (\n        self._abscissa.is_wrapping\n        and self._ordinate.is_linking\n        and self._ordinate.is_wrapping\n    ):\n        raise NotImplementedError\n\n    out = utils.gaussian_filter(out, **kwargs)\n    out.__renew__()\n\n    if self._ordinate.is_wrapping:\n        if ord_is_wrapped:\n            out = out.wrap()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.standardize","title":"<code>standardize(inplace=False)</code>","text":"<p>Standardize the data to zero mean and unit standard deviation along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The standardized signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; standardized = asa.standardize()\n&gt;&gt;&gt; standardized.mean(), standardized.std()\n(0.0, 1.0)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def standardize(self, inplace=False):\n    \"\"\"\n    Standardize the data to zero mean and unit standard deviation along the sample axis.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The standardized signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; standardized = asa.standardize()\n    &gt;&gt;&gt; standardized.mean(), standardized.std()\n    (0.0, 1.0)\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n    out._data = (out._data.T - out.mean()).T\n    std = np.atleast_1d(out.std())\n    std[std == 0] = 1\n    out._data = (out._data.T / std).T\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.std","title":"<code>std(*, axis=1)</code>","text":"<p>Compute the standard deviation of the data along the specified axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>Axis along which to compute the standard deviation (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>std</code> <code>ndarray</code> <p>Standard deviation values along the specified axis.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def std(self, *, axis=1):\n    \"\"\"\n    Compute the standard deviation of the data along the specified axis.\n\n    Parameters\n    ----------\n    axis : int, optional\n        Axis along which to compute the standard deviation (default is 1).\n\n    Returns\n    -------\n    std : np.ndarray\n        Standard deviation values along the specified axis.\n    \"\"\"\n    try:\n        stds = np.nanstd(self.data, axis=axis).squeeze()\n        if stds.size == 1:\n            return stds.item()\n        return stds\n    except IndexError:\n        raise IndexError(\n            \"Empty RegularlySampledAnalogSignalArray cannot calculate standard deviation\"\n        )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.subsample","title":"<code>subsample(*, fs)</code>","text":"<p>Subsamples a RegularlySampledAnalogSignalArray</p> <p>WARNING! Aliasing can occur! It is better to use downsample when lowering the sampling rate substantially.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>Desired output sampling rate, in Hz</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Copy of RegularlySampledAnalogSignalArray where data is only stored at the new subset of points.</p> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def subsample(self, *, fs):\n    \"\"\"Subsamples a RegularlySampledAnalogSignalArray\n\n    WARNING! Aliasing can occur! It is better to use downsample when\n    lowering the sampling rate substantially.\n\n    Parameters\n    ----------\n    fs : float, optional\n        Desired output sampling rate, in Hz\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Copy of RegularlySampledAnalogSignalArray where data is only stored at the\n        new subset of points.\n    \"\"\"\n\n    return self.simplify(ds=1 / fs)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.trim","title":"<code>trim(start, stop=None, *, fs=None)</code>","text":"<p>Trim the signal to the specified start and stop times.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>Start time.</p> required <code>stop</code> <code>float</code> <p>Stop time. If None, trims to the end.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling frequency. If None, uses self.fs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>Trimmed signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trimmed = asa.trim(0, 10)\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def trim(self, start, stop=None, *, fs=None):\n    \"\"\"\n    Trim the signal to the specified start and stop times.\n\n    Parameters\n    ----------\n    start : float\n        Start time.\n    stop : float, optional\n        Stop time. If None, trims to the end.\n    fs : float, optional\n        Sampling frequency. If None, uses self.fs.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        Trimmed signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; trimmed = asa.trim(0, 10)\n    \"\"\"\n    logging.warning(\"RegularlySampledAnalogSignalArray: Trim may not work!\")\n    # TODO: do comprehensive input validation\n    if stop is not None:\n        try:\n            start = np.array(start, ndmin=1)\n            if len(start) != 1:\n                raise TypeError(\"start must be a scalar float\")\n        except TypeError:\n            raise TypeError(\"start must be a scalar float\")\n        try:\n            stop = np.array(stop, ndmin=1)\n            if len(stop) != 1:\n                raise TypeError(\"stop must be a scalar float\")\n        except TypeError:\n            raise TypeError(\"stop must be a scalar float\")\n    else:  # start must have two elements\n        try:\n            if len(np.array(start, ndmin=1)) &gt; 2:\n                raise TypeError(\n                    \"unsupported input to RegularlySampledAnalogSignalArray.trim()\"\n                )\n            stop = np.array(start[1], ndmin=1)\n            start = np.array(start[0], ndmin=1)\n            if len(start) != 1 or len(stop) != 1:\n                raise TypeError(\"start and stop must be scalar floats\")\n        except TypeError:\n            raise TypeError(\"start and stop must be scalar floats\")\n\n    logging.disable(logging.CRITICAL)\n    interval = self._abscissa.support.intersect(\n        type(self.support)([start, stop], fs=fs)\n    )\n    if not interval.isempty:\n        analogsignalarray = self[interval]\n    else:\n        analogsignalarray = type(self)([], empty=True)\n    logging.disable(0)\n    analogsignalarray.__renew__()\n    return analogsignalarray\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.unwrap","title":"<code>unwrap(inplace=False)</code>","text":"<p>Unwrap the ordinate values by minimizing total displacement, useful for phase data.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The unwrapped signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unwrapped = asa.unwrap()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def unwrap(self, inplace=False):\n    \"\"\"\n    Unwrap the ordinate values by minimizing total displacement, useful for phase data.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The unwrapped signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; unwrapped = asa.unwrap()\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    out.data = np.atleast_2d(\n        out._unwrap(out._data, out._ordinate.range.min, out._ordinate.range.max)\n    )\n    # out._is_wrapped = False\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.wrap","title":"<code>wrap(inplace=False)</code>","text":"<p>Wrap the ordinate values within the finite range defined by the ordinate's range.</p> <p>Parameters:</p> Name Type Description Default <code>inplace</code> <code>bool</code> <p>If True, modifies the data in place. If False (default), returns a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>The wrapped signal array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; wrapped = asa.wrap()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def wrap(self, inplace=False):\n    \"\"\"\n    Wrap the ordinate values within the finite range defined by the ordinate's range.\n\n    Parameters\n    ----------\n    inplace : bool, optional\n        If True, modifies the data in place. If False (default), returns a new object.\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        The wrapped signal array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; wrapped = asa.wrap()\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    out.data = np.atleast_2d(\n        out._wrap(out.data, out._ordinate.range.min, out._ordinate.range.max)\n    )\n    # out._is_wrapped = True\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.RegularlySampledAnalogSignalArray.zscore","title":"<code>zscore()</code>","text":"<p>Normalize each signal in the array using z-scores (zero mean, unit variance).</p> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>New object with z-scored data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zscored = asa.zscore()\n</code></pre> Source code in <code>nelpy/core/_analogsignalarray.py</code> <pre><code>def zscore(self):\n    \"\"\"\n    Normalize each signal in the array using z-scores (zero mean, unit variance).\n\n    Returns\n    -------\n    out : RegularlySampledAnalogSignalArray\n        New object with z-scored data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; zscored = asa.zscore()\n    \"\"\"\n    out = self.copy()\n    out._data = zscore(out._data, axis=1)\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.SpaceArray","title":"<code>SpaceArray</code>","text":"<p>               Bases: <code>IntervalArray</code></p> <p>IntervalArray containing spatial intervals (in centimeters).</p> <p>This class extends <code>IntervalArray</code> to specifically handle space-based intervals, such as linear or 2D spatial regions. It provides a formatter for displaying spatial lengths and can be used for spatial segmentation in behavioral or neural data analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>If shape (n_intervals, 1) or (n_intervals,), the start position for each interval (which then requires a <code>length</code> to be specified). If shape (n_intervals, 2), the start and stop positions for each interval. Defaults to None, creating an empty <code>SpaceArray</code>.</p> required <code>length</code> <code>np.array, float, or None</code> <p>The length of the interval (in base units, centimeters). If a float, the same length is assumed for every interval. Only used if <code>data</code> is a 1D array of start positions.</p> required <code>meta</code> <code>dict</code> <p>Metadata associated with the spatial intervals.</p> required <code>empty</code> <code>bool</code> <p>If True, an empty <code>SpaceArray</code> is returned, ignoring <code>data</code> and <code>length</code>. Defaults to False.</p> required <code>domain</code> <code>IntervalArray</code> <p>The domain within which the spatial intervals are defined. If None, it defaults to an infinite domain.</p> required <code>label</code> <code>str</code> <p>A descriptive label for the space array.</p> required <p>Attributes:</p> Name Type Description <code>data</code> <code>array</code> <p>The start and stop positions for each interval, with shape (n_intervals, 2).</p> <code>n_intervals</code> <code>int</code> <p>The number of spatial intervals in the array.</p> <code>lengths</code> <code>array</code> <p>The length of each spatial interval (in centimeters).</p> <code>formatter</code> <code>PrettySpace</code> <p>The formatter used for displaying spatial lengths.</p> <code>base_unit</code> <code>str</code> <p>The base unit of the intervals, which is 'cm' for SpaceArray.</p> Notes <p>This class inherits all methods and properties from <code>IntervalArray</code>. It is intended for use with spatial data, such as segmenting a linear track or defining regions of interest in a behavioral arena.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy.core import SpaceArray\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a SpaceArray from start and stop positions\n&gt;&gt;&gt; regions = SpaceArray(data=np.array([[0, 50], [100, 150]]))\n&gt;&gt;&gt; print(regions)\n&lt;SpaceArray at 0x...: 2 intervals&gt; of length 100 cm\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a SpaceArray from start positions and a common length\n&gt;&gt;&gt; starts = np.array([0, 100])\n&gt;&gt;&gt; length = 25.0\n&gt;&gt;&gt; regions_with_length = SpaceArray(data=starts, length=length)\n&gt;&gt;&gt; print(regions_with_length)\n&lt;SpaceArray at 0x...: 2 intervals&gt; of length 50 cm\n</code></pre> <pre><code>&gt;&gt;&gt; # Accessing attributes\n&gt;&gt;&gt; print(f\"Number of regions: {regions.n_intervals}\")\nNumber of regions: 2\n&gt;&gt;&gt; print(f\"Lengths: {regions.lengths}\")\nLengths: [50 50]\n</code></pre> Source code in <code>nelpy/core/_intervalarray.py</code> <pre><code>class SpaceArray(IntervalArray):\n    \"\"\"\n    IntervalArray containing spatial intervals (in centimeters).\n\n    This class extends `IntervalArray` to specifically handle space-based\n    intervals, such as linear or 2D spatial regions. It provides a formatter\n    for displaying spatial lengths and can be used for spatial segmentation\n    in behavioral or neural data analysis.\n\n    Parameters\n    ----------\n    data : np.array, optional\n        If shape (n_intervals, 1) or (n_intervals,), the start position for each\n        interval (which then requires a `length` to be specified).\n        If shape (n_intervals, 2), the start and stop positions for each interval.\n        Defaults to None, creating an empty `SpaceArray`.\n    length : np.array, float, or None, optional\n        The length of the interval (in base units, centimeters). If a float,\n        the same length is assumed for every interval. Only used if `data`\n        is a 1D array of start positions.\n    meta : dict, optional\n        Metadata associated with the spatial intervals.\n    empty : bool, optional\n        If True, an empty `SpaceArray` is returned, ignoring `data` and `length`.\n        Defaults to False.\n    domain : IntervalArray, optional\n        The domain within which the spatial intervals are defined. If None, it defaults\n        to an infinite domain.\n    label : str, optional\n        A descriptive label for the space array.\n\n    Attributes\n    ----------\n    data : np.array\n        The start and stop positions for each interval, with shape (n_intervals, 2).\n    n_intervals : int\n        The number of spatial intervals in the array.\n    lengths : np.array\n        The length of each spatial interval (in centimeters).\n    formatter : formatters.PrettySpace\n        The formatter used for displaying spatial lengths.\n    base_unit : str\n        The base unit of the intervals, which is 'cm' for SpaceArray.\n\n    Notes\n    -----\n    This class inherits all methods and properties from `IntervalArray`.\n    It is intended for use with spatial data, such as segmenting a linear track\n    or defining regions of interest in a behavioral arena.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy.core import SpaceArray\n\n    &gt;&gt;&gt; # Create a SpaceArray from start and stop positions\n    &gt;&gt;&gt; regions = SpaceArray(data=np.array([[0, 50], [100, 150]]))\n    &gt;&gt;&gt; print(regions)\n    &lt;SpaceArray at 0x...: 2 intervals&gt; of length 100 cm\n\n    &gt;&gt;&gt; # Create a SpaceArray from start positions and a common length\n    &gt;&gt;&gt; starts = np.array([0, 100])\n    &gt;&gt;&gt; length = 25.0\n    &gt;&gt;&gt; regions_with_length = SpaceArray(data=starts, length=length)\n    &gt;&gt;&gt; print(regions_with_length)\n    &lt;SpaceArray at 0x...: 2 intervals&gt; of length 50 cm\n\n    &gt;&gt;&gt; # Accessing attributes\n    &gt;&gt;&gt; print(f\"Number of regions: {regions.n_intervals}\")\n    Number of regions: 2\n    &gt;&gt;&gt; print(f\"Lengths: {regions.lengths}\")\n    Lengths: [50 50]\n    \"\"\"\n\n    __aliases__ = {}\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        super().__init__(*args, **kwargs)\n\n        self.formatter = formatters.PrettySpace\n        self.base_unit = self.formatter.base_unit\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.SpikeTrainArray","title":"<code>SpikeTrainArray</code>","text":"<p>               Bases: <code>EventArray</code></p> <p>A multiseries spike train array with shared support.</p> <p>SpikeTrainArray is a specialized EventArray for handling neural spike train data. It provides unit-specific aliases and methods for analyzing spike timing data across multiple recording units with a common temporal support.</p> <p>Parameters:</p> Name Type Description Default <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000.</p> required <code>support</code> <code>IntervalArray</code> <p>IntervalArray on which spike trains are defined. Default is [0, last spike] inclusive.</p> required <code>unit_ids</code> <code>list of int</code> <p>Unit IDs. Alias for series_ids.</p> required <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Default casts unit_ids to str. Alias for series_labels.</p> required <code>unit_tags</code> <code>optional</code> <p>Tags corresponding to units. Alias for series_tags. NOTE: Currently we do not do any input validation so these can be any type. We also don't use these for anything yet.</p> required <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the spike train array.</p> required <code>empty</code> <code>bool</code> <p>Whether an empty SpikeTrainArray should be constructed (no data).</p> required <code>**kwargs</code> <code>optional</code> <p>Additional keyword arguments forwarded to the BaseEventArray constructor.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>Note</code> <code>Read the docstring for the BaseEventArray and EventArray superclasses</code> <code>for additional attributes that are defined there.</code> <code>isempty</code> <code>bool</code> <p>Whether the SpikeTrainArray is empty (no data).</p> <code>n_units</code> <code>int</code> <p>The number of units. Alias for n_series.</p> <code>n_active</code> <code>int</code> <p>The number of active units. A unit is considered active if it fired at least one spike.</p> <code>time</code> <code>array of np.array(dtype=np.float64)</code> <p>Spike time data in seconds. Array of length n_units, each entry with shape (n_spikes,). Alias for data.</p> <code>n_spikes</code> <code>ndarray</code> <p>The number of spikes in each unit. Alias for n_events.</p> <code>issorted</code> <code>bool</code> <p>Whether the spike times are sorted.</p> <code>first_spike</code> <code>float</code> <p>The time of the very first spike, across all units.</p> <code>last_spike</code> <code>float</code> <p>The time of the very last spike, across all units.</p> <code>unit_ids</code> <code>list of int</code> <p>Unit IDs. Alias for series_ids.</p> <code>unit_labels</code> <code>list of str</code> <p>Labels corresponding to units. Alias for series_labels.</p> <code>unit_tags</code> <p>Tags corresponding to units. Alias for series_tags.</p> <code>n_epochs</code> <code>int</code> <p>The number of epochs/intervals. Alias for n_intervals.</p> <code>support</code> <code>IntervalArray</code> <p>The support of the SpikeTrainArray.</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz).</p> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the spike train array.</p> <p>Methods:</p> Name Description <code>bin</code> <p>Return a BinnedSpikeTrainArray.</p> <code>get_spike_firing_order</code> <p>Returns unit_ids ordered by when they first fire.</p> <code>reorder_units_by_ids</code> <p>Reorder units according to specified unit_ids.</p> <code>flatten</code> <p>Collapse spikes across units into a single unit.</p> <code>partition</code> <p>Returns a SpikeTrainArray whose support has been partitioned.</p> <code>copy</code> <p>Returns a copy of the SpikeTrainArray.</p> <code>empty</code> <p>Remove data (but not metadata) from SpikeTrainArray.</p> <p>Examples:</p> <p>Create a SpikeTrainArray with two units:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n&gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n&gt;&gt;&gt; print(sta.n_units)\n2\n&gt;&gt;&gt; print(sta.n_spikes)\n[3 3]\n</code></pre> <p>Access spike times using the time alias:</p> <pre><code>&gt;&gt;&gt; print(sta.time[0])  # First unit's spike times\n[0.1 0.3 0.7]\n</code></pre> <p>Create a binned version:</p> <pre><code>&gt;&gt;&gt; binned = sta.bin(ds=0.1)  # 100ms bins\n</code></pre> Notes <p>SpikeTrainArray provides neuroscience-specific aliases for EventArray functionality. For example, 'units' instead of 'series', 'spikes' instead of 'events', and 'time' instead of 'data'. This makes the API more intuitive for neuroscience applications while maintaining full compatibility with the underlying EventArray infrastructure.</p> <p>The class automatically handles spike time sorting and provides efficient methods for common spike train analyses. All spike times are stored in seconds and should be non-negative values within the support interval.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>class SpikeTrainArray(EventArray):\n    \"\"\"A multiseries spike train array with shared support.\n\n    SpikeTrainArray is a specialized EventArray for handling neural spike train data.\n    It provides unit-specific aliases and methods for analyzing spike timing data\n    across multiple recording units with a common temporal support.\n\n    Parameters\n    ----------\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000.\n    support : IntervalArray, optional\n        IntervalArray on which spike trains are defined.\n        Default is [0, last spike] inclusive.\n    unit_ids : list of int, optional\n        Unit IDs. Alias for series_ids.\n    unit_labels : list of str, optional\n        Labels corresponding to units. Default casts unit_ids to str.\n        Alias for series_labels.\n    unit_tags : optional\n        Tags corresponding to units. Alias for series_tags.\n        NOTE: Currently we do not do any input validation so these can\n        be any type. We also don't use these for anything yet.\n    label : str or None, optional\n        Information pertaining to the source of the spike train array.\n    empty : bool, optional\n        Whether an empty SpikeTrainArray should be constructed (no data).\n    **kwargs : optional\n        Additional keyword arguments forwarded to the BaseEventArray\n        constructor.\n\n    Attributes\n    ----------\n    Note : Read the docstring for the BaseEventArray and EventArray superclasses\n    for additional attributes that are defined there.\n\n    isempty : bool\n        Whether the SpikeTrainArray is empty (no data).\n    n_units : int\n        The number of units. Alias for n_series.\n    n_active : int\n        The number of active units. A unit is considered active if\n        it fired at least one spike.\n    time : array of np.array(dtype=np.float64)\n        Spike time data in seconds. Array of length n_units, each entry with\n        shape (n_spikes,). Alias for data.\n    n_spikes : np.ndarray\n        The number of spikes in each unit. Alias for n_events.\n    issorted : bool\n        Whether the spike times are sorted.\n    first_spike : float\n        The time of the very first spike, across all units.\n    last_spike : float\n        The time of the very last spike, across all units.\n    unit_ids : list of int\n        Unit IDs. Alias for series_ids.\n    unit_labels : list of str\n        Labels corresponding to units. Alias for series_labels.\n    unit_tags :\n        Tags corresponding to units. Alias for series_tags.\n    n_epochs : int\n        The number of epochs/intervals. Alias for n_intervals.\n    support : IntervalArray\n        The support of the SpikeTrainArray.\n    fs : float\n        Sampling frequency (Hz).\n    label : str or None\n        Information pertaining to the source of the spike train array.\n\n    Methods\n    -------\n    bin(ds=None)\n        Return a BinnedSpikeTrainArray.\n    get_spike_firing_order()\n        Returns unit_ids ordered by when they first fire.\n    reorder_units_by_ids(neworder, inplace=False)\n        Reorder units according to specified unit_ids.\n    flatten(unit_id=None, unit_label=None)\n        Collapse spikes across units into a single unit.\n    partition(ds=None, n_epochs=None)\n        Returns a SpikeTrainArray whose support has been partitioned.\n    copy()\n        Returns a copy of the SpikeTrainArray.\n    empty(inplace=False)\n        Remove data (but not metadata) from SpikeTrainArray.\n\n    Examples\n    --------\n    Create a SpikeTrainArray with two units:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; spike_times = [np.array([0.1, 0.3, 0.7]), np.array([0.2, 0.5, 0.8])]\n    &gt;&gt;&gt; sta = SpikeTrainArray(spike_times, unit_ids=[1, 2], fs=1000)\n    &gt;&gt;&gt; print(sta.n_units)\n    2\n    &gt;&gt;&gt; print(sta.n_spikes)\n    [3 3]\n\n    Access spike times using the time alias:\n\n    &gt;&gt;&gt; print(sta.time[0])  # First unit's spike times\n    [0.1 0.3 0.7]\n\n    Create a binned version:\n\n    &gt;&gt;&gt; binned = sta.bin(ds=0.1)  # 100ms bins\n\n    Notes\n    -----\n    SpikeTrainArray provides neuroscience-specific aliases for EventArray\n    functionality. For example, 'units' instead of 'series', 'spikes' instead\n    of 'events', and 'time' instead of 'data'. This makes the API more intuitive\n    for neuroscience applications while maintaining full compatibility with the\n    underlying EventArray infrastructure.\n\n    The class automatically handles spike time sorting and provides efficient\n    methods for common spike train analyses. All spike times are stored in\n    seconds and should be non-negative values within the support interval.\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n    }\n\n    def __init__(self, *args, **kwargs):\n        # add class-specific aliases to existing aliases:\n        self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n\n        series_label = kwargs.pop(\"series_label\", None)\n        if series_label is None:\n            series_label = \"units\"\n        kwargs[\"series_label\"] = series_label\n\n        # legacy STA constructor support for backward compatibility\n        kwargs = legacySTAkwargs(**kwargs)\n\n        support = kwargs.get(\"support\", None)\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        super().__init__(*args, **kwargs)\n\n    # @keyword_equivalence(this_or_that={'n_intervals':'n_epochs'})\n    # def partition(self, ds=None, n_intervals=None, n_epochs=None):\n    #     if n_intervals is None:\n    #         n_intervals = n_epochs\n    #     kwargs = {'ds':ds, 'n_intervals': n_intervals}\n    #     return super().partition(**kwargs)\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n        return BinnedSpikeTrainArray(self, ds=ds)  # TODO #FIXME\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.SpikeTrainArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedSpikeTrainArray.</p> Source code in <code>nelpy/core/_eventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedSpikeTrainArray.\"\"\"\n    return BinnedSpikeTrainArray(self, ds=ds)  # TODO #FIXME\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray","title":"<code>StatefulValueEventArray</code>","text":"<p>               Bases: <code>BaseValueEventArray</code></p> <p>StatefulValueEventArray for storing events with associated values and states.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>array - like</code> <p>Event times for each series. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>values</code> <code>array - like</code> <p>Values associated with each event. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>states</code> <code>array - like</code> <p>States associated with each event. List of arrays, shape (n_series, n_events_i).</p> <code>None</code> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the events. If None, inferred from events.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz. Default is 30000.</p> <code>None</code> <code>series_ids</code> <code>list</code> <p>List of series IDs. If None, defaults to [1, ..., n_series].</p> <code>None</code> <code>empty</code> <code>bool</code> <p>If True, create an empty object.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>events</code> <code>ndarray</code> <p>Event times for each series. Ragged array, shape (n_series, n_events_i).</p> <code>values</code> <code>ndarray</code> <p>Values for each event. Ragged array, shape (n_series, n_events_i).</p> <code>states</code> <code>ndarray</code> <p>States for each event. Ragged array, shape (n_series, n_events_i).</p> <code>support</code> <code>IntervalArray</code> <p>Support intervals for the events.</p> <code>fs</code> <code>float</code> <p>Sampling frequency in Hz.</p> <code>n_series</code> <code>int</code> <p>Number of series.</p> <code>n_events</code> <code>ndarray</code> <p>Number of events in each series.</p> <code>series_ids</code> <code>list</code> <p>List of series IDs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n&gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; states = [[10, 20, 30], [40, 50, 60]]\n&gt;&gt;&gt; sveva = nel.StatefulValueEventArray(\n...     events=events, values=values, states=states, fs=10\n... )\n&gt;&gt;&gt; sveva.n_series\n2\n&gt;&gt;&gt; sveva.n_events\narray([3, 3])\n&gt;&gt;&gt; sveva.events[0]\narray([0.1, 0.5, 1.0])\n&gt;&gt;&gt; sveva.values[0]\narray([1, 2, 3])\n&gt;&gt;&gt; sveva.states[0]\narray([10, 20, 30])\n</code></pre> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class StatefulValueEventArray(BaseValueEventArray):\n    \"\"\"\n    StatefulValueEventArray for storing events with associated values and states.\n\n    Parameters\n    ----------\n    events : array-like\n        Event times for each series. List of arrays, shape (n_series, n_events_i).\n    values : array-like\n        Values associated with each event. List of arrays, shape (n_series, n_events_i).\n    states : array-like\n        States associated with each event. List of arrays, shape (n_series, n_events_i).\n    support : nelpy.IntervalArray, optional\n        Support intervals for the events. If None, inferred from events.\n    fs : float, optional\n        Sampling frequency in Hz. Default is 30000.\n    series_ids : list, optional\n        List of series IDs. If None, defaults to [1, ..., n_series].\n    empty : bool, optional\n        If True, create an empty object.\n    **kwargs\n        Additional keyword arguments passed to the parent class.\n\n    Attributes\n    ----------\n    events : np.ndarray\n        Event times for each series. Ragged array, shape (n_series, n_events_i).\n    values : np.ndarray\n        Values for each event. Ragged array, shape (n_series, n_events_i).\n    states : np.ndarray\n        States for each event. Ragged array, shape (n_series, n_events_i).\n    support : nelpy.IntervalArray\n        Support intervals for the events.\n    fs : float\n        Sampling frequency in Hz.\n    n_series : int\n        Number of series.\n    n_events : np.ndarray\n        Number of events in each series.\n    series_ids : list\n        List of series IDs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; events = [[0.1, 0.5, 1.0], [0.2, 0.6, 1.2]]\n    &gt;&gt;&gt; values = [[1, 2, 3], [4, 5, 6]]\n    &gt;&gt;&gt; states = [[10, 20, 30], [40, 50, 60]]\n    &gt;&gt;&gt; sveva = nel.StatefulValueEventArray(\n    ...     events=events, values=values, states=states, fs=10\n    ... )\n    &gt;&gt;&gt; sveva.n_series\n    2\n    &gt;&gt;&gt; sveva.n_events\n    array([3, 3])\n    &gt;&gt;&gt; sveva.events[0]\n    array([0.1, 0.5, 1.0])\n    &gt;&gt;&gt; sveva.values[0]\n    array([1, 2, 3])\n    &gt;&gt;&gt; sveva.states[0]\n    array([10, 20, 30])\n    \"\"\"\n\n    # specify class-specific aliases:\n    __aliases__ = {\n        \"time\": \"data\",\n        \"_time\": \"_data\",\n        \"n_epochs\": \"n_intervals\",\n        \"n_units\": \"n_series\",\n        \"_unit_subset\": \"_series_subset\",  # requires kw change\n        \"get_event_firing_order\": \"get_spike_firing_order\",\n        \"reorder_units_by_ids\": \"reorder_series_by_ids\",\n        \"reorder_units\": \"reorder_series\",\n        \"_reorder_units_by_idx\": \"_reorder_series_by_idx\",\n        \"n_spikes\": \"n_events\",\n        \"n_marks\": \"n_values\",\n        \"unit_ids\": \"series_ids\",\n        \"unit_labels\": \"series_labels\",\n        \"unit_tags\": \"series_tags\",\n        \"_unit_ids\": \"_series_ids\",\n        \"_unit_labels\": \"_series_labels\",\n        \"_unit_tags\": \"_series_tags\",\n        \"first_spike\": \"first_event\",\n        \"last_spike\": \"last_event\",\n        \"marks\": \"values\",\n        \"spikes\": \"events\",\n    }\n\n    def __init__(\n        self,\n        events=None,\n        values=None,\n        states=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        # add class-specific aliases to existing aliases:\n        # self.__aliases__ = {**super().__aliases__, **self.__aliases__}\n        # print('in init')\n        if support is not None:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa(support=support))\n        else:\n            abscissa = kwargs.get(\"abscissa\", core.TemporalAbscissa())\n        ordinate = kwargs.get(\"ordinate\", core.AnalogSignalArrayOrdinate())\n\n        kwargs[\"abscissa\"] = abscissa\n        kwargs[\"ordinate\"] = ordinate\n\n        # print('non-stateful preprocessing')\n        self._val_init(\n            events=events,\n            values=values,\n            states=states,\n            fs=fs,\n            support=support,\n            series_ids=series_ids,\n            empty=empty,\n            **kwargs,\n        )\n\n        # print('making stateful')\n        data = self._make_stateful(data=self.data)\n        self._data = data\n\n    def _val_init(\n        self,\n        events=None,\n        values=None,\n        states=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        #############################################\n        #            standardize kwargs             #\n        #############################################\n        if events is not None:\n            kwargs[\"events\"] = events\n        if values is not None:\n            kwargs[\"values\"] = values\n        if states is not None:\n            kwargs[\"states\"] = states\n        kwargs = self._standardize_kwargs(**kwargs)\n        events = kwargs.pop(\"events\", None)\n        values = kwargs.pop(\"values\", None)\n        states = kwargs.pop(\"states\", None)\n        #############################################\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.info(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            data = np.array(data)\n            try:\n                if data.shape[-1] &lt; 2 and np.max(data.shape) &gt; 1:\n                    return True\n                if max(np.array(data).shape[:-1]) &gt; 1 and data.shape[-1] == 1:\n                    return True\n            except (IndexError, TypeError, ValueError):\n                return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            ========\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            try:\n                if isinstance(data[0][0], list) or isinstance(data[0][0], np.ndarray):\n                    logging.info(\"event datas input has too many layers!\")\n                    try:\n                        if max(np.array(data).shape[:-1]) &gt; 1:\n                            #                 singletons = True\n                            return False\n                    except ValueError:\n                        return False\n                    data = np.squeeze(data)\n            except (IndexError, TypeError):\n                pass\n            try:\n                if isinstance(data[1], list) or isinstance(data[1], np.ndarray):\n                    return False\n            except (IndexError, TypeError):\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            # Handle ragged input: list/tuple or np.ndarray of dtype=object\n            is_ragged = False\n            if isinstance(data, (list, tuple)):\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            elif isinstance(data, np.ndarray) and data.dtype == object:\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            if is_ragged:\n                return utils.ragged_array([np.array(st, ndmin=1) for st in data])\n            # Only here, if not ragged, use np.array/np.squeeze\n            return np.array(np.squeeze(data), ndmin=2)\n\n        def standardize_values_to_2d(data):\n            data = standardize_to_2d(data)\n            for ii, series in enumerate(data):\n                if len(series.shape) == 2:\n                    pass\n                else:\n                    for xx in series:\n                        if len(np.atleast_1d(xx)) &gt; 1:\n                            raise ValueError(\n                                \"each series must have a fixed number of values; mismatch in series {}\".format(\n                                    ii\n                                )\n                            )\n            return data\n\n        events = standardize_to_2d(events)\n        values = standardize_values_to_2d(values)\n        states = standardize_to_2d(states)\n\n        data = []\n        for a, v, s in zip(events, values, states):\n            data.append(np.vstack((a, v.T, s.T)).T)\n        data = np.array(data)\n\n        # sort event series, but only if necessary:\n        for ii, train in enumerate(events):\n            if not utils.is_sorted(train):\n                sortidx = np.argsort(train)\n                data[ii] = (data[ii])[sortidx, :]\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            self._abscissa.support = type(self._abscissa.support)(\n                np.array([self.first_event, self.last_event + 1 / fs])\n            )\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            self._abscissa.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        data = self._restrict_to_interval_array_fast(\n            intervalarray=self.support, data=data\n        )\n\n        self._data = data\n        return\n\n    @property\n    def data(self):\n        \"\"\"Event datas in seconds.\"\"\"\n        return self._data\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to ValueEventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def support(self):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        return self._abscissa.support\n\n    @support.setter\n    def support(self, val):\n        \"\"\"(nelpy.IntervalArray) The support of the underlying EventArray.\"\"\"\n        # modify support\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.support = val\n        elif isinstance(val, (tuple, list)):\n            prev_domain = self._abscissa.domain\n            self._abscissa.support = type(self._abscissa.support)([val[0], val[1]])\n            self._abscissa.domain = prev_domain\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_value_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    @property\n    def domain(self):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        return self._abscissa.domain\n\n    @domain.setter\n    def domain(self, val):\n        \"\"\"(nelpy.IntervalArray) The domain of the underlying EventArray.\"\"\"\n        # modify domain\n        if isinstance(val, type(self._abscissa.support)):\n            self._abscissa.domain = val\n        elif isinstance(val, (tuple, list)):\n            self._abscissa.domain = type(self._abscissa.support)([val[0], val[1]])\n        else:\n            raise TypeError(\n                \"support must be of type {}\".format(str(type(self._abscissa.support)))\n            )\n        # restrict data to new support\n        self._data = self._restrict_to_interval_array_value_fast(\n            intervalarray=self._abscissa.support, data=self.data, copyover=True\n        )\n\n    def _intervalslicer(self, idx):\n        \"\"\"Helper function to restrict object to EpochArray.\"\"\"\n        # if self.isempty:\n        #     return self\n\n        if isinstance(idx, core.IntervalArray):\n            if idx.isempty:\n                return type(self)(empty=True)\n            support = self._abscissa.support.intersect(\n                interval=idx, boundaries=True\n            )  # what if fs of slicing interval is different?\n            if support.isempty:\n                return type(self)(empty=True)\n\n            logging.disable(logging.CRITICAL)\n            data = self._restrict_to_interval_array_value_fast(\n                intervalarray=support, data=self.data, copyover=True\n            )\n            eventarray = self._copy_without_data()\n            eventarray._data = data\n            eventarray._abscissa.support = support\n            eventarray.__renew__()\n            logging.disable(0)\n            return eventarray\n        elif isinstance(idx, int):\n            eventarray = self._copy_without_data()\n            support = self._abscissa.support[idx]\n            eventarray._abscissa.support = support\n            if (idx &gt;= self._abscissa.support.n_intervals) or idx &lt; (\n                -self._abscissa.support.n_intervals\n            ):\n                eventarray.__renew__()\n                return eventarray\n            else:\n                data = self._restrict_to_interval_array_value_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                return eventarray\n        else:  # most likely slice indexing\n            try:\n                logging.disable(logging.CRITICAL)\n                support = self._abscissa.support[idx]\n                data = self._restrict_to_interval_array_value_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray = self._copy_without_data()\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                logging.disable(0)\n                return eventarray\n            except Exception:\n                raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    @staticmethod\n    def _restrict_to_interval_array_fast(intervalarray, data, copyover=True):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                frm, to = np.searchsorted(evt_data[:, 0], (t_start, t_stop))\n                indices.append((frm, to))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n        return data\n\n    def _restrict_to_interval_array_value_fast(\n        self, intervalarray, data, copyover=True\n    ):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        # plan of action\n        # create pseudo events supporting each interval\n        # then restrict existing data (pseudo and real events)\n        # then merge in all pseudo events that don't exist yet\n        starts = intervalarray.starts\n        stops = intervalarray.stops\n\n        kinds = []\n        events = []\n        states = []\n\n        for series in data:\n            tvect = series[:, 0].astype(float)\n            statevals = series[:, 2:]\n\n            kind = []\n            state = []\n\n            for start in starts:\n                idx = np.max((np.searchsorted(tvect, start, side=\"right\") - 1, 0))\n                kind.append(0)\n                state.append(statevals[[idx]])\n\n            for stop in stops:\n                idx = np.max((np.searchsorted(tvect, stop, side=\"right\") - 1, 0))\n                kind.append(2)\n                state.append(statevals[[idx]])\n\n            states.append(np.array(state).squeeze())  ## squeeze???\n            events.append(np.hstack((starts, stops)))\n            kinds.append(np.array(kind))\n\n        pseudodata = []\n        for e, k, s in zip(events, kinds, states):\n            pseudodata.append(np.vstack((e, k, s.T)).T)\n\n        pseudodata = utils.ragged_array(pseudodata)\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                frm, to = np.searchsorted(evt_data[:, 0], (t_start, t_stop))\n                indices.append((frm, to))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n\n        # now add in all pseudo events that don't already exist in data\n\n        kinds = []\n        events = []\n        states = []\n\n        for pseries, series in zip(pseudodata, data):\n            ptvect = pseries[:, 0].astype(float)\n            pkind = pseries[:, 1].astype(int)\n            pstatevals = pseries[:, 2:]\n\n            try:\n                tvect = series[:, 0].astype(float)\n                kind = series[:, 1]\n                statevals = series[:, 2:]\n            except IndexError:\n                tvect = np.zeros((0))\n                kind = np.zeros((0))\n                statevals = np.zeros((0,))\n\n            for tt, kk, psv in zip(ptvect, pkind, pstatevals):\n                # print(tt, kk, psv)\n                idx = np.searchsorted(tvect, tt, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                try:\n                    if tt == tvect[idx2]:\n                        pass\n                        # print('pseudo event {} not necessary...'.format(tt))\n                    else:\n                        # print('pseudo event {} necessary...'.format(tt))\n                        kind = np.insert(kind, idx, kk)\n                        tvect = np.insert(tvect, idx, tt)\n                        statevals = np.insert(statevals, idx, psv, axis=0)\n                except IndexError:\n                    kind = np.insert(kind, idx, kk)\n                    tvect = np.insert(tvect, idx, tt)\n                    statevals = np.insert(statevals, idx, psv, axis=0)\n\n            states.append(np.array(statevals).squeeze())\n            events.append(tvect)\n            kinds.append(kind)\n\n        # print(states)\n        # print(tvect)\n        # print(kinds)\n\n        data = []\n        for e, k, s in zip(events, kinds, states):\n            data.append(np.vstack((e, k, s.T)).T)\n\n        data = utils.ragged_array(data)\n\n        return data\n\n    def bin(self, *, ds=None):\n        \"\"\"Return a BinnedValueEventArray.\"\"\"\n        raise NotImplementedError\n        return BinnedValueEventArray(self, ds=ds)\n\n    def __call__(self, *args):\n        \"\"\"StatefulValueEventArray callable method; by default returns state values\"\"\"\n        values = []\n        for events, vals in zip(self.state_events, self.state_values):\n            idx = np.searchsorted(events, args, side=\"right\") - 1\n            idx[idx &lt; 0] = 0\n            values.append(vals[[idx]])\n        values = np.asarray(values)\n        return values\n\n    def _make_stateful(self, data, intervalarray=None, initial_state=np.nan):\n        \"\"\"\n        [i, e0, e1, e2, ..., f] for every epoch\n\n        matrix of size (n_values x (n_epochs*2 + n_events) )\n        matrix of size (nSeries: n_values x (n_epochs*2 + n_events) )\n\n        needs to change when calling loc, iloc, restrict, getitem, ...\n\n        TODO: initial_state is not used yet!!!\n        \"\"\"\n        kinds = []\n        events = []\n        states = []\n\n        if intervalarray is None:\n            intervalarray = self.support\n\n        for series in data:\n            starts = intervalarray.starts\n            stops = intervalarray.stops\n            tvect = series[:, 0].astype(float)\n            statevals = series[:, 1:]\n            kind = np.ones(tvect.size).astype(int)\n\n            for start in starts:\n                idx = np.searchsorted(tvect, start, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                if start == tvect[idx2]:\n                    continue\n                else:\n                    kind = np.insert(kind, idx, 0)\n                    tvect = np.insert(tvect, idx, start)\n                    statevals = np.insert(statevals, idx, statevals[idx2], axis=0)\n\n            for stop in stops:\n                idx = np.searchsorted(tvect, stop, side=\"right\")\n                idx2 = np.max((idx - 1, 0))\n                if stop == tvect[idx2]:\n                    continue\n                else:\n                    kind = np.insert(kind, idx, 2)\n                    tvect = np.insert(tvect, idx, stop)\n                    statevals = np.insert(statevals, idx, statevals[idx2], axis=0)\n\n            states.append(statevals)\n            events.append(tvect)\n            kinds.append(kind)\n\n        data = []\n        for e, k, s in zip(events, kinds, states):\n            data.append(np.vstack((e, k, s.T)).T)\n        data = utils.ragged_array(data)\n\n        return data\n\n    @property\n    def n_values(self):\n        \"\"\"(int) The number of values associated with each event series.\"\"\"\n        if self.isempty:\n            return 0\n        n_values = []\n        for series in self.data:\n            n_values.append(series.squeeze().shape[1] - 2)\n        return n_values\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s\" % (self.type_name, address_str, numstr, epstr, fsstr)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.events])\n\n    @property\n    def events(self):\n        events = []\n        for series, kinds in zip(self.state_events, self.state_kinds):\n            keep_idx = np.argwhere(kinds == 1)\n            events.append(series[keep_idx].squeeze())\n        return np.asarray(events)\n\n    @property\n    def values(self):\n        values = []\n        for series, kinds in zip(self.state_values, self.state_kinds):\n            keep_idx = np.argwhere(kinds == 1)\n            values.append(series[keep_idx].squeeze())\n        return np.asarray(values)\n\n    @property\n    def state_events(self):\n        events = []\n        for series in self.data:\n            events.append(series[:, 0].squeeze())\n\n        return np.asarray(events)\n\n    @property\n    def state_values(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 2:].squeeze())\n\n        return np.asarray(values)\n\n    @property\n    def state_kinds(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 1].squeeze())\n\n        return np.asarray(values)\n\n    def _plot(self, *args, **kwargs):\n        if self.n_series &gt; 1:\n            raise NotImplementedError\n        if np.any(np.array(self.n_values) &gt; 1):\n            raise NotImplementedError\n\n        import matplotlib.pyplot as plt\n\n        events = self.state_events.squeeze()\n        values = self.state_values.squeeze()\n        kinds = self.state_kinds.squeeze()\n\n        for (a, b), val, (ka, kb) in zip(\n            utils.pairwise(events), values, utils.pairwise(kinds)\n        ):\n            if kb == 1:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"b\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n            if ka == 1:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"g\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n            if kb == 1:\n                plt.plot(b, val, \"o\", color=\"k\", markerfacecolor=\"w\", lw=1.5, mew=1.5)\n            if ka == 1:\n                plt.plot(a, val, \"o\", color=\"k\", markerfacecolor=\"k\", lw=1.5, mew=1.5)\n            if ka == 0 and kb == 2:\n                plt.plot(\n                    [a, b],\n                    [val, val],\n                    \"-\",\n                    color=\"r\",\n                    markerfacecolor=\"w\",\n                    lw=1.5,\n                    mew=1.5,\n                )\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray.data","title":"<code>data</code>  <code>property</code>","text":"<p>Event datas in seconds.</p>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray.domain","title":"<code>domain</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The domain of the underlying EventArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray.n_values","title":"<code>n_values</code>  <code>property</code>","text":"<p>(int) The number of values associated with each event series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray.support","title":"<code>support</code>  <code>property</code> <code>writable</code>","text":"<p>(nelpy.IntervalArray) The support of the underlying EventArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray.bin","title":"<code>bin(*, ds=None)</code>","text":"<p>Return a BinnedValueEventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None):\n    \"\"\"Return a BinnedValueEventArray.\"\"\"\n    raise NotImplementedError\n    return BinnedValueEventArray(self, ds=ds)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.StatefulValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.TemporalAbscissa","title":"<code>TemporalAbscissa</code>","text":"<p>               Bases: <code>Abscissa</code></p> <p>Abscissa for time series data.</p> Source code in <code>nelpy/core/_coordinates.py</code> <pre><code>class TemporalAbscissa(Abscissa):\n    \"\"\"Abscissa for time series data.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        support = kwargs.get(\"support\", core.EpochArray(empty=True))\n        labelstring = kwargs.get(\n            \"labelstring\", \"time ({})\"\n        )  # TODO FIXME after unit inheritance; inherit from formatter?\n\n        if support is None:\n            support = core.EpochArray(empty=True)\n\n        kwargs[\"support\"] = support\n        kwargs[\"labelstring\"] = labelstring\n\n        super().__init__(*args, **kwargs)\n\n        self.formatter = self.support.formatter\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray","title":"<code>ValueEventArray</code>","text":"<p>               Bases: <code>BaseValueEventArray</code></p> <p>A multiseries eventarray with shared support.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,)</p> required <code>fs</code> <code>float</code> <p>Sampling rate in Hz. Default is 30,000</p> <code>None</code> <code>support</code> <code>EpochArray</code> <p>EpochArray on which eventarrays are defined. Default is [0, last event] inclusive.</p> <code>None</code> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> required <code>cell_type</code> <code>list (of length n_series) of str or other</code> <p>Identified cell type indicator, e.g., 'pyr', 'int'.</p> required <code>series_ids</code> <code>list (of length n_series) of indices corresponding to</code> <p>curated data. If no series_ids are specified, then [1,...,n_series] will be used. WARNING! The first series will have index 1, not 0!</p> <code>None</code> <code>meta</code> <code>dict</code> <p>Metadata associated with eventarray.</p> required <p>Attributes:</p> Name Type Description <code>data</code> <code>array of np.array(dtype=np.float64) event datas in seconds.</code> <p>Array of length n_series, each entry with shape (n_data,)</p> <code>support</code> <code>EpochArray on which eventarray is defined.</code> <code>n_events</code> <code>np.array(dtype=np.int) of shape (n_series,)</code> <p>Number of events in each series.</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz).</p> <code>cell_types</code> <code>np.array of str or other</code> <p>Identified cell type for each series.</p> <code>label</code> <code>str or None</code> <p>Information pertaining to the source of the eventarray.</p> <code>meta</code> <code>dict</code> <p>Metadata associated with eventseries.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>class ValueEventArray(BaseValueEventArray):\n    \"\"\"A multiseries eventarray with shared support.\n\n    Parameters\n    ----------\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,)\n    fs : float, optional\n        Sampling rate in Hz. Default is 30,000\n    support : EpochArray, optional\n        EpochArray on which eventarrays are defined.\n        Default is [0, last event] inclusive.\n    label : str or None, optional\n        Information pertaining to the source of the eventarray.\n    cell_type : list (of length n_series) of str or other, optional\n        Identified cell type indicator, e.g., 'pyr', 'int'.\n    series_ids : list (of length n_series) of indices corresponding to\n        curated data. If no series_ids are specified, then [1,...,n_series]\n        will be used. WARNING! The first series will have index 1, not 0!\n    meta : dict\n        Metadata associated with eventarray.\n\n    Attributes\n    ----------\n    data : array of np.array(dtype=np.float64) event datas in seconds.\n        Array of length n_series, each entry with shape (n_data,)\n    support : EpochArray on which eventarray is defined.\n    n_events: np.array(dtype=np.int) of shape (n_series,)\n        Number of events in each series.\n    fs: float\n        Sampling frequency (Hz).\n    cell_types : np.array of str or other\n        Identified cell type for each series.\n    label : str or None\n        Information pertaining to the source of the eventarray.\n    meta : dict\n        Metadata associated with eventseries.\n    \"\"\"\n\n    __attributes__ = [\"_data\"]\n    __attributes__.extend(BaseValueEventArray.__attributes__)\n\n    def __init__(\n        self,\n        events=None,\n        values=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        self._val_init(\n            events=events,\n            values=values,\n            fs=fs,\n            support=support,\n            series_ids=series_ids,\n            empty=empty,\n            **kwargs,\n        )\n\n    def _val_init(\n        self,\n        events=None,\n        values=None,\n        *,\n        fs=None,\n        support=None,\n        series_ids=None,\n        empty=False,\n        **kwargs,\n    ):\n        #############################################\n        #            standardize kwargs             #\n        #############################################\n        if events is not None:\n            kwargs[\"events\"] = events\n        if values is not None:\n            kwargs[\"values\"] = values\n        kwargs = self._standardize_kwargs(**kwargs)\n        events = kwargs.pop(\"events\", None)\n        values = kwargs.pop(\"values\", None)\n        #############################################\n\n        # if an empty object is requested, return it:\n        if empty:\n            super().__init__(empty=True)\n            for attr in self.__attributes__:\n                exec(\"self.\" + attr + \" = None\")\n            self._abscissa.support = type(self._abscissa.support)(empty=True)\n            return\n\n        # set default sampling rate\n        if fs is None:\n            fs = 30000\n            logging.info(\n                \"No sampling rate was specified! Assuming default of {} Hz.\".format(fs)\n            )\n\n        def is_singletons(data):\n            \"\"\"Returns True if data is a list of singletons (more than one).\"\"\"\n            # Avoid np.array on jagged input\n            if isinstance(data, (list, tuple)) and len(data) &gt; 1:\n                # If all elements are scalars or 1-element lists/arrays\n                try:\n                    if all(\n                        (not hasattr(x, \"__len__\") or len(np.atleast_1d(x)) == 1)\n                        for x in data\n                    ):\n                        return True\n                except Exception:\n                    return False\n            return False\n\n        def is_single_series(data):\n            \"\"\"Returns True if data represents event datas from a single series.\n\n            Examples\n            --------\n            [1, 2, 3]           : True\n            [[1, 2, 3]]         : True\n            [[1, 2, 3], []]     : False\n            [[], [], []]        : False\n            [[[[1, 2, 3]]]]     : True\n            [[[[[1],[2],[3]]]]] : False\n            \"\"\"\n            # Avoid np.array on jagged input\n            try:\n                # If first element is a list/array and has more than 1 element, not single series\n                if hasattr(data[0], \"__len__\") and len(data[0]) &gt; 1:\n                    # If data[0][0] is also a list/array, too many layers\n                    if hasattr(data[0][0], \"__len__\"):\n                        return False\n                # If second element exists and is a list/array, not single series\n                if len(data) &gt; 1 and hasattr(data[1], \"__len__\"):\n                    return False\n            except Exception:\n                pass\n            return True\n\n        def standardize_to_2d(data):\n            # Handle ragged input: list/tuple or np.ndarray of dtype=object\n            is_ragged = False\n            if isinstance(data, (list, tuple)):\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            elif isinstance(data, np.ndarray) and data.dtype == object:\n                try:\n                    lengths = [len(np.atleast_1d(x)) for x in data]\n                    if len(set(lengths)) &gt; 1:\n                        is_ragged = True\n                except Exception:\n                    pass\n            if is_ragged:\n                return utils.ragged_array([np.array(st, ndmin=1) for st in data])\n            # Only here, if not ragged, use np.array/np.squeeze\n            return np.array(np.squeeze(data), ndmin=2)\n\n        def standardize_values_to_2d(data):\n            data = standardize_to_2d(data)\n            for ii, series in enumerate(data):\n                if len(series.shape) == 2:\n                    pass\n                else:\n                    for xx in series:\n                        if len(np.atleast_1d(xx)) &gt; 1:\n                            raise ValueError(\n                                \"each series must have a fixed number of values; mismatch in series {}\".format(\n                                    ii\n                                )\n                            )\n            return data\n\n        events = standardize_to_2d(events)\n        values = standardize_values_to_2d(values)\n\n        data = []\n        for a, v in zip(events, values):\n            data.append(np.vstack((a, v.T)).T)\n        # Use ragged_array to support jagged arrays (multi-series with different numbers of events)\n        data = utils.ragged_array(data)\n\n        # sort event series, but only if necessary:\n        for ii, train in enumerate(events):\n            if not utils.is_sorted(train):\n                sortidx = np.argsort(train)\n                data[ii] = (data[ii])[sortidx, :]\n\n        kwargs[\"fs\"] = fs\n        kwargs[\"series_ids\"] = series_ids\n\n        self._data = data  # this is necessary so that\n        # super() can determine self.n_series when initializing.\n\n        # initialize super so that self.fs is set:\n        super().__init__(**kwargs)\n\n        # print(self.type_name, kwargs)\n\n        # if only empty data were received AND no support, attach an\n        # empty support:\n        if np.sum([st.size for st in data]) == 0 and support is None:\n            logging.warning(\"no events; cannot automatically determine support\")\n            support = type(self._abscissa.support)(empty=True)\n\n        # determine eventarray support:\n        if support is None:\n            self.support = type(self._abscissa.support)(\n                np.array([self.first_event, self.last_event + 1 / fs])\n            )\n        else:\n            # restrict events to only those within the eventseries\n            # array's support:\n            # print('restricting, here')\n            self.support = support\n\n        # TODO: if sorted, we may as well use the fast restrict here as well?\n        data = self._restrict_to_interval_array_fast(\n            intervalarray=self.support, data=data\n        )\n\n        self._data = data\n        return\n\n    @keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\n    def partition(self, ds=None, n_intervals=None):\n        \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n        Parameters\n        ----------\n        ds : float, optional\n            Maximum duration (in seconds), for each interval.\n        n_points : int, optional\n            Number of intervals. If ds is None and n_intervals is None, then\n            default is to use n_intervals = 100\n\n        Returns\n        -------\n        out : BaseEventArray\n            BaseEventArray that has been partitioned.\n\n        Notes\n        -----\n        Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n        underlying support is propagated, and the first and last points\n        of the supports are always included, even if this would cause\n        n_points or ds to be violated.\n        \"\"\"\n\n        out = self.copy()\n        abscissa = copy.deepcopy(out._abscissa)\n        abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n        out._abscissa = abscissa\n        out.__renew__()\n\n        return out\n\n    def __iter__(self):\n        \"\"\"EventArray iterator initialization.\"\"\"\n        self._index = 0\n        return self\n\n    def __next__(self):\n        \"\"\"EventArray iterator advancer.\"\"\"\n        index = self._index\n\n        if index &gt; self._abscissa.support.n_intervals - 1:\n            raise StopIteration\n\n        self._index += 1\n        return self.loc[index]\n\n    def _intervalslicer(self, idx):\n        \"\"\"Helper function to restrict object to EpochArray.\"\"\"\n        # if self.isempty:\n        #     return self\n\n        if isinstance(idx, core.IntervalArray):\n            if idx.isempty:\n                return type(self)(empty=True)\n            support = self._abscissa.support.intersect(\n                interval=idx, boundaries=True\n            )  # what if fs of slicing interval is different?\n            if support.isempty:\n                return type(self)(empty=True)\n\n            logging.disable(logging.CRITICAL)\n            data = self._restrict_to_interval_array_fast(\n                intervalarray=support, data=self.data, copyover=True\n            )\n            eventarray = self._copy_without_data()\n            eventarray._data = data\n            eventarray._abscissa.support = support\n            eventarray.__renew__()\n            logging.disable(0)\n            return eventarray\n        elif isinstance(idx, int):\n            eventarray = self._copy_without_data()\n            support = self._abscissa.support[idx]\n            eventarray._abscissa.support = support\n            if (idx &gt;= self._abscissa.support.n_intervals) or idx &lt; (\n                -self._abscissa.support.n_intervals\n            ):\n                eventarray.__renew__()\n                return eventarray\n            else:\n                data = self._restrict_to_interval_array_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                return eventarray\n        else:  # most likely slice indexing\n            try:\n                logging.disable(logging.CRITICAL)\n                support = self._abscissa.support[idx]\n                data = self._restrict_to_interval_array_fast(\n                    intervalarray=support, data=self.data, copyover=True\n                )\n                eventarray = self._copy_without_data()\n                eventarray._data = data\n                eventarray._abscissa.support = support\n                eventarray.__renew__()\n                logging.disable(0)\n                return eventarray\n            except Exception:\n                raise TypeError(\"unsupported subsctipting type {}\".format(type(idx)))\n\n    def __getitem__(self, idx):\n        \"\"\"EventArray index access.\n\n        By default, this method is bound to ValueEventArray.loc\n        \"\"\"\n        return self.loc[idx]\n\n    @property\n    def n_active(self):\n        \"\"\"(int) The number of active series.\n\n        A series is considered active if it fired at least one event.\n        \"\"\"\n        if self.isempty:\n            return 0\n        return utils.PrettyInt(np.count_nonzero(self.n_events))\n\n    @property\n    def events(self):\n        events = []\n        for series in self.data:\n            events.append(series[:, 0].squeeze())\n        return utils.ragged_array(events)\n\n    @property\n    def values(self):\n        values = []\n        for series in self.data:\n            values.append(series[:, 1:].squeeze())\n        return utils.ragged_array(values)\n\n    def flatten(self, *, series_id=None):\n        \"\"\"Collapse events across series.\n\n        Parameters\n        ----------\n        series_id: (int)\n            (series) ID to assign to flattened event series, default is 0.\n        \"\"\"\n        if self.n_series &lt; 2:  # already flattened\n            return self\n\n        # default args:\n        if series_id is None:\n            series_id = 0\n\n        flattened = self._copy_without_data()\n\n        flattened._series_ids = [series_id]\n\n        raise NotImplementedError\n        alldatas = self.data[0]\n        for series in range(1, self.n_series):\n            alldatas = utils.linear_merge(alldatas, self.data[series])\n\n        flattened._data = np.array(list(alldatas), ndmin=2)\n        flattened.__renew__()\n        return flattened\n\n    @staticmethod\n    def _restrict_to_interval_array_fast(intervalarray, data, copyover=True):\n        \"\"\"Return data restricted to an IntervalArray.\n\n        This function assumes sorted event datas, so that binary search can\n        be used to quickly identify slices that should be kept in the\n        restriction. It does not check every event data.\n\n        Parameters\n        ----------\n        intervalarray : IntervalArray or EpochArray\n        data : list or array-like, each element of size (n_events, n_values).\n        \"\"\"\n        if intervalarray.isempty:\n            n_series = len(data)\n            data = np.zeros((n_series, 0))\n            return data\n\n        singleseries = len(data) == 1  # bool\n\n        # TODO: is this copy even necessary?\n        if copyover:\n            data = copy.copy(data)\n\n        # NOTE: this used to assume multiple series for the enumeration to work\n        for series, evt_data in enumerate(data):\n            evt_data = ValueEventArray._to_2d_array(evt_data)\n            if evt_data.size == 0 or evt_data.shape[1] &lt; 1:\n                if singleseries:\n                    data = np.array([[]])\n                else:\n                    data_ = data.tolist()\n                    data_[series] = np.array([])\n                    data = utils.ragged_array(data_)\n                continue\n            indices = []\n            for epdata in intervalarray.data:\n                t_start = epdata[0]\n                t_stop = epdata[1]\n                # Ensure we have a proper 1D array of event times\n                if evt_data.ndim &gt; 1:\n                    event_times = evt_data[:, 0].flatten()\n                else:\n                    event_times = evt_data.flatten()\n                # Ensure event_times is a proper 1D array and not an object array\n                if event_times.dtype == object:\n                    # Handle object array by extracting the actual values\n                    event_times = np.array(\n                        [\n                            float(t) if hasattr(t, \"__float__\") else t\n                            for t in event_times\n                        ]\n                    )\n                # Ensure event_times is a proper 1D array\n                if event_times.size == 0:\n                    indices.append((0, 0))\n                else:\n                    try:\n                        frm, to = np.searchsorted(event_times, (t_start, t_stop))\n                        indices.append((frm, to))\n                    except (ValueError, TypeError):\n                        # Fallback: handle case where searchsorted fails\n                        indices.append((0, 0))\n            indices = np.array(indices, ndmin=2)\n            if np.diff(indices).sum() &lt; len(evt_data):\n                logging.info(\"ignoring events outside of eventarray support\")\n            if singleseries:\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data = np.array([data_list])\n            else:\n                # here we have to do some annoying conversion between\n                # arrays and lists to fully support jagged array\n                # mutation\n                data_list = []\n                for start, stop in indices:\n                    data_list.extend(evt_data[start:stop])\n                data_ = data.tolist()\n                data_[series] = np.array(data_list)\n                data = utils.ragged_array(data_)\n        return data\n\n    def __repr__(self):\n        address_str = \" at \" + str(hex(id(self)))\n        logging.disable(logging.CRITICAL)\n        if self.isempty:\n            return \"&lt;empty \" + self.type_name + address_str + \"&gt;\"\n        if self._abscissa.support.n_intervals &gt; 1:\n            epstr = \" ({} segments)\".format(self._abscissa.support.n_intervals)\n        else:\n            epstr = \"\"\n        if self.fs is not None:\n            fsstr = \" at %s Hz\" % self.fs\n        else:\n            fsstr = \"\"\n        numstr = \" %s %s\" % (self.n_series, self._series_label)\n        logging.disable(0)\n        return \"&lt;%s%s:%s%s&gt;%s\" % (self.type_name, address_str, numstr, epstr, fsstr)\n\n    def bin(self, *, ds=None, method=\"mean\", **kwargs):\n        \"\"\"Return a binned value event array.\n\n        method in [sum, mean, median, min, max] or a custom function.\n        Additional keyword arguments are passed to BinnedValueEventArray.\n        \"\"\"\n        return BinnedValueEventArray(self, ds=ds, method=method, **kwargs)\n\n    @property\n    def n_events(self):\n        \"\"\"(np.array) The number of events in each series.\"\"\"\n        if self.isempty:\n            return 0\n        return np.array([len(series) for series in self.data])\n\n    @property\n    def n_values(self):\n        \"\"\"(int) The number of values associated with each event series.\"\"\"\n        if self.isempty:\n            return 0\n        n_values = []\n        for series in self.data:\n            n_values.append(series.squeeze().shape[1] - 1)\n        return n_values\n\n    @property\n    def issorted(self):\n        \"\"\"(bool) Sorted EventArray.\"\"\"\n        if self.isempty:\n            return True\n        return np.array(\n            [utils.is_sorted(eventarray[:, 0]) for eventarray in self.data]\n        ).all()\n\n    def _reorder_series_by_idx(self, neworder, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,)\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n        out.__renew__()\n\n        return out\n\n    def reorder_series_by_ids(self, neworder, *, inplace=False):\n        \"\"\"Reorder series according to a specified order.\n\n        neworder must be list-like, of size (n_series,) and in terms of\n        series_ids\n\n        Return\n        ------\n        out : reordered EventArray\n        \"\"\"\n        if inplace:\n            out = self\n        else:\n            out = self.copy()\n\n        neworder = [self.series_ids.index(x) for x in neworder]\n\n        oldorder = list(range(len(neworder)))\n        for oi, ni in enumerate(neworder):\n            frm = oldorder.index(ni)\n            to = oi\n            utils.swap_rows(out._data, frm, to)\n            out._series_ids[frm], out._series_ids[to] = (\n                out._series_ids[to],\n                out._series_ids[frm],\n            )\n            # TODO: re-build series tags (tag system not yet implemented)\n            oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n        out.__renew__()\n        return out\n\n    def make_stateful(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def _to_2d_array(arr):\n        \"\"\"Convert array to 2D numpy array, handling object arrays properly.\"\"\"\n        if isinstance(arr, np.ndarray) and arr.dtype == object:\n            # Handle object arrays by extracting the actual data\n            if arr.size == 1:\n                # Single element object array\n                return np.atleast_2d(arr[0])\n            else:\n                # Multiple element object array - concatenate\n                flattened = []\n                for item in arr:\n                    if isinstance(item, np.ndarray):\n                        flattened.append(item)\n                    else:\n                        flattened.append(np.array(item))\n                if flattened:\n                    return np.vstack(flattened)\n                else:\n                    return np.array([]).reshape(0, 0)\n        else:\n            return np.atleast_2d(arr)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.issorted","title":"<code>issorted</code>  <code>property</code>","text":"<p>(bool) Sorted EventArray.</p>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.n_active","title":"<code>n_active</code>  <code>property</code>","text":"<p>(int) The number of active series.</p> <p>A series is considered active if it fired at least one event.</p>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.n_events","title":"<code>n_events</code>  <code>property</code>","text":"<p>(np.array) The number of events in each series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.n_values","title":"<code>n_values</code>  <code>property</code>","text":"<p>(int) The number of values associated with each event series.</p>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.bin","title":"<code>bin(*, ds=None, method='mean', **kwargs)</code>","text":"<p>Return a binned value event array.</p> <p>method in [sum, mean, median, min, max] or a custom function. Additional keyword arguments are passed to BinnedValueEventArray.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def bin(self, *, ds=None, method=\"mean\", **kwargs):\n    \"\"\"Return a binned value event array.\n\n    method in [sum, mean, median, min, max] or a custom function.\n    Additional keyword arguments are passed to BinnedValueEventArray.\n    \"\"\"\n    return BinnedValueEventArray(self, ds=ds, method=method, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.flatten","title":"<code>flatten(*, series_id=None)</code>","text":"<p>Collapse events across series.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <p>(series) ID to assign to flattened event series, default is 0.</p> <code>None</code> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def flatten(self, *, series_id=None):\n    \"\"\"Collapse events across series.\n\n    Parameters\n    ----------\n    series_id: (int)\n        (series) ID to assign to flattened event series, default is 0.\n    \"\"\"\n    if self.n_series &lt; 2:  # already flattened\n        return self\n\n    # default args:\n    if series_id is None:\n        series_id = 0\n\n    flattened = self._copy_without_data()\n\n    flattened._series_ids = [series_id]\n\n    raise NotImplementedError\n    alldatas = self.data[0]\n    for series in range(1, self.n_series):\n        alldatas = utils.linear_merge(alldatas, self.data[series])\n\n    flattened._data = np.array(list(alldatas), ndmin=2)\n    flattened.__renew__()\n    return flattened\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.partition","title":"<code>partition(ds=None, n_intervals=None)</code>","text":"<p>Returns a BaseEventArray whose support has been partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>float</code> <p>Maximum duration (in seconds), for each interval.</p> <code>None</code> <code>n_points</code> <code>int</code> <p>Number of intervals. If ds is None and n_intervals is None, then default is to use n_intervals = 100</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BaseEventArray</code> <p>BaseEventArray that has been partitioned.</p> Notes <p>Irrespective of whether 'ds' or 'n_intervals' are used, the exact underlying support is propagated, and the first and last points of the supports are always included, even if this would cause n_points or ds to be violated.</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>@keyword_equivalence(this_or_that={\"n_intervals\": \"n_epochs\"})\ndef partition(self, ds=None, n_intervals=None):\n    \"\"\"Returns a BaseEventArray whose support has been partitioned.\n\n    Parameters\n    ----------\n    ds : float, optional\n        Maximum duration (in seconds), for each interval.\n    n_points : int, optional\n        Number of intervals. If ds is None and n_intervals is None, then\n        default is to use n_intervals = 100\n\n    Returns\n    -------\n    out : BaseEventArray\n        BaseEventArray that has been partitioned.\n\n    Notes\n    -----\n    Irrespective of whether 'ds' or 'n_intervals' are used, the exact\n    underlying support is propagated, and the first and last points\n    of the supports are always included, even if this would cause\n    n_points or ds to be violated.\n    \"\"\"\n\n    out = self.copy()\n    abscissa = copy.deepcopy(out._abscissa)\n    abscissa.support = abscissa.support.partition(ds=ds, n_intervals=n_intervals)\n    out._abscissa = abscissa\n    out.__renew__()\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/min/#nelpy.min.ValueEventArray.reorder_series_by_ids","title":"<code>reorder_series_by_ids(neworder, *, inplace=False)</code>","text":"<p>Reorder series according to a specified order.</p> <p>neworder must be list-like, of size (n_series,) and in terms of series_ids</p> Return <p>out : reordered EventArray</p> Source code in <code>nelpy/core/_valeventarray.py</code> <pre><code>def reorder_series_by_ids(self, neworder, *, inplace=False):\n    \"\"\"Reorder series according to a specified order.\n\n    neworder must be list-like, of size (n_series,) and in terms of\n    series_ids\n\n    Return\n    ------\n    out : reordered EventArray\n    \"\"\"\n    if inplace:\n        out = self\n    else:\n        out = self.copy()\n\n    neworder = [self.series_ids.index(x) for x in neworder]\n\n    oldorder = list(range(len(neworder)))\n    for oi, ni in enumerate(neworder):\n        frm = oldorder.index(ni)\n        to = oi\n        utils.swap_rows(out._data, frm, to)\n        out._series_ids[frm], out._series_ids[to] = (\n            out._series_ids[to],\n            out._series_ids[frm],\n        )\n        # TODO: re-build series tags (tag system not yet implemented)\n        oldorder[frm], oldorder[to] = oldorder[to], oldorder[frm]\n\n    out.__renew__()\n    return out\n</code></pre>"},{"location":"reference/nelpy/preprocessing/","title":"nelpy.preprocessing","text":"<p>Data preprocessing objects and functions.</p>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.DataWindow","title":"<code>DataWindow</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>DataWindow Data window description to describe stride and/or data aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>bins_before</code> <code>(int, optional(default=0))</code> <p>How many bins before the output to include in the window.</p> <code>0</code> <code>bins_after</code> <code>(int, optional(default=0))</code> <p>How many bins after the output to include in the window.</p> <code>0</code> <code>bins_current</code> <code>(int, optional(default=1))</code> <p>Whether (1) or not (0) to include the concurrent bin in the window.</p> <code>1</code> <code>bins_stride</code> <code>(int, optional(default=1))</code> <p>Number of bins to advance the window during each time step.</p> <code>1</code> <code>bin_width</code> <code>(float, optional(default=None))</code> <p>Width of single bin (default units are in seconds).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = DataWindow(1, 1, 1, 1)\nDataWindow(bins_before=1, bins_after=1, bins_current=1, bins_stride=1, bin_width=None)\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.DataWindow--implicit-bin-size-of-1-second-centered-window-of-duration-5-seconds-stride-of-2-seconds","title":"Implicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:","text":"<pre><code>&gt;&gt;&gt; w = DataWindow(2, 2, 1, 2)\nDataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2)\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.DataWindow--excplicit-bin-size-of-1-second-centered-window-of-duration-5-seconds-stride-of-2-seconds","title":"Excplicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:","text":"<pre><code>&gt;&gt;&gt; w = DataWindow(2, 2, 1, 2, 1)\nDataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2, bin_width=1)\n        Total bin width = 5 seconds\n</code></pre> Source code in <code>nelpy/preprocessing.py</code> <pre><code>class DataWindow(BaseEstimator):\n    \"\"\"\n    DataWindow\n    Data window description to describe stride and/or data aggregation.\n\n    Parameters\n    ----------\n    bins_before : int, optional (default=0)\n        How many bins before the output to include in the window.\n    bins_after : int, optional (default=0)\n        How many bins after the output to include in the window.\n    bins_current : int, optional (default=1)\n        Whether (1) or not (0) to include the concurrent bin in the window.\n    bins_stride : int, optional (default=1)\n        Number of bins to advance the window during each time step.\n    bin_width : float, optional (default=None)\n        Width of single bin (default units are in seconds).\n\n    Examples\n    --------\n    &gt;&gt;&gt; w = DataWindow(1, 1, 1, 1)\n    DataWindow(bins_before=1, bins_after=1, bins_current=1, bins_stride=1, bin_width=None)\n\n    # Implicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:\n    &gt;&gt;&gt; w = DataWindow(2, 2, 1, 2)\n    DataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2)\n\n    # Excplicit bin size of 1 second, centered window of duration 5 seconds, stride of 2 seconds:\n    &gt;&gt;&gt; w = DataWindow(2, 2, 1, 2, 1)\n    DataWindow(bins_before=2, bins_after=2, bins_current=1, bins_stride=2, bin_width=1)\n            Total bin width = 5 seconds\n    \"\"\"\n\n    def __init__(\n        self,\n        bins_before=0,\n        bins_after=0,\n        bins_current=1,\n        bins_stride=1,\n        bin_width=None,\n        flatten=False,\n        sum=False,\n    ):\n        self.bins_before = bins_before\n        self.bins_after = bins_after\n        self.bins_current = bins_current\n        self.bins_stride = bins_stride\n        self.bin_width = bin_width\n        self._flatten = flatten\n        self._sum = sum\n\n    def __str__(self):\n        if self.bin_width is not None:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={}, bin_width={})\".format(\n                self._bins_before,\n                self._bins_after,\n                self._bins_current,\n                self._bins_stride,\n                self._bin_width,\n            )\n        else:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={})\".format(\n                self._bins_before,\n                self._bins_after,\n                self._bins_current,\n                self._bins_stride,\n            )\n        return repr_string\n\n    def __repr__(self):\n        if self.bin_width is not None:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={}, bin_width={})\".format(\n                self.bins_before,\n                self.bins_after,\n                self.bins_current,\n                self.bins_stride,\n                self.bin_width,\n            )\n            repr_string += \"\\n\\tTotal bin width = {}\".format(\n                PrettyDuration(\n                    (self.bins_before + self.bins_after + self.bins_current)\n                    * self.bin_width\n                )\n            )\n        else:\n            repr_string = \"DataWindow(bins_before={}, bins_after={}, bins_current={}, bins_stride={})\".format(\n                self.bins_before, self.bins_after, self.bins_current, self.bins_stride\n            )\n        return repr_string\n\n    def fit(self, X, y=None, *, T=None, lengths=None, flatten=None):\n        \"\"\"Dummy fit function to support sklearn pipelines.\n        Parameters\n        ----------\n        X\n            Ignored\n        y\n            Ignored\n        flatten : bool, optional (default=False)\n            Whether or not to flatten the output data during transformation.\n        \"\"\"\n        if flatten is not None:\n            self._flatten = flatten\n\n        bins_before = self.bins_before\n        bins_after = self.bins_after\n        # bins_current = self.bins_current\n        stride = self.bins_stride\n\n        X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n        L = np.insert(np.cumsum(lengths), 0, 0)\n        idx = []\n        n_zamples_tot = 0\n        for kk, (ii, jj) in enumerate(self._iter_from_X_lengths(X=X, lengths=lengths)):\n            X_ = X[ii:jj]  # , T[ii:jj]\n            n_samples, n_features = X_.shape\n            n_zamples = int(np.ceil((n_samples - bins_before - bins_after) / stride))\n            n_zamples_tot += n_zamples\n            idx += list(\n                L[kk] + np.array(range(bins_before, n_samples - bins_after, stride))\n            )\n\n        self.n_samples = n_zamples_tot\n        self.idx = idx\n        self.T = T[idx]\n        return self\n\n    def transform(self, X, T=None, lengths=None, flatten=None, sum=None):\n        \"\"\"\n        Apply window specification to data in X.\n\n        NOTE: this function is epoch-aware.\n\n        WARNING: this function works in-core, and may use a lot of memory\n                 to represent the unwrapped (windowed) data. If you have\n                 a large dataset, using the streaming version may be better.\n\n        Parameters\n        ----------\n        X : numpy 2d array of shape (n_samples, n_features)\n                OR\n            array-like of shape (n_epochs, ), each element of which is\n            a numpy 2d array of shape (n_samples, n_features)\n                OR\n            nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n                The number of spikes in each time bin for each neuron/unit.\n        T : array-like of shape (n_samples,), optional (default=None)\n                Timestamps / sample numbers corresponding to data in X.\n        lengths : array-like, optional (default=None)\n                Only used / allowed when X is a 2d numpy array, in which case\n                sum(lengths) must equal n_samples.\n                Array of lengths (in number of bins) for each contiguous segment\n                in X.\n        flatten : int, optional (default=False)\n            Whether or not to flatten the output data.\n        sum : boolean, optional (default=False)\n            Whether or not to sum all the spikes in the window per time bin. If\n            sum==True, then the dimensions of Z will be (n_samples, n_features).\n\n        Returns\n        -------\n        Z : Windowed data of shape (n_samples, window_size, n_features).\n            Note that n_samples in the output may not be the same as n_samples\n            in the input, since window specifications can affect which and how\n            many samples to return.\n            When flatten is True, then Z has shape (n_samples, window_size*n_features).\n            When sum is True, then Z has shape (n_samples, n_features)\n        T : array-like of shape (n_samples,)\n            Timestamps associated with data contained in Z.\n        \"\"\"\n        if flatten is None:\n            flatten = self._flatten\n\n        if sum is None:\n            sum = self._sum\n\n        X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n        z = []\n        t = []\n        for ii, jj in self._iter_from_X_lengths(X=X, lengths=lengths):\n            x, tx = self._apply_contiguous(X[ii:jj], T[ii:jj], flatten=flatten, sum=sum)\n            if x is not None:\n                z.append(x)\n                t.extend(tx)\n\n        Z = np.vstack(z)\n        T = np.array(t)\n\n        return Z, T\n\n    def _apply_contiguous(self, X, T=None, flatten=None, sum=False):\n        \"\"\"\n        Apply window specification to data in X.\n\n        NOTE: this function works on a single epoch only (i.e. assumes data\n              is contiguous).\n\n        NOTE: instead of returning partial data (with NaNs filling the rest),\n              we only return those bins (windows) whose specifications are wholly\n              contained in the data, similar to how binning in nelpy only includes\n              those bins that fit wholly in the data support.\n\n        WARNING: this function works in-core, and may use a lot of memory\n                 to represent the unwrapped (windowed) data. If you have\n                 a large dataset, using the streaming version may be better.\n\n        Parameters\n        ----------\n        X : numpy 2d array of shape (n_samples, n_features)\n        T : array-like of shape (n_samples,), optional (default=None)\n                Timestamps / sample numbers corresponding to data in X.\n        flatten : int, optional (default=False)\n            Whether or not to flatten the output data.\n        sum : boolean, optional (default=False)\n            Whether or not to sum all the spikes in the window per time bin. If\n            sum==True, then the dimensions of Z will be (n_samples, n_features).\n\n        Returns\n        -------\n        Z : Windowed data of shape (n_samples, window_size, n_features).\n            Note that n_samples in the output may not be the same as n_samples\n            in the input, since window specifications can affect which and how\n            many samples to return.\n            When flatten is True, then Z has shape (n_samples, window_size*n_features).\n        T : array-like of shape (n_samples,)\n            Timestamps associated with data contained in Z.\n        \"\"\"\n        if flatten is None:\n            flatten = self._flatten\n\n        bins_before = self.bins_before\n        bins_after = self.bins_after\n        bins_current = self.bins_current\n        stride = self.bins_stride\n\n        n_samples, n_features = X.shape\n        n_zamples = int(np.ceil((n_samples - bins_before - bins_after) / stride))\n\n        if n_zamples &lt; 1:\n            Z = None\n            T = None\n            return Z, T\n\n        Z = np.empty([n_zamples, bins_before + bins_after + bins_current, n_features])\n        Z[:] = np.nan\n\n        frm_idx = 0\n        curr_idx = bins_before\n\n        for zz in range(n_zamples):\n            if bins_current == 1:\n                idx = np.arange(\n                    frm_idx, frm_idx + bins_before + bins_after + bins_current\n                )\n            else:\n                idx = list(range(frm_idx, frm_idx + bins_before))\n                idx.extend(\n                    list(\n                        range(\n                            frm_idx + bins_before + 1,\n                            frm_idx + bins_before + 1 + bins_after,\n                        )\n                    )\n                )\n\n            #     print('{}  @ {}'.format(idx, curr_idx))\n\n            Z[zz, :] = X[idx, :]\n            curr_idx += stride\n            frm_idx += stride\n\n        if sum:\n            Z = Z.sum(axis=1)\n        elif flatten:\n            Z = Z.reshape(Z.shape[0], (Z.shape[1] * Z.shape[2]))\n\n        if T is not None:\n            t_idx = list(range(bins_before, n_samples - bins_after, stride))\n            T = T[t_idx]\n\n        return Z, T\n\n    def stream(self, X, chunk_size=1, flatten=False):\n        \"\"\"Streaming window specification on data X.\n\n        Q. Should this return a generator? Should it BE a generator? I think we\n            should return an iterable?\n\n        Examples\n        --------\n        &gt;&gt;&gt; w = DataWindow()\n        &gt;&gt;&gt; ws = w.stream(X)\n        &gt;&gt;&gt; for x in ws:\n                print(x)\n\n        \"\"\"\n        X, T, lengths = self._tidy(X)\n        return StreamingDataWindow(self, X=X, flatten=flatten)\n\n    def _tidy(self, X, T=None, lengths=None):\n        \"\"\"Transform data into a tidy, standardized, minimalist form.\n\n        NOTE: No windowing is present in tidy data; windowing is APPLIED\n              to tidy data when using DataWindow.apply().\n\n        Parameters\n        ----------\n        X : numpy 2d array of shape (n_samples, n_features)\n                OR\n            array-like of shape (n_epochs, ), each element of which is\n            a numpy 2d array of shape (n_samples, n_features)\n                OR\n            nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n                The number of spikes in each time bin for each neuron/unit.\n        T : array-like of shape (n_samples,), optional (default=None)\n                Timestamps / sample numbers corresponding to data in X.\n        lengths : array-like, optional (default=None)\n                Only used / allowed when X is a 2d numpy array, in which case\n                sum(lengths) must equal n_samples.\n                Array of lengths (in number of bins) for each contiguous segment\n                in X.\n\n        Returns\n        -------\n        tidyX : numpy 2d array of shape (n_samples, n_features)\n            The number of spikes in each time bin for each neuron/unit.\n        tidyT : array-like of shape (n_samples,)\n            Timestamps / sample numbers corresponding to data in X.\n        lengths : array-like\n            Array of lengths (in number of bins) for each contiguous segment\n            in tidyX.\n\n        Examples\n        --------\n\n        X = np.zeros((20, 8))\n        X = [np.zeros((20,50)), np.zeros((30, 50)), np.zeros((80, 50))]\n        X = [np.zeros((20,50)), np.zeros((30, 50)), np.zeros((80, 30))]\n        w = DataWindow(bin_width=0.02)\n\n        X, T, lengths = w._tidy(X)\n        X, T, lengths = w._tidy(X, T=np.arange(50))\n        X, T, lengths = w._tidy(X, lengths=[20,5,10])\n        \"\"\"\n\n        # here we should transform BSTs, numpy arrays, check for dimensions, etc\n        if isinstance(X, core.BinnedEventArray):\n            if self._bin_width is not None:\n                if self._bin_width != X.ds:\n                    raise ValueError(\n                        \"The DataWindow has ``bin_width``={}, whereas ``X.ds``={}.\".format(\n                            self._bin_width, X.ds\n                        )\n                    )\n\n            if (T is not None) or (lengths is not None):\n                logging.warning(\n                    \"A {} was passed in, so 'T' and 'lengths' will be ignored...\".format(\n                        X.type_name\n                    )\n                )\n\n            T = X.bin_centers\n            lengths = X.lengths\n            X = X.data.T\n\n            return X, T, lengths\n\n        try:\n            x = X[0, 0]\n            if X.ndim != 2:\n                raise ValueError(\n                    \"X is expected to be array-like with shape (n_samples, n_features).\"\n                )\n            n_samples, n_features = X.shape\n            if lengths is not None:\n                tot_length = np.sum(lengths)\n                if tot_length != n_samples:\n                    raise ValueError(\n                        \"The sum of ``lengths`` should equal ``n_samples``. [sum(lengths)={}; n_samples={}]\".format(\n                            tot_length, n_samples\n                        )\n                    )\n        except (IndexError, TypeError):\n            try:\n                x = X[0]\n                if x.ndim != 2:\n                    raise ValueError(\n                        \"Each element of X is expected to be array-like with shape (n_samples, n_features).\"\n                    )\n                if lengths is not None:\n                    raise ValueError(\n                        \"``lengths`` should not be specified when the shape of X is (n_epochs,)\"\n                    )\n                n_samples, n_features = x.shape\n                lengths = []\n                for x in X:\n                    lengths.append(x.shape[0])\n                    if x.ndim != 2:\n                        raise ValueError(\n                            \"Each element of X is expected to be array-like with shape (n_samples, n_features).\"\n                        )\n                    if x.shape[1] != n_features:\n                        raise ValueError(\n                            \"Each element of X is expected to have the same number of features.\"\n                        )\n                X = np.vstack(X)\n            except (IndexError, TypeError):\n                raise TypeError(\n                    \"Windowing of type {} not supported!\".format(str(type(X)))\n                )\n        n_samples, n_features = X.shape\n        if T is not None:\n            assert len(T) == n_samples, (\n                \"T must have the same number of elements as n_samples.\"\n            )\n        else:\n            if self._bin_width is not None:\n                ds = self._bin_width\n            else:\n                ds = 1\n            T = np.arange(n_samples) * ds + ds / 2\n\n        return X, T, lengths\n\n    def _iter_from_X_lengths(self, X, lengths=None):\n        \"\"\"\n        Helper function to iterate over contiguous segments of data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n                Feature matrix of individual samples.\n                Typically the number of spikes in each time bin for each neuron.\n        lengths : array-like of integers, shape (n_epochs, ), optional\n                Lengths of the individual epochs in ``X``. The sum of\n                these should be ``n_samples``.\n                Array of lengths (in number of bins) for each contiguous segment\n                in X.\n\n        Returns\n        -------\n        start, end : indices of a contiguous segment in data, so that\n                     segment = data[start:end]\n        \"\"\"\n\n        if X.ndim != 2:\n            raise ValueError(\n                \"X is expected to be array-like with shape (n_samples, n_features).\"\n            )\n\n        n_samples = X.shape[0]\n\n        if lengths is None:\n            try:\n                yield 0, n_samples\n            except StopIteration:\n                return\n        else:\n            end = np.cumsum(lengths).astype(np.int)\n\n            if end[-1] != n_samples:\n                raise ValueError(\n                    \"The sum of ``lengths`` should equal ``n_samples``. [sum(lengths)={}; n_samples={}]\".format(\n                        end[-1], n_samples\n                    )\n                )\n\n            start = end - lengths\n\n            for i in range(len(lengths)):\n                try:\n                    yield start[i], end[i]\n                except StopIteration:\n                    return\n\n    @property\n    def bins_before(self):\n        return self._bins_before\n\n    @bins_before.setter\n    def bins_before(self, val):\n        assert float(val).is_integer(), (\n            \"``bins_before`` must be a non-negative integer!\"\n        )\n        assert val &gt;= 0, \"``bins_before`` must be a non-negative integer!\"\n        self._bins_before = int(val)\n\n    @property\n    def bins_after(self):\n        return self._bins_after\n\n    @bins_after.setter\n    def bins_after(self, val):\n        assert float(val).is_integer(), \"``bins_after`` must be a non-negative integer!\"\n        assert val &gt;= 0, \"``bins_after`` must be a non-negative integer!\"\n        self._bins_after = int(val)\n\n    @property\n    def bins_current(self):\n        return self._bins_current\n\n    @bins_current.setter\n    def bins_current(self, val):\n        assert float(val).is_integer(), \"``bins_current`` must be a either 1 or 0!\"\n        assert val in [0, 1], \"``bins_current`` must be a either 1 or 0!\"\n        self._bins_current = int(val)\n\n    @property\n    def bins_stride(self):\n        return self._bins_stride\n\n    @bins_stride.setter\n    def bins_stride(self, val):\n        assert float(val).is_integer(), (\n            \"``bins_stride`` must be a non-negative integer!\"\n        )\n        assert val &gt;= 0, \"``bins_stride`` must be a non-negative integer!\"\n        self._bins_stride = int(val)\n\n    @property\n    def bin_width(self):\n        return self._bin_width\n\n    @bin_width.setter\n    def bin_width(self, val):\n        if val is not None:\n            assert float(val) &gt; 0, (\n                \"``bin_width`` must be a non-negative number (float)!\"\n            )\n        self._bin_width = val\n\n    @property\n    def flatten(self):\n        return self._flatten\n\n    @flatten.setter\n    def flatten(self, val):\n        try:\n            if val:\n                val = True\n        except Exception:\n            val = False\n        self._flatten = val\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.DataWindow.fit","title":"<code>fit(X, y=None, *, T=None, lengths=None, flatten=None)</code>","text":"<p>Dummy fit function to support sklearn pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Ignored</p> required <code>y</code> <p>Ignored</p> <code>None</code> <code>flatten</code> <code>(bool, optional(default=False))</code> <p>Whether or not to flatten the output data during transformation.</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def fit(self, X, y=None, *, T=None, lengths=None, flatten=None):\n    \"\"\"Dummy fit function to support sklearn pipelines.\n    Parameters\n    ----------\n    X\n        Ignored\n    y\n        Ignored\n    flatten : bool, optional (default=False)\n        Whether or not to flatten the output data during transformation.\n    \"\"\"\n    if flatten is not None:\n        self._flatten = flatten\n\n    bins_before = self.bins_before\n    bins_after = self.bins_after\n    # bins_current = self.bins_current\n    stride = self.bins_stride\n\n    X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n    L = np.insert(np.cumsum(lengths), 0, 0)\n    idx = []\n    n_zamples_tot = 0\n    for kk, (ii, jj) in enumerate(self._iter_from_X_lengths(X=X, lengths=lengths)):\n        X_ = X[ii:jj]  # , T[ii:jj]\n        n_samples, n_features = X_.shape\n        n_zamples = int(np.ceil((n_samples - bins_before - bins_after) / stride))\n        n_zamples_tot += n_zamples\n        idx += list(\n            L[kk] + np.array(range(bins_before, n_samples - bins_after, stride))\n        )\n\n    self.n_samples = n_zamples_tot\n    self.idx = idx\n    self.T = T[idx]\n    return self\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.DataWindow.stream","title":"<code>stream(X, chunk_size=1, flatten=False)</code>","text":"<p>Streaming window specification on data X.</p> <p>Q. Should this return a generator? Should it BE a generator? I think we     should return an iterable?</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; w = DataWindow()\n&gt;&gt;&gt; ws = w.stream(X)\n&gt;&gt;&gt; for x in ws:\n        print(x)\n</code></pre> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def stream(self, X, chunk_size=1, flatten=False):\n    \"\"\"Streaming window specification on data X.\n\n    Q. Should this return a generator? Should it BE a generator? I think we\n        should return an iterable?\n\n    Examples\n    --------\n    &gt;&gt;&gt; w = DataWindow()\n    &gt;&gt;&gt; ws = w.stream(X)\n    &gt;&gt;&gt; for x in ws:\n            print(x)\n\n    \"\"\"\n    X, T, lengths = self._tidy(X)\n    return StreamingDataWindow(self, X=X, flatten=flatten)\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.DataWindow.transform","title":"<code>transform(X, T=None, lengths=None, flatten=None, sum=None)</code>","text":"<p>Apply window specification to data in X.</p> <p>NOTE: this function is epoch-aware.</p> <p>WARNING: this function works in-core, and may use a lot of memory          to represent the unwrapped (windowed) data. If you have          a large dataset, using the streaming version may be better.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy 2d array of shape (n_samples, n_features)</code> <pre><code>OR\n</code></pre> <p>array-like of shape (n_epochs, ), each element of which is a numpy 2d array of shape (n_samples, n_features)     OR nelpy.core.BinnedEventArray / BinnedSpikeTrainArray     The number of spikes in each time bin for each neuron/unit.</p> required <code>T</code> <code>array-like of shape (n_samples,), optional (default=None)</code> <pre><code>Timestamps / sample numbers corresponding to data in X.\n</code></pre> <code>None</code> <code>lengths</code> <code>(array - like, optional(default=None))</code> <pre><code>Only used / allowed when X is a 2d numpy array, in which case\nsum(lengths) must equal n_samples.\nArray of lengths (in number of bins) for each contiguous segment\nin X.\n</code></pre> <code>None</code> <code>flatten</code> <code>(int, optional(default=False))</code> <p>Whether or not to flatten the output data.</p> <code>None</code> <code>sum</code> <code>(boolean, optional(default=False))</code> <p>Whether or not to sum all the spikes in the window per time bin. If sum==True, then the dimensions of Z will be (n_samples, n_features).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Z</code> <code>Windowed data of shape (n_samples, window_size, n_features).</code> <p>Note that n_samples in the output may not be the same as n_samples in the input, since window specifications can affect which and how many samples to return. When flatten is True, then Z has shape (n_samples, window_size*n_features). When sum is True, then Z has shape (n_samples, n_features)</p> <code>T</code> <code>array-like of shape (n_samples,)</code> <p>Timestamps associated with data contained in Z.</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def transform(self, X, T=None, lengths=None, flatten=None, sum=None):\n    \"\"\"\n    Apply window specification to data in X.\n\n    NOTE: this function is epoch-aware.\n\n    WARNING: this function works in-core, and may use a lot of memory\n             to represent the unwrapped (windowed) data. If you have\n             a large dataset, using the streaming version may be better.\n\n    Parameters\n    ----------\n    X : numpy 2d array of shape (n_samples, n_features)\n            OR\n        array-like of shape (n_epochs, ), each element of which is\n        a numpy 2d array of shape (n_samples, n_features)\n            OR\n        nelpy.core.BinnedEventArray / BinnedSpikeTrainArray\n            The number of spikes in each time bin for each neuron/unit.\n    T : array-like of shape (n_samples,), optional (default=None)\n            Timestamps / sample numbers corresponding to data in X.\n    lengths : array-like, optional (default=None)\n            Only used / allowed when X is a 2d numpy array, in which case\n            sum(lengths) must equal n_samples.\n            Array of lengths (in number of bins) for each contiguous segment\n            in X.\n    flatten : int, optional (default=False)\n        Whether or not to flatten the output data.\n    sum : boolean, optional (default=False)\n        Whether or not to sum all the spikes in the window per time bin. If\n        sum==True, then the dimensions of Z will be (n_samples, n_features).\n\n    Returns\n    -------\n    Z : Windowed data of shape (n_samples, window_size, n_features).\n        Note that n_samples in the output may not be the same as n_samples\n        in the input, since window specifications can affect which and how\n        many samples to return.\n        When flatten is True, then Z has shape (n_samples, window_size*n_features).\n        When sum is True, then Z has shape (n_samples, n_features)\n    T : array-like of shape (n_samples,)\n        Timestamps associated with data contained in Z.\n    \"\"\"\n    if flatten is None:\n        flatten = self._flatten\n\n    if sum is None:\n        sum = self._sum\n\n    X, T, lengths = self._tidy(X=X, T=T, lengths=lengths)\n    z = []\n    t = []\n    for ii, jj in self._iter_from_X_lengths(X=X, lengths=lengths):\n        x, tx = self._apply_contiguous(X[ii:jj], T[ii:jj], flatten=flatten, sum=sum)\n        if x is not None:\n            z.append(x)\n            t.extend(tx)\n\n    Z = np.vstack(z)\n    T = np.array(t)\n\n    return Z, T\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.StandardScaler","title":"<code>StandardScaler</code>","text":"<p>               Bases: <code>StandardScaler</code></p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>class StandardScaler(SklearnStandardScaler):\n    def __init__(self, copy=True, with_mean=True, with_std=True):\n        super().__init__(copy=copy, with_mean=with_mean, with_std=with_std)\n\n    def fit(self, X, y=None):\n        \"\"\"Compute the mean and std to be used for later scaling.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n        y\n            Ignored\n        \"\"\"\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            X = X.data.T\n\n        return super().fit(X, y)\n\n    def partial_fit(self, X, y=None, sample_weight=None):\n        \"\"\"Online computation of mean and std on X for later scaling.\n        All of X is processed as a single batch. This is intended for cases\n        when `fit` is not feasible due to very large number of `n_samples`\n        or because X is read from a continuous stream.\n        The algorithm for incremental mean and std is given in Equation 1.5a,b\n        in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n        for computing the sample variance: Analysis and recommendations.\"\n        The American Statistician 37.3 (1983): 242-247:\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n        y\n            Ignored\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n        \"\"\"\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            X = X.data.T\n\n        return super().partial_fit(X, y, sample_weight)\n\n    def transform(self, X, copy=None):\n        \"\"\"Perform standardization by centering and scaling\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        \"\"\"\n\n        if copy is None:\n            copy = self.copy\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            if copy:\n                Xdata = copycopy(X.data.T)\n                X = X.copy()\n            else:\n                Xdata = X.data.T\n            Xdata = super().transform(Xdata, copy).T\n\n            X._data = Xdata\n        else:\n            X = super().transform(X, copy)\n        return X\n\n    def inverse_transform(self, X, copy=None):\n        \"\"\"Scale back the data to the original representation\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data used to scale along the features axis.\n        copy : bool, optional (default: None)\n            Copy the input X or not.\n        Returns\n        -------\n        X_tr : array-like, shape [n_samples, n_features]\n            Transformed array.\n        \"\"\"\n\n        if copy is None:\n            copy = self.copy\n\n        if isinstance(\n            X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n        ):\n            if copy:\n                Xdata = copycopy(X.data.T)\n                X = X.copy()\n            else:\n                Xdata = X.data.T\n            Xdata = super().inverse_transform(Xdata, copy).T\n\n            X._data = Xdata\n        else:\n            X = super().inverse_transform(X, copy)\n\n        return X\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.StandardScaler.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the mean and std to be used for later scaling.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, sparse matrix</code> <p>The data used to compute the mean and standard deviation used for later scaling along the features axis.</p> <code>array-like</code> <code>y</code> <p>Ignored</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Compute the mean and std to be used for later scaling.\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        The data used to compute the mean and standard deviation\n        used for later scaling along the features axis.\n    y\n        Ignored\n    \"\"\"\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        X = X.data.T\n\n    return super().fit(X, y)\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.StandardScaler.inverse_transform","title":"<code>inverse_transform(X, copy=None)</code>","text":"<p>Scale back the data to the original representation</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape[n_samples, n_features])</code> <p>The data used to scale along the features axis.</p> required <code>copy</code> <code>bool, optional (default: None)</code> <p>Copy the input X or not.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_tr</code> <code>(array - like, shape[n_samples, n_features])</code> <p>Transformed array.</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def inverse_transform(self, X, copy=None):\n    \"\"\"Scale back the data to the original representation\n    Parameters\n    ----------\n    X : array-like, shape [n_samples, n_features]\n        The data used to scale along the features axis.\n    copy : bool, optional (default: None)\n        Copy the input X or not.\n    Returns\n    -------\n    X_tr : array-like, shape [n_samples, n_features]\n        Transformed array.\n    \"\"\"\n\n    if copy is None:\n        copy = self.copy\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        if copy:\n            Xdata = copycopy(X.data.T)\n            X = X.copy()\n        else:\n            Xdata = X.data.T\n        Xdata = super().inverse_transform(Xdata, copy).T\n\n        X._data = Xdata\n    else:\n        X = super().inverse_transform(X, copy)\n\n    return X\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.StandardScaler.partial_fit","title":"<code>partial_fit(X, y=None, sample_weight=None)</code>","text":"<p>Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when <code>fit</code> is not feasible due to very large number of <code>n_samples</code> or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247:</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, sparse matrix</code> <p>The data used to compute the mean and standard deviation used for later scaling along the features axis.</p> <code>array-like</code> <code>y</code> <p>Ignored</p> <code>None</code> <code>sample_weight</code> <code>array-like of shape (n_samples,)</code> <p>Individual weights for each sample.</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def partial_fit(self, X, y=None, sample_weight=None):\n    \"\"\"Online computation of mean and std on X for later scaling.\n    All of X is processed as a single batch. This is intended for cases\n    when `fit` is not feasible due to very large number of `n_samples`\n    or because X is read from a continuous stream.\n    The algorithm for incremental mean and std is given in Equation 1.5a,b\n    in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n    for computing the sample variance: Analysis and recommendations.\"\n    The American Statistician 37.3 (1983): 242-247:\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        The data used to compute the mean and standard deviation\n        used for later scaling along the features axis.\n    y\n        Ignored\n    sample_weight : array-like of shape (n_samples,), default=None\n        Individual weights for each sample.\n    \"\"\"\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        X = X.data.T\n\n    return super().partial_fit(X, y, sample_weight)\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.StandardScaler.transform","title":"<code>transform(X, copy=None)</code>","text":"<p>Perform standardization by centering and scaling</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape[n_samples, n_features])</code> <p>The data used to scale along the features axis.</p> required <code>copy</code> <code>bool, optional (default: None)</code> <p>Copy the input X or not.</p> <code>None</code> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def transform(self, X, copy=None):\n    \"\"\"Perform standardization by centering and scaling\n    Parameters\n    ----------\n    X : array-like, shape [n_samples, n_features]\n        The data used to scale along the features axis.\n    copy : bool, optional (default: None)\n        Copy the input X or not.\n    \"\"\"\n\n    if copy is None:\n        copy = self.copy\n\n    if isinstance(\n        X, (core.RegularlySampledAnalogSignalArray, core.BinnedEventArray)\n    ):\n        if copy:\n            Xdata = copycopy(X.data.T)\n            X = X.copy()\n        else:\n            Xdata = X.data.T\n        Xdata = super().transform(Xdata, copy).T\n\n        X._data = Xdata\n    else:\n        X = super().transform(X, copy)\n    return X\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.StreamingDataWindow","title":"<code>StreamingDataWindow</code>","text":"<p>StreamingDataWindow</p> <p>StreamingDataWindow is an iterable with an associated data object.</p> <p>See https://hackmag.com/coding/lets-tame-data-streams-with-python/</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>class StreamingDataWindow:\n    \"\"\"\n    StreamingDataWindow\n\n    StreamingDataWindow is an iterable with an associated data object.\n\n    See https://hackmag.com/coding/lets-tame-data-streams-with-python/\n    \"\"\"\n\n    def __init__(self, w, X, flatten=False):\n        self._w = w\n        self.X = X\n        self._flatten = False\n\n    def flatten(self, inplace=False):\n        # what's the opposite of flatten?\n        pass\n\n    def __repr__(self):\n        return \"StreamingDataWindow(\\n\\tw={},\\n\\tX={},\\n\\tflatten={})\".format(\n            str(self.w), str(self.X), str(self._flatten)\n        )  # + str(self.w)\n\n    def __iter__(self):\n        # initialize the internal index to zero when used as iterator\n        self._index = 0\n        return self\n\n    def __next__(self):\n        # index = self._index\n        # if index &gt; self.n_intervals - 1:\n        #     raise StopIteration\n\n        self._index += 1\n\n    @property\n    def w(self):\n        return self._w\n\n    @w.setter\n    def w(self, val):\n        if not isinstance(val, DataWindow):\n            raise TypeError(\"w must be a nelpy.preprocessing.DataWindow type!\")\n        else:\n            self._w = val\n</code></pre>"},{"location":"reference/nelpy/preprocessing/#nelpy.preprocessing.standardize_asa","title":"<code>standardize_asa(func=None, *, asa, lengths=None, timestamps=None, fs=None, n_signals=None)</code>","text":"<p>Standardize nelpy RegularlySampledAnalogSignalArray to numpy representation.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>string</code> <p>Argument name corresponding to 'asa' in decorated function.</p> required <code>lengths</code> <code>string</code> <p>Argument name corresponding to 'lengths' in decorated function.</p> <code>None</code> <code>timestamps</code> <code>string</code> <p>Argument name corresponding to 'timestamps' in decorated function.</p> <code>None</code> <code>fs</code> <code>string</code> <p>Argument name corresponding to 'fs' in decorated function.</p> <code>None</code> <code>n_signals</code> <code>int</code> <p>Number of signals required in asa.</p> <code>None</code> Notes <ul> <li>asa is replaced with a (n_samples, n_signals) numpy array</li> <li>lenghts is replaced with a (n_intervals, ) numpy array, each containing    the number of samples in the associated interval.</li> <li>timestmaps is replaced with an (n_samples, ) numpy array, containing the    timestamps or abscissa_vals of the RegularlySampledAnalogSignalArray.</li> <li>fs is replaced with the float corresponding to the sampling frequency.</li> </ul> <p>Examples:</p> <p>@standardize_asa(asa='X', lengths='lengths', n_signals=2) def myfunc(*args, X=None, lengths=None):     pass</p> Source code in <code>nelpy/preprocessing.py</code> <pre><code>def standardize_asa(\n    func=None, *, asa, lengths=None, timestamps=None, fs=None, n_signals=None\n):\n    \"\"\"\n    Standardize nelpy RegularlySampledAnalogSignalArray to numpy representation.\n\n    Parameters\n    ----------\n    asa : string\n        Argument name corresponding to 'asa' in decorated function.\n    lengths : string, optional\n        Argument name corresponding to 'lengths' in decorated function.\n    timestamps : string, optional\n        Argument name corresponding to 'timestamps' in decorated function.\n    fs : string, optional\n        Argument name corresponding to 'fs' in decorated function.\n    n_signals : int, optional\n        Number of signals required in asa.\n\n    Notes\n    -----\n     - asa is replaced with a (n_samples, n_signals) numpy array\n     - lenghts is replaced with a (n_intervals, ) numpy array, each containing\n       the number of samples in the associated interval.\n     - timestmaps is replaced with an (n_samples, ) numpy array, containing the\n       timestamps or abscissa_vals of the RegularlySampledAnalogSignalArray.\n     - fs is replaced with the float corresponding to the sampling frequency.\n\n    Examples\n    --------\n    @standardize_asa(asa='X', lengths='lengths', n_signals=2)\n    def myfunc(*args, X=None, lengths=None):\n        pass\n\n    \"\"\"\n    if n_signals is not None:\n        try:\n            assert float(n_signals).is_integer(), (\n                \"'n_signals' must be a positive integer!\"\n            )\n            n_signals = int(n_signals)\n        except ValueError:\n            raise ValueError(\"'n_signals' must be a positive integer!\")\n        assert n_signals &gt; 0, \"'n_signals' must be a positive integer!\"\n\n    assert isinstance(asa, str), \"'asa' decorator argument must be a string!\"\n    if lengths is not None:\n        assert isinstance(lengths, str), (\n            \"'lengths' decorator argument must be a string!\"\n        )\n    if timestamps is not None:\n        assert isinstance(timestamps, str), (\n            \"'timestamps' decorator argument must be a string!\"\n        )\n    if fs is not None:\n        assert isinstance(fs, str), \"'fs' decorator argument must be a string!\"\n\n    def _decorate(function):\n        @wraps(function)\n        def wrapped_function(*args, **kwargs):\n            kw = True\n            # TODO: check that all decorator kwargs are strings\n            asa_ = kwargs.pop(asa, None)\n            lengths_ = kwargs.pop(lengths, None)\n            fs_ = kwargs.pop(fs, None)\n            timestamps_ = kwargs.pop(timestamps, None)\n\n            if asa_ is None:\n                try:\n                    asa_ = args[0]\n                    kw = False\n                except IndexError:\n                    raise TypeError(\n                        \"{}() missing 1 required positional argument: '{}'\".format(\n                            function.__name__, asa\n                        )\n                    )\n\n            # standardize asa_ here...\n            if isinstance(asa_, core.RegularlySampledAnalogSignalArray):\n                if n_signals is not None:\n                    if not asa_.n_signals == n_signals:\n                        raise ValueError(\n                            \"Input object '{}'.n_signals=={}, but {} was expected!\".format(\n                                asa, asa_.n_signals, n_signals\n                            )\n                        )\n                if lengths_ is not None:\n                    logging.warning(\n                        \"'{}' was passed in, but will be overwritten\"\n                        \" by '{}'s 'lengths' attribute\".format(lengths, asa)\n                    )\n                if timestamps_ is not None:\n                    logging.warning(\n                        \"'{}' was passed in, but will be overwritten\"\n                        \" by '{}'s 'abscissa_vals' attribute\".format(timestamps, asa)\n                    )\n                if fs_ is not None:\n                    logging.warning(\n                        \"'{}' was passed in, but will be overwritten\"\n                        \" by '{}'s 'fs' attribute\".format(fs, asa)\n                    )\n\n                fs_ = asa_.fs\n                lengths_ = asa_.lengths\n                timestamps_ = asa_.abscissa_vals\n                asa_ = asa_.data.squeeze().copy()\n\n            elif not isinstance(asa_, np.ndarray):\n                raise TypeError(\n                    \"'{}' was not a nelpy.RegularlySampledAnalogSignalArray\"\n                    \" so expected a numpy ndarray but got {}\".format(asa, type(asa_))\n                )\n\n            if kw:\n                kwargs[asa] = asa_\n            else:\n                args = tuple([arg if ii &gt; 0 else asa_ for (ii, arg) in enumerate(args)])\n\n            if lengths is not None:\n                if lengths_ is None:\n                    lengths_ = np.array([len(asa_)])\n                kwargs[lengths] = lengths_\n            if timestamps is not None:\n                if timestamps_ is None:\n                    raise TypeError(\n                        \"{}() missing 1 required keyword argument: '{}'\".format(\n                            function.__name__, timestamps\n                        )\n                    )\n                kwargs[timestamps] = timestamps_\n            if fs is not None:\n                if fs_ is None:\n                    raise TypeError(\n                        \"{}() missing 1 required keyword argument: '{}'\".format(\n                            function.__name__, fs\n                        )\n                    )\n                kwargs[fs] = fs_\n\n            return function(*args, **kwargs)\n\n        return wrapped_function\n\n    if func:\n        return _decorate(func)\n\n    return _decorate\n</code></pre>"},{"location":"reference/nelpy/scoring/","title":"nelpy.scoring","text":"<p>Temporary scoring functions. Needs a lot of work. DEPRECATED</p>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.scoreOrderD","title":"<code>scoreOrderD(hmm, state_sequences)</code>","text":"<p>Compute order score of state sequences</p> <p>A score of 0 means there's only one state.</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def scoreOrderD(hmm, state_sequences):\n    \"\"\"Compute order score of state sequences\n\n    A score of 0 means there's only one state.\n    \"\"\"\n\n    scoresND = []  # scores with no adjacent duplicates\n    for seqid in range(len(state_sequences)):\n        logP = np.log(hmm.transmat_)\n        pth = state_sequences[seqid]\n        plen = len(pth)\n        logPseq = 0\n        for ii in range(plen - 1):\n            logPseq += logP[pth[ii], pth[ii + 1]]\n        score = logPseq - np.log(plen)\n        scoresND.append(score)\n\n    return np.array(scoresND)\n</code></pre>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.scoreOrderD_time_swap","title":"<code>scoreOrderD_time_swap(hmm, state_sequences, n_shuffles=250)</code>","text":"<p>Compute order score of state sequences</p> <p>A score of 0 means there's only one state.</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def scoreOrderD_time_swap(hmm, state_sequences, n_shuffles=250):\n    \"\"\"Compute order score of state sequences\n\n    A score of 0 means there's only one state.\n    \"\"\"\n\n    scoresD = []  # scores with no adjacent duplicates\n    n_sequences = len(state_sequences)\n    shuffled = np.zeros((n_shuffles, n_sequences))\n\n    for seqid in range(n_sequences):\n        logP = np.log(hmm.transmat_)\n        pth = state_sequences[seqid]\n        plen = len(pth)\n        logPseq = 0\n        for ii in range(plen - 1):\n            logPseq += logP[pth[ii], pth[ii + 1]]\n        score = logPseq - np.log(plen)\n        scoresD.append(score)\n        for nn in range(n_shuffles):\n            logPseq = 0\n            pth = np.random.permutation(pth)\n            for ii in range(plen - 1):\n                logPseq += logP[pth[ii], pth[ii + 1]]\n            score = logPseq - np.log(plen)\n            shuffled[nn, seqid] = score\n\n    scoresD = np.array(scoresD)\n    return scoresD, shuffled\n</code></pre>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.scoreOrderNA","title":"<code>scoreOrderNA(hmm, state_sequences)</code>","text":"<p>Compute order score of state sequences, not averaging</p> <p>A score of 0 means there's only one state.</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def scoreOrderNA(hmm, state_sequences):\n    \"\"\"Compute order score of state sequences, not averaging\n\n    A score of 0 means there's only one state.\n    \"\"\"\n\n    scoresND = []  # scores with no adjacent duplicates\n\n    for seqid in range(len(state_sequences)):\n        logP = np.log(hmm.transmat_)\n        pth = state_sequences[seqid]\n        plen = len(pth)\n        logPseq = 0\n        for ii in range(plen - 1):\n            logPseq += logP[pth[ii], pth[ii + 1]]\n        score = logPseq\n        scoresND.append(score)\n\n    return np.array(scoresND)\n</code></pre>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.scoreOrderNAND","title":"<code>scoreOrderNAND(hmm, state_sequences)</code>","text":"<p>Compute order score of state sequences, not averaging, but with adj dupes removed</p> <p>A score of 0 means there's only one state.</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def scoreOrderNAND(hmm, state_sequences):\n    \"\"\"Compute order score of state sequences, not averaging, but with adj dupes removed\n\n    A score of 0 means there's only one state.\n    \"\"\"\n\n    scoresND = []  # scores with no adjacent duplicates\n\n    for seqid in range(len(state_sequences)):\n        logP = np.log(hmm.transmat_)\n        pth = [\n            x[0] for x in groupby(state_sequences[seqid])\n        ]  # remove adjacent duplicates\n        plen = len(pth)\n        logPseq = 0\n        for ii in range(plen - 1):\n            logPseq += logP[pth[ii], pth[ii + 1]]\n        score = logPseq\n        scoresND.append(score)\n\n    return np.array(scoresND)\n</code></pre>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.scoreOrderND","title":"<code>scoreOrderND(hmm, state_sequences)</code>","text":"<p>Compute order score with no adjacent duplicates in state sequences</p> <p>A score of 0 means there's only one state.</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def scoreOrderND(hmm, state_sequences):\n    \"\"\"Compute order score with no adjacent duplicates in state sequences\n\n    A score of 0 means there's only one state.\n    \"\"\"\n\n    scoresND = []  # scores with no adjacent duplicates\n\n    for seqid in range(len(state_sequences)):\n        logP = np.log(hmm.transmat_)\n        pth = [\n            x[0] for x in groupby(state_sequences[seqid])\n        ]  # remove adjacent duplicates\n        plen = len(pth)\n        logPseq = 0\n        for ii in range(plen - 1):\n            logPseq += logP[pth[ii], pth[ii + 1]]\n        score = logPseq - np.log(plen)\n        scoresND.append(score)\n\n    return np.array(scoresND)\n</code></pre>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.score_SD","title":"<code>score_SD(hmm, state_sequences)</code>","text":"<p>returns State Diversity --- number of unique decoded states</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def score_SD(hmm, state_sequences):\n    \"\"\"returns State Diversity --- number of unique decoded states\"\"\"\n    plens = []  # scores with no adjacent duplicates\n\n    for seqid in range(len(state_sequences)):\n        pth = state_sequences[seqid]\n        sd = len(set(pth))\n        plens.append(sd)\n\n    return np.array(plens)\n</code></pre>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.score_plen","title":"<code>score_plen(hmm, state_sequences)</code>","text":"<p>returns path length</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def score_plen(hmm, state_sequences):\n    \"\"\"returns path length\"\"\"\n    plens = []  # scores with no adjacent duplicates\n\n    for seqid in range(len(state_sequences)):\n        pth = state_sequences[seqid]\n        plen = len(pth)\n        plens.append(plen)\n\n    return np.array(plens)\n</code></pre>"},{"location":"reference/nelpy/scoring/#nelpy.scoring.score_plenND","title":"<code>score_plenND(hmm, state_sequences)</code>","text":"<p>returns path length, no adjacent duplicates</p> Source code in <code>nelpy/scoring.py</code> <pre><code>def score_plenND(hmm, state_sequences):\n    \"\"\"returns path length, no adjacent duplicates\"\"\"\n    plens = []  # scores with no adjacent duplicates\n\n    for seqid in range(len(state_sequences)):\n        pth = [\n            x[0] for x in groupby(state_sequences[seqid])\n        ]  # remove adjacent duplicates\n        plen = len(pth)\n        plens.append(plen)\n\n    return np.array(plens)\n</code></pre>"},{"location":"reference/nelpy/utils/","title":"nelpy.utils","text":"<p>This module contains helper functions and utilities for nelpy.</p>"},{"location":"reference/nelpy/utils/#nelpy.utils.PrettyBytes","title":"<code>PrettyBytes</code>","text":"<p>               Bases: <code>int</code></p> <p>Prints number of bytes in a more readable format</p> Source code in <code>nelpy/utils.py</code> <pre><code>class PrettyBytes(int):\n    \"\"\"Prints number of bytes in a more readable format\"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    def __str__(self):\n        if self.val &lt; 1024:\n            return \"{} bytes\".format(self.val)\n        elif self.val &lt; 1024**2:\n            return \"{:.3f} kilobytes\".format(self.val / 1024)\n        elif self.val &lt; 1024**3:\n            return \"{:.3f} megabytes\".format(self.val / 1024**2)\n        elif self.val &lt; 1024**4:\n            return \"{:.3f} gigabytes\".format(self.val / 1024**3)\n\n    def __repr__(self):\n        return self.__str__()\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.PrettyDuration","title":"<code>PrettyDuration</code>","text":"<p>               Bases: <code>float</code></p> <p>Time duration with pretty print.</p> <p>Behaves like a float, and can always be cast to a float.</p> Source code in <code>nelpy/utils.py</code> <pre><code>class PrettyDuration(float):\n    \"\"\"Time duration with pretty print.\n\n    Behaves like a float, and can always be cast to a float.\n    \"\"\"\n\n    def __init__(self, seconds):\n        self.duration = seconds\n\n    def __str__(self):\n        return self.time_string(self.duration)\n\n    def __repr__(self):\n        return self.time_string(self.duration)\n\n    @staticmethod\n    def to_dhms(seconds):\n        \"\"\"\n        Convert seconds into days, hours, minutes, seconds, and milliseconds.\n\n        Parameters\n        ----------\n        seconds : float\n            Time duration in seconds.\n\n        Returns\n        -------\n        namedtuple\n            Named tuple with fields: pos (bool), dd (days), hh (hours),\n            mm (minutes), ss (seconds), ms (milliseconds).\n\n        Examples\n        --------\n        &gt;&gt;&gt; PrettyDuration.to_dhms(3661.5)\n        Time(pos=True, dd=0, hh=1, mm=1, ss=1, ms=500.0)\n        \"\"\"\n        pos = seconds &gt;= 0\n        if not pos:\n            seconds = -seconds\n        ms = seconds % 1\n        ms = round(ms * 10000) / 10\n        seconds = floor(seconds)\n        m, s = divmod(seconds, 60)\n        h, m = divmod(m, 60)\n        d, h = divmod(h, 24)\n        Time = namedtuple(\"Time\", \"pos dd hh mm ss ms\")\n        time = Time(pos=pos, dd=d, hh=h, mm=m, ss=s, ms=ms)\n        return time\n\n    @staticmethod\n    def time_string(seconds):\n        \"\"\"\n        Return a formatted time string.\n\n        Parameters\n        ----------\n        seconds : float\n            Time duration in seconds.\n\n        Returns\n        -------\n        str\n            Formatted time string (e.g., \"1:01:01.5 hours\", \"30.5 seconds\").\n\n        Examples\n        --------\n        &gt;&gt;&gt; PrettyDuration.time_string(3661.5)\n        '1:01:01.5 hours'\n        &gt;&gt;&gt; PrettyDuration.time_string(30.5)\n        '30.5 seconds'\n        \"\"\"\n        if np.isinf(seconds):\n            return \"inf\"\n        pos, dd, hh, mm, ss, s = PrettyDuration.to_dhms(seconds)\n        if s &gt; 0:\n            if mm == 0:\n                # in this case, represent milliseconds in terms of\n                # seconds (i.e. a decimal)\n                sstr = str(s / 1000).lstrip(\"0\")\n                if s &gt;= 999.5:\n                    ss += 1\n                    s = 0\n                    sstr = \"\"\n                    # now propagate the carry:\n                    if ss == 60:\n                        mm += 1\n                        ss = 0\n                    if mm == 60:\n                        hh += 1\n                        mm = 0\n                    if hh == 24:\n                        dd += 1\n                        hh = 0\n            else:\n                # for all other cases, milliseconds will be represented\n                # as an integer\n                if s &gt;= 999.5:\n                    ss += 1\n                    s = 0\n                    sstr = \"\"\n                    # now propagate the carry:\n                    if ss == 60:\n                        mm += 1\n                        ss = 0\n                    if mm == 60:\n                        hh += 1\n                        mm = 0\n                    if hh == 24:\n                        dd += 1\n                        hh = 0\n                else:\n                    sstr = \":{:03d}\".format(int(s))\n        else:\n            sstr = \"\"\n        if dd &gt; 0:\n            daystr = \"{:01d} days \".format(dd)\n        else:\n            daystr = \"\"\n        if hh &gt; 0:\n            timestr = daystr + \"{:01d}:{:02d}:{:02d}{} hours\".format(hh, mm, ss, sstr)\n        elif mm &gt; 0:\n            timestr = daystr + \"{:01d}:{:02d}{} minutes\".format(mm, ss, sstr)\n        elif ss &gt; 0:\n            timestr = daystr + \"{:01d}{} seconds\".format(ss, sstr)\n        else:\n            timestr = daystr + \"{} milliseconds\".format(s)\n        if not pos:\n            timestr = \"-\" + timestr\n        return timestr\n\n    def __add__(self, other):\n        \"\"\"\n        Add another value to this duration.\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to add.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration + other)\n\n    def __radd__(self, other):\n        \"\"\"\n        Add this duration to another value (right addition).\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to add to.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return self.__add__(other)\n\n    def __sub__(self, other):\n        \"\"\"\n        Subtract another value from this duration.\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to subtract.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration - other)\n\n    def __rsub__(self, other):\n        \"\"\"\n        Subtract this duration from another value (right subtraction).\n\n        Parameters\n        ----------\n        other : float or PrettyDuration\n            Value to subtract from.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return other - self.duration\n\n    def __mul__(self, other):\n        \"\"\"\n        Multiply this duration by another value.\n\n        Parameters\n        ----------\n        other : float\n            Value to multiply by.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration * other)\n\n    def __rmul__(self, other):\n        \"\"\"\n        Multiply another value by this duration (right multiplication).\n\n        Parameters\n        ----------\n        other : float\n            Value to multiply.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return self.__mul__(other)\n\n    def __truediv__(self, other):\n        \"\"\"\n        Divide this duration by another value.\n\n        Parameters\n        ----------\n        other : float\n            Value to divide by.\n\n        Returns\n        -------\n        PrettyDuration\n            New duration object.\n        \"\"\"\n        return PrettyDuration(self.duration / other)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.PrettyDuration.time_string","title":"<code>time_string(seconds)</code>  <code>staticmethod</code>","text":"<p>Return a formatted time string.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Time duration in seconds.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted time string (e.g., \"1:01:01.5 hours\", \"30.5 seconds\").</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PrettyDuration.time_string(3661.5)\n'1:01:01.5 hours'\n&gt;&gt;&gt; PrettyDuration.time_string(30.5)\n'30.5 seconds'\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>@staticmethod\ndef time_string(seconds):\n    \"\"\"\n    Return a formatted time string.\n\n    Parameters\n    ----------\n    seconds : float\n        Time duration in seconds.\n\n    Returns\n    -------\n    str\n        Formatted time string (e.g., \"1:01:01.5 hours\", \"30.5 seconds\").\n\n    Examples\n    --------\n    &gt;&gt;&gt; PrettyDuration.time_string(3661.5)\n    '1:01:01.5 hours'\n    &gt;&gt;&gt; PrettyDuration.time_string(30.5)\n    '30.5 seconds'\n    \"\"\"\n    if np.isinf(seconds):\n        return \"inf\"\n    pos, dd, hh, mm, ss, s = PrettyDuration.to_dhms(seconds)\n    if s &gt; 0:\n        if mm == 0:\n            # in this case, represent milliseconds in terms of\n            # seconds (i.e. a decimal)\n            sstr = str(s / 1000).lstrip(\"0\")\n            if s &gt;= 999.5:\n                ss += 1\n                s = 0\n                sstr = \"\"\n                # now propagate the carry:\n                if ss == 60:\n                    mm += 1\n                    ss = 0\n                if mm == 60:\n                    hh += 1\n                    mm = 0\n                if hh == 24:\n                    dd += 1\n                    hh = 0\n        else:\n            # for all other cases, milliseconds will be represented\n            # as an integer\n            if s &gt;= 999.5:\n                ss += 1\n                s = 0\n                sstr = \"\"\n                # now propagate the carry:\n                if ss == 60:\n                    mm += 1\n                    ss = 0\n                if mm == 60:\n                    hh += 1\n                    mm = 0\n                if hh == 24:\n                    dd += 1\n                    hh = 0\n            else:\n                sstr = \":{:03d}\".format(int(s))\n    else:\n        sstr = \"\"\n    if dd &gt; 0:\n        daystr = \"{:01d} days \".format(dd)\n    else:\n        daystr = \"\"\n    if hh &gt; 0:\n        timestr = daystr + \"{:01d}:{:02d}:{:02d}{} hours\".format(hh, mm, ss, sstr)\n    elif mm &gt; 0:\n        timestr = daystr + \"{:01d}:{:02d}{} minutes\".format(mm, ss, sstr)\n    elif ss &gt; 0:\n        timestr = daystr + \"{:01d}{} seconds\".format(ss, sstr)\n    else:\n        timestr = daystr + \"{} milliseconds\".format(s)\n    if not pos:\n        timestr = \"-\" + timestr\n    return timestr\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.PrettyDuration.to_dhms","title":"<code>to_dhms(seconds)</code>  <code>staticmethod</code>","text":"<p>Convert seconds into days, hours, minutes, seconds, and milliseconds.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Time duration in seconds.</p> required <p>Returns:</p> Type Description <code>namedtuple</code> <p>Named tuple with fields: pos (bool), dd (days), hh (hours), mm (minutes), ss (seconds), ms (milliseconds).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PrettyDuration.to_dhms(3661.5)\nTime(pos=True, dd=0, hh=1, mm=1, ss=1, ms=500.0)\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>@staticmethod\ndef to_dhms(seconds):\n    \"\"\"\n    Convert seconds into days, hours, minutes, seconds, and milliseconds.\n\n    Parameters\n    ----------\n    seconds : float\n        Time duration in seconds.\n\n    Returns\n    -------\n    namedtuple\n        Named tuple with fields: pos (bool), dd (days), hh (hours),\n        mm (minutes), ss (seconds), ms (milliseconds).\n\n    Examples\n    --------\n    &gt;&gt;&gt; PrettyDuration.to_dhms(3661.5)\n    Time(pos=True, dd=0, hh=1, mm=1, ss=1, ms=500.0)\n    \"\"\"\n    pos = seconds &gt;= 0\n    if not pos:\n        seconds = -seconds\n    ms = seconds % 1\n    ms = round(ms * 10000) / 10\n    seconds = floor(seconds)\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    d, h = divmod(h, 24)\n    Time = namedtuple(\"Time\", \"pos dd hh mm ss ms\")\n    time = Time(pos=pos, dd=d, hh=h, mm=m, ss=s, ms=ms)\n    return time\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.PrettyInt","title":"<code>PrettyInt</code>","text":"<p>               Bases: <code>int</code></p> <p>Prints integers in a more readable format</p> Source code in <code>nelpy/utils.py</code> <pre><code>class PrettyInt(int):\n    \"\"\"Prints integers in a more readable format\"\"\"\n\n    def __init__(self, val):\n        self.val = val\n\n    def __str__(self):\n        return \"{:,}\".format(self.val)\n\n    def __repr__(self):\n        return \"{:,}\".format(self.val)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.argsort","title":"<code>argsort(seq)</code>","text":"<p>Return indices that would sort a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>sequence</code> <p>Sequence to sort (list, tuple, array, etc.).</p> required <p>Returns:</p> Name Type Description <code>indices</code> <code>list</code> <p>List of indices that would sort the sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; argsort([3, 1, 4, 1, 5])\n[1, 3, 0, 2, 4]\n&gt;&gt;&gt; seq = [\"c\", \"a\", \"b\"]\n&gt;&gt;&gt; [seq[i] for i in argsort(seq)]\n['a', 'b', 'c']\n</code></pre> Notes <p>Based on http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python</p> Source code in <code>nelpy/utils.py</code> <pre><code>def argsort(seq):\n    \"\"\"\n    Return indices that would sort a sequence.\n\n    Parameters\n    ----------\n    seq : sequence\n        Sequence to sort (list, tuple, array, etc.).\n\n    Returns\n    -------\n    indices : list\n        List of indices that would sort the sequence.\n\n    Examples\n    --------\n    &gt;&gt;&gt; argsort([3, 1, 4, 1, 5])\n    [1, 3, 0, 2, 4]\n    &gt;&gt;&gt; seq = [\"c\", \"a\", \"b\"]\n    &gt;&gt;&gt; [seq[i] for i in argsort(seq)]\n    ['a', 'b', 'c']\n\n    Notes\n    -----\n    Based on http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python\n    \"\"\"\n    return sorted(range(len(seq)), key=seq.__getitem__)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.asa_indices_within_epochs","title":"<code>asa_indices_within_epochs(asa, intervalarray)</code>","text":"<p>Return indices of ASA within epochs.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray object.</p> required <code>intervalarray</code> <code>IntervalArray</code> <p>IntervalArray containing epochs of interest.</p> required <p>Returns:</p> Name Type Description <code>indices</code> <code>ndarray</code> <p>Array of shape (n_epochs, 2) containing [start, stop] indices for each epoch, so that data can be associated with asa._data[:,start:stop] for each epoch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epochs = nelpy.EpochArray([[0, 10], [20, 30]])\n&gt;&gt;&gt; indices = asa_indices_within_epochs(asa, epochs)\n&gt;&gt;&gt; data_in_epochs = [asa._data[:, start:stop] for start, stop in indices]\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def asa_indices_within_epochs(asa, intervalarray):\n    \"\"\"\n    Return indices of ASA within epochs.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray\n        AnalogSignalArray object.\n    intervalarray : IntervalArray\n        IntervalArray containing epochs of interest.\n\n    Returns\n    -------\n    indices : np.ndarray\n        Array of shape (n_epochs, 2) containing [start, stop] indices\n        for each epoch, so that data can be associated with\n        asa._data[:,start:stop] for each epoch.\n\n    Examples\n    --------\n    &gt;&gt;&gt; epochs = nelpy.EpochArray([[0, 10], [20, 30]])\n    &gt;&gt;&gt; indices = asa_indices_within_epochs(asa, epochs)\n    &gt;&gt;&gt; data_in_epochs = [asa._data[:, start:stop] for start, stop in indices]\n    \"\"\"\n    indices = []\n    intervalarray = intervalarray[asa.support]\n    for interval in intervalarray.merge().data:\n        a_start = interval[0]\n        a_stop = interval[1]\n        frm, to = np.searchsorted(asa._abscissa_vals, (a_start, a_stop))\n        indices.append((frm, to))\n    indices = np.array(indices, ndmin=2)\n\n    return indices\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.cartesian","title":"<code>cartesian(xcenters, ycenters)</code>","text":"<p>Find every combination of elements in two arrays (Cartesian product).</p> <p>Parameters:</p> Name Type Description Default <code>xcenters</code> <code>ndarray</code> <p>First array of values.</p> required <code>ycenters</code> <code>ndarray</code> <p>Second array of values.</p> required <p>Returns:</p> Name Type Description <code>cartesian</code> <code>ndarray</code> <p>Array of shape (n_samples, 2) containing all combinations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.array([1, 2])\n&gt;&gt;&gt; y = np.array([3, 4, 5])\n&gt;&gt;&gt; cartesian(x, y)\narray([[1, 3],\n       [1, 4],\n       [1, 5],\n       [2, 3],\n       [2, 4],\n       [2, 5]])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def cartesian(xcenters, ycenters):\n    \"\"\"\n    Find every combination of elements in two arrays (Cartesian product).\n\n    Parameters\n    ----------\n    xcenters : np.ndarray\n        First array of values.\n    ycenters : np.ndarray\n        Second array of values.\n\n    Returns\n    -------\n    cartesian : np.ndarray\n        Array of shape (n_samples, 2) containing all combinations.\n\n    Examples\n    --------\n    &gt;&gt;&gt; x = np.array([1, 2])\n    &gt;&gt;&gt; y = np.array([3, 4, 5])\n    &gt;&gt;&gt; cartesian(x, y)\n    array([[1, 3],\n           [1, 4],\n           [1, 5],\n           [2, 3],\n           [2, 4],\n           [2, 5]])\n    \"\"\"\n    return np.transpose(\n        [np.tile(xcenters, len(ycenters)), np.repeat(ycenters, len(xcenters))]\n    )\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.collapse_time","title":"<code>collapse_time(obj, gap=0)</code>","text":"<p>Collapse all epochs in an object into a single, contiguous object.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>SpikeTrainArray or AnalogSignalArray</code> <p>Object with multiple epochs to collapse.</p> required <code>gap</code> <code>float</code> <p>Gap to insert between epochs in the collapsed object. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>obj</code> <p>New object of the same type with all epochs collapsed into a single contiguous epoch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Collapse a SpikeTrainArray with multiple epochs\n&gt;&gt;&gt; collapsed_st = collapse_time(spiketrain)\n&gt;&gt;&gt; # Collapse an AnalogSignalArray with gaps between epochs\n&gt;&gt;&gt; collapsed_asa = collapse_time(analogsignal, gap=0.1)\n</code></pre> Notes <p>For SpikeTrainArrays, gaps are not yet supported. The function adjusts spike times or signal times to create a continuous timeline while preserving the original epoch boundaries information.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def collapse_time(obj, gap=0):\n    \"\"\"\n    Collapse all epochs in an object into a single, contiguous object.\n\n    Parameters\n    ----------\n    obj : SpikeTrainArray or AnalogSignalArray\n        Object with multiple epochs to collapse.\n    gap : float, optional\n        Gap to insert between epochs in the collapsed object. Default is 0.\n\n    Returns\n    -------\n    obj\n        New object of the same type with all epochs collapsed into a single\n        contiguous epoch.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Collapse a SpikeTrainArray with multiple epochs\n    &gt;&gt;&gt; collapsed_st = collapse_time(spiketrain)\n    &gt;&gt;&gt; # Collapse an AnalogSignalArray with gaps between epochs\n    &gt;&gt;&gt; collapsed_asa = collapse_time(analogsignal, gap=0.1)\n\n    Notes\n    -----\n    For SpikeTrainArrays, gaps are not yet supported.\n    The function adjusts spike times or signal times to create a continuous\n    timeline while preserving the original epoch boundaries information.\n    \"\"\"\n\n    # TODO: redo SpikeTrainArray so as to keep the epochs separate!, and to support gaps!\n\n    # We'll have to ajust all the spikes per epoch... and we'll have to compute a new support. Also set a flag!\n\n    # If it's a SpikeTrainArray, then we left-shift the spike times. If it's an AnalogSignalArray, then we\n    # left-shift the time and tdata.\n\n    # Also set a new attribute, with the boundaries in seconds.\n\n    if isinstance(obj, core.RegularlySampledAnalogSignalArray):\n        new_obj = type(obj)(empty=True)\n        new_obj._data = obj._data\n\n        durations = obj.support.durations\n        starts = np.insert(np.cumsum(durations + gap), 0, 0)[:-1]\n        stops = starts + durations\n        newsupport = type(obj._abscissa.support)(np.vstack((starts, stops)).T)\n        new_obj._support = newsupport\n\n        new_time = obj.time.astype(float)  # fast copy\n        time_idx = np.insert(np.cumsum(obj.lengths), 0, 0)\n\n        new_offset = 0\n        for epidx in range(obj.n_epochs):\n            if epidx &gt; 0:\n                new_time[time_idx[epidx] : time_idx[epidx + 1]] = (\n                    new_time[time_idx[epidx] : time_idx[epidx + 1]]\n                    - obj.time[time_idx[epidx]]\n                    + new_offset\n                    + gap\n                )\n                new_offset += durations[epidx] + gap\n            else:\n                new_time[time_idx[epidx] : time_idx[epidx + 1]] = (\n                    new_time[time_idx[epidx] : time_idx[epidx + 1]]\n                    - obj.time[time_idx[epidx]]\n                    + new_offset\n                )\n                new_offset += durations[epidx]\n        new_obj._time = new_time\n\n        new_obj._fs = obj._fs\n\n    elif isinstance(obj, core.EventArray):\n        if gap &gt; 0:\n            raise ValueError(\"gaps not supported for SpikeTrainArrays yet!\")\n        new_obj = type(obj)(empty=True)\n        new_time = [[] for _ in range(obj.n_series)]\n        duration = 0\n        for st_ in obj:\n            le = st_.support.start\n            for unit_ in range(obj.n_series):\n                new_time[unit_].extend(st_._data[unit_] - le + duration)\n            duration += st_.support.duration\n        new_time = np.asanyarray(\n            [np.asanyarray(unittime) for unittime in new_time], dtype=object\n        )\n        new_obj._data = new_time\n        new_obj.support = type(obj._abscissa.support)([0, duration])\n        new_obj._series_ids = obj._series_ids\n        new_obj._series_labels = obj._series_labels\n        new_obj._series_tags = obj._series_tags\n    elif isinstance(obj, core.BinnedEventArray):\n        raise NotImplementedError(\n            \"BinnedEventArrays are not yet supported, but bst.data is essentially already collapsed!\"\n        )\n    else:\n        raise TypeError(\"unsupported type for collapse_time\")\n\n    return new_obj\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.ddt_asa","title":"<code>ddt_asa(asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None, norm=False)</code>","text":"<p>Numerical differentiation of a regularly sampled AnalogSignalArray.</p> <p>Optionally also smooths result with a Gaussian kernel.</p> <p>Smoothing is applied in time, and the same smoothing is applied to each signal in the AnalogSignalArray.</p> <p>Differentiation, (and if requested, smoothing) is applied within each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>RegularlySampledAnalogSignalArray</code> <p>Input object.</p> required <code>fs</code> <code>float</code> <p>Sampling rate (in Hz) of input RSASA. If not provided, it will be obtained from asa.fs.</p> <code>None</code> <code>smooth</code> <code>bool</code> <p>If true, result will be smoothed. Default is False</p> <code>False</code> <code>rectify</code> <code>bool</code> <p>If True, absolute value of derivative is computed. Default is True.</p> <code>True</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.05 (50 ms).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <code>norm</code> <p>If True, then apply the L2 norm to the result.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>RegularlySampledAnalogSignalArray</code> <p>A RegularlySampledAnalogSignalArray with derivative data (in units per second) is returned.</p> Notes <p>Central differences are used here.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef ddt_asa(\n    asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None, norm=False\n):\n    \"\"\"Numerical differentiation of a regularly sampled AnalogSignalArray.\n\n    Optionally also smooths result with a Gaussian kernel.\n\n    Smoothing is applied in time, and the same smoothing is applied to each\n    signal in the AnalogSignalArray.\n\n    Differentiation, (and if requested, smoothing) is applied within each epoch.\n\n    Parameters\n    ----------\n    asa : nelpy.RegularlySampledAnalogSignalArray\n        Input object.\n    fs : float, optional\n        Sampling rate (in Hz) of input RSASA. If not provided, it will be obtained\n        from asa.fs.\n    smooth : bool, optional\n        If true, result will be smoothed. Default is False\n    rectify : bool, optional\n        If True, absolute value of derivative is computed. Default is True.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.05\n        (50 ms).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n    norm: boolean, optional\n        If True, then apply the L2 norm to the result.\n    Returns\n    -------\n    out : nelpy.RegularlySampledAnalogSignalArray\n        A RegularlySampledAnalogSignalArray with derivative data (in units\n        per second) is returned.\n\n    Notes\n    -----\n    Central differences are used here.\n    \"\"\"\n\n    if not (\n        isinstance(asa, core.RegularlySampledAnalogSignalArray)\n        or isinstance(asa, core._analogsignalarray.PositionArray)\n    ):\n        raise TypeError(\"Input object must be a RegularlySampledAnalogSignalArray!\")\n    if fs is None:\n        fs = asa.fs\n    if sigma is None:\n        sigma = 0.05  # 50 ms default\n\n    out = asa.copy()\n    cum_lengths = np.insert(np.cumsum(asa.lengths), 0, 0)\n\n    # ensure that datatype is float\n    # TODO: this will break complex data\n    out._data = out.data.astype(float)\n\n    # now obtain the derivative for each epoch separately\n    for idx in range(asa.n_epochs):\n        # if 1D:\n        if asa.n_signals == 1:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = np.gradient(\n                    asa._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]], axis=1\n                )\n        else:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = np.gradient(\n                    asa._data[:, cum_lengths[idx] : cum_lengths[idx + 1]], axis=1\n                )\n\n    out._data = out._data * fs\n\n    if norm:\n        out._data = np.atleast_2d(np.linalg.norm(out._data, axis=0))\n\n    if rectify:\n        out._data = np.abs(out._data)\n\n    if smooth:\n        out = gaussian_filter(out, fs=fs, sigma=sigma, truncate=truncate)\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.dxdt_AnalogSignalArray","title":"<code>dxdt_AnalogSignalArray(asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None)</code>","text":"<p>Numerical differentiation of a regularly sampled AnalogSignalArray.</p> <p>Optionally also smooths result with a Gaussian kernel.</p> <p>Smoothing is applied in time, and the same smoothing is applied to each signal in the AnalogSignalArray.</p> <p>Differentiation, (and if requested, smoothing) is applied within each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray</code> required <code>fs</code> <code>float</code> <p>Sampling rate (in Hz) of AnalogSignalArray. If not provided, it will be obtained from asa.fs</p> <code>None</code> <code>smooth</code> <code>bool</code> <p>If true, result will be smoothed. Default is False</p> <code>False</code> <code>rectify</code> <code>bool</code> <p>If True, absolute value of derivative is computed. Default is True.</p> <code>True</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in seconds. Default is 0.05 (50 ms).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>AnalogSignalArray</code> <p>An AnalogSignalArray with derivative data (in units per second) is returned.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef dxdt_AnalogSignalArray(\n    asa, *, fs=None, smooth=False, rectify=True, sigma=None, truncate=None\n):\n    \"\"\"Numerical differentiation of a regularly sampled AnalogSignalArray.\n\n    Optionally also smooths result with a Gaussian kernel.\n\n    Smoothing is applied in time, and the same smoothing is applied to each\n    signal in the AnalogSignalArray.\n\n    Differentiation, (and if requested, smoothing) is applied within each epoch.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray\n    fs : float, optional\n        Sampling rate (in Hz) of AnalogSignalArray. If not provided, it will\n        be obtained from asa.fs\n    smooth : bool, optional\n        If true, result will be smoothed. Default is False\n    rectify : bool, optional\n        If True, absolute value of derivative is computed. Default is True.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in seconds. Default is 0.05\n        (50 ms).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0\n\n    Returns\n    -------\n    out : AnalogSignalArray\n        An AnalogSignalArray with derivative data (in units per second) is returned.\n    \"\"\"\n\n    raise DeprecationWarning(\"use ddt_asa instead!\")\n\n    if fs is None:\n        fs = asa.fs\n    if fs is None:\n        raise ValueError(\n            \"fs must either be specified, or must be contained in the AnalogSignalArray!\"\n        )\n    if sigma is None:\n        sigma = 0.05  # 50 ms default\n\n    out = copy.deepcopy(asa)\n    cum_lengths = np.insert(np.cumsum(asa.lengths), 0, 0)\n\n    # ensure that datatype is float\n    out._data = out.data.astype(float)\n\n    if asa.n_signals == 2:\n        out._data = out._data[[0], :]\n\n    # now obtain the derivative for each epoch separately\n    for idx in range(asa.n_epochs):\n        # if 1D:\n        if asa.n_signals == 1:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = np.gradient(\n                    asa._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]], axis=1\n                )\n        elif asa.n_signals == 2:\n            if (cum_lengths[idx + 1] - cum_lengths[idx]) &lt; 2:\n                # only single sample\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = 0\n            else:\n                out._data[[0], cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                    np.linalg.norm(\n                        np.gradient(\n                            asa._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                            axis=1,\n                        ),\n                        axis=0,\n                    )\n                )\n        else:\n            raise TypeError(\"more than 2D not currently supported!\")\n\n    out._data = out._data * fs\n\n    if rectify:\n        out._data = np.abs(out._data)\n\n    if smooth:\n        out = gaussian_filter(out, fs=fs, sigma=sigma, truncate=truncate)\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.find_nearest_idx","title":"<code>find_nearest_idx(array, val)</code>","text":"<p>Find the index of the nearest value in an array.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array to search in.</p> required <code>val</code> <code>float</code> <p>Value to find the nearest index for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index into array that is closest to val.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n&gt;&gt;&gt; find_nearest_idx(array, 4)\n1\n&gt;&gt;&gt; find_nearest_idx(array, 6)\n2\n</code></pre> Notes <p>TODO: A more efficient version using searchsorted should be incorporated: Based on answer here: http://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array</p> Source code in <code>nelpy/utils.py</code> <pre><code>def find_nearest_idx(array, val):\n    \"\"\"\n    Find the index of the nearest value in an array.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Input array to search in.\n    val : float\n        Value to find the nearest index for.\n\n    Returns\n    -------\n    int\n        Index into array that is closest to val.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n    &gt;&gt;&gt; find_nearest_idx(array, 4)\n    1\n    &gt;&gt;&gt; find_nearest_idx(array, 6)\n    2\n\n    Notes\n    -----\n    TODO: A more efficient version using searchsorted should be incorporated:\n    Based on answer here: http://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array\n    \"\"\"\n    return (np.abs(array - val)).argmin()\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.find_nearest_indices","title":"<code>find_nearest_indices(array, vals)</code>","text":"<p>Find indices of nearest values in an array for multiple values.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array to search in.</p> required <code>vals</code> <code>ndarray</code> <p>Array of values to find nearest indices for.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of indices into array that are closest to vals.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n&gt;&gt;&gt; vals = np.array([2, 4, 6, 8])\n&gt;&gt;&gt; find_nearest_indices(array, vals)\narray([0, 1, 2, 3])\n</code></pre> Notes <p>Wrapper around find_nearest_idx().</p> Source code in <code>nelpy/utils.py</code> <pre><code>def find_nearest_indices(array, vals):\n    \"\"\"\n    Find indices of nearest values in an array for multiple values.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Input array to search in.\n    vals : np.ndarray\n        Array of values to find nearest indices for.\n\n    Returns\n    -------\n    np.ndarray\n        Array of indices into array that are closest to vals.\n\n    Examples\n    --------\n    &gt;&gt;&gt; array = np.array([1, 3, 5, 7, 9])\n    &gt;&gt;&gt; vals = np.array([2, 4, 6, 8])\n    &gt;&gt;&gt; find_nearest_indices(array, vals)\n    array([0, 1, 2, 3])\n\n    Notes\n    -----\n    Wrapper around find_nearest_idx().\n    \"\"\"\n    return np.array([find_nearest_idx(array, val) for val in vals], dtype=int)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.find_threshold_crossing_events","title":"<code>find_threshold_crossing_events(x, threshold, *, mode='above')</code>","text":"<p>Find threshold crossing events. INCLUSIVE</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>numpy array</code> <p>Input data</p> required <code>threshold</code> <code>float</code> <p>The value whose crossing triggers an event</p> required <code>mode</code> <code>string, optional in ['above', 'below']; default 'above'</code> <p>event triggering above, or below threshold</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>eventlist</code> <code>list</code> <p>List containing the indices corresponding to threshold crossings</p> <code>eventmax</code> <code>list</code> <p>List containing the maximum value of each event</p> Source code in <code>nelpy/utils.py</code> <pre><code>def find_threshold_crossing_events(x, threshold, *, mode=\"above\"):\n    \"\"\"Find threshold crossing events. INCLUSIVE\n\n    Parameters\n    ----------\n    x : numpy array\n        Input data\n    threshold : float\n        The value whose crossing triggers an event\n    mode : string, optional in ['above', 'below']; default 'above'\n        event triggering above, or below threshold\n\n    Returns\n    -------\n    eventlist : list\n        List containing the indices corresponding to threshold crossings\n    eventmax : list\n        List containing the maximum value of each event\n    \"\"\"\n    from itertools import groupby\n    from operator import itemgetter\n\n    if mode == \"below\":\n        cross_threshold = np.where(x &lt;= threshold, 1, 0)\n    elif mode == \"above\":\n        cross_threshold = np.where(x &gt;= threshold, 1, 0)\n    else:\n        raise NotImplementedError(\n            \"mode {} not understood for find_threshold_crossing_events\".format(\n                str(mode)\n            )\n        )\n    eventlist = []\n    eventmax = []\n    for k, v in groupby(enumerate(cross_threshold), key=itemgetter(1)):\n        if k:\n            v = list(v)\n            eventlist.append([v[0][0], v[-1][0]])\n            try:\n                eventmax.append(x[v[0][0] : (v[-1][0] + 1)].max())\n            except Exception:\n                print(v, x[v[0][0] : v[-1][0]])\n    eventmax = np.asarray(eventmax)\n    eventlist = np.asarray(eventlist)\n    return eventlist, eventmax\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.frange","title":"<code>frange(start, stop, step)</code>","text":"<p>Generate a range of floating point values with a given step size.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>Start value.</p> required <code>stop</code> <code>float</code> <p>Stop value (exclusive).</p> required <code>step</code> <code>float</code> <p>Step size.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Array of values from start to stop (exclusive) with given step.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; frange(0, 1, 0.2)\narray([0. , 0.2, 0.4, 0.6, 0.8])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def frange(start, stop, step):\n    \"\"\"\n    Generate a range of floating point values with a given step size.\n\n    Parameters\n    ----------\n    start : float\n        Start value.\n    stop : float\n        Stop value (exclusive).\n    step : float\n        Step size.\n\n    Returns\n    -------\n    out : np.ndarray\n        Array of values from start to stop (exclusive) with given step.\n\n    Examples\n    --------\n    &gt;&gt;&gt; frange(0, 1, 0.2)\n    array([0. , 0.2, 0.4, 0.6, 0.8])\n    \"\"\"\n    num_steps = int(np.floor((stop - start) / step))\n    return np.linspace(start, stop, num=num_steps, endpoint=False)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.gaussian_filter","title":"<code>gaussian_filter(obj, *, fs=None, sigma=None, truncate=None, inplace=False, mode=None, cval=None, within_intervals=False)</code>","text":"<p>Smooths with a Gaussian kernel.</p> <p>Smoothing is applied along the abscissa, and the same smoothing is applied to each signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.</p> <p>Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.</code> required <code>fs</code> <code>float</code> <p>Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will be inferred.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05 (50 ms if base_unit=seconds).</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth outside of which the filter value will be zero. Default is 4.0.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True the data will be replaced with the smoothed data. Default is False.</p> <code>False</code> <code>mode</code> <code>(reflect, constant, nearest, mirror, wrap)</code> <p>The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to 'constant'. Default is 'reflect'.</p> <code>'reflect'</code> <code>cval</code> <code>scalar</code> <p>Value to fill past edges of input if mode is 'constant'. Default is 0.0.</p> <code>None</code> <code>within_intervals</code> <code>boolean</code> <p>If True, then smooth within each epoch. Otherwise smooth across epochs. Default is False. Note that when mode = 'wrap', then smoothing within epochs aren't affected by wrapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as obj</code> <p>An object with smoothed data is returned.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef gaussian_filter(\n    obj,\n    *,\n    fs=None,\n    sigma=None,\n    truncate=None,\n    inplace=False,\n    mode=None,\n    cval=None,\n    within_intervals=False,\n):\n    \"\"\"Smooths with a Gaussian kernel.\n\n    Smoothing is applied along the abscissa, and the same smoothing is applied to each\n    signal in the RegularlySampledAnalogSignalArray, or to each unit in a BinnedSpikeTrainArray.\n\n    Smoothing is applied ACROSS intervals, but smoothing WITHIN intervals is also supported.\n\n    Parameters\n    ----------\n    obj : RegularlySampledAnalogSignalArray or BinnedSpikeTrainArray.\n    fs : float, optional\n        Sampling rate (in obj.base_unit^-1) of obj. If not provided, it will\n        be inferred.\n    sigma : float, optional\n        Standard deviation of Gaussian kernel, in obj.base_units. Default is 0.05\n        (50 ms if base_unit=seconds).\n    truncate : float, optional\n        Bandwidth outside of which the filter value will be zero. Default is 4.0.\n    inplace : bool\n        If True the data will be replaced with the smoothed data.\n        Default is False.\n    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional\n        The mode parameter determines how the array borders are handled,\n        where cval is the value when mode is equal to 'constant'. Default is\n        'reflect'.\n    cval : scalar, optional\n        Value to fill past edges of input if mode is 'constant'. Default is 0.0.\n    within_intervals : boolean, optional\n        If True, then smooth within each epoch. Otherwise smooth across epochs.\n        Default is False.\n        Note that when mode = 'wrap', then smoothing within epochs aren't affected\n        by wrapping.\n\n    Returns\n    -------\n    out : same type as obj\n        An object with smoothed data is returned.\n\n    \"\"\"\n    if sigma is None:\n        sigma = 0.05\n    if truncate is None:\n        truncate = 4\n    if mode is None:\n        mode = \"reflect\"\n    if cval is None:\n        cval = 0.0\n\n    if not inplace:\n        out = copy.deepcopy(obj)\n    else:\n        out = obj\n\n    if isinstance(out, core.RegularlySampledAnalogSignalArray) or isinstance(\n        out, core._analogsignalarray.PositionArray\n    ):\n        if fs is None:\n            fs = out.fs\n        if fs is None:\n            raise ValueError(\n                \"fs must either be specified, or must be contained in the {}!\".format(\n                    out.type_name\n                )\n            )\n    elif isinstance(out, core.BinnedEventArray) or isinstance(\n        out, core._valeventarray.BinnedValueEventArray\n    ):\n        bst = out\n        if fs is None:\n            fs = 1 / bst.ds\n        if fs is None:\n            raise ValueError(\n                \"fs must either be specified, or must be contained in the {}!\".format(\n                    out.type_name\n                )\n            )\n    else:\n        raise NotImplementedError(\n            \"gaussian_filter for {} is not yet supported!\".format(str(type(out)))\n        )\n\n    sigma = sigma * fs\n\n    if not within_intervals:\n        # see https://stackoverflow.com/questions/18697532/gaussian-filtering-a-image-with-nan-in-python\n        # (1) if smoothing across intervals, we work on a merged support\n        # (2) build abscissa_vals, including existing ones, and out-of-support ones\n        # (3) to smooth U, build auxiliary arrays V and W, with (V=U).nan=0, and (W=1).nan=0\n        # (4) Z = smooth(V)/smooth(W)\n        # (5) only keep original support, and original abscissa_vals\n\n        if isinstance(\n            out,\n            (\n                core.RegularlySampledAnalogSignalArray,\n                core.BinnedEventArray,\n                core._analogsignalarray.PositionArray,\n                core._valeventarray.BinnedValueEventArray,\n            ),\n        ):\n            support = out._abscissa.support.merge()\n            if not support.domain.is_finite:\n                support.domain = (\n                    support.start,\n                    support.stop,\n                )  # TODO: #FIXME might come from abscissa definition, and not from support\n\n            missing_abscissa_vals = []\n            for interval in ~support:\n                missing_vals = frange(interval.start, interval.stop, 1 / fs)\n                missing_abscissa_vals.extend(missing_vals)\n\n            if isinstance(\n                out,\n                (\n                    core.RegularlySampledAnalogSignalArray,\n                    core._analogsignalarray.PositionArray,\n                ),\n            ):\n                n_signals = out.n_signals\n                n_samples = out.n_samples\n                V = np.zeros((n_signals, n_samples + len(missing_abscissa_vals)))\n                W = np.ones(V.shape)\n                all_abscissa_vals = np.sort(\n                    np.append(out._abscissa_vals, missing_abscissa_vals)\n                )\n                data_idx = np.searchsorted(all_abscissa_vals, out._abscissa_vals)\n                missing_idx = np.searchsorted(all_abscissa_vals, missing_abscissa_vals)\n                V[:, data_idx] = out.data\n                W[:, missing_idx] = 0\n\n                VV = scipy_gaussian_filter(\n                    V, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                )\n                WW = scipy_gaussian_filter(\n                    W, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                )\n\n                Z = VV[:, data_idx] / WW[:, data_idx]\n                out._data = Z\n            elif isinstance(\n                out, (core.BinnedEventArray, core._valeventarray.BinnedValueEventArray)\n            ):\n                n_signals = out.n_series\n                n_samples = out.n_bins\n                if out.data.ndim == 2:\n                    # (n_series, n_bins) - legacy BinnedEventArray\n                    V = np.zeros((n_signals, n_samples + len(missing_abscissa_vals)))\n                    W = np.ones(V.shape)\n                    all_abscissa_vals = np.sort(\n                        np.append(out._abscissa_vals, missing_abscissa_vals)\n                    )\n                    data_idx = np.searchsorted(all_abscissa_vals, out._abscissa_vals)\n                    missing_idx = np.searchsorted(\n                        all_abscissa_vals, missing_abscissa_vals\n                    )\n                    V[:, data_idx] = out.data\n                    W[:, missing_idx] = 0\n\n                    VV = scipy_gaussian_filter(\n                        V, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                    )\n                    WW = scipy_gaussian_filter(\n                        W, sigma=(0, sigma), truncate=truncate, mode=mode, cval=cval\n                    )\n\n                    Z = VV[:, data_idx] / WW[:, data_idx]\n                    out._data = Z\n                elif out.data.ndim == 3:\n                    # (n_series, n_bins, n_values) - BinnedValueEventArray\n                    n_values = out.data.shape[2]\n                    V = np.zeros(\n                        (n_signals, n_samples + len(missing_abscissa_vals), n_values)\n                    )\n                    W = np.ones(V.shape)\n                    all_abscissa_vals = np.sort(\n                        np.append(out._abscissa_vals, missing_abscissa_vals)\n                    )\n                    data_idx = np.searchsorted(all_abscissa_vals, out._abscissa_vals)\n                    missing_idx = np.searchsorted(\n                        all_abscissa_vals, missing_abscissa_vals\n                    )\n                    V[:, data_idx, :] = out.data\n                    W[:, missing_idx, :] = 0\n                    VV = np.empty_like(V)\n                    WW = np.empty_like(W)\n                    for v in range(n_values):\n                        VV[..., v] = scipy_gaussian_filter(\n                            V[..., v],\n                            sigma=(0, sigma),\n                            truncate=truncate,\n                            mode=mode,\n                            cval=cval,\n                        )\n                        WW[..., v] = scipy_gaussian_filter(\n                            W[..., v],\n                            sigma=(0, sigma),\n                            truncate=truncate,\n                            mode=mode,\n                            cval=cval,\n                        )\n                    Z = VV[:, data_idx, :] / WW[:, data_idx, :]\n                    out._data = Z\n                else:\n                    raise ValueError(\n                        \"Unsupported data shape for BinnedValueEventArray: {}\".format(\n                            out.data.shape\n                        )\n                    )\n        else:\n            raise NotImplementedError(\n                \"gaussian_filter across intervals for {} is not yet supported!\".format(\n                    str(type(out))\n                )\n            )\n    else:  # within intervals:\n        cum_lengths = np.insert(np.cumsum(out.lengths), 0, 0)\n        out._data = out._data.astype(float)\n\n        if isinstance(out, core.RegularlySampledAnalogSignalArray):\n            # now smooth each interval separately\n            for idx in range(out.n_intervals):\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                    scipy_gaussian_filter(\n                        out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                        sigma=(0, sigma),\n                        truncate=truncate,\n                    )\n                )\n        elif isinstance(out, core.BinnedSpikeTrainArray):\n            # now smooth each interval separately\n            for idx in range(out.n_epochs):\n                out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                    scipy_gaussian_filter(\n                        out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                        sigma=(0, sigma),\n                        truncate=truncate,\n                    )\n                )\n                # out._data[:,cum_lengths[idx]:cum_lengths[idx+1]] = self._smooth_array(out._data[:,cum_lengths[idx]:cum_lengths[idx+1]], w=w)\n        elif isinstance(out, core._valeventarray.BinnedValueEventArray):\n            # now smooth each interval separately for each value type\n            if out.data.ndim == 3:\n                for idx in range(out.n_intervals):\n                    for v in range(out.data.shape[2]):\n                        out._data[:, cum_lengths[idx] : cum_lengths[idx + 1], v] = (\n                            scipy_gaussian_filter(\n                                out._data[\n                                    :, cum_lengths[idx] : cum_lengths[idx + 1], v\n                                ],\n                                sigma=(0, sigma),\n                                truncate=truncate,\n                            )\n                        )\n            else:\n                # fallback to 2D smoothing (shouldn't happen for BinnedValueEventArray, but for safety)\n                for idx in range(out.n_intervals):\n                    out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = (\n                        scipy_gaussian_filter(\n                            out._data[:, cum_lengths[idx] : cum_lengths[idx + 1]],\n                            sigma=(0, sigma),\n                            truncate=truncate,\n                        )\n                    )\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_PBEs","title":"<code>get_PBEs(data, fs=None, ds=None, sigma=None, truncate=None, unsorted_id=0, min_active=None, minLength=None, maxLength=None, PrimaryThreshold=None, minThresholdLength=None, SecondaryThreshold=None)</code>","text":"<p>Determine PBEs from multiunit activity or spike trains.</p> Definitions <p>MUA : multiunit activity PBE : population burst event</p> Summary <p>This function can be used to identify PBE epochs from spike trains, binned spike trains, or multiunit activity (in the form of an AnalogSignalArray).</p> <p>It is recommended to either pass in a SpikeTrainArray or a BinnedSpikeTrainArray, so that a <code>min_active</code> number of sorted units can be set.</p> <p>It is also recommended that the unsorted units (but not noise artifacts!) should be included in the spike train that is used to estimate the PBEs. By default, unit_id=0 is assumed to be unsorted, but this can be changed, or if no unsorted units are present, you can set unsorted_id=None. Equivalently, if min_active=0, then no restriction will apply, and the unsorted_id will have no effect on the final PBE epochs.</p> <p>Examples:</p> <p>PBE_epochs = get_PBEs(mua_asa) PBE_epochs = get_PBEs(spiketrain, min_active=5) PBE_epochs = get_PBEs(binnedspiketrain, min_active=5)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz]. -- OR --</p> required <code>data</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray with multiple units, including unsorted unit(s), but excluding any noise artifects. -- OR --</p> required <code>data</code> <code>BinnedSpikeTrainArray</code> <p>BinnedSpikeTrainArray containing multiunit activity.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of mua, in Hz. If not specified, it will be inferred from data.</p> <code>None</code> <code>ds</code> <code>float</code> <p>Time step in which to bin spikes. Default is 1 ms.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation (in seconds) of Gaussian smoothing kernel. Default is 10 ms. If sigma==0 then no smoothing is applied.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth of the Gaussian filter. Default is 6.</p> <code>None</code> <code>unsorted_id</code> <code>int</code> <p>unit_id of the unsorted unit. Default is 0. If no unsorted unit is present, then set unsorted_id = None</p> <code>0</code> <code>min_active</code> <code>int</code> <p>Minimum number of active units per event, excluding unsorted unit. Default is 5.</p> <code>None</code> <code>minLength</code> <code>float</code> <p>Minimum event duration in seconds. Default is 50 ms.</p> <code>None</code> <code>maxLength</code> <code>float</code> <p>Maximum event duration in seconds. Default is 750 ms.</p> <code>None</code> <code>PrimaryThreshold</code> <code>float</code> <p>Primary threshold to exceed. Default is mean() + 3*std()</p> <code>None</code> <code>SecondaryThreshold</code> <code>float</code> <p>Secondary threshold to fall back to. Default is mean().</p> <code>None</code> <code>minThresholdLength</code> <code>float</code> <p>Minimum duration to stay above PrimaryThreshold. Default is 0 ms.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>PBE_epochs</code> <code>EpochArray</code> <p>EpochArray containing all the PBEs.</p> Future improvements <p>As of now, it is possible, but not easy to specify the Primary and Secondary thresholds for event detection. A slight change in API might be needed to make this specification more flexible.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef get_PBEs(\n    data,\n    fs=None,\n    ds=None,\n    sigma=None,\n    truncate=None,\n    unsorted_id=0,\n    min_active=None,\n    minLength=None,\n    maxLength=None,\n    PrimaryThreshold=None,\n    minThresholdLength=None,\n    SecondaryThreshold=None,\n):\n    \"\"\"Determine PBEs from multiunit activity or spike trains.\n\n    Definitions\n    -----------\n    MUA : multiunit activity\n    PBE : population burst event\n\n    Summary\n    -------\n    This function can be used to identify PBE epochs from spike trains, binned\n    spike trains, or multiunit activity (in the form of an AnalogSignalArray).\n\n    It is recommended to either pass in a SpikeTrainArray or a\n    BinnedSpikeTrainArray, so that a `min_active` number of sorted units can be\n    set.\n\n    It is also recommended that the unsorted units (but not noise artifacts!)\n    should be included in the spike train that is used to estimate the PBEs. By\n    default, unit_id=0 is assumed to be unsorted, but this can be changed, or if\n    no unsorted units are present, you can set unsorted_id=None. Equivalently,\n    if min_active=0, then no restriction will apply, and the unsorted_id will\n    have no effect on the final PBE epochs.\n\n    Examples\n    --------\n    PBE_epochs = get_PBEs(mua_asa)\n    PBE_epochs = get_PBEs(spiketrain, min_active=5)\n    PBE_epochs = get_PBEs(binnedspiketrain, min_active=5)\n\n    Parameters\n    ----------\n    data : AnalogSignalArray\n        AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz].\n     -- OR --\n    data : SpikeTrainArray\n        SpikeTrainArray with multiple units, including unsorted unit(s), but\n        excluding any noise artifects.\n     -- OR --\n    data : BinnedSpikeTrainArray\n        BinnedSpikeTrainArray containing multiunit activity.\n    fs : float, optional\n        Sampling frequency of mua, in Hz. If not specified, it will be inferred\n        from data.\n    ds : float, optional\n        Time step in which to bin spikes. Default is 1 ms.\n    sigma : float, optional\n        Standard deviation (in seconds) of Gaussian smoothing kernel.\n        Default is 10 ms. If sigma==0 then no smoothing is applied.\n    truncate : float, optional\n        Bandwidth of the Gaussian filter. Default is 6.\n    unsorted_id : int, optional\n        unit_id of the unsorted unit. Default is 0. If no unsorted unit is\n        present, then set unsorted_id = None\n    min_active : int, optional\n        Minimum number of active units per event, excluding unsorted unit.\n        Default is 5.\n    minLength : float, optional\n        Minimum event duration in seconds. Default is 50 ms.\n    maxLength : float, optional\n        Maximum event duration in seconds. Default is 750 ms.\n    PrimaryThreshold : float, optional\n        Primary threshold to exceed. Default is mean() + 3*std()\n    SecondaryThreshold : float, optional\n        Secondary threshold to fall back to. Default is mean().\n    minThresholdLength : float, optional\n        Minimum duration to stay above PrimaryThreshold. Default is 0 ms.\n\n    Returns\n    -------\n    PBE_epochs : EpochArray\n        EpochArray containing all the PBEs.\n\n    Future improvements\n    -------------------\n    As of now, it is possible, but not easy to specify the Primary and Secondary\n    thresholds for event detection. A slight change in API might be needed to\n    make this specification more flexible.\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.01  # 10 ms standard deviation\n    if truncate is None:\n        truncate = 6\n\n    if isinstance(data, core.AnalogSignalArray):\n        # if we have only mua, then we cannot set (ds, unsorted_id, min_active)\n        if ds is not None:\n            raise ValueError(\n                \"if data is an AnalogSignalArray then ds cannot be specified!\"\n            )\n        if unsorted_id:\n            raise ValueError(\n                \"if data is an AnalogSignalArray then unsorted_id cannot be specified!\"\n            )\n        if min_active is not None:\n            raise ValueError(\n                \"if data is an AnalogSignalArray then min_active cannot be specified!\"\n            )\n        mua = data\n        mua._data = mua._data.astype(float)\n        if (sigma != 0) and (truncate &gt; 0):\n            mua = gaussian_filter(mua, sigma=sigma, truncate=truncate)\n\n    elif isinstance(data, (core.EventArray, core.BinnedEventArray)):\n        # set default parameter values:\n        if ds is None:\n            ds = 0.001  # default 1 ms\n        if min_active is None:\n            min_active = 5\n        mua = get_mua(data, ds=ds, sigma=sigma, truncate=truncate, _fast=True)\n    else:\n        raise TypeError(\n            \"data has to be one of (AnalogSignalArray, SpikeTrainArray, BinnedSpikeTrainArray)\"\n        )\n\n    # set default parameter values:\n    if fs is None:\n        fs = mua.fs\n    if minLength is None:\n        minLength = 0.050  # 50 ms minimum event duration\n    if maxLength is None:\n        maxLength = 0.750  # 750 ms maximum event duration\n    if minThresholdLength is None:\n        minThresholdLength = 0.0\n    # if PrimaryThreshold is None:\n    #         PrimaryThreshold =\n    # if SecondaryThreshold is None:\n    #     SecondaryThreshold =\n    PBE_epochs = get_mua_events(\n        mua=mua,\n        fs=fs,\n        minLength=minLength,\n        maxLength=maxLength,\n        PrimaryThreshold=PrimaryThreshold,\n        minThresholdLength=minThresholdLength,\n        SecondaryThreshold=SecondaryThreshold,\n    )\n\n    # now require min_active number of sorted cells\n    if isinstance(data, (core.EventArray, core.BinnedEventArray)):\n        if min_active &gt; 0:\n            if unsorted_id is not None:\n                # remove unsorted unit, if present:\n                unit_ids = copy.deepcopy(data.unit_ids)\n                try:\n                    unit_ids.remove(unsorted_id)\n                except ValueError:\n                    pass\n                # data_ = data._unit_subset(unit_ids)\n                data_ = data.loc[:, unit_ids]\n            else:\n                data_ = data\n            # determine number of active units per epoch:\n            n_active = np.array([snippet.n_active for snippet in data_[PBE_epochs]])\n            active_epochs_idx = np.argwhere(n_active &gt; min_active).squeeze()\n            # only keep those epochs where sufficiently many units are active:\n            PBE_epochs = PBE_epochs[active_epochs_idx]\n    return PBE_epochs\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_contiguous_segments","title":"<code>get_contiguous_segments(data, *, step=None, assume_sorted=None, in_core=True, index=False, inclusive=False, fs=None, sort=None, in_memory=None)</code>","text":"<p>Compute contiguous segments (seperated by step) in a list.</p> <p>Note! This function requires that a sorted list is passed. It first checks if the list is sorted O(n), and only sorts O(n log(n)) if necessary. But if you know that the list is already sorted, you can pass assume_sorted=True, in which case it will skip the O(n) check.</p> <p>Returns an array of size (n_segments, 2), with each row being of the form ([start, stop]) [inclusive, exclusive].</p> <p>NOTE: when possible, use assume_sorted=True, and step=1 as explicit       arguments to function call.</p> <p>WARNING! Step is robustly computed in-core (i.e., when in_core is     True), but is assumed to be 1 when out-of-core.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [1, 2, 3, 4, 10, 11, 12]\n&gt;&gt;&gt; get_contiguous_segments(data)\n([1,5], [10,13])\n&gt;&gt;&gt; get_contiguous_segments(data, index=True)\n([0,4], [4,7])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>1D array of sequential data, typically assumed to be integral (sample numbers).</p> required <code>step</code> <code>float</code> <p>Expected step size for neighboring samples. Default uses numpy to find the median, but it is much faster and memory efficient to explicitly pass in step=1.</p> <code>None</code> <code>assume_sorted</code> <code>bool</code> <p>If assume_sorted == True, then data is not inspected or re-ordered. This can be significantly faster, especially for out-of-core computation, but it should only be used when you are confident that the data is indeed sorted, otherwise the results from get_contiguous_segments will not be reliable.</p> <code>None</code> <code>in_core</code> <code>bool</code> <p>If True, then we use np.diff which requires all the data to fit into memory simultaneously, otherwise we use groupby, which uses a generator to process potentially much larger chunks of data, but also much slower.</p> <code>True</code> <code>index</code> <code>bool</code> <p>If True, the indices of segment boundaries will be returned. Otherwise, the segment boundaries will be returned in terms of the data itself. Default is False.</p> <code>False</code> <code>inclusive</code> <code>bool</code> <p>If True, the boundaries are returned as [(inclusive idx, inclusive idx)] Default is False, and can only be used when index==True.</p> <code>False</code> Source code in <code>nelpy/utils.py</code> <pre><code>def get_contiguous_segments(\n    data,\n    *,\n    step=None,\n    assume_sorted=None,\n    in_core=True,\n    index=False,\n    inclusive=False,\n    fs=None,\n    sort=None,\n    in_memory=None,\n):\n    \"\"\"Compute contiguous segments (seperated by step) in a list.\n\n    Note! This function requires that a sorted list is passed.\n    It first checks if the list is sorted O(n), and only sorts O(n log(n))\n    if necessary. But if you know that the list is already sorted,\n    you can pass assume_sorted=True, in which case it will skip\n    the O(n) check.\n\n    Returns an array of size (n_segments, 2), with each row\n    being of the form ([start, stop]) [inclusive, exclusive].\n\n    NOTE: when possible, use assume_sorted=True, and step=1 as explicit\n          arguments to function call.\n\n    WARNING! Step is robustly computed in-core (i.e., when in_core is\n        True), but is assumed to be 1 when out-of-core.\n\n    Examples\n    -------\n    &gt;&gt;&gt; data = [1, 2, 3, 4, 10, 11, 12]\n    &gt;&gt;&gt; get_contiguous_segments(data)\n    ([1,5], [10,13])\n    &gt;&gt;&gt; get_contiguous_segments(data, index=True)\n    ([0,4], [4,7])\n\n    Parameters\n    ----------\n    data : array-like\n        1D array of sequential data, typically assumed to be integral (sample\n        numbers).\n    step : float, optional\n        Expected step size for neighboring samples. Default uses numpy to find\n        the median, but it is much faster and memory efficient to explicitly\n        pass in step=1.\n    assume_sorted : bool, optional\n        If assume_sorted == True, then data is not inspected or re-ordered. This\n        can be significantly faster, especially for out-of-core computation, but\n        it should only be used when you are confident that the data is indeed\n        sorted, otherwise the results from get_contiguous_segments will not be\n        reliable.\n    in_core : bool, optional\n        If True, then we use np.diff which requires all the data to fit\n        into memory simultaneously, otherwise we use groupby, which uses\n        a generator to process potentially much larger chunks of data,\n        but also much slower.\n    index : bool, optional\n        If True, the indices of segment boundaries will be returned. Otherwise,\n        the segment boundaries will be returned in terms of the data itself.\n        Default is False.\n    inclusive : bool, optional\n        If True, the boundaries are returned as [(inclusive idx, inclusive idx)]\n        Default is False, and can only be used when index==True.\n\n    Deprecated\n    ----------\n    in_memory : bool, optional\n        This is equivalent to the new 'in-core'.\n    sort : bool, optional\n        This is equivalent to the new 'assume_sorted'\n    fs : sampling rate (Hz) used to extend half-open interval support by 1/fs\n    \"\"\"\n\n    # handle deprecated API calls:\n    if in_memory:\n        in_core = in_memory\n        logging.warning(\"'in_memory' has been deprecated; use 'in_core' instead\")\n    if sort:\n        assume_sorted = sort\n        logging.warning(\"'sort' has been deprecated; use 'assume_sorted' instead\")\n    if fs:\n        step = 1 / fs\n        logging.warning(\"'fs' has been deprecated; use 'step' instead\")\n\n    if inclusive:\n        assert index, \"option 'inclusive' can only be used with 'index=True'\"\n    if in_core:\n        data = np.asarray(data)\n\n        if not assume_sorted:\n            if not is_sorted(data):\n                data = np.sort(data)  # algorithm assumes sorted list\n\n        if step is None:\n            step = np.median(np.diff(data))\n\n        # assuming that data(t1) is sampled somewhere on [t, t+1/fs) we have a 'continuous' signal as long as\n        # data(t2 = t1+1/fs) is sampled somewhere on [t+1/fs, t+2/fs). In the most extreme case, it could happen\n        # that t1 = t and t2 = t + 2/fs, i.e. a difference of 2 steps.\n\n        if np.any(np.diff(data) &lt; step):\n            logging.warning(\n                \"some steps in the data are smaller than the requested step size.\"\n            )\n\n        breaks = np.argwhere(np.diff(data) &gt;= 2 * step)\n        starts = np.insert(breaks + 1, 0, 0)\n        stops = np.append(breaks, len(data) - 1)\n        bdries = np.vstack((data[starts], data[stops] + step)).T\n        if index:\n            if inclusive:\n                indices = np.vstack((starts, stops)).T\n            else:\n                indices = np.vstack((starts, stops + 1)).T\n            return indices\n    else:\n        from itertools import groupby\n        from operator import itemgetter\n\n        if not assume_sorted:\n            if not is_sorted(data):\n                # data = np.sort(data)  # algorithm assumes sorted list\n                raise NotImplementedError(\n                    \"out-of-core sorting has not been implemented yet...\"\n                )\n\n        if step is None:\n            step = 1\n\n        bdries = []\n\n        if not index:\n            for k, g in groupby(enumerate(data), lambda ix: (ix[0] - ix[1])):\n                f = itemgetter(1)\n                gen = (f(x) for x in g)\n                start = next(gen)\n                stop = start\n                for stop in gen:\n                    pass\n                bdries.append([start, stop + step])\n        else:\n            counter = 0\n            for k, g in groupby(enumerate(data), lambda ix: (ix[0] - ix[1])):\n                f = itemgetter(1)\n                gen = (f(x) for x in g)\n                _ = next(gen)\n                start = counter\n                stop = start\n                for _ in gen:\n                    stop += 1\n                if inclusive:\n                    bdries.append([start, stop])\n                else:\n                    bdries.append([start, stop + 1])\n                counter = stop + 1\n\n    return np.asarray(bdries)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_direction","title":"<code>get_direction(asa, *, sigma=None)</code>","text":"<p>Return epochs during which an animal was running left to right, or right to left.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray 1D</code> <p>AnalogSignalArray containing the 1D position data.</p> required <code>sigma</code> <code>float</code> <p>Smoothing to apply to position (x) before computing gradient estimate. Default is 0.</p> <code>None</code> <p>Returns:</p> Type Description <code>l2r, r2l : EpochArrays</code> <p>EpochArrays corresponding to left-to-right and right-to-left movement.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_direction(asa, *, sigma=None):\n    \"\"\"Return epochs during which an animal was running left to right, or right\n    to left.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray 1D\n        AnalogSignalArray containing the 1D position data.\n    sigma : float, optional\n        Smoothing to apply to position (x) before computing gradient estimate.\n        Default is 0.\n\n    Returns\n    -------\n    l2r, r2l : EpochArrays\n        EpochArrays corresponding to left-to-right and right-to-left movement.\n    \"\"\"\n    if sigma is None:\n        sigma = 0\n    if not isinstance(asa, core.AnalogSignalArray):\n        raise TypeError(\"AnalogSignalArray expected!\")\n    assert asa.n_signals == 1, \"1D AnalogSignalArray expected!\"\n\n    direction = dxdt_AnalogSignalArray(asa.smooth(sigma=sigma), rectify=False).data\n    direction[direction &gt;= 0] = 1\n    direction[direction &lt; 0] = -1\n    direction = direction.squeeze()\n\n    l2r = get_contiguous_segments(np.argwhere(direction &gt; 0).squeeze(), step=1)\n    l2r[:, 1] -= (\n        1  # change bounds from [inclusive, exclusive] to [inclusive, inclusive]\n    )\n    l2r = core.EpochArray(asa.abscissa_vals[l2r])\n\n    r2l = get_contiguous_segments(np.argwhere(direction &lt; 0).squeeze(), step=1)\n    r2l[:, 1] -= (\n        1  # change bounds from [inclusive, exclusive] to [inclusive, inclusive]\n    )\n    r2l = core.EpochArray(asa.abscissa_vals[r2l])\n\n    return l2r, r2l\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_events_boundaries","title":"<code>get_events_boundaries(x, *, PrimaryThreshold=None, SecondaryThreshold=None, minThresholdLength=None, minLength=None, maxLength=None, ds=None, mode='above')</code>","text":"<p>get event boundaries such that event.max &gt;= PrimaryThreshold and the event extent is defined by SecondaryThreshold.</p> <p>Note that when PrimaryThreshold==SecondaryThreshold, then this is a simple threshold crossing algorithm.</p> <p>NB. minLength and maxLength are applied to the SecondaryThreshold     events, whereas minThresholdLength is applied to the     PrimaryThreshold events.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>numpy array</code> <p>Input data</p> required <code>mode</code> <code>string, optional in ['above', 'below']; default 'above'</code> <p>event triggering above, or below threshold</p> <code>'above'</code> <code>PrimaryThreshold</code> <code>float</code> <p>If mode=='above', requires that event.max &gt;= PrimaryThreshold If mode=='below', requires that event.min &lt;= PrimaryThreshold</p> <code>None</code> <code>SecondaryThreshold</code> <code>float</code> <p>The value that defines the event extent</p> <code>None</code> <code>minThresholdLength</code> <code>float</code> <p>Minimum duration for which the PrimaryThreshold is crossed</p> <code>None</code> <code>minLength</code> <code>float</code> <p>Minimum duration for which the SecondaryThreshold is crossed</p> <code>None</code> <code>maxLength</code> <code>float</code> <p>Maximum duration for which the SecondaryThreshold is crossed</p> <code>None</code> <code>ds</code> <code>float</code> <p>Time step of the input data x</p> <code>None</code> <p>Returns:</p> Type Description <code>returns bounds, maxes, events</code> <p>where bounds &lt;==&gt; SecondaryThreshold to SecondaryThreshold, inclusive       maxes  &lt;==&gt; maximum value during each event       events &lt;==&gt; PrimaryThreshold to PrimaryThreshold, inclusive</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_events_boundaries(\n    x,\n    *,\n    PrimaryThreshold=None,\n    SecondaryThreshold=None,\n    minThresholdLength=None,\n    minLength=None,\n    maxLength=None,\n    ds=None,\n    mode=\"above\",\n):\n    \"\"\"get event boundaries such that event.max &gt;= PrimaryThreshold\n    and the event extent is defined by SecondaryThreshold.\n\n    Note that when PrimaryThreshold==SecondaryThreshold, then this is a\n    simple threshold crossing algorithm.\n\n    NB. minLength and maxLength are applied to the SecondaryThreshold\n        events, whereas minThresholdLength is applied to the\n        PrimaryThreshold events.\n\n    Parameters\n    ----------\n    x : numpy array\n        Input data\n    mode : string, optional in ['above', 'below']; default 'above'\n        event triggering above, or below threshold\n    PrimaryThreshold : float, optional\n        If mode=='above', requires that event.max &gt;= PrimaryThreshold\n        If mode=='below', requires that event.min &lt;= PrimaryThreshold\n    SecondaryThreshold : float, optional\n        The value that defines the event extent\n    minThresholdLength : float, optional\n        Minimum duration for which the PrimaryThreshold is crossed\n    minLength : float, optional\n        Minimum duration for which the SecondaryThreshold is crossed\n    maxLength : float, optional\n        Maximum duration for which the SecondaryThreshold is crossed\n    ds : float, optional\n        Time step of the input data x\n\n    Returns\n    -------\n    returns bounds, maxes, events\n        where bounds &lt;==&gt; SecondaryThreshold to SecondaryThreshold, inclusive\n              maxes  &lt;==&gt; maximum value during each event\n              events &lt;==&gt; PrimaryThreshold to PrimaryThreshold, inclusive\n    \"\"\"\n\n    # TODO: x must be a numpy array\n    # TODO: ds is often used, but we have no default, and no check for when\n    #       it is left as None.\n    # TODO: the Docstring should equally be improved.\n\n    x = x.squeeze()\n    if x.ndim &gt; 1:\n        raise TypeError(\"multidimensional arrays not supported!\")\n\n    if PrimaryThreshold is None:  # by default, threshold is 3 SDs above mean of x\n        PrimaryThreshold = np.mean(x) + 3 * np.std(x)\n\n    if SecondaryThreshold is None:  # by default, revert back to mean of x\n        SecondaryThreshold = np.mean(x)  # + 0*np.std(x)\n\n    events, primary_maxes = find_threshold_crossing_events(\n        x=x, threshold=PrimaryThreshold, mode=mode\n    )\n\n    # apply minThresholdLength criterion:\n    if minThresholdLength is not None and len(events) &gt; 0:\n        durations = (events[:, 1] - events[:, 0] + 1) * ds\n        events = events[durations &gt;= minThresholdLength]\n\n    if len(events) == 0:\n        bounds, maxes, events = [], [], []\n        logging.warning(\"no events satisfied criteria\")\n        return bounds, maxes, events\n\n    # Find periods where value is &gt; SecondaryThreshold; note that the previous periods should be within these!\n    if mode == \"above\":\n        assert SecondaryThreshold &lt;= PrimaryThreshold, (\n            \"Secondary Threshold by definition should include more data than Primary Threshold\"\n        )\n    elif mode == \"below\":\n        assert SecondaryThreshold &gt;= PrimaryThreshold, (\n            \"Secondary Threshold by definition should include more data than Primary Threshold\"\n        )\n    else:\n        raise NotImplementedError(\n            \"mode {} not understood for find_threshold_crossing_events\".format(\n                str(mode)\n            )\n        )\n\n    bounds, broader_maxes = find_threshold_crossing_events(\n        x=x, threshold=SecondaryThreshold, mode=mode\n    )\n\n    # Find corresponding big windows for potential events\n    #  Specifically, look for closest left edge that is just smaller\n    outer_boundary_indices = np.searchsorted(bounds[:, 0], events[:, 0], side=\"right\")\n    #  searchsorted finds the index after, so subtract one to get index before\n    outer_boundary_indices = outer_boundary_indices - 1\n\n    # Find extended boundaries for events by pairing to larger windows\n    #   (Note that there may be repeats if the larger window contains multiple &gt; 3SD sections)\n    bounds = bounds[outer_boundary_indices, :]\n    maxes = broader_maxes[outer_boundary_indices]\n\n    if minLength is not None and len(events) &gt; 0:\n        durations = (bounds[:, 1] - bounds[:, 0] + 1) * ds\n        # TODO: refactor [durations &lt;= maxLength] but be careful about edge cases\n        bounds = bounds[durations &gt;= minLength]\n        maxes = maxes[durations &gt;= minLength]\n        events = events[durations &gt;= minLength]\n\n    if maxLength is not None and len(events) &gt; 0:\n        durations = (bounds[:, 1] - bounds[:, 0] + 1) * ds\n        # TODO: refactor [durations &lt;= maxLength] but be careful about edge cases\n        bounds = bounds[durations &lt;= maxLength]\n        maxes = maxes[durations &lt;= maxLength]\n        events = events[durations &lt;= maxLength]\n\n    if len(events) == 0:\n        bounds, maxes, events = [], [], []\n        logging.warning(\"no events satisfied criteria\")\n        return bounds, maxes, events\n\n    # Now, since all that we care about are the larger windows, so we should get rid of repeats\n    _, unique_idx = np.unique(bounds[:, 0], return_index=True)\n    bounds = bounds[unique_idx, :]  # SecondaryThreshold to SecondaryThreshold\n    maxes = maxes[unique_idx]  # maximum value during event\n    events = events[unique_idx, :]  # PrimaryThreshold to PrimaryThreshold\n\n    return bounds, maxes, events\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_inactive_epochs","title":"<code>get_inactive_epochs(speed, v1=5, v2=7)</code>","text":"<p>Return epochs where animal is running no faster than specified by v1 and v2.</p> <p>Parameters:</p> Name Type Description Default <code>speed</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray containing single channel speed, in units/sec</p> required <code>v1</code> <code>float</code> <p>Minimum speed (in same units as speed) that has to be reached / exceeded during an event. Default is 10 [units/sec]</p> <code>5</code> <code>v2</code> <code>float</code> <p>Speed that defines the event boundaries. Default is 8 [units/sec]</p> <code>7</code> <p>Returns:</p> Name Type Description <code>inactive_epochs</code> <code>EpochArray</code> <p>EpochArray with all the epochs where speed satisfied the criteria.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_inactive_epochs(speed, v1=5, v2=7):\n    \"\"\"Return epochs where animal is running no faster than specified by\n    v1 and v2.\n\n    Parameters\n    ----------\n    speed : AnalogSignalArray\n        AnalogSignalArray containing single channel speed, in units/sec\n    v1 : float, optional\n        Minimum speed (in same units as speed) that has to be reached /\n        exceeded during an event. Default is 10 [units/sec]\n    v2 : float, optional\n        Speed that defines the event boundaries. Default is 8 [units/sec]\n    Returns\n    -------\n    inactive_epochs : EpochArray\n        EpochArray with all the epochs where speed satisfied the criteria.\n    \"\"\"\n    inactive_epochs = get_threshold_crossing_epochs(\n        asa=speed, t1=v1, t2=v2, mode=\"below\"\n    )\n    return inactive_epochs\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_mua","title":"<code>get_mua(st, ds=None, sigma=None, truncate=None, _fast=True)</code>","text":"<p>Compute the multiunit activity (MUA) from a spike train.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray containing one or more units. -- OR --</p> required <code>st</code> <code>BinnedSpikeTrainArray</code> <p>BinnedSpikeTrainArray containing multiunit activity.</p> required <code>ds</code> <code>float</code> <p>Time step in which to bin spikes. Default is 1 ms.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation (in seconds) of Gaussian smoothing kernel. Default is 10 ms. If sigma==0 then no smoothing is applied.</p> <code>None</code> <code>truncate</code> <code>float</code> <p>Bandwidth of the Gaussian filter. Default is 6.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>mua</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray with MUA.</p> Source code in <code>nelpy/utils.py</code> <pre><code>@keyword_deprecation(replace_x_with_y={\"bw\": \"truncate\"})\ndef get_mua(st, ds=None, sigma=None, truncate=None, _fast=True):\n    \"\"\"Compute the multiunit activity (MUA) from a spike train.\n\n    Parameters\n    ----------\n    st : SpikeTrainArray\n        SpikeTrainArray containing one or more units.\n     -- OR --\n    st : BinnedSpikeTrainArray\n        BinnedSpikeTrainArray containing multiunit activity.\n    ds : float, optional\n        Time step in which to bin spikes. Default is 1 ms.\n    sigma : float, optional\n        Standard deviation (in seconds) of Gaussian smoothing kernel.\n        Default is 10 ms. If sigma==0 then no smoothing is applied.\n    truncate : float, optional\n        Bandwidth of the Gaussian filter. Default is 6.\n\n    Returns\n    -------\n    mua : AnalogSignalArray\n        AnalogSignalArray with MUA.\n    \"\"\"\n\n    if ds is None:\n        ds = 0.001  # 1 ms bin size\n    if sigma is None:\n        sigma = 0.01  # 10 ms standard deviation\n    if truncate is None:\n        truncate = 6\n\n    if isinstance(st, core.EventArray):\n        # bin spikes, so that we can count the spikes\n        mua_binned = st.bin(ds=ds).flatten()\n    elif isinstance(st, core.BinnedEventArray):\n        mua_binned = st.flatten()\n        ds = mua_binned.ds\n    else:\n        raise TypeError(\"st has to be one of (SpikeTrainArray, BinnedSpikeTrainArray)\")\n\n    # make sure data type is float, so that smoothing works, and convert to rate\n    mua_binned._data = mua_binned._data.astype(float) / ds\n\n    # TODO: now that we can simply cast from BST to ASA and back, the following logic could be simplified:\n    # put mua rate inside an AnalogSignalArray\n    if _fast:\n        mua = core.AnalogSignalArray([], empty=True)\n        mua._data = mua_binned.data\n        mua._abscissa_vals = mua_binned.bin_centers\n        mua._abscissa.support = mua_binned.support\n    else:\n        mua = core.AnalogSignalArray(\n            mua_binned.data, timestamps=mua_binned.bin_centers, fs=1 / ds\n        )\n\n    mua._fs = 1 / ds\n\n    if (sigma != 0) and (truncate &gt; 0):\n        mua = gaussian_filter(mua, sigma=sigma, truncate=truncate)\n\n    return mua\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_mua_events","title":"<code>get_mua_events(mua, fs=None, minLength=None, maxLength=None, PrimaryThreshold=None, minThresholdLength=None, SecondaryThreshold=None)</code>","text":"<p>Determine MUA/PBEs from multiunit activity.</p> <p>MUA : multiunit activity PBE : population burst event</p> <p>Parameters:</p> Name Type Description Default <code>mua</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz].</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of mua, in Hz. If not specified, it will be inferred from mua.fs</p> <code>None</code> <code>minLength</code> <code>float</code> <code>None</code> <code>maxLength</code> <code>float</code> <code>None</code> <code>PrimaryThreshold</code> <code>float</code> <code>None</code> <code>SecondaryThreshold</code> <code>float</code> <code>None</code> <code>minThresholdLength</code> <code>float</code> <code>None</code> <p>Returns:</p> Name Type Description <code>mua_epochs</code> <code>EpochArray</code> <p>EpochArray containing all the MUA events / PBEs.</p> <p>Examples:</p> <p>mua = get_mua(spiketrain) mua_epochs = get_mua_events(mua) PBEs = get_PBEs(spiketrain, min_active=5)      = get_PBEs(get_mua_events(get_mua(*)), spiketrain, min_active=5)</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_mua_events(\n    mua,\n    fs=None,\n    minLength=None,\n    maxLength=None,\n    PrimaryThreshold=None,\n    minThresholdLength=None,\n    SecondaryThreshold=None,\n):\n    \"\"\"Determine MUA/PBEs from multiunit activity.\n\n    MUA : multiunit activity\n    PBE : population burst event\n\n    Parameters\n    ----------\n    mua : AnalogSignalArray\n        AnalogSignalArray with one signal, namely the multiunit firing rate [in Hz].\n    fs : float, optional\n        Sampling frequency of mua, in Hz. If not specified, it will be inferred from\n        mua.fs\n    minLength : float, optional\n    maxLength : float, optional\n    PrimaryThreshold : float, optional\n    SecondaryThreshold : float, optional\n    minThresholdLength : float, optional\n\n    Returns\n    -------\n    mua_epochs : EpochArray\n        EpochArray containing all the MUA events / PBEs.\n\n    Examples\n    --------\n    mua = get_mua(spiketrain)\n    mua_epochs = get_mua_events(mua)\n    PBEs = get_PBEs(spiketrain, min_active=5)\n         = get_PBEs(get_mua_events(get_mua(*)), spiketrain, min_active=5)\n    \"\"\"\n\n    if fs is None:\n        fs = mua.fs\n    if fs is None:\n        raise ValueError(\"fs must either be specified, or must be contained in mua!\")\n\n    if PrimaryThreshold is None:\n        PrimaryThreshold = mua.mean() + 3 * mua.std()\n    if SecondaryThreshold is None:\n        SecondaryThreshold = mua.mean()\n    if minLength is None:\n        minLength = 0.050  # 50 ms minimum event duration\n    if maxLength is None:\n        maxLength = 0.750  # 750 ms maximum event duration\n    if minThresholdLength is None:\n        minThresholdLength = 0.0\n\n    # determine MUA event bounds:\n    mua_bounds_idx, maxes, _ = get_events_boundaries(\n        x=mua.data,\n        PrimaryThreshold=PrimaryThreshold,\n        SecondaryThreshold=SecondaryThreshold,\n        minThresholdLength=minThresholdLength,\n        minLength=minLength,\n        maxLength=maxLength,\n        ds=1 / fs,\n    )\n\n    if len(mua_bounds_idx) == 0:\n        logging.warning(\"no mua events detected\")\n        return core.EpochArray(empty=True)\n\n    # store MUA bounds in an EpochArray\n    mua_epochs = core.EpochArray(mua.time[mua_bounds_idx])\n\n    return mua_epochs\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_run_epochs","title":"<code>get_run_epochs(speed, v1=10, v2=8)</code>","text":"<p>Return epochs where animal is running at least as fast as specified by v1 and v2.</p> <p>Parameters:</p> Name Type Description Default <code>speed</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray containing single channel speed, in units/sec</p> required <code>v1</code> <code>float</code> <p>Minimum speed (in same units as speed) that has to be reached / exceeded during an event. Default is 10 [units/sec]</p> <code>10</code> <code>v2</code> <code>float</code> <p>Speed that defines the event boundaries. Default is 8 [units/sec]</p> <code>8</code> <p>Returns:</p> Name Type Description <code>run_epochs</code> <code>EpochArray</code> <p>EpochArray with all the epochs where speed satisfied the criteria.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_run_epochs(speed, v1=10, v2=8):\n    \"\"\"Return epochs where animal is running at least as fast as\n    specified by v1 and v2.\n\n    Parameters\n    ----------\n    speed : AnalogSignalArray\n        AnalogSignalArray containing single channel speed, in units/sec\n    v1 : float, optional\n        Minimum speed (in same units as speed) that has to be reached /\n        exceeded during an event. Default is 10 [units/sec]\n    v2 : float, optional\n        Speed that defines the event boundaries. Default is 8 [units/sec]\n\n    Returns\n    -------\n    run_epochs : EpochArray\n        EpochArray with all the epochs where speed satisfied the criteria.\n    \"\"\"\n\n    run_epochs = get_threshold_crossing_epochs(asa=speed, t1=v1, t2=v2, mode=\"above\")\n\n    return run_epochs\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_sort_idx","title":"<code>get_sort_idx(tuning_curves)</code>","text":"<p>Find indices to sort neurons by maximum firing rate location in tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>list of lists</code> <p>List where each inner list contains the tuning curve for an individual neuron.</p> required <p>Returns:</p> Name Type Description <code>sorted_idx</code> <code>list</code> <p>List of integers that correspond to neurons sorted by the location of their maximum firing rate in the tuning curve.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tuning_curves = [[1, 5, 2], [3, 1, 4], [2, 3, 6]]\n&gt;&gt;&gt; get_sort_idx(tuning_curves)\n[1, 0, 2]  # sorted by position of max firing rate\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def get_sort_idx(tuning_curves):\n    \"\"\"\n    Find indices to sort neurons by maximum firing rate location in tuning curve.\n\n    Parameters\n    ----------\n    tuning_curves : list of lists\n        List where each inner list contains the tuning curve for an individual\n        neuron.\n\n    Returns\n    -------\n    sorted_idx : list\n        List of integers that correspond to neurons sorted by the location\n        of their maximum firing rate in the tuning curve.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tuning_curves = [[1, 5, 2], [3, 1, 4], [2, 3, 6]]\n    &gt;&gt;&gt; get_sort_idx(tuning_curves)\n    [1, 0, 2]  # sorted by position of max firing rate\n    \"\"\"\n    tc_max_loc = []\n    for i, neuron_tc in enumerate(tuning_curves):\n        tc_max_loc.append((i, np.where(neuron_tc == np.max(neuron_tc))[0][0]))\n    sorted_by_tc = sorted(tc_max_loc, key=lambda x: x[1])\n\n    sorted_idx = []\n    for idx in sorted_by_tc:\n        sorted_idx.append(idx[0])\n\n    return sorted_idx\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.get_threshold_crossing_epochs","title":"<code>get_threshold_crossing_epochs(asa, t1=None, t2=None, mode='above')</code>","text":"<p>Return epochs where a signal crosses a compound threshold specified by t1 and t2.</p> <p>Parameters:</p> Name Type Description Default <code>asa</code> <code>AnalogSignalArray</code> <p>AnalogSignalArray containing a single channel</p> required <code>t1</code> <code>float</code> <p>Primary threshold. Minimum signal value that has to be reached / exceeded during an event. Default is 3 standard deviations above signal mean.</p> <code>None</code> <code>t2</code> <code>float</code> <p>Secondary threshold. Signal value that defines the event boundaries. Default is signal mean.</p> <code>None</code> <code>mode</code> <code>string</code> <p>Mode of operation. One of ['above', 'below']. If 'above', then return epochs where the signal exceeds the compound threshold, and if 'below', then return epochs where the signal falls below the compound threshold. Default is 'above'.</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>epochs</code> <code>EpochArray</code> <p>EpochArray with all the epochs where the signal satisfied the criteria.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def get_threshold_crossing_epochs(asa, t1=None, t2=None, mode=\"above\"):\n    \"\"\"Return epochs where a signal crosses a compound threshold specified by t1\n    and t2.\n\n    Parameters\n    ----------\n    asa : AnalogSignalArray\n        AnalogSignalArray containing a single channel\n    t1 : float, optional\n        Primary threshold. Minimum signal value that has to be reached /\n        exceeded during an event. Default is 3 standard deviations above signal\n        mean.\n    t2 : float, optional\n        Secondary threshold. Signal value that defines the event boundaries.\n        Default is signal mean.\n    mode : string, optional\n        Mode of operation. One of ['above', 'below']. If 'above', then return\n        epochs where the signal exceeds the compound threshold, and if 'below',\n        then return epochs where the signal falls below the compound threshold.\n        Default is 'above'.\n\n    Returns\n    -------\n    epochs : EpochArray\n        EpochArray with all the epochs where the signal satisfied the criteria.\n    \"\"\"\n\n    if asa.n_signals &gt; 1:\n        raise TypeError(\"multidimensional AnalogSignalArrays not supported!\")\n    x = asa.data.squeeze()\n\n    if t1 is None:  # by default, threshold is 3 SDs above mean of x\n        t1 = np.mean(x) + 3 * np.std(x)\n\n    if t2 is None:  # by default, revert back to mean of x\n        t2 = np.mean(x)\n\n    # compute periods where signal exceeds compound threshold\n    epoch_bounds, _, _ = get_events_boundaries(\n        x=x, PrimaryThreshold=t1, SecondaryThreshold=t2, mode=mode\n    )\n    # convert bounds to time in seconds\n    epoch_bounds = asa.time[epoch_bounds]\n    if len(epoch_bounds) == 0:\n        return type(asa._abscissa.support)(empty=True)\n    # add 1/fs to stops for open interval\n    epoch_bounds[:, 1] += 1 / asa.fs\n    # create EpochArray with threshould exceeding bounds\n    epochs = type(asa._abscissa.support)(epoch_bounds)\n    return epochs\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.information_rate","title":"<code>information_rate(ratemap, Pi=1)</code>","text":"<p>This computes the spatial information rate of cell spikes given variable x in bits/second.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>ndarray</code> <p>A firing rate map, any number of dimensions.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution over the bins of the rate map. If not provided, it is assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>ir</code> <code>ndarray</code> <p>The information rate of the cell, in bits/second.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def information_rate(ratemap, Pi=1):\n    \"\"\"\n    This computes the spatial information rate of cell spikes given variable x in\n    bits/second.\n\n    Parameters\n    ----------\n    ratemap : numpy.ndarray\n        A firing rate map, any number of dimensions.\n    Pi : numpy.ndarray\n        A probability distribution over the bins of the rate map. If not\n        provided, it is assumed to be uniform.\n\n    Returns\n    -------\n    ir : numpy.ndarray\n        The information rate of the cell, in bits/second.\n\n    \"\"\"\n    # convert Pi to probability distribution\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)\n    return spatial_information(ratemap, Pi) * R\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.is_odd","title":"<code>is_odd(n)</code>","text":"<p>Check if a number is odd.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Integer to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if n is odd, False if n is even.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_odd(3)\nTrue\n&gt;&gt;&gt; is_odd(4)\nFalse\n&gt;&gt;&gt; is_odd(-1)\nTrue\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def is_odd(n):\n    \"\"\"\n    Check if a number is odd.\n\n    Parameters\n    ----------\n    n : int\n        Integer to check.\n\n    Returns\n    -------\n    bool\n        True if n is odd, False if n is even.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_odd(3)\n    True\n    &gt;&gt;&gt; is_odd(4)\n    False\n    &gt;&gt;&gt; is_odd(-1)\n    True\n    \"\"\"\n    return bool(n &amp; 1)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.is_sorted","title":"<code>is_sorted(x, chunk_size=None)</code>","text":"<p>Check if a 1D array, list, or tuple is monotonically increasing (sorted).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>1D array, list, or tuple to check.</p> required <code>chunk_size</code> <code>int</code> <p>Size of chunks to check at a time (for large arrays).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>sorted</code> <code>bool</code> <p>True if x is sorted, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_sorted([1, 2, 3])\nTrue\n&gt;&gt;&gt; is_sorted([1, 3, 2])\nFalse\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def is_sorted(x, chunk_size=None):\n    \"\"\"\n    Check if a 1D array, list, or tuple is monotonically increasing (sorted).\n\n    Parameters\n    ----------\n    x : array-like\n        1D array, list, or tuple to check.\n    chunk_size : int, optional\n        Size of chunks to check at a time (for large arrays).\n\n    Returns\n    -------\n    sorted : bool\n        True if x is sorted, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_sorted([1, 2, 3])\n    True\n    &gt;&gt;&gt; is_sorted([1, 3, 2])\n    False\n    \"\"\"\n\n    if not isinstance(x, (tuple, list, np.ndarray)):\n        raise TypeError(\"Unsupported type {}\".format(type(x)))\n\n    x = np.atleast_1d(np.array(x).squeeze())\n    if x.ndim &gt; 1:\n        raise ValueError(\"Input x must be 1-dimensional\")\n\n    if chunk_size is None:\n        chunk_size = 500000\n    stop = x.size\n    for chunk_start in range(0, stop, chunk_size):\n        chunk_stop = int(min(stop, chunk_start + chunk_size + 1))\n        chunk = x[chunk_start:chunk_stop]\n        if not np.all(chunk[:-1] &lt;= chunk[1:]):\n            return False\n    return True\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.is_sorted_general","title":"<code>is_sorted_general(iterable, key=lambda a, b: a &lt;= b)</code>","text":"<p>Check if an iterable is sorted according to a custom key function.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>iterable</code> <p>Sequence to check.</p> required <code>key</code> <code>callable</code> <p>Function that takes two elements and returns True if they are in order. Default is lambda a, b: a &lt;= b (monotonic increasing).</p> <code>lambda a, b: a &lt;= b</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if iterable is sorted according to key function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_sorted_general([1, 2, 3, 4])\nTrue\n&gt;&gt;&gt; is_sorted_general([4, 3, 2, 1])\nFalse\n&gt;&gt;&gt; is_sorted_general([4, 3, 2, 1], key=lambda a, b: a &gt;= b)  # decreasing\nTrue\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def is_sorted_general(iterable, key=lambda a, b: a &lt;= b):\n    \"\"\"\n    Check if an iterable is sorted according to a custom key function.\n\n    Parameters\n    ----------\n    iterable : iterable\n        Sequence to check.\n    key : callable, optional\n        Function that takes two elements and returns True if they are in order.\n        Default is lambda a, b: a &lt;= b (monotonic increasing).\n\n    Returns\n    -------\n    bool\n        True if iterable is sorted according to key function.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_sorted_general([1, 2, 3, 4])\n    True\n    &gt;&gt;&gt; is_sorted_general([4, 3, 2, 1])\n    False\n    &gt;&gt;&gt; is_sorted_general([4, 3, 2, 1], key=lambda a, b: a &gt;= b)  # decreasing\n    True\n    \"\"\"\n    return all(key(a, b) for a, b in pairwise(iterable))\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.linear_merge","title":"<code>linear_merge(list1, list2)</code>","text":"<p>Merge two sorted lists in linear time.</p> <p>Parameters:</p> Name Type Description Default <code>list1</code> <code>list or ndarray</code> <p>First sorted list.</p> required <code>list2</code> <code>list or ndarray</code> <p>Second sorted list.</p> required <p>Returns:</p> Name Type Description <code>merged</code> <code>generator</code> <p>Generator yielding merged, sorted elements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(linear_merge([1, 3, 5], [2, 4, 6]))\n[1, 2, 3, 4, 5, 6]\n&gt;&gt;&gt; list(linear_merge([1, 2, 2, 3], [2, 2, 4, 4]))\n[1, 2, 2, 2, 2, 3, 4, 4]\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def linear_merge(list1, list2):\n    \"\"\"\n    Merge two sorted lists in linear time.\n\n    Parameters\n    ----------\n    list1 : list or np.ndarray\n        First sorted list.\n    list2 : list or np.ndarray\n        Second sorted list.\n\n    Returns\n    -------\n    merged : generator\n        Generator yielding merged, sorted elements.\n\n    Examples\n    --------\n    &gt;&gt;&gt; list(linear_merge([1, 3, 5], [2, 4, 6]))\n    [1, 2, 3, 4, 5, 6]\n    &gt;&gt;&gt; list(linear_merge([1, 2, 2, 3], [2, 2, 4, 4]))\n    [1, 2, 2, 2, 2, 3, 4, 4]\n    \"\"\"\n\n    # if any of the lists are empty, return the other (possibly also\n    # empty) list: (this is necessary because having either list1 or\n    # list2 be empty makes this quite a bit more complicated...)\n    if isinstance(list1, (list, np.ndarray)):\n        if len(list1) == 0:\n            list2 = iter(list2)\n            while True:\n                try:\n                    yield next(list2)\n                except StopIteration:\n                    return\n    if isinstance(list2, (list, np.ndarray)):\n        if len(list2) == 0:\n            list1 = iter(list1)\n            while True:\n                try:\n                    yield next(list1)\n                except StopIteration:\n                    return\n\n    list1 = iter(list1)\n    list2 = iter(list2)\n\n    value1 = next(list1)\n    value2 = next(list2)\n\n    # We'll normally exit this loop from a next() call raising\n    # StopIteration, which is how a generator function exits anyway.\n    while True:\n        if value1 &lt;= value2:\n            # Yield the lower value.\n            try:\n                yield value1\n            except StopIteration:\n                return\n            try:\n                # Grab the next value from list1.\n                value1 = next(list1)\n            except StopIteration:\n                # list1 is empty.  Yield the last value we received from list2, then\n                # yield the rest of list2.\n                try:\n                    yield value2\n                except StopIteration:\n                    return\n                while True:\n                    try:\n                        yield next(list2)\n                    except StopIteration:\n                        return\n        else:\n            try:\n                yield value2\n            except StopIteration:\n                return\n            try:\n                value2 = next(list2)\n\n            except StopIteration:\n                # list2 is empty.\n                try:\n                    yield value1\n                except StopIteration:\n                    return\n                while True:\n                    try:\n                        yield next(list1)\n                    except StopIteration:\n                        return\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.nextfastpower","title":"<code>nextfastpower(n)</code>","text":"<p>Return the next integral power of small factors greater than the given number.  Specifically, return m such that     m &gt;= n     m == 2x * 3y * 5**z where x, y, and z are integers. This is useful for ensuring fast FFT sizes.</p> <p>From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)</p> <p>See also http://scipy.github.io/devdocs/generated/scipy.fftpack.next_fast_len.html</p> Source code in <code>nelpy/utils.py</code> <pre><code>def nextfastpower(n):\n    \"\"\"Return the next integral power of small factors greater than the given\n    number.  Specifically, return m such that\n        m &gt;= n\n        m == 2**x * 3**y * 5**z\n    where x, y, and z are integers.\n    This is useful for ensuring fast FFT sizes.\n\n    From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)\n\n    See also http://scipy.github.io/devdocs/generated/scipy.fftpack.next_fast_len.html\n    \"\"\"\n    if n &lt; 7:\n        return max(n, 1)\n\n    # x, y, and z are all bounded from above by the formula of nextpower.\n    # Compute all possible combinations for powers of 3 and 5.\n    # (Not too many for reasonable FFT sizes.)\n    def power_series(x, base):\n        nmax = int(ceil(log(x) / log(base)))\n        return np.logspace(0.0, nmax, num=nmax + 1, base=base)\n\n    n35 = np.outer(power_series(n, 3.0), power_series(n, 5.0))\n    n35 = n35[n35 &lt;= n]\n    # Lump the powers of 3 and 5 together and solve for the powers of 2.\n    n2 = nextpower(n / n35)\n    return int(min(n2 * n35))\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.nextpower","title":"<code>nextpower(n, base=2.0)</code>","text":"<p>Return the next integral power of two greater than the given number. Specifically, return m such that     m &gt;= n     m == 2**x where x is an integer. Use base argument to specify a base other than 2. This is useful for ensuring fast FFT sizes.</p> <p>From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)</p> Source code in <code>nelpy/utils.py</code> <pre><code>def nextpower(n, base=2.0):\n    \"\"\"Return the next integral power of two greater than the given number.\n    Specifically, return m such that\n        m &gt;= n\n        m == 2**x\n    where x is an integer. Use base argument to specify a base other than 2.\n    This is useful for ensuring fast FFT sizes.\n\n    From https://gist.github.com/bhawkins/4479607 (Brian Hawkins)\n    \"\"\"\n    x = base ** ceil(log(n) / log(base))\n    if isinstance(n, np.ndarray):\n        return np.asarray(x, dtype=int)\n    else:\n        return int(x)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.pairwise","title":"<code>pairwise(iterable)</code>","text":"<p>Return a zip of all neighboring pairs in an iterable.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>iterable</code> <p>Input iterable.</p> required <p>Returns:</p> Name Type Description <code>pairs</code> <code>zip</code> <p>Iterator of pairs (a, b).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(pairwise([2, 3, 6, 8, 7]))\n[(2, 3), (3, 6), (6, 8), (8, 7)]\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def pairwise(iterable):\n    \"\"\"\n    Return a zip of all neighboring pairs in an iterable.\n\n    Parameters\n    ----------\n    iterable : iterable\n        Input iterable.\n\n    Returns\n    -------\n    pairs : zip\n        Iterator of pairs (a, b).\n\n    Examples\n    --------\n    &gt;&gt;&gt; list(pairwise([2, 3, 6, 8, 7]))\n    [(2, 3), (3, 6), (6, 8), (8, 7)]\n    \"\"\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.ragged_array","title":"<code>ragged_array(arr)</code>","text":"<p>Convert a list of arrays into a ragged array (object dtype).</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>list of np.ndarray</code> <p>List of arrays to combine into a ragged array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Ragged array of dtype object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ragged_array([np.arange(3), np.arange(5)])\narray([array([0, 1, 2]), array([0, 1, 2, 3, 4])], dtype=object)\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def ragged_array(arr):\n    \"\"\"\n    Convert a list of arrays into a ragged array (object dtype).\n\n    Parameters\n    ----------\n    arr : list of np.ndarray\n        List of arrays to combine into a ragged array.\n\n    Returns\n    -------\n    out : np.ndarray\n        Ragged array of dtype object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; ragged_array([np.arange(3), np.arange(5)])\n    array([array([0, 1, 2]), array([0, 1, 2, 3, 4])], dtype=object)\n    \"\"\"\n    n_elem = len(arr)\n    out = np.array(n_elem * [None])\n    for ii in range(out.shape[0]):\n        out[ii] = arr[ii]\n    return out\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.shrinkMatColsTo","title":"<code>shrinkMatColsTo(mat, numCols)</code>","text":"<p>Shrink a matrix by reducing the number of columns using interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>mat</code> <code>ndarray</code> <p>Input matrix of shape (N, M1).</p> required <code>numCols</code> <code>int</code> <p>Target number of columns M2, where M2 &lt;= M1.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Resized matrix of shape (N, numCols).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mat = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n&gt;&gt;&gt; shrinkMatColsTo(mat, 2)\narray([[1.5, 3.5],\n       [5.5, 7.5]])\n</code></pre> Notes <p>Uses scipy.ndimage.zoom with order=1 (linear interpolation).</p> Source code in <code>nelpy/utils.py</code> <pre><code>def shrinkMatColsTo(mat, numCols):\n    \"\"\"\n    Shrink a matrix by reducing the number of columns using interpolation.\n\n    Parameters\n    ----------\n    mat : np.ndarray\n        Input matrix of shape (N, M1).\n    numCols : int\n        Target number of columns M2, where M2 &lt;= M1.\n\n    Returns\n    -------\n    np.ndarray\n        Resized matrix of shape (N, numCols).\n\n    Examples\n    --------\n    &gt;&gt;&gt; mat = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n    &gt;&gt;&gt; shrinkMatColsTo(mat, 2)\n    array([[1.5, 3.5],\n           [5.5, 7.5]])\n\n    Notes\n    -----\n    Uses scipy.ndimage.zoom with order=1 (linear interpolation).\n    \"\"\"\n    from scipy.ndimage import zoom\n\n    numCells = mat.shape[0]\n    numColsMat = mat.shape[1]\n    a = np.zeros((numCells, numCols))\n    for row in np.arange(numCells):\n        niurou = zoom(input=mat[row, :], zoom=(numCols / numColsMat), order=1)\n        a[row, :] = niurou\n    return a\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.signal_envelope1D","title":"<code>signal_envelope1D(data, *, sigma=None, fs=None)</code>","text":"<p>Find the signal envelope using the Hilbert transform (deprecated).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>numpy array, list, or RegularlySampledAnalogSignalArray</code> <p>Input data.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian smoothing kernel in seconds.</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate of the signal.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as input</code> <p>Signal envelope.</p> Notes <p>This function is deprecated. Use <code>signal_envelope_1d</code> instead.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def signal_envelope1D(data, *, sigma=None, fs=None):\n    \"\"\"\n    Find the signal envelope using the Hilbert transform (deprecated).\n\n    Parameters\n    ----------\n    data : numpy array, list, or RegularlySampledAnalogSignalArray\n        Input data.\n    sigma : float, optional\n        Standard deviation of Gaussian smoothing kernel in seconds.\n    fs : float, optional\n        Sampling rate of the signal.\n\n    Returns\n    -------\n    out : same type as input\n        Signal envelope.\n\n    Notes\n    -----\n    This function is deprecated. Use `signal_envelope_1d` instead.\n    \"\"\"\n    logging.warnings(\n        \"'signal_envelope1D' is deprecated; use 'signal_envelope_1d' instead!\"\n    )\n    return signal_envelope_1d(data, sigma=sigma, fs=fs)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.signal_envelope_1d","title":"<code>signal_envelope_1d(data, *, sigma=None, fs=None)</code>","text":"<p>Finds the signal envelope by taking the absolute value of the Hilbert transform</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>numpy array, list, or RegularlySampledAnalogSignalArray</code> <p>Input data If data is a numpy array, it is expected to have shape (n_signals, n_samples) If data is a list, it is expected to have length n_signals, where each sublist has length n_samples, i.e. data is not jagged</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of the Gaussian kernel used to smooth the envelope after applying the Hilbert transform. Units of seconds. Default is 4 ms</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate of the signal</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>same type as the input object</code> <p>An object containing the signal envelope</p> <code>TODO</code> <code>this is not yet epoch-aware!</code> <code>UPDATE</code> <code>this is actually epoch-aware by now!</code> Source code in <code>nelpy/utils.py</code> <pre><code>def signal_envelope_1d(data, *, sigma=None, fs=None):\n    \"\"\"Finds the signal envelope by taking the absolute value\n    of the Hilbert transform\n\n    Parameters\n    ----------\n    data : numpy array, list, or RegularlySampledAnalogSignalArray\n        Input data\n        If data is a numpy array, it is expected to have shape\n        (n_signals, n_samples)\n        If data is a list, it is expected to have length n_signals,\n        where each sublist has length n_samples, i.e. data is not\n        jagged\n    sigma : float, optional\n        Standard deviation of the Gaussian kernel used to\n        smooth the envelope after applying the Hilbert transform.\n        Units of seconds. Default is 4 ms\n    fs : float, optional\n        Sampling rate of the signal\n\n    Returns\n    -------\n    out : same type as the input object\n        An object containing the signal envelope\n\n    TODO: this is not yet epoch-aware!\n    UPDATE: this is actually epoch-aware by now!\n    \"\"\"\n\n    if sigma is None:\n        sigma = 0.004  # 4 ms standard deviation\n    if fs is None:\n        if isinstance(data, (np.ndarray, list)):\n            raise ValueError(\"sampling frequency must be specified!\")\n        elif isinstance(data, core.RegularlySampledAnalogSignalArray):\n            fs = data.fs\n\n    if isinstance(data, (np.ndarray, list)):\n        data_array = np.array(data)\n        n_dims = np.array(data).ndim\n        assert n_dims &lt;= 2, \"Only 1D signals supported!\"\n        if n_dims == 1:\n            input_data = data_array.reshape((1, data_array.size))\n        else:\n            input_data = data_array\n        n_signals, n_samples = input_data.shape\n        # Compute number of samples to compute fast FFTs\n        padlen = next_fast_len(n_samples) - n_samples\n        # Pad data\n        paddeddata = np.hstack((input_data, np.zeros((n_signals, padlen))))\n        # Use hilbert transform to get an envelope\n        envelope = np.absolute(hilbert(paddeddata, axis=-1))\n        # free up memory\n        del paddeddata\n        # Truncate results back to original length\n        envelope = envelope[..., :n_samples]\n        if sigma:\n            # Smooth envelope with a gaussian (sigma = 4 ms default)\n            EnvelopeSmoothingSD = sigma * fs\n            smoothed_envelope = gaussian_filter1d(\n                envelope, EnvelopeSmoothingSD, mode=\"constant\", axis=-1\n            )\n            envelope = smoothed_envelope\n        if isinstance(data, list):\n            envelope = envelope.tolist()\n        return envelope\n    elif isinstance(data, core.RegularlySampledAnalogSignalArray):\n        # Only ASA data of shape (n_signals, n_timepoints) -&gt; 2D currently supported\n        assert data.data.ndim == 2\n        cum_lengths = np.insert(np.cumsum(data.lengths), 0, 0)\n\n        newasa = data.copy()\n        # for segment in data:\n        for idx in range(data.n_epochs):\n            # print('hilberting epoch {}/{}'.format(idx+1, data.n_epochs))\n            segment_data = data._data[:, cum_lengths[idx] : cum_lengths[idx + 1]]\n            n_signals, n_samples = segment_data.shape\n            # Compute number of samples to compute fast FFTs:\n            padlen = next_fast_len(n_samples) - n_samples\n            # Pad data\n            paddeddata = np.hstack((segment_data, np.zeros((n_signals, padlen))))\n            # Use hilbert transform to get an envelope\n            envelope = np.absolute(hilbert(paddeddata, axis=-1))\n            # free up memory\n            del paddeddata\n            # Truncate results back to original length\n            envelope = envelope[..., :n_samples]\n            if sigma:\n                # Smooth envelope with a gaussian (sigma = 4 ms default)\n                EnvelopeSmoothingSD = sigma * fs\n                smoothed_envelope = gaussian_filter1d(\n                    envelope, EnvelopeSmoothingSD, mode=\"constant\", axis=-1\n                )\n                envelope = smoothed_envelope\n            newasa._data[:, cum_lengths[idx] : cum_lengths[idx + 1]] = np.atleast_2d(\n                envelope\n            )\n        return newasa\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.spatial_information","title":"<code>spatial_information(ratemap, Pi=1)</code>","text":"<p>Compute the spatial information and firing sparsity...</p> <p>The specificity index examines the amount of information (in bits) that a single spike conveys about the animal's location (i.e., how well cell firing predicts the animal's location).The spatial information content of cell discharge was calculated using the formula:     information content = \\Sum P_i(R_i/R)log_2(R_i/R) where i is the bin number, P_i, is the probability for occupancy of bin i, R_i, is the mean firing rate for bin i, and R is the overall mean firing rate.</p> <p>In order to account for the effects of low firing rates (with fewer spikes there is a tendency toward higher information content) or random bursts of firing, the spike firing time-series was randomly offset in time from the rat location time-series, and the information content was calculated. A distribution of the information content based on 100 such random shifts was obtained and was used to compute a standardized score (Zscore) of information content for that cell. While the distribution is not composed of independent samples, it was nominally normally distributed, and a Z value of 2.29 was chosen as a cut-off for significance (the equivalent of a one-tailed t-test with P = 0.01 under a normal distribution).</p> Reference(s) <p>Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,     and Skaggs, W. E. (1994). \"Spatial information content and     reliability of hippocampal CA1 neurons: effects of visual     input\", Hippocampus, 4(4), 410-421.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>array of shape (n_units, n_bins)</code> <p>Rate map in Hz.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution over the bins of the rate map. If not provided, it is assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>si</code> <code>array of shape (n_units,)</code> <p>spatial information (in bits per spike)</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spatial_information(ratemap, Pi=1):\n    \"\"\"Compute the spatial information and firing sparsity...\n\n    The specificity index examines the amount of information\n    (in bits) that a single spike conveys about the animal's\n    location (i.e., how well cell firing predicts the animal's\n    location).The spatial information content of cell discharge was\n    calculated using the formula:\n        information content = \\\\Sum P_i(R_i/R)log_2(R_i/R)\n    where i is the bin number, P_i, is the probability for occupancy\n    of bin i, R_i, is the mean firing rate for bin i, and R is the\n    overall mean firing rate.\n\n    In order to account for the effects of low firing rates (with\n    fewer spikes there is a tendency toward higher information\n    content) or random bursts of firing, the spike firing\n    time-series was randomly offset in time from the rat location\n    time-series, and the information content was calculated. A\n    distribution of the information content based on 100 such random\n    shifts was obtained and was used to compute a standardized score\n    (Zscore) of information content for that cell. While the\n    distribution is not composed of independent samples, it was\n    nominally normally distributed, and a Z value of 2.29 was chosen\n    as a cut-off for significance (the equivalent of a one-tailed\n    t-test with P = 0.01 under a normal distribution).\n\n    Reference(s)\n    ------------\n    Markus, E. J., Barnes, C. A., McNaughton, B. L., Gladden, V. L.,\n        and Skaggs, W. E. (1994). \"Spatial information content and\n        reliability of hippocampal CA1 neurons: effects of visual\n        input\", Hippocampus, 4(4), 410-421.\n\n    Parameters\n    ----------\n    ratemap : array of shape (n_units, n_bins)\n        Rate map in Hz.\n\n    Pi : numpy.ndarray\n        A probability distribution over the bins of the rate map. If not\n        provided, it is assumed to be uniform.\n    Returns\n    -------\n    si : array of shape (n_units,)\n        spatial information (in bits per spike)\n    \"\"\"\n    # convert Pi to probability distribution\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    ratemap = copy.deepcopy(ratemap)\n    # ensure that the ratemap always has nonzero firing rates,\n    # otherwise the spatial information might return NaNs:\n    ratemap[np.isnan(ratemap) | (ratemap == 0)] = np.finfo(float).eps\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)  # mean firing rate for each unit\n\n    # Compute spatial information for each unit\n    # We need to compute sum over spatial bins of: Pi * (Ri/R) * log2(Ri/R)\n    # where Ri is the firing rate in each spatial bin\n\n    si = np.zeros(ratemap.shape[0])  # one value per unit\n\n    for unit_idx in range(ratemap.shape[0]):\n        unit_ratemap = ratemap[unit_idx]  # spatial dimensions only\n        unit_mean_rate = R[unit_idx]\n\n        # Compute (Ri / R) * log2(Ri / R) for each spatial bin\n        rate_ratio = unit_ratemap / unit_mean_rate\n        log_term = rate_ratio * np.log2(rate_ratio)\n\n        # Weight by occupancy probability and sum over all spatial dimensions\n        si[unit_idx] = np.sum(Pi * log_term)\n\n    return si\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.spatial_selectivity","title":"<code>spatial_selectivity(ratemap, Pi=1)</code>","text":"<p>The selectivity measure max(rate)/mean(rate)  of the cell. The more tightly concentrated the cell's activity, the higher the selectivity. A cell with no spatial tuning at all will have a selectivity of 1.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>A firing rate map, any number of dimensions.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution of the occupancy of each bin in the rate map. If not provided, the occupancy will be assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>selectivity</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spatial_selectivity(ratemap, Pi=1):\n    \"\"\"\n    The selectivity measure max(rate)/mean(rate)  of the cell. The more\n    tightly concentrated the cell's activity, the higher the selectivity.\n    A cell with no spatial tuning at all will have a selectivity of 1.\n\n    Parameters\n    ----------\n    rate_map : numpy.ndarray\n        A firing rate map, any number of dimensions.\n    Pi : numpy.ndarray\n        A probability distribution of the occupancy of each bin in the\n        rate map. If not provided, the occupancy will be assumed to be uniform.\n\n    Returns\n    -------\n    out : float\n        selectivity\n    \"\"\"\n    # convert Pi to probability distribution\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)\n\n    # Get maximum rate over all spatial dimensions for each unit\n    max_rate = np.max(ratemap, axis=spatial_axes)\n    return max_rate / R\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.spatial_sparsity","title":"<code>spatial_sparsity(ratemap, Pi=1)</code>","text":"<p>Compute the firing sparsity... Compute sparsity of a rate map, The sparsity  measure is an adaptation to space. The adaptation measures the fraction of the environment  in which a cell is  active. A sparsity of, 0.1 means that the place field of the cell occupies 1/10 of the area the subject traverses</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>A firing rate map, any number of dimensions.</p> required <code>Pi</code> <code>ndarray</code> <p>A probability distribution over the bins of the rate map. If not provided, it is assumed to be uniform.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>sparsity</p> References <p>.. [2] Skaggs, W. E., McNaughton, B. L., Wilson, M., &amp; Barnes, C. (1996). Theta phase precession in hippocampal neuronal populations and the compression of temporal sequences. Hippocampus, 6, 149-172.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>array of shape (n_units, n_bins)</code> <p>Rate map in Hz.</p> required <code>Pi</code> <code>array of shape (n_bins,)</code> <p>Occupancy of the animal.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>sparsity</code> <code>array of shape (n_units,)</code> <p>sparsity (in percent) for each unit</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spatial_sparsity(ratemap, Pi=1):\n    \"\"\"Compute the firing sparsity...\n    Compute sparsity of a rate map, The sparsity  measure is an adaptation\n    to space. The adaptation measures the fraction of the environment  in which\n    a cell is  active. A sparsity of, 0.1 means that the place field of the\n    cell occupies 1/10 of the area the subject traverses\n\n    Parameters\n    ----------\n    rate_map : numpy.ndarray\n        A firing rate map, any number of dimensions.\n    Pi : numpy.ndarray\n        A probability distribution over the bins of the rate map. If not\n        provided, it is assumed to be uniform.\n    Returns\n    -------\n    out : float\n        sparsity\n\n    References\n    ----------\n    .. [2] Skaggs, W. E., McNaughton, B. L., Wilson, M., &amp; Barnes, C. (1996).\n    Theta phase precession in hippocampal neuronal populations and the\n    compression of temporal sequences. Hippocampus, 6, 149-172.\n\n    Parameters\n    ----------\n\n    ratemap : array of shape (n_units, n_bins)\n        Rate map in Hz.\n    Pi : array of shape (n_bins,)\n        Occupancy of the animal.\n    Returns\n    -------\n    sparsity: array of shape (n_units,)\n        sparsity (in percent) for each unit\n    \"\"\"\n\n    Pi[np.isnan(Pi) | (Pi == 0)] = np.finfo(float).eps\n    Pi = Pi / (np.sum(Pi) + np.finfo(float).eps)\n\n    ratemap = copy.deepcopy(ratemap)\n    # ensure that the ratemap always has nonzero firing rates,\n    # otherwise the spatial information might return NaNs:\n    ratemap[np.isnan(ratemap) | (ratemap == 0)] = np.finfo(float).eps\n\n    # Handle N-dimensional ratemaps (n_units, *spatial_dims)\n    if len(ratemap.shape) &lt; 2:\n        raise TypeError(\n            \"rate map must have at least 2 dimensions (n_units, *spatial_dims)!\"\n        )\n\n    # Sum over all spatial dimensions to get mean firing rate for each unit\n    spatial_axes = tuple(range(1, len(ratemap.shape)))\n    R = (ratemap * Pi).sum(axis=spatial_axes)\n\n    # Compute average squared rate over all spatial dimensions\n    avg_sqr_rate = np.sum(ratemap**2 * Pi, axis=spatial_axes)\n\n    return R**2 / avg_sqr_rate\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.spiketrain_union","title":"<code>spiketrain_union(st1, st2)</code>","text":"<p>Join two spiketrains together.</p> <p>Parameters:</p> Name Type Description Default <code>st1</code> <code>SpikeTrainArray</code> <p>First spiketrain.</p> required <code>st2</code> <code>SpikeTrainArray</code> <p>Second spiketrain.</p> required <p>Returns:</p> Type Description <code>SpikeTrainArray</code> <p>Combined spiketrain with joined support.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combined_st = spiketrain_union(st1, st2)\n</code></pre> Notes <p>WARNING! This function should be improved a lot! Currently assumes both spiketrains have the same number of units.</p> Source code in <code>nelpy/utils.py</code> <pre><code>def spiketrain_union(st1, st2):\n    \"\"\"\n    Join two spiketrains together.\n\n    Parameters\n    ----------\n    st1 : SpikeTrainArray\n        First spiketrain.\n    st2 : SpikeTrainArray\n        Second spiketrain.\n\n    Returns\n    -------\n    SpikeTrainArray\n        Combined spiketrain with joined support.\n\n    Examples\n    --------\n    &gt;&gt;&gt; combined_st = spiketrain_union(st1, st2)\n\n    Notes\n    -----\n    WARNING! This function should be improved a lot!\n    Currently assumes both spiketrains have the same number of units.\n    \"\"\"\n    assert st1.n_units == st2.n_units\n    support = st1.support.join(st2.support)\n\n    newdata = []\n    for unit in range(st1.n_units):\n        newdata.append(np.append(st1.time[unit], st2.time[unit]))\n\n    fs = None\n    if st1.fs == st2.fs:\n        fs = st1.fs\n\n    return core.SpikeTrainArray(newdata, support=support, fs=fs)\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.swap_cols","title":"<code>swap_cols(arr, frm, to)</code>","text":"<p>Swap columns of a 2D numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>2D array to modify.</p> required <code>frm</code> <code>int</code> <p>Index of first column to swap.</p> required <code>to</code> <code>int</code> <p>Index of second column to swap.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies array in-place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; swap_cols(arr, 0, 2)\n&gt;&gt;&gt; arr\narray([[3, 2, 1],\n       [6, 5, 4]])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def swap_cols(arr, frm, to):\n    \"\"\"\n    Swap columns of a 2D numpy array.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        2D array to modify.\n    frm : int\n        Index of first column to swap.\n    to : int\n        Index of second column to swap.\n\n    Returns\n    -------\n    None\n        Modifies array in-place.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6]])\n    &gt;&gt;&gt; swap_cols(arr, 0, 2)\n    &gt;&gt;&gt; arr\n    array([[3, 2, 1],\n           [6, 5, 4]])\n    \"\"\"\n    if arr.ndim &gt; 1:\n        arr[:, [frm, to]] = arr[:, [to, frm]]\n    else:\n        arr[frm], arr[to] = arr[to], arr[frm]\n</code></pre>"},{"location":"reference/nelpy/utils/#nelpy.utils.swap_rows","title":"<code>swap_rows(arr, frm, to)</code>","text":"<p>Swap rows of a 2D numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>2D array to modify.</p> required <code>frm</code> <code>int</code> <p>Index of first row to swap.</p> required <code>to</code> <code>int</code> <p>Index of second row to swap.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Modifies array in-place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; swap_rows(arr, 0, 2)\n&gt;&gt;&gt; arr\narray([[7, 8, 9],\n       [4, 5, 6],\n       [1, 2, 3]])\n</code></pre> Source code in <code>nelpy/utils.py</code> <pre><code>def swap_rows(arr, frm, to):\n    \"\"\"\n    Swap rows of a 2D numpy array.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        2D array to modify.\n    frm : int\n        Index of first row to swap.\n    to : int\n        Index of second row to swap.\n\n    Returns\n    -------\n    None\n        Modifies array in-place.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    &gt;&gt;&gt; swap_rows(arr, 0, 2)\n    &gt;&gt;&gt; arr\n    array([[7, 8, 9],\n           [4, 5, 6],\n           [1, 2, 3]])\n    \"\"\"\n    if arr.ndim &gt; 1:\n        arr[[frm, to], :] = arr[[to, frm], :]\n    else:\n        arr[frm], arr[to] = arr[to], arr[frm]\n</code></pre>"},{"location":"reference/nelpy/analysis/ergodic/","title":"nelpy.analysis.ergodic","text":""},{"location":"reference/nelpy/analysis/ergodic/#nelpy.analysis.ergodic--modergodic-summary-measures-for-ergodic-markov-chains","title":":mod:<code>ergodic</code> --- summary measures for ergodic Markov chains","text":""},{"location":"reference/nelpy/analysis/ergodic/#nelpy.analysis.ergodic.fmpt","title":"<code>fmpt(P)</code>","text":"<p>Calculates the matrix of first mean passage times for an ergodic transition probability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <p>an ergodic Markov transition probability matrix</p> required <p>Returns:</p> Name Type Description <code>M</code> <code>array(kxk)</code> <p>elements are the expected value for the number of intervals required for  a chain starting in state i to first enter state j If i=j then this is the recurrence time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n&gt;&gt;&gt; fm = fmpt(p)\n&gt;&gt;&gt; fm\narray([[ 2.5       ,  4.        ,  3.33333333],\n       [ 2.66666667,  5.        ,  2.66666667],\n       [ 3.33333333,  4.        ,  2.5       ]])\nThus, if it is raining today in Oz we can expect a nice day to come\nalong in another 4 days, on average, and snow to hit in 3.33 days. We can\nexpect another rainy day in 2.5 days. If it is nice today in Oz, we would\nexperience a change in the weather (either rain or snow) in 2.67 days from\ntoday. (That wicked witch can only die once so I reckon that is the\nultimate absorbing state).\n</code></pre> <p>Notes ----- Uses formulation (and examples on p. 218) in Kemeny and Snell (1976) [1]_ References</p> <p>.. [1] Kemeny, John, G. and J. Laurie Snell (1976) Finite Markov    Chains. Springer-Verlag. Berlin</p> Source code in <code>nelpy/analysis/ergodic.py</code> <pre><code>def fmpt(P):\n    \"\"\"\n    Calculates the matrix of first mean passage times for an\n    ergodic transition probability matrix.\n    Parameters\n    ----------\n    P    : array (kxk)\n           an ergodic Markov transition probability matrix\n    Returns\n    -------\n    M    : array (kxk)\n           elements are the expected value for the number of intervals\n           required for  a chain starting in state i to first enter state j\n           If i=j then this is the recurrence time.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n    &gt;&gt;&gt; fm = fmpt(p)\n    &gt;&gt;&gt; fm\n    array([[ 2.5       ,  4.        ,  3.33333333],\n           [ 2.66666667,  5.        ,  2.66666667],\n           [ 3.33333333,  4.        ,  2.5       ]])\n    Thus, if it is raining today in Oz we can expect a nice day to come\n    along in another 4 days, on average, and snow to hit in 3.33 days. We can\n    expect another rainy day in 2.5 days. If it is nice today in Oz, we would\n    experience a change in the weather (either rain or snow) in 2.67 days from\n    today. (That wicked witch can only die once so I reckon that is the\n    ultimate absorbing state).\n\n    Notes -----\n    Uses formulation (and examples on p. 218) in Kemeny and Snell (1976) [1]_\n    References\n    ----------\n\n    .. [1] Kemeny, John, G. and J. Laurie Snell (1976) Finite Markov\n       Chains. Springer-Verlag. Berlin\n    \"\"\"\n    P = np.asarray(P)\n    A = np.zeros_like(P)\n    ss = steady_state(P)\n    k = ss.shape[0]\n    for i in range(k):\n        A[:, i] = ss.flatten()\n    A = A.T\n    identity_matrix = np.identity(k)\n    Z = la.inv(identity_matrix - P + A)\n    E = np.ones_like(Z)\n    D = np.diag(1.0 / np.diag(A))\n    Zdg = np.diag(np.diag(Z))\n    M = (identity_matrix - Z + np.multiply(E, Zdg)) @ D\n    return M\n</code></pre>"},{"location":"reference/nelpy/analysis/ergodic/#nelpy.analysis.ergodic.set_self_transition_zero","title":"<code>set_self_transition_zero(x)</code>","text":"<p>Set cost/length of self-transition to zero.</p> Source code in <code>nelpy/analysis/ergodic.py</code> <pre><code>def set_self_transition_zero(x):\n    \"\"\"Set cost/length of self-transition to zero.\"\"\"\n    np.fill_diagonal(x, 0.0)\n</code></pre>"},{"location":"reference/nelpy/analysis/ergodic/#nelpy.analysis.ergodic.steady_state","title":"<code>steady_state(P)</code>","text":"<p>Calculates the steady state probability vector for a regular Markov transition matrix P</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <pre><code>   an ergodic Markov transition probability matrix\n</code></pre> required <p>Returns:</p> Name Type Description <code>implicit</code> <code>array(kx1)</code> <p>steady state distribution</p> <p>Examples:</p> <p>Taken from Kemeny and Snell. [1]_ Land of Oz example where the states are Rain, Nice and Snow - so there is 25 percent chance that if it rained in Oz today, it will snow tomorrow, while if it snowed today in Oz there is a 50 percent chance of snow again tomorrow and a 25 percent chance of a nice day (nice, like when the witch with the monkeys is melting).</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n&gt;&gt;&gt; steady_state(p)\narray([[ 0.4],\n       [ 0.2],\n       [ 0.4]])\nThus, the long run distribution for Oz is to have 40 percent of the\ndays classified as Rain, 20 percent as Nice, and 40 percent as Snow\n(states are mutually exclusive).\n</code></pre> Source code in <code>nelpy/analysis/ergodic.py</code> <pre><code>def steady_state(P):\n    \"\"\"\n    Calculates the steady state probability vector for a regular Markov\n    transition matrix P\n    Parameters\n    ----------\n    P        : array (kxk)\n               an ergodic Markov transition probability matrix\n    Returns\n    -------\n    implicit : array (kx1)\n               steady state distribution\n    Examples\n    --------\n    Taken from Kemeny and Snell. [1]_ Land of Oz example where the states are\n    Rain, Nice and Snow - so there is 25 percent chance that if it\n    rained in Oz today, it will snow tomorrow, while if it snowed today in\n    Oz there is a 50 percent chance of snow again tomorrow and a 25\n    percent chance of a nice day (nice, like when the witch with the monkeys\n    is melting).\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n    &gt;&gt;&gt; steady_state(p)\n    array([[ 0.4],\n           [ 0.2],\n           [ 0.4]])\n    Thus, the long run distribution for Oz is to have 40 percent of the\n    days classified as Rain, 20 percent as Nice, and 40 percent as Snow\n    (states are mutually exclusive).\n    \"\"\"\n\n    v, d = la.eig(np.transpose(P))\n\n    # for a regular P maximum eigenvalue will be 1\n    mv = max(v.real)  # Use real part for comparison\n    # find its position\n    i = v.real.tolist().index(mv)\n\n    # normalize eigenvector corresponding to the eigenvalue 1\n    # Take real part to avoid complex warning\n    eigenvector = d[:, i].real\n    return eigenvector / np.sum(eigenvector)\n</code></pre>"},{"location":"reference/nelpy/analysis/ergodic/#nelpy.analysis.ergodic.var_fmpt","title":"<code>var_fmpt(P)</code>","text":"<p>Variances of first mean passage times for an ergodic transition probability matrix</p> <p>Parameters:</p> Name Type Description Default <code>P</code> <p>an ergodic Markov transition probability matrix</p> required <p>Returns:</p> Name Type Description <code>implic</code> <code>array(kxk)</code> <p>elements are the variances for the number of intervals required for  a chain starting in state i to first enter state j</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n&gt;&gt;&gt; vfm = var_fmpt(p)\n&gt;&gt;&gt; vfm\narray([[  5.58333333,  12.        ,   6.88888889],\n       [  6.22222222,  12.        ,   6.22222222],\n       [  6.88888889,  12.        ,   5.58333333]])\n</code></pre> Notes <p>Uses formulation (and examples on p. 83) in Kemeny and Snell (1976) [1]_</p> References <p>.. [1] Kemeny, John, G. and J. Laurie Snell (1976) Finite Markov    Chains. Springer-Verlag. Berlin</p> Source code in <code>nelpy/analysis/ergodic.py</code> <pre><code>def var_fmpt(P):\n    \"\"\"\n    Variances of first mean passage times for an ergodic transition\n    probability matrix\n    Parameters\n    ----------\n    P    : array (kxk)\n           an ergodic Markov transition probability matrix\n    Returns\n    -------\n    implic : array (kxk)\n             elements are the variances for the number of intervals\n             required for  a chain starting in state i to first enter state j\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; p = np.array([[0.5, 0.25, 0.25], [0.5, 0, 0.5], [0.25, 0.25, 0.5]])\n    &gt;&gt;&gt; vfm = var_fmpt(p)\n    &gt;&gt;&gt; vfm\n    array([[  5.58333333,  12.        ,   6.88888889],\n           [  6.22222222,  12.        ,   6.22222222],\n           [  6.88888889,  12.        ,   5.58333333]])\n\n    Notes\n    -----\n    Uses formulation (and examples on p. 83) in Kemeny and Snell (1976) [1]_\n    References\n    ----------\n\n    .. [1] Kemeny, John, G. and J. Laurie Snell (1976) Finite Markov\n       Chains. Springer-Verlag. Berlin\n    \"\"\"\n    P = np.asarray(P)\n    A = np.linalg.matrix_power(P, 1000)\n    n, k = A.shape\n    identity_matrix = np.identity(k)\n    Z = la.inv(identity_matrix - P + A)\n    E = np.ones_like(Z)\n    D = np.diag(1.0 / np.diag(A))\n    Zdg = np.diag(np.diag(Z))\n    M = (identity_matrix - Z + np.multiply(E, Zdg)) * D\n    ZM = Z @ M\n    ZMdg = np.diag(np.diag(ZM))\n    W = M @ (2 * Zdg @ D - identity_matrix) + 2 * (ZM - np.multiply(E, ZMdg))\n    return W - np.multiply(M, M)\n</code></pre>"},{"location":"reference/nelpy/analysis/hmm_sparsity/","title":"nelpy.analysis.hmm_sparsity","text":""},{"location":"reference/nelpy/analysis/hmm_sparsity/#nelpy.analysis.hmm_sparsity.HMMSurrogate","title":"<code>HMMSurrogate</code>","text":"Source code in <code>nelpy/analysis/hmm_sparsity.py</code> <pre><code>class HMMSurrogate:\n    def __init__(\n        self,\n        *,\n        kind,\n        st,\n        num_states=None,\n        ds=None,\n        test_size=None,\n        random_state=None,\n        verbose=False,\n        description=\"\",\n        PBE_idx=None,\n    ):\n        \"\"\"\n        Initialize an HMMSurrogate for generating surrogate HMM data.\n\n        Parameters\n        ----------\n        kind : str\n            One of ['actual', 'incoherent', 'coherent', 'poisson', 'unit_id', 'spike_id'].\n        st : SpikeTrainArray\n            SpikeTrainArray restricted to the PBEs.\n        num_states : int, optional\n            Number of states in the hidden Markov model. Default is 50.\n        ds : float, optional\n            Bin size for PBEs. Default is 0.02 (=20 ms).\n        test_size : float, optional\n            Proportion of data to use as test data. Default is 0.2 (=20%).\n        random_state : int, optional\n            Random seed for numpy, default is 1.\n        verbose : bool, optional\n            If True, print progress information. Default is False.\n        description : str, optional\n            Description of the surrogate.\n        PBE_idx : tuple of lists, optional\n            (PBE_trainidx, PBE_testidx)\n\n        Notes\n        -----\n        All the shuffle methods currently operate directly on PBEs_train, and PBEs_test is left unmodified. If you want to evaluate test sets, this behavior needs to change!\n        \"\"\"\n\n        if kind == \"actual\":\n            self.shuffle = self._do_nothing\n        elif kind == \"incoherent\":\n            self.shuffle = self._within_event_incoherent_shuffle\n        elif kind == \"coherent\":\n            self.shuffle = self._within_event_coherent_shuffle\n        elif kind == \"poisson\":\n            self.shuffle = self._within_event_homogeneous_poisson\n        elif kind == \"unit_id\":\n            self.shuffle = self._within_event_unit_id_shuffle\n        elif kind == \"spike_id\":\n            self.shuffle = self._spike_id_shuffle\n        else:\n            raise ValueError(\"unknown data surrogate kind!\")\n\n        if num_states is None:\n            num_states = 50\n        if ds is None:\n            ds = 0.02  # 20 ms bin size\n        if test_size is None:\n            test_size = 0.2\n        if random_state is not None:\n            np.random.seed(random_state)\n\n        self._random_state = random_state\n        self._st = st\n        self._num_states = num_states\n        self._ds = ds\n        self._test_size = test_size\n        self._random_state = random_state\n        self._verbose = verbose\n        self.label = kind\n        self.description = description\n\n        self._preprocess(PBE_idx=PBE_idx)\n        self.hmm = PoissonHMM(\n            n_components=self._num_states,\n            random_state=self._random_state,\n            verbose=self._verbose,\n        )\n\n        self.results = defaultdict(list)\n\n    @property\n    def transmat(self):\n        return self.hmm.transmat_\n\n    @property\n    def means(self):\n        return self.hmm.means_\n\n    def clear_results(self):\n        self.results = defaultdict(list)\n\n    def score_gini(self, kind=\"tmat_departure\"):\n        \"\"\"Calculate and record the gini coefficients.\"\"\"\n        # kinds = ['tmat_arrival', 'tmat_departure', 'tmat', 'lambda', 'lambda_across_states', 'lambda_across_units']\n\n        gini_distr = []\n\n        # transmat departure sparsity\n        if kind == \"tmat_departure\":\n            for row in self.hmm.transmat_:\n                gini_distr.append(self._gini(row))\n            self.results[\"gini_tmat_departure\"].append(gini_distr)\n\n        # transmat arrival sparsity\n        if kind == \"tmat_arrival\":\n            for row in self.hmm.transmat_.T:\n                gini_distr.append(self._gini(row))\n            self.results[\"gini_tmat_arrival\"].append(gini_distr)\n\n        # transmat sparsity\n        if kind == \"tmat\":\n            gini_distr = self._gini(self.hmm.transmat_)\n            self.results[\"gini_tmat\"].append(gini_distr)\n\n        # lambda sparsity\n        if kind == \"lambda\":\n            gini_distr = self._gini(self.hmm.means_)\n            self.results[\"gini_lambda\"].append(gini_distr)\n\n        # lambda_across_states sparsity\n        if kind == \"lambda_across_states\":\n            for row in self.hmm.means_.T:\n                gini_distr.append(self._gini(row))\n            self.results[\"gini_lambda_across_states\"].append(gini_distr)\n\n        # lambda_across_units sparsity\n        if kind == \"lambda_across_units\":\n            for row in self.hmm.means_:\n                gini_distr.append(self._gini(row))\n            self.results[\"gini_lambda_across_units\"].append(gini_distr)\n\n    def score_bottleneck_ratio(self, n_samples=50000):\n        def Qij(i, j, P, pi):\n            return pi[i] * P[i, j]\n\n        def QAB(A, B, P, pi):\n            sumQ = 0\n            for i in A:\n                for j in B:\n                    sumQ += Qij(i, j, P, pi)\n            return sumQ\n\n        def complement(S, Omega):\n            return Omega - S\n\n        def Pi(S, pi):\n            sumS = 0\n            for i in S:\n                sumS += pi[i]\n            return sumS\n\n        def Phi(S, P, pi, Omega):\n            Sc = complement(S, Omega)\n            return QAB(S, Sc, P, pi) / Pi(S, pi)\n\n        P = self.hmm.transmat_\n        num_states = self._num_states\n        Omega = set(range(num_states))\n        pi_ = steady_state(P).real\n\n        min_Phi = 1\n        for nn in range(n_samples):\n            n_samp_in_subset = np.random.randint(1, num_states - 1)\n            S = set(np.random.choice(num_states, n_samp_in_subset, replace=False))\n            while Pi(S, pi_) &gt; 0.5:\n                n_samp_in_subset -= 1\n                if n_samp_in_subset &lt; 1:\n                    n_samp_in_subset = 1\n                S = set(np.random.choice(num_states, n_samp_in_subset, replace=False))\n            candidate_Phi = Phi(S, P, pi_, Omega)\n            if candidate_Phi &lt; min_Phi:\n                min_Phi = candidate_Phi\n                if self._verbose:\n                    print(\"{}: {} (|S| = {})\".format(nn, min_Phi, len(S)))\n\n        self.results[\"bottleneck\"].append(min_Phi)\n\n    def score_mixing_time(self, eps=0.25):\n        raise NotImplementedError\n\n    def score_spectrum(self):\n        pass\n\n    def _preprocess_PBEs(self, PBE_idx=None):\n        \"\"\"used for most types of shuffles\"\"\"\n        # compute PBEs\n        self.PBEs = self._st.bin(ds=self._ds)\n\n        if self.PBEs.n_epochs == 1:\n            raise ValueError(\n                \"spike train is continuous, and does not have more than one event!\"\n            )\n\n        if PBE_idx is not None:\n            self._trainidx, self._testidx = PBE_idx  # tuple unpacking\n        else:\n            # split into train and test data\n            if self._random_state is not None:\n                self._trainidx, self._testidx = train_test_split(\n                    np.arange(self.PBEs.n_epochs),\n                    test_size=self._test_size,\n                    random_state=self._random_state,\n                )\n            else:\n                self._trainidx, self._testidx = train_test_split(\n                    np.arange(self.PBEs.n_epochs),\n                    test_size=self._test_size,\n                    random_state=1,\n                )\n\n        self._trainidx.sort()\n        self._testidx.sort()\n\n        self.PBEs_train = self.PBEs[self._trainidx]\n        self.PBEs_test = self.PBEs[self._testidx]\n\n    def _preprocess_STs(self):\n        \"\"\"used for spike ID shuffle, where spike times must be shuffled\"\"\"\n        # split into train and test data\n        self._preprocess_PBEs()\n        self._st_flat = self._st.flatten().time.squeeze()\n\n    def _preprocess(self, PBE_idx=None):\n        self._preprocess_PBEs(PBE_idx=PBE_idx)\n        if self.label == \"spike_id\":\n            self._preprocess_STs()\n\n    def fit(self):\n        self.hmm = PoissonHMM(n_components=self._num_states, verbose=self._verbose)\n\n        # train HMM on all training PBEs\n        self.hmm.fit(self.PBEs_train)\n\n        # re-order states of hmm\n        transmat_order = self.hmm.get_state_order(\"transmat\")\n        self.hmm.reorder_states(transmat_order)\n\n    def score_loglikelihood(self):\n        # record log-likelihood on both train and validation sets after fitting model:\n\n        # train_LL = np.array(self.hmm.score(self.PBEs_train)).sum() # one scalar for each sequence in training set\n        # test_LL = np.array(self.hmm.score(self.PBEs_test)).sum() # one scalar for each sequence in training set\n        train_LL = self.hmm.score(\n            self.PBEs_train\n        )  # one scalar for each sequence in training set\n        test_LL = self.hmm.score(\n            self.PBEs_test\n        )  # one scalar for each sequence in training set\n\n        self.results[\"loglikelihood_train\"].extend(train_LL)\n        self.results[\"loglikelihood_test\"].extend(test_LL)\n\n    def _gini(self, array):\n        \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n        # https://github.com/oliviaguest/gini\n        # based on bottom eq:\n        # http://www.statsdirect.com/help/generatedimages/equations/equation154.svg\n        # from:\n        # http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n        # All values are treated equally, arrays must be 1d:\n        array = array.flatten()\n        if np.amin(array) &lt; 0:\n            # Values cannot be negative:\n            array -= np.amin(array)\n        # Values cannot be 0:\n        array += 0.0000001\n        # Values must be sorted:\n        array = np.sort(array)\n        # Index per array element:\n        index = np.arange(1, array.shape[0] + 1)\n        # Number of array elements:\n        n = array.shape[0]\n        # Gini coefficient:\n        return (np.sum((2 * index - n - 1) * array)) / (n * np.sum(array))\n\n    def _do_nothing(self, kind=\"train\"):\n        \"\"\"Do nothing to the data.\"\"\"\n        pass\n\n    def _within_event_coherent_shuffle(self, kind=\"train\"):\n        \"\"\"Time swap on BinnedSpikeTrainArray, swapping only within each epoch.\"\"\"\n        if kind == \"train\":\n            bst = self.PBEs_train\n        elif kind == \"test\":\n            bst = self.PBEs_test\n        else:\n            raise ValueError(\"kind '{}' not understood!\".format(kind))\n\n        out = copy.deepcopy(bst)  # should this be deep?\n        shuffled = np.arange(bst.n_bins)\n        edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n        for ii in range(bst.n_epochs):\n            segment = shuffled[edges[ii] : edges[ii + 1]]\n            shuffled[edges[ii] : edges[ii + 1]] = np.random.permutation(segment)\n\n        out._data = out._data[:, shuffled]\n\n        if kind == \"train\":\n            self.PBEs_train = out\n        else:\n            self.PBEs_test = out\n\n    def _within_event_incoherent_shuffle(self, kind=\"train\"):\n        \"\"\"Time cycle on BinnedSpikeTrainArray, cycling only within each epoch.\n        We cycle each unit independently, within each epoch.\n        \"\"\"\n        if kind == \"train\":\n            bst = self.PBEs_train\n        elif kind == \"test\":\n            bst = self.PBEs_test\n        else:\n            raise ValueError(\"kind '{}' not understood!\".format(kind))\n\n        out = copy.deepcopy(bst)  # should this be deep?\n        data = out._data\n        edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n\n        for uu in range(bst.n_units):\n            for ii in range(bst.n_epochs):\n                segment = np.squeeze(data[uu, edges[ii] : edges[ii + 1]])\n                segment = np.roll(segment, np.random.randint(len(segment)))\n                data[uu, edges[ii] : edges[ii + 1]] = segment\n\n        if kind == \"train\":\n            self.PBEs_train = out\n        else:\n            self.PBEs_test = out\n\n    def _within_event_homogeneous_poisson(self, kind=\"train\"):\n        \"\"\"Estimate mean firing rates across all events, and then generate\n        homogeneous Poisson spikes within each event. That is, ISIs do not\n        span multiple events, but are started fresh within each event.\"\"\"\n\n        if kind == \"train\":\n            bst = self.PBEs_train\n        elif kind == \"test\":\n            bst = self.PBEs_test\n        else:\n            raise ValueError(\"kind '{}' not understood!\".format(kind))\n\n        firing_rates = bst.n_spikes / bst.support.duration  # firing rate in Hz\n\n        spikes = []\n\n        for rate in firing_rates:\n            unit_spikes = []\n            for start, stop in bst.support.time:\n                evt_duration = stop - start\n                n_evt_spikes = np.random.poisson(rate * evt_duration)\n                spike_times = start + np.random.uniform(0, evt_duration, n_evt_spikes)\n                unit_spikes.extend(spike_times)\n\n            spikes.append(unit_spikes)\n\n        support = bst.support.expand(bst.ds / 2, direction=\"stop\")\n        poisson_st = SpikeTrainArray(\n            timestamps=spikes, support=support, unit_ids=bst.unit_ids\n        )\n\n        if kind == \"train\":\n            self.PBEs_train = poisson_st.bin(ds=bst.ds)\n        else:\n            self.PBEs_test = poisson_st.bin(ds=bst.ds)\n\n    def _spike_id_shuffle(self, proportional=False, kind=\"train\"):\n        \"\"\"Shuffle the cell/unit identity of each spike, independently.\n\n        If 'proportional' is True, then sample spike IDs in proportion to\n        the original data numbers of spikes. Otherwise, sample spike IDs\n        uniformly.\n        \"\"\"\n\n        all_spiketimes = self._st_flat\n        spike_ids = np.zeros(\n            len(all_spiketimes)\n        )  # WARNING! if we don't have the same unit_ids, it could cause problems later on...\n\n        if proportional:\n            n_spikes = self._st.n_spikes\n        else:\n            n_spikes = np.ones(self._st.n_units) * np.floor(\n                self._st.n_spikes.sum() / self._st.n_units\n            )\n\n        pointer = 0\n        for uu, n_spikes in enumerate(n_spikes):\n            spike_ids[pointer : pointer + int(n_spikes)] = uu\n            pointer += int(n_spikes)\n\n        # permute spike IDs\n        spike_ids = np.random.permutation(spike_ids)\n\n        # now re-assign all spike times according to sampling above\n        spikes = []\n        for unit in range(self._st.n_units):\n            spikes.append(all_spiketimes[spike_ids == unit])\n\n        shuffled_st = SpikeTrainArray(timestamps=spikes, support=self._st.support)\n\n        if kind == \"train\":\n            self.PBEs_train = shuffled_st.bin(ds=self._ds)[self._trainidx]\n        elif kind == \"test\":\n            self.PBEs_test = shuffled_st.bin(ds=self._ds)[self._testidx]\n        else:\n            raise ValueError(\"kind '{}' not understood!\".format(kind))\n\n    def _within_event_unit_id_shuffle(self, kind=\"train\"):\n        \"\"\"Unit ID shuffle on BinnedSpikeTrainArray, shuffling independently within each epoch.\"\"\"\n\n        if kind == \"train\":\n            bst = self.PBEs_train\n        elif kind == \"test\":\n            bst = self.PBEs_test\n        else:\n            raise ValueError(\"kind '{}' not understood!\".format(kind))\n\n        out = copy.deepcopy(bst)  # should this be deep?\n        data = out._data\n        edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n\n        unit_list = np.arange(bst.n_units)\n\n        for ii in range(bst.n_epochs):\n            segment = data[:, edges[ii] : edges[ii + 1]]\n            out._data[:, edges[ii] : edges[ii + 1]] = segment[\n                np.random.permutation(unit_list)\n            ]\n\n        if kind == \"train\":\n            self.PBEs_train = out\n        else:\n            self.PBEs_test = out\n</code></pre>"},{"location":"reference/nelpy/analysis/hmm_sparsity/#nelpy.analysis.hmm_sparsity.HMMSurrogate.score_gini","title":"<code>score_gini(kind='tmat_departure')</code>","text":"<p>Calculate and record the gini coefficients.</p> Source code in <code>nelpy/analysis/hmm_sparsity.py</code> <pre><code>def score_gini(self, kind=\"tmat_departure\"):\n    \"\"\"Calculate and record the gini coefficients.\"\"\"\n    # kinds = ['tmat_arrival', 'tmat_departure', 'tmat', 'lambda', 'lambda_across_states', 'lambda_across_units']\n\n    gini_distr = []\n\n    # transmat departure sparsity\n    if kind == \"tmat_departure\":\n        for row in self.hmm.transmat_:\n            gini_distr.append(self._gini(row))\n        self.results[\"gini_tmat_departure\"].append(gini_distr)\n\n    # transmat arrival sparsity\n    if kind == \"tmat_arrival\":\n        for row in self.hmm.transmat_.T:\n            gini_distr.append(self._gini(row))\n        self.results[\"gini_tmat_arrival\"].append(gini_distr)\n\n    # transmat sparsity\n    if kind == \"tmat\":\n        gini_distr = self._gini(self.hmm.transmat_)\n        self.results[\"gini_tmat\"].append(gini_distr)\n\n    # lambda sparsity\n    if kind == \"lambda\":\n        gini_distr = self._gini(self.hmm.means_)\n        self.results[\"gini_lambda\"].append(gini_distr)\n\n    # lambda_across_states sparsity\n    if kind == \"lambda_across_states\":\n        for row in self.hmm.means_.T:\n            gini_distr.append(self._gini(row))\n        self.results[\"gini_lambda_across_states\"].append(gini_distr)\n\n    # lambda_across_units sparsity\n    if kind == \"lambda_across_units\":\n        for row in self.hmm.means_:\n            gini_distr.append(self._gini(row))\n        self.results[\"gini_lambda_across_units\"].append(gini_distr)\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/","title":"nelpy.analysis.replay","text":""},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.column_cycle_array","title":"<code>column_cycle_array(posterior, amt=None)</code>","text":"<p>Cycle each column of the posterior matrix by a random or specified amount.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>amt</code> <code>array - like or None</code> <p>Amount to cycle each column. If None, random cycling is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Cycled posterior matrix.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def column_cycle_array(posterior, amt=None):\n    \"\"\"\n    Cycle each column of the posterior matrix by a random or specified amount.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    amt : array-like or None, optional\n        Amount to cycle each column. If None, random cycling is used.\n\n    Returns\n    -------\n    out : np.ndarray\n        Cycled posterior matrix.\n    \"\"\"\n    out = copy.deepcopy(posterior)\n    rows, cols = posterior.shape\n\n    if amt is None:\n        for col in range(cols):\n            if np.isnan(np.sum(posterior[:, col])):\n                continue\n            else:\n                out[:, col] = np.roll(posterior[:, col], np.random.randint(1, rows))\n    else:\n        if len(amt) == cols:\n            for col in range(cols):\n                if np.isnan(np.sum(posterior[:, col])):\n                    continue\n                else:\n                    out[:, col] = np.roll(posterior[:, col], int(amt[col]))\n        else:\n            raise TypeError(\"amt does not seem to be the correct shape!\")\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.get_line_of_best_Davidson_score","title":"<code>get_line_of_best_Davidson_score(bst, tuningcurve, w=3, n_samples=50000)</code>","text":"<p>Find the best-fit line through the decoded posterior for a single event using the Davidson et al. 2009 method.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing a single event (PBE).</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for decoding.</p> required <code>w</code> <code>int</code> <p>Half-width of the band for scoring (default is 3).</p> <code>3</code> <code>n_samples</code> <code>int</code> <p>Number of random lines to sample (default is 50000).</p> <code>50000</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>The best trajectory score found.</p> <code>ri</code> <code>ndarray</code> <p>Row indices of the best-fit line.</p> <code>ci</code> <code>ndarray</code> <p>Column indices (time bins).</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If more than one event is passed in bst.</p> Notes <p>This function decodes the posterior, samples random lines, and finds the one with the highest score.</p> References <p>Davidson TJ, Kloosterman F, Wilson MA (2009)     Hippocampal replay of extended experience. Neuron 63:497\u2013507</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def get_line_of_best_Davidson_score(bst, tuningcurve, w=3, n_samples=50000):\n    \"\"\"\n    Find the best-fit line through the decoded posterior for a single event using the Davidson et al. 2009 method.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing a single event (PBE).\n    tuningcurve : TuningCurve1D\n        Tuning curve for decoding.\n    w : int, optional\n        Half-width of the band for scoring (default is 3).\n    n_samples : int, optional\n        Number of random lines to sample (default is 50000).\n\n    Returns\n    -------\n    score : float\n        The best trajectory score found.\n    ri : np.ndarray\n        Row indices of the best-fit line.\n    ci : np.ndarray\n        Column indices (time bins).\n\n    Raises\n    ------\n    TypeError\n        If more than one event is passed in bst.\n\n    Notes\n    -----\n    This function decodes the posterior, samples random lines, and finds the one with the highest score.\n\n    References\n    ----------\n    Davidson TJ, Kloosterman F, Wilson MA (2009)\n        Hippocampal replay of extended experience. Neuron 63:497\u2013507\n    \"\"\"\n    tc = tuningcurve\n\n    if bst.n_epochs &gt; 1:\n        raise TypeError(\"You can only pass one PBE at a time!\")\n\n    def sub2ind(array_shape, rows, cols):\n        return rows * array_shape[1] + cols\n\n    def calc_ri(NT, NP, phi, rho, ci, ci_mid, ri_mid):\n        \"\"\"\n        Note: Not matrix dimensions! Think of dim0 as\n        x coordinate, dim1 as y coordinate, etc.\n        \"\"\"\n        ri = (rho - (ci - ci_mid) * np.cos(phi)) / np.sin(phi) + ri_mid\n        ri = np.around(ri).astype(int)  # Find nearest position bin\n\n        return ri\n\n    def _score_line_ri_ci(posterior, precond_posterior, NT, NP, ri, ci):\n        scores_outside_track = np.nanmedian(\n            posterior[:, (ri &gt; NP - 1) | (ri &lt; 0)], axis=0\n        )\n\n        coords = sub2ind(posterior.shape, ri, ci)\n        coords = coords[(ri &lt; NP) &amp; (ri &gt;= 0)]\n        scores_within_track = np.take(precond_posterior, coords)\n\n        num_empty_bins = (\n            np.isnan(scores_outside_track).sum() + np.isnan(scores_within_track).sum()\n        )\n\n        score_within_track = np.nansum(scores_within_track)\n        if (score_within_track) &gt; 0 &amp; (num_empty_bins &gt; 0):\n            temp = np.nanmedian(scores_within_track) * num_empty_bins\n        else:\n            temp = 0\n        score_outside_track = np.nansum(scores_outside_track) + temp\n\n        score = score_within_track + score_outside_track\n\n        # we divide by NT later on to be more efficient\n        # final_score = score/NT\n\n        return score\n\n    def find_best_line(posterior, precond_posterior, phis, rhos):\n        best_score = 0\n        best_ri = []\n\n        NP, NT = posterior.shape\n\n        ci_mid = (NT + 1) / 2  # CONST\n        ri_mid = (NP + 1) / 2  # CONST\n        ci = np.arange(NT)  # CONST\n\n        for phi, rho in zip(phis, rhos):\n            # parameterize line\n            ri = calc_ri(NT, NP, phi, rho, ci, ci_mid, ri_mid)\n\n            score = _score_line_ri_ci(posterior, precond_posterior, NT, NP, ri, ci)\n\n            if score &gt; best_score:\n                best_score = score\n                best_ri = ri\n\n        score = (\n            _score_line_ri_ci(posterior, precond_posterior, NT, NP, best_ri, ci) / NT\n        )\n\n        return score, best_ri\n\n    # decode neural activity\n    posterior_array, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tc, xmax=310)\n\n    # precondition matrix kernel for banded summation\n    k = np.zeros((2 * w + 1, 3))\n    k[:, 1] = 1\n\n    NP, NT = posterior_array.shape\n\n    D = np.sqrt((NT - 1) ** 2 + (NP - 1) ** 2)\n    phi_range = (-0.5 * np.pi, 0.5 * np.pi)\n    rho_range = (-0.5 * D, 0.5 * D)\n\n    phis = phi_range[0] + np.random.rand(n_samples) * (phi_range[1] - phi_range[0])\n    phis[(phis &lt; 0.0001) &amp; (phis &gt; -0.0001)] = 0.0001\n    rhos = rho_range[0] + np.random.rand(n_samples) * (rho_range[1] - rho_range[0])\n\n    precond_posterior = convolve(posterior_array, k, mode=\"constant\", cval=0.0)\n\n    score, ri = find_best_line(\n        posterior=posterior_array,\n        precond_posterior=precond_posterior,\n        phis=phis,\n        rhos=rhos,\n    )\n\n    ci = np.arange(NT)\n\n    return score, ri, ci\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.get_significant_events","title":"<code>get_significant_events(scores, shuffled_scores, q=95)</code>","text":"<p>Return the significant events based on percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray</code> <p>Scores for each event.</p> required <code>shuffled_scores</code> <code>ndarray</code> <p>Shuffled scores for each event and shuffle.</p> required <code>q</code> <code>float</code> <p>Percentile to compute (default is 95).</p> <code>95</code> <p>Returns:</p> Name Type Description <code>sig_event_idx</code> <code>ndarray</code> <p>Indices of significant events.</p> <code>pvalues</code> <code>ndarray</code> <p>Monte Carlo p-values for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def get_significant_events(scores, shuffled_scores, q=95):\n    \"\"\"\n    Return the significant events based on percentiles.\n\n    Parameters\n    ----------\n    scores : np.ndarray\n        Scores for each event.\n    shuffled_scores : np.ndarray\n        Shuffled scores for each event and shuffle.\n    q : float, optional\n        Percentile to compute (default is 95).\n\n    Returns\n    -------\n    sig_event_idx : np.ndarray\n        Indices of significant events.\n    pvalues : np.ndarray\n        Monte Carlo p-values for each event.\n    \"\"\"\n\n    n, _ = shuffled_scores.shape\n    r = np.sum(shuffled_scores &gt;= scores, axis=0)\n    pvalues = (r + 1) / (n + 1)\n\n    sig_event_idx = np.argwhere(\n        scores &gt; np.percentile(shuffled_scores, axis=0, q=q)\n    ).squeeze()\n\n    return np.atleast_1d(sig_event_idx), np.atleast_1d(pvalues)\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.incoherent_shuffle_bst","title":"<code>incoherent_shuffle_bst(bst)</code>","text":"<p>Incoherent shuffle on BinnedSpikeTrainArray, swapping only within each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to shuffle.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Shuffled spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def incoherent_shuffle_bst(bst):\n    \"\"\"\n    Incoherent shuffle on BinnedSpikeTrainArray, swapping only within each epoch.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to shuffle.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Shuffled spike train array.\n    \"\"\"\n    out = copy.deepcopy(bst)  # should this be deep? YES! Oh my goodness, yes!\n    data = out._data\n    edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n\n    for uu in range(bst.n_units):\n        for ii in range(bst.n_epochs):\n            segment = np.atleast_1d(np.squeeze(data[uu, edges[ii] : edges[ii + 1]]))\n            segment = np.roll(segment, np.random.randint(len(segment)))\n            data[uu, edges[ii] : edges[ii + 1]] = segment\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.linregress_array","title":"<code>linregress_array(posterior)</code>","text":"<p>Perform linear regression on the posterior matrix, and return the slope, intercept, and R^2 value.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <p>Returns:</p> Name Type Description <code>slope</code> <code>float</code> <p>Slope of the best-fit line.</p> <code>intercept</code> <code>float</code> <p>Intercept of the best-fit line.</p> <code>r2</code> <code>float</code> <p>R^2 value of the fit.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def linregress_array(posterior):\n    \"\"\"\n    Perform linear regression on the posterior matrix, and return the slope, intercept, and R^2 value.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n\n    Returns\n    -------\n    slope : float\n        Slope of the best-fit line.\n    intercept : float\n        Intercept of the best-fit line.\n    r2 : float\n        R^2 value of the fit.\n    \"\"\"\n\n    mode_pth = get_mode_pth_from_array(posterior)\n\n    y = mode_pth\n    x = np.arange(len(y))\n    x = x[~np.isnan(y)]\n    y = y[~np.isnan(y)]\n\n    if len(y) &gt; 0:\n        slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)\n        return slope, intercept, rvalue**2\n    else:\n        return np.nan, np.nan, np.nan\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.linregress_bst","title":"<code>linregress_bst(bst, tuningcurve)</code>","text":"<p>Perform linear regression on all the events in bst, and return the slopes, intercepts, and R^2 values.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for decoding.</p> required <p>Returns:</p> Name Type Description <code>slopes</code> <code>ndarray</code> <p>Slopes for each event.</p> <code>intercepts</code> <code>ndarray</code> <p>Intercepts for each event.</p> <code>r2values</code> <code>ndarray</code> <p>R^2 values for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def linregress_bst(bst, tuningcurve):\n    \"\"\"\n    Perform linear regression on all the events in bst, and return the slopes, intercepts, and R^2 values.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve for decoding.\n\n    Returns\n    -------\n    slopes : np.ndarray\n        Slopes for each event.\n    intercepts : np.ndarray\n        Intercepts for each event.\n    r2values : np.ndarray\n        R^2 values for each event.\n    \"\"\"\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    slopes = np.zeros(bst.n_epochs)\n    intercepts = np.zeros(bst.n_epochs)\n    r2values = np.zeros(bst.n_epochs)\n    for idx in range(bst.n_epochs):\n        y = mode_pth[bdries[idx] : bdries[idx + 1]]\n        x = np.arange(bdries[idx], bdries[idx + 1], step=1)\n        x = x[~np.isnan(y)]\n        y = y[~np.isnan(y)]\n\n        if len(y) &gt; 0:\n            slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)\n            slopes[idx] = slope\n            intercepts[idx] = intercept\n            r2values[idx] = rvalue**2\n        else:\n            slopes[idx] = np.nan\n            intercepts[idx] = np.nan\n            r2values[idx] = np.nan  #\n    #     if bst.n_epochs == 1:\n    #         return np.asscalar(slopes), np.asscalar(intercepts), np.asscalar(r2values)\n    return slopes, intercepts, r2values\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.linregress_ting","title":"<code>linregress_ting(bst, tuningcurve, n_shuffles=250)</code>","text":"<p>Perform linear regression on all the events in bst, and return the R^2 values.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for decoding.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution (default is 250).</p> <code>250</code> <p>Returns:</p> Name Type Description <code>r2values</code> <code>ndarray</code> <p>R^2 values for each event.</p> <code>r2values_shuffled</code> <code>ndarray</code> <p>Shuffled R^2 values for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def linregress_ting(bst, tuningcurve, n_shuffles=250):\n    \"\"\"\n    Perform linear regression on all the events in bst, and return the R^2 values.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve for decoding.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution (default is 250).\n\n    Returns\n    -------\n    r2values : np.ndarray\n        R^2 values for each event.\n    r2values_shuffled : np.ndarray\n        Shuffled R^2 values for each event and shuffle.\n    \"\"\"\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    #     bdries = np.insert(np.cumsum(bst.lengths), 0, 0)\n    r2values = np.zeros(bst.n_epochs)\n    r2values_shuffled = np.zeros((n_shuffles, bst.n_epochs))\n    for idx in range(bst.n_epochs):\n        y = mode_pth[bdries[idx] : bdries[idx + 1]]\n        x = np.arange(bdries[idx], bdries[idx + 1], step=1)\n        x = x[~np.isnan(y)]\n        y = y[~np.isnan(y)]\n\n        if len(y) &gt; 0:\n            slope, intercept, rvalue, pvalue, stderr = stats.linregress(x, y)\n            r2values[idx] = rvalue**2\n        else:\n            r2values[idx] = np.nan  #\n        for ss in range(n_shuffles):\n            if len(y) &gt; 0:\n                slope, intercept, rvalue, pvalue, stderr = stats.linregress(\n                    np.random.permutation(x), y\n                )\n                r2values_shuffled[ss, idx] = rvalue**2\n            else:\n                r2values_shuffled[ss, idx] = (\n                    np.nan\n                )  # event contained NO decoded activity... unlikely or even impossible with current code\n\n    #     sig_idx = np.argwhere(r2values[0,:] &gt; np.percentile(r2values, q=q, axis=0))\n    #     np.argwhere(((R2[1:,:] &gt;= R2[0,:]).sum(axis=0))/(R2.shape[0]-1)&lt;0.05) # equivalent to above\n    if n_shuffles &gt; 0:\n        return r2values, r2values_shuffled\n    return r2values\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.poisson_surrogate_bst","title":"<code>poisson_surrogate_bst(bst)</code>","text":"<p>Create a Poisson surrogate of BinnedSpikeTrainArray.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Poisson surrogate spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def poisson_surrogate_bst(bst):\n    \"\"\"\n    Create a Poisson surrogate of BinnedSpikeTrainArray.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Poisson surrogate spike train array.\n    \"\"\"\n    firing_rates = bst.n_spikes / bst.support.duration  # firing rates in Hz\n\n    spikes = []\n\n    for rate in firing_rates:\n        unit_spikes = []\n        for start, stop in bst.support.time:\n            evt_duration = stop - start\n            n_evt_spikes = np.random.poisson(rate * evt_duration)\n            spike_times = start + np.random.uniform(0, evt_duration, n_evt_spikes)\n            unit_spikes.extend(spike_times)\n\n        spikes.append(unit_spikes)\n\n    support = bst.support.expand(bst.ds / 2, direction=\"stop\")\n    poisson_st = SpikeTrainArray(\n        timestamps=spikes, support=support, unit_ids=bst.unit_ids\n    )\n\n    out = poisson_st.bin(ds=bst.ds)\n    # out = out[bst.support]\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.pooled_incoherent_shuffle_bst","title":"<code>pooled_incoherent_shuffle_bst(bst)</code>","text":"<p>Perform incoherent shuffle on BinnedSpikeTrainArray, swapping within the entire array.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to shuffle.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Shuffled spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def pooled_incoherent_shuffle_bst(bst):\n    \"\"\"\n    Perform incoherent shuffle on BinnedSpikeTrainArray, swapping within the entire array.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to shuffle.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Shuffled spike train array.\n    \"\"\"\n    raise NotImplementedError(\"function not done yet!\")\n    out = copy.deepcopy(bst)  # should this be deep? YES! Oh my goodness, yes!\n    data = out._data\n    edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n\n    for uu in range(bst.n_units):\n        for ii in range(bst.n_epochs):\n            segment = np.atleast_1d(np.squeeze(data[uu, edges[ii] : edges[ii + 1]]))\n            segment = np.roll(segment, np.random.randint(len(segment)))\n            data[uu, edges[ii] : edges[ii + 1]] = segment\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.pooled_time_swap_bst","title":"<code>pooled_time_swap_bst(bst)</code>","text":"<p>Time swap on BinnedSpikeTrainArray, swapping within entire bst.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to swap.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Time-swapped spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def pooled_time_swap_bst(bst):\n    \"\"\"\n    Time swap on BinnedSpikeTrainArray, swapping within entire bst.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to swap.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Time-swapped spike train array.\n    \"\"\"\n    out = copy.deepcopy(bst)  # should this be deep? YES! Oh my goodness, yes!\n    shuffled = np.random.permutation(bst.n_bins)\n    out._data = out._data[:, shuffled]\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_Davidson_final_bst","title":"<code>score_Davidson_final_bst(bst, tuningcurve, w=None, n_shuffles=2000, n_samples=35000, verbose=False)</code>","text":"<p>Compute trajectory scores for each event in the BinnedSpikeTrainArray using the Davidson et al. 2009 method.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for decoding.</p> required <code>w</code> <code>int</code> <p>Half-width of the band for scoring (default is None, treated as 0).</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution (default is 2000).</p> <code>2000</code> <code>n_samples</code> <code>int</code> <p>Number of random lines to sample (default is 35000).</p> <code>35000</code> <code>verbose</code> <code>bool</code> <p>If True, print progress information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores_bayes</code> <code>ndarray</code> <p>Trajectory scores for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_Davidson_final_bst(\n    bst, tuningcurve, w=None, n_shuffles=2000, n_samples=35000, verbose=False\n):\n    \"\"\"\n    Compute trajectory scores for each event in the BinnedSpikeTrainArray using the Davidson et al. 2009 method.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve for decoding.\n    w : int, optional\n        Half-width of the band for scoring (default is None, treated as 0).\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution (default is 2000).\n    n_samples : int, optional\n        Number of random lines to sample (default is 35000).\n    verbose : bool, optional\n        If True, print progress information.\n\n    Returns\n    -------\n    scores_bayes : np.ndarray\n        Trajectory scores for each event.\n    \"\"\"\n\n    def sub2ind(array_shape, rows, cols):\n        return rows * array_shape[1] + cols\n\n    def calc_ri(NT, NP, phi, rho, ci, ci_mid, ri_mid):\n        \"\"\"\n        Note: Not matrix dimensions! Think of dim0 as\n        x coordinate, dim1 as y coordinate, etc.\n        \"\"\"\n        ri = (rho - (ci - ci_mid) * np.cos(phi)) / np.sin(phi) + ri_mid\n        ri = np.around(ri).astype(int)  # Find nearest position bin\n\n        return ri\n\n    def _score_line_ri_ci(posterior, precond_posterior, NT, NP, ri, ci):\n        scores_outside_track = np.nanmedian(\n            posterior[:, (ri &gt; NP - 1) | (ri &lt; 0)], axis=0\n        )\n\n        coords = sub2ind(posterior.shape, ri, ci)\n        coords = coords[(ri &lt; NP) &amp; (ri &gt;= 0)]\n        scores_within_track = np.take(precond_posterior, coords)\n\n        num_empty_bins = (\n            np.isnan(scores_outside_track).sum() + np.isnan(scores_within_track).sum()\n        )\n\n        score_within_track = np.nansum(scores_within_track)\n        if (score_within_track) &gt; 0 &amp; (num_empty_bins &gt; 0):\n            temp = np.nanmedian(scores_within_track) * num_empty_bins\n        else:\n            temp = 0\n        score_outside_track = np.nansum(scores_outside_track) + temp\n\n        score = score_within_track + score_outside_track\n\n        # we divide by NT later on to be more efficient\n        # final_score = score/NT\n\n        return score\n\n    def find_best_line(posterior, precond_posterior, phis, rhos):\n        best_score = 0\n        best_ri = []\n\n        NP, NT = posterior.shape\n\n        ci_mid = (NT + 1) / 2  # CONST\n        ri_mid = (NP + 1) / 2  # CONST\n        ci = np.arange(NT)  # CONST\n\n        for phi, rho in zip(phis, rhos):\n            # parameterize line\n            ri = calc_ri(NT, NP, phi, rho, ci, ci_mid, ri_mid)\n\n            score = _score_line_ri_ci(posterior, precond_posterior, NT, NP, ri, ci)\n            if score &gt; best_score:\n                best_score = score\n                best_ri = ri\n\n        score = (\n            _score_line_ri_ci(posterior, precond_posterior, NT, NP, best_ri, ci) / NT\n        )\n        return score, best_ri\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    # precondition matrix kernel for banded summation\n    k = np.zeros((2 * w + 1, 3))\n    k[:, 1] = 1\n\n    scores_bayes = np.zeros(bst.n_epochs)\n\n    if n_shuffles &gt; 0:\n        scores_bayes_shuffled = np.zeros((n_shuffles, bst.n_epochs))\n\n    for idx in range(bst.n_epochs):\n        if verbose:\n            print(\"scoring event \", idx + 1, \"/\", bst.n_epochs)\n\n        posterior_array = posterior[:, bdries[idx] : bdries[idx + 1]]\n\n        NP, NT = posterior_array.shape\n\n        D = np.sqrt((NT - 1) ** 2 + (NP - 1) ** 2)\n        phi_range = (-0.5 * np.pi, 0.5 * np.pi)\n        rho_range = (-0.5 * D, 0.5 * D)\n\n        phis = phi_range[0] + np.random.rand(n_samples) * (phi_range[1] - phi_range[0])\n        phis[(phis &lt; 0.0001) &amp; (phis &gt; -0.0001)] = 0.0001\n        rhos = rho_range[0] + np.random.rand(n_samples) * (rho_range[1] - rho_range[0])\n\n        precond_posterior = convolve(posterior_array, k, mode=\"constant\", cval=0.0)\n\n        scores_bayes[idx], _ = find_best_line(\n            posterior=posterior_array,\n            precond_posterior=precond_posterior,\n            phis=phis,\n            rhos=rhos,\n        )\n        if n_shuffles &gt; 0:\n            posterior_cs = copy.deepcopy(posterior_array)\n            precond_posterior_cs = copy.deepcopy(precond_posterior)\n\n            for shflidx in range(n_shuffles):\n                for col in range(NT):\n                    random_offset = np.random.randint(1, NP)\n                    posterior_cs[:, col] = np.roll(posterior_cs[:, col], random_offset)\n                    precond_posterior_cs[:, col] = np.roll(\n                        precond_posterior_cs[:, col], random_offset\n                    )\n\n                # ideally we should re-sample phi and rho here for every sequence, but to save time, we don't...\n                scores_bayes_shuffled[shflidx, idx], _ = find_best_line(\n                    posterior=posterior_cs,\n                    precond_posterior=precond_posterior_cs,\n                    phis=phis,\n                    rhos=rhos,\n                )\n    if n_shuffles &gt; 0:\n        scores_bayes_shuffled = scores_bayes_shuffled.T\n        n_scores = len(scores_bayes)\n        scores_bayes_percentile = np.array(\n            [\n                stats.percentileofscore(\n                    scores_bayes_shuffled[idx], scores_bayes[idx], kind=\"mean\"\n                )\n                for idx in range(n_scores)\n            ]\n        )\n        return scores_bayes, scores_bayes_shuffled, scores_bayes_percentile\n    return scores_bayes\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_Davidson_final_bst_fast","title":"<code>score_Davidson_final_bst_fast(bst, tuningcurve, w=None, n_shuffles=2000, n_samples=35000, verbose=False)</code>","text":"<p>Compute trajectory scores for each event in the BinnedSpikeTrainArray using the Davidson et al. 2009 method (fast version).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve for decoding.</p> required <code>w</code> <code>int</code> <p>Half-width of the band for scoring (default is None, treated as 0).</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution (default is 2000).</p> <code>2000</code> <code>n_samples</code> <code>int</code> <p>Number of random lines to sample (default is 35000).</p> <code>35000</code> <code>verbose</code> <code>bool</code> <p>If True, print progress information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores_bayes</code> <code>ndarray</code> <p>Trajectory scores for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_Davidson_final_bst_fast(\n    bst, tuningcurve, w=None, n_shuffles=2000, n_samples=35000, verbose=False\n):\n    \"\"\"\n    Compute trajectory scores for each event in the BinnedSpikeTrainArray using the Davidson et al. 2009 method (fast version).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve for decoding.\n    w : int, optional\n        Half-width of the band for scoring (default is None, treated as 0).\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution (default is 2000).\n    n_samples : int, optional\n        Number of random lines to sample (default is 35000).\n    verbose : bool, optional\n        If True, print progress information.\n\n    Returns\n    -------\n    scores_bayes : np.ndarray\n        Trajectory scores for each event.\n    \"\"\"\n\n    def sub2ind(n_cols, rows, cols):\n        return rows * n_cols + cols\n\n    def calc_ri(NT, NP, phi, rho, ci, ci_mid, ri_mid):\n        \"\"\"\n        Note: Not matrix dimensions! Think of dim0 as\n        x coordinate, dim1 as y coordinate, etc.\n        \"\"\"\n        ri = (rho - (ci - ci_mid) * np.cos(phi)) / np.sin(phi) + ri_mid\n        ri = np.around(ri).astype(int)  # Find nearest position bin\n\n        return ri\n\n    def _score_line_ri_ci(\n        posterior,\n        precond_posterior,\n        NT,\n        NP,\n        ri,\n        ci,\n        ncols,\n        median_post,\n        nanbins,\n        n_nanbins,\n    ):\n        scores_outside_track = median_post[\n            ((ri &gt; NP - 1) &amp; ~nanbins) | ((ri &lt; 0) &amp; ~nanbins)\n        ]\n\n        coords = sub2ind(NT, ri, ci)\n        coords = coords[(ri &lt; NP) &amp; (ri &gt;= 0) &amp; (~nanbins)]\n        scores_within_track = np.take(precond_posterior, coords)\n\n        score_within_track = np.sum(scores_within_track)\n        score_outside_track = np.sum(scores_outside_track)\n\n        score = score_within_track + score_outside_track\n\n        # we divide by NT later on to be more efficient\n        # final_score = score/NT\n\n        return score\n\n    def find_best_line(\n        posterior,\n        precond_posterior,\n        phis,\n        rhos,\n        NP,\n        NT,\n        median_post,\n        nanbins,\n        n_nanbins,\n    ):\n        best_score = 0\n        best_ri = []\n\n        # n_rows, n_cols = posterior.shape\n        #         NP, NT = posterior.shape\n\n        ci_mid = (NT + 1) / 2  # CONST\n        ri_mid = (NP + 1) / 2  # CONST\n        ci = np.arange(NT)  # CONST\n\n        for phi, rho in zip(phis, rhos):\n            # parameterize line\n            ri = calc_ri(NT, NP, phi, rho, ci, ci_mid, ri_mid)\n\n            score = _score_line_ri_ci(\n                posterior,\n                precond_posterior,\n                NT,\n                NP,\n                ri,\n                ci,\n                NT,\n                median_post,\n                nanbins,\n                n_nanbins,\n            )\n            if score &gt; best_score:\n                best_score = score\n                best_ri = ri\n\n        score = (\n            _score_line_ri_ci(\n                posterior,\n                precond_posterior,\n                NT,\n                NP,\n                best_ri,\n                ci,\n                NT,\n                median_post,\n                nanbins,\n                n_nanbins,\n            )\n            / NT\n        )\n        return score, best_ri\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    # precondition matrix kernel for banded summation\n    k = np.zeros((2 * w + 1, 3))\n    k[:, 1] = 1\n\n    scores_bayes = np.zeros(bst.n_epochs)\n\n    if n_shuffles &gt; 0:\n        scores_bayes_shuffled = np.zeros((n_shuffles, bst.n_epochs))\n\n    for idx in range(bst.n_epochs):\n        if verbose:\n            print(\"scoring event \", idx + 1, \"/\", bst.n_epochs)\n\n        posterior_array = posterior[:, bdries[idx] : bdries[idx + 1]]\n\n        # now we zero out all the nan bins (we compensate for them later...)\n        nanbins = np.isnan(np.max(posterior_array, axis=0))\n        n_nanbins = np.count_nonzero(nanbins)\n        posterior_array[:, nanbins] = 0\n\n        # now pre-compute median of entire array\n        posterior_median = np.median(posterior_array, axis=0)\n\n        NP, NT = posterior_array.shape\n\n        D = np.sqrt((NT - 1) ** 2 + (NP - 1) ** 2)\n        phi_range = (-0.5 * np.pi, 0.5 * np.pi)\n        rho_range = (-0.5 * D, 0.5 * D)\n\n        phis = phi_range[0] + np.random.rand(n_samples) * (phi_range[1] - phi_range[0])\n        phis[(phis &lt; 0.0001) &amp; (phis &gt; -0.0001)] = 0.0001\n        rhos = rho_range[0] + np.random.rand(n_samples) * (rho_range[1] - rho_range[0])\n\n        precond_posterior = convolve(posterior_array, k, mode=\"constant\", cval=0.0)\n\n        scores_bayes[idx], _ = find_best_line(\n            posterior=posterior_array,\n            precond_posterior=precond_posterior,\n            phis=phis,\n            rhos=rhos,\n            NP=NP,\n            NT=NT,\n            median_post=posterior_median,\n            nanbins=nanbins,\n            n_nanbins=n_nanbins,\n        )\n        if n_shuffles &gt; 0:\n            posterior_cs = copy.deepcopy(posterior_array)\n            precond_posterior_cs = copy.deepcopy(precond_posterior)\n\n            for shflidx in range(n_shuffles):\n                # do column cycle shuffle on each column independently\n                for col in range(NT):\n                    random_offset = np.random.randint(1, NP)\n                    posterior_cs[:, col] = np.roll(posterior_cs[:, col], random_offset)\n                    precond_posterior_cs[:, col] = np.roll(\n                        precond_posterior_cs[:, col], random_offset\n                    )\n\n                # ideally we should re-sample phi and rho here for every sequence, but to save time, we don't...\n                scores_bayes_shuffled[shflidx, idx], _ = find_best_line(\n                    posterior=posterior_cs,\n                    precond_posterior=precond_posterior_cs,\n                    phis=phis,\n                    rhos=rhos,\n                    NP=NP,\n                    NT=NT,\n                    median_post=posterior_median,\n                    nanbins=nanbins,\n                    n_nanbins=n_nanbins,\n                )\n    if n_shuffles &gt; 0:\n        scores_bayes_shuffled = scores_bayes_shuffled.T\n        n_scores = len(scores_bayes)\n        scores_bayes_percentile = np.array(\n            [\n                stats.percentileofscore(\n                    scores_bayes_shuffled[idx], scores_bayes[idx], kind=\"mean\"\n                )\n                for idx in range(n_scores)\n            ]\n        )\n        return scores_bayes, scores_bayes_shuffled, scores_bayes_percentile\n    return scores_bayes\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_events","title":"<code>score_hmm_events(bst, k_folds=None, num_states=30, n_shuffles=5000, shuffle='row-wise', verbose=False)</code>","text":"<p>Score all sequences in the entire BinnedSpikeTrainArray using HMM transition matrix shuffling and cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>k_folds</code> <code>int</code> <p>Number of cross-validation folds (default is 5).</p> <code>None</code> <code>num_states</code> <code>int</code> <p>Number of hidden states in the HMM (default is 30).</p> <code>30</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution (default is 5000).</p> <code>5000</code> <code>shuffle</code> <code>(row - wise, col - wise, timeswap, pooled - timeswap)</code> <p>Type of shuffling to use for the null distribution (default is 'row-wise').</p> <code>'row-wise'</code> <code>verbose</code> <code>bool</code> <p>If True, print progress information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores_hmm</code> <code>ndarray</code> <p>Log-likelihood scores for each event.</p> <code>scores_hmm_shuffled</code> <code>ndarray</code> <p>Shuffled log-likelihood scores for each event and shuffle.</p> <code>scores_hmm_percentile</code> <code>ndarray</code> <p>Percentile of the real score within the shuffled distribution for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_events(\n    bst, k_folds=None, num_states=30, n_shuffles=5000, shuffle=\"row-wise\", verbose=False\n):\n    \"\"\"\n    Score all sequences in the entire BinnedSpikeTrainArray using HMM transition matrix shuffling and cross-validation.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    k_folds : int, optional\n        Number of cross-validation folds (default is 5).\n    num_states : int, optional\n        Number of hidden states in the HMM (default is 30).\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution (default is 5000).\n    shuffle : {'row-wise', 'col-wise', 'timeswap', 'pooled-timeswap'}, optional\n        Type of shuffling to use for the null distribution (default is 'row-wise').\n    verbose : bool, optional\n        If True, print progress information.\n\n    Returns\n    -------\n    scores_hmm : np.ndarray\n        Log-likelihood scores for each event.\n    scores_hmm_shuffled : np.ndarray\n        Shuffled log-likelihood scores for each event and shuffle.\n    scores_hmm_percentile : np.ndarray\n        Percentile of the real score within the shuffled distribution for each event.\n    \"\"\"\n    # lazy import hmmutils\n    from .. import hmmutils\n\n    if k_folds is None:\n        k_folds = 5\n\n    if shuffle == \"row-wise\":\n        rowwise = True\n    elif shuffle == \"col-wise\":\n        rowwise = False\n    elif shuffle == \"timeswap\":\n        pass\n    elif shuffle == \"pooled-timeswap\":\n        pass\n    else:\n        raise ValueError(\"unknown shuffle\")\n\n    # else:\n    #     raise ValueError(\"tmat must be either 'row-wise' or 'col-wise'\")\n\n    X = [ii for ii in range(bst.n_epochs)]\n\n    scores_hmm = np.zeros(bst.n_epochs)\n    scores_hmm_shuffled = np.zeros((bst.n_epochs, n_shuffles))\n\n    for kk, (training, validation) in enumerate(k_fold_cross_validation(X, k=k_folds)):\n        if verbose:\n            print(\"  fold {}/{}\".format(kk + 1, k_folds))\n\n        PBEs_train = bst[training]\n        PBEs_test = bst[validation]\n\n        # train HMM on all training PBEs\n        hmm = hmmutils.PoissonHMM(\n            n_components=num_states, random_state=0, verbose=False\n        )\n        hmm.fit(PBEs_train)\n\n        # reorder states according to transmat ordering\n        transmat_order = hmm.get_state_order(\"transmat\")\n        hmm.reorder_states(transmat_order)\n\n        # compute scores_hmm (log likelihoods) of validation set:\n        scores_hmm[validation] = hmm.score(PBEs_test)\n\n        if shuffle == \"timeswap\":\n            _, scores_tswap_hmm = score_hmm_timeswap_shuffle(\n                bst=PBEs_test, hmm=hmm, n_shuffles=n_shuffles\n            )\n\n            scores_hmm_shuffled[validation, :] = scores_tswap_hmm.T\n\n        elif shuffle == \"row-wise\" or shuffle == \"col-wise\":\n            hmm_shuffled = copy.deepcopy(hmm)\n            for nn in range(n_shuffles):\n                # shuffle transition matrix:\n                if rowwise:\n                    hmm_shuffled.transmat_ = shuffle_transmat(hmm_shuffled.transmat)\n                else:\n                    hmm_shuffled.transmat_ = (\n                        shuffle_transmat_Kourosh_breaks_stochasticity(\n                            hmm_shuffled.transmat\n                        )\n                    )\n                    hmm_shuffled.transmat_ = (\n                        hmm_shuffled.transmat\n                        / np.tile(\n                            hmm_shuffled.transmat.sum(axis=1),\n                            (hmm_shuffled.n_components, 1),\n                        ).T\n                    )\n\n                # score validation set with shuffled HMM\n                scores_hmm_shuffled[validation, nn] = hmm_shuffled.score(PBEs_test)\n        elif shuffle == \"pooled-timeswap\":\n            _, scores_tswap_hmm = score_hmm_pooled_timeswap_shuffle(\n                bst=PBEs_test, hmm=hmm, n_shuffles=n_shuffles\n            )\n\n            scores_hmm_shuffled[validation, :] = scores_tswap_hmm.T\n\n    n_scores = len(scores_hmm)\n    scores_hmm_percentile = np.array(\n        [\n            stats.percentileofscore(\n                scores_hmm_shuffled[idx], scores_hmm[idx], kind=\"mean\"\n            )\n            for idx in range(n_scores)\n        ]\n    )\n\n    return scores_hmm, scores_hmm_shuffled, scores_hmm_percentile\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_events_no_xval","title":"<code>score_hmm_events_no_xval(bst, training=None, validation=None, num_states=30, n_shuffles=5000, shuffle='row-wise', verbose=False)</code>","text":"<p>Score sequences using HMM, training on a specified training set and scoring a validation set.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>training</code> <code>list or ndarray</code> <p>Indices for training events.</p> <code>None</code> <code>validation</code> <code>list or ndarray</code> <p>Indices for validation events.</p> <code>None</code> <code>num_states</code> <code>int</code> <p>Number of hidden states in the HMM (default is 30).</p> <code>30</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution (default is 5000).</p> <code>5000</code> <code>shuffle</code> <code>(row - wise, col - wise, timeswap)</code> <p>Type of shuffling to use for the null distribution (default is 'row-wise').</p> <code>'row-wise'</code> <code>verbose</code> <code>bool</code> <p>If True, print progress information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores_hmm</code> <code>ndarray</code> <p>Log-likelihood scores for each event in the validation set.</p> <code>scores_hmm_shuffled</code> <code>ndarray</code> <p>Shuffled log-likelihood scores for each event and shuffle.</p> <code>scores_hmm_percentile</code> <code>ndarray</code> <p>Percentile of the real score within the shuffled distribution for each event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_events_no_xval(\n    bst,\n    training=None,\n    validation=None,\n    num_states=30,\n    n_shuffles=5000,\n    shuffle=\"row-wise\",\n    verbose=False,\n):\n    \"\"\"\n    Score sequences using HMM, training on a specified training set and scoring a validation set.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    training : list or np.ndarray\n        Indices for training events.\n    validation : list or np.ndarray\n        Indices for validation events.\n    num_states : int, optional\n        Number of hidden states in the HMM (default is 30).\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution (default is 5000).\n    shuffle : {'row-wise', 'col-wise', 'timeswap'}, optional\n        Type of shuffling to use for the null distribution (default is 'row-wise').\n    verbose : bool, optional\n        If True, print progress information.\n\n    Returns\n    -------\n    scores_hmm : np.ndarray\n        Log-likelihood scores for each event in the validation set.\n    scores_hmm_shuffled : np.ndarray\n        Shuffled log-likelihood scores for each event and shuffle.\n    scores_hmm_percentile : np.ndarray\n        Percentile of the real score within the shuffled distribution for each event.\n    \"\"\"\n    # lazy import hmmutils\n    from .. import hmmutils\n\n    if shuffle == \"row-wise\":\n        rowwise = True\n    elif shuffle == \"col-wise\":\n        rowwise = False\n    else:\n        shuffle = \"timeswap\"\n\n    scores_hmm = np.zeros(len(validation))\n    scores_hmm_shuffled = np.zeros((len(validation), n_shuffles))\n\n    PBEs_train = bst[training]\n    PBEs_test = bst[validation]\n\n    # train HMM on all training PBEs\n    hmm = hmmutils.PoissonHMM(n_components=num_states, random_state=0, verbose=False)\n    hmm.fit(PBEs_train)\n\n    # reorder states according to transmat ordering\n    transmat_order = hmm.get_state_order(\"transmat\")\n    hmm.reorder_states(transmat_order)\n\n    # compute scores_hmm (log likelihoods) of validation set:\n    scores_hmm[:] = hmm.score(PBEs_test)\n\n    if shuffle == \"timeswap\":\n        _, scores_tswap_hmm = score_hmm_timeswap_shuffle(\n            bst=PBEs_test, hmm=hmm, n_shuffles=n_shuffles\n        )\n\n        scores_hmm_shuffled[:, :] = scores_tswap_hmm.T\n    else:\n        hmm_shuffled = copy.deepcopy(hmm)\n        for nn in range(n_shuffles):\n            # shuffle transition matrix:\n            if rowwise:\n                hmm_shuffled.transmat_ = shuffle_transmat(hmm_shuffled.transmat)\n            else:\n                hmm_shuffled.transmat_ = shuffle_transmat_Kourosh_breaks_stochasticity(\n                    hmm_shuffled.transmat\n                )\n                hmm_shuffled.transmat_ = (\n                    hmm_shuffled.transmat\n                    / np.tile(\n                        hmm_shuffled.transmat.sum(axis=1),\n                        (hmm_shuffled.n_components, 1),\n                    ).T\n                )\n\n            # score validation set with shuffled HMM\n            scores_hmm_shuffled[:, nn] = hmm_shuffled.score(PBEs_test)\n\n    n_scores = len(scores_hmm)\n    scores_hmm_percentile = np.array(\n        [\n            stats.percentileofscore(\n                scores_hmm_shuffled[idx], scores_hmm[idx], kind=\"mean\"\n            )\n            for idx in range(n_scores)\n        ]\n    )\n\n    return scores_hmm, scores_hmm_shuffled, scores_hmm_percentile\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_incoherent_shuffle","title":"<code>score_hmm_incoherent_shuffle(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (incoherent shuffle).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_incoherent_shuffle(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (incoherent shuffle).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    scores = score_hmm_logprob(bst=bst, hmm=hmm, normalize=normalize)\n    n_events = bst.n_epochs\n    shuffled = np.zeros((n_shuffles, n_events))\n    for ii in range(n_shuffles):\n        bst_shuffled = incoherent_shuffle_bst(bst=bst)\n        shuffled[ii, :] = score_hmm_logprob(\n            bst=bst_shuffled, hmm=hmm, normalize=normalize\n        )\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_logprob","title":"<code>score_hmm_logprob(bst, hmm, normalize=False)</code>","text":"<p>Score events in a BinnedSpikeTrainArray by computing the log probability under the model.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>normalize</code> <code>bool</code> <p>If True, log probabilities will be normalized by their sequence lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>ndarray</code> <p>Log probabilities, one for each event in bst.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_logprob(bst, hmm, normalize=False):\n    \"\"\"\n    Score events in a BinnedSpikeTrainArray by computing the log probability under the model.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    normalize : bool, optional\n        If True, log probabilities will be normalized by their sequence lengths.\n\n    Returns\n    -------\n    logprob : np.ndarray\n        Log probabilities, one for each event in bst.\n    \"\"\"\n\n    logprob = np.atleast_1d(hmm.score(bst))\n    if normalize:\n        logprob = np.atleast_1d(logprob) / bst.lengths\n\n    return logprob\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_logprob_cumulative","title":"<code>score_hmm_logprob_cumulative(bst, hmm, normalize=False)</code>","text":"<p>Score events in a BinnedSpikeTrainArray by computing the cumulative log probability under the model.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>normalize</code> <code>bool</code> <p>If True, log probabilities will be normalized by their sequence lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>logprob</code> <code>ndarray</code> <p>Cumulative log probabilities for each event in bst.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_logprob_cumulative(bst, hmm, normalize=False):\n    \"\"\"\n    Score events in a BinnedSpikeTrainArray by computing the cumulative log probability under the model.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    normalize : bool, optional\n        If True, log probabilities will be normalized by their sequence lengths.\n\n    Returns\n    -------\n    logprob : np.ndarray\n        Cumulative log probabilities for each event in bst.\n    \"\"\"\n\n    logprob = np.atleast_1d(hmm._cum_score_per_bin(bst))\n    if normalize:\n        cumlengths = []\n        for evt in bst.lengths:\n            cumlengths.extend(np.arange(1, evt + 1).tolist())\n        cumlengths = np.array(cumlengths)\n        logprob = np.atleast_1d(logprob) / cumlengths\n\n    return logprob\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_poisson_shuffle","title":"<code>score_hmm_poisson_shuffle(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (Poisson shuffle).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_poisson_shuffle(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (Poisson shuffle).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    scores = score_hmm_logprob(bst=bst, hmm=hmm, normalize=normalize)\n    n_events = bst.n_epochs\n    shuffled = np.zeros((n_shuffles, n_events))\n    for ii in range(n_shuffles):\n        bst_shuffled = poisson_surrogate_bst(bst=bst)\n        shuffled[ii, :] = score_hmm_logprob(\n            bst=bst_shuffled, hmm=hmm, normalize=normalize\n        )\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_pooled_timeswap_shuffle","title":"<code>score_hmm_pooled_timeswap_shuffle(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (pooled time swap).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_pooled_timeswap_shuffle(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (pooled time swap).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    scores = score_hmm_logprob(bst=bst, hmm=hmm, normalize=normalize)\n    n_events = bst.n_epochs\n    shuffled = np.zeros((n_shuffles, n_events))\n    for ii in range(n_shuffles):\n        bst_shuffled = pooled_time_swap_bst(bst=bst)\n        shuffled[ii, :] = score_hmm_logprob(\n            bst=bst_shuffled, hmm=hmm, normalize=normalize\n        )\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_spike_id_shuffle","title":"<code>score_hmm_spike_id_shuffle(bst, hmm, st_flat, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where spike IDs are shuffled.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>st_flat</code> <code>ndarray</code> <p>Flattened spike train array for shuffling.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_spike_id_shuffle(bst, hmm, st_flat, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where spike IDs are shuffled.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    st_flat : np.ndarray\n        Flattened spike train array for shuffling.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    scores = score_hmm_logprob(bst=bst, hmm=hmm, normalize=normalize)\n    n_events = bst.n_epochs\n    shuffled = np.zeros((n_shuffles, n_events))\n    for ii in range(n_shuffles):\n        bst_shuffled = spike_id_shuffle_bst(bst=bst, st_flat=st_flat)\n        shuffled[ii, :] = score_hmm_logprob(\n            bst=bst_shuffled, hmm=hmm, normalize=normalize\n        )\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_time_resolved","title":"<code>score_hmm_time_resolved(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (time-resolved).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_time_resolved(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (time-resolved).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    hmm_shuffled = copy.deepcopy(hmm)\n    Lbraw = score_hmm_logprob_cumulative(bst=bst, hmm=hmm, normalize=normalize)\n\n    # per event, compute L(:b|raw) - L(:b-1|raw)\n    Lb = copy.deepcopy(Lbraw)\n\n    cumLengths = np.cumsum(bst.lengths)\n    cumLengths = np.insert(cumLengths, 0, 0)\n\n    for ii in range(bst.n_epochs):\n        LE = cumLengths[ii]\n        RE = cumLengths[ii + 1]\n        Lb[LE + 1 : RE] -= Lbraw[LE : RE - 1]\n\n    n_bins = bst.n_bins\n    shuffled = np.zeros((n_shuffles, n_bins))\n    for ii in range(n_shuffles):\n        hmm_shuffled.transmat_ = shuffle_transmat(hmm_shuffled.transmat_)\n        Lbtmat = score_hmm_logprob_cumulative(\n            bst=bst, hmm=hmm_shuffled, normalize=normalize\n        )\n\n        # per event, compute L(:b|tmat) - L(:b-1|raw)\n        NL = copy.deepcopy(Lbtmat)\n        for jj in range(bst.n_epochs):\n            LE = cumLengths[jj]\n            RE = cumLengths[jj + 1]\n            NL[LE + 1 : RE] -= Lbraw[LE : RE - 1]\n\n        shuffled[ii, :] = NL\n\n    scores = Lb\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_timeswap_shuffle","title":"<code>score_hmm_timeswap_shuffle(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (time swap).</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_timeswap_shuffle(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled (time swap).\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    scores = score_hmm_logprob(bst=bst, hmm=hmm, normalize=normalize)\n    n_events = bst.n_epochs\n    shuffled = np.zeros((n_shuffles, n_events))\n    for ii in range(n_shuffles):\n        bst_shuffled = time_swap_bst(bst=bst)\n        shuffled[ii, :] = score_hmm_logprob(\n            bst=bst_shuffled, hmm=hmm, normalize=normalize\n        )\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_transmat_shuffle","title":"<code>score_hmm_transmat_shuffle(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_transmat_shuffle(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where the transition probability matrix has been shuffled.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    hmm_shuffled = copy.deepcopy(hmm)\n    scores = score_hmm_logprob(bst=bst, hmm=hmm, normalize=normalize)\n    n_events = bst.n_epochs\n    shuffled = np.zeros((n_shuffles, n_events))\n    for ii in range(n_shuffles):\n        hmm_shuffled.transmat_ = shuffle_transmat(hmm_shuffled.transmat_)\n        shuffled[ii, :] = score_hmm_logprob(\n            bst=bst, hmm=hmm_shuffled, normalize=normalize\n        )\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.score_hmm_unit_id_shuffle","title":"<code>score_hmm_unit_id_shuffle(bst, hmm, n_shuffles=250, normalize=False)</code>","text":"<p>Score sequences using a hidden Markov model and a model where unit IDs are shuffled.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>hmm</code> <code>PoissonHMM</code> <p>Trained hidden Markov model.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the scores by event lengths.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Log probabilities for each event.</p> <code>shuffled</code> <code>ndarray</code> <p>Shuffled log probabilities for each event and shuffle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def score_hmm_unit_id_shuffle(bst, hmm, n_shuffles=250, normalize=False):\n    \"\"\"\n    Score sequences using a hidden Markov model and a model where unit IDs are shuffled.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    hmm : PoissonHMM\n        Trained hidden Markov model.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    normalize : bool, optional\n        If True, normalize the scores by event lengths.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Log probabilities for each event.\n    shuffled : np.ndarray\n        Shuffled log probabilities for each event and shuffle.\n    \"\"\"\n\n    scores = score_hmm_logprob(bst=bst, hmm=hmm, normalize=normalize)\n    n_events = bst.n_epochs\n    shuffled = np.zeros((n_shuffles, n_events))\n    for ii in range(n_shuffles):\n        bst_shuffled = unit_id_shuffle_bst(bst=bst)\n        shuffled[ii, :] = score_hmm_logprob(\n            bst=bst_shuffled, hmm=hmm, normalize=normalize\n        )\n\n    return scores, shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.shuffle_transmat","title":"<code>shuffle_transmat(transmat)</code>","text":"<p>Shuffle transition probability matrix within each row, leaving self transitions in tact.</p> <p>It is assumed that the transmat is stochastic-row-wise, meaning that A_{ij} = Pr(S_{t+1}=j|S_t=i).</p> <p>Parameters:</p> Name Type Description Default <code>transmat</code> <code>array of size (n_states, n_states)</code> <p>Transition probability matrix, where A_{ij} = Pr(S_{t+1}=j|S_t=i).</p> required <p>Returns:</p> Name Type Description <code>shuffled</code> <code>array of size (n_states, n_states)</code> <p>Shuffled transition probability matrix.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def shuffle_transmat(transmat):\n    \"\"\"\n    Shuffle transition probability matrix within each row, leaving self transitions in tact.\n\n    It is assumed that the transmat is stochastic-row-wise, meaning that A_{ij} = Pr(S_{t+1}=j|S_t=i).\n\n    Parameters\n    ----------\n    transmat : array of size (n_states, n_states)\n        Transition probability matrix, where A_{ij} = Pr(S_{t+1}=j|S_t=i).\n\n    Returns\n    -------\n    shuffled : array of size (n_states, n_states)\n        Shuffled transition probability matrix.\n    \"\"\"\n    shuffled = transmat.copy()\n\n    nrows, ncols = transmat.shape\n    for rowidx in range(nrows):\n        all_but_diagonal = np.append(np.arange(rowidx), np.arange(rowidx + 1, ncols))\n        shuffle_idx = np.random.permutation(all_but_diagonal)\n        shuffle_idx = np.insert(shuffle_idx, rowidx, rowidx)\n        shuffled[rowidx, :] = shuffled[rowidx, shuffle_idx]\n\n    return shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.shuffle_transmat_Kourosh_breaks_stochasticity","title":"<code>shuffle_transmat_Kourosh_breaks_stochasticity(transmat)</code>","text":"<p>Shuffle transition probability matrix within each column, leaving self transitions in tact.</p> <p>It is assumed that the transmat is stochastic-row-wise, meaning that A_{ij} = Pr(S_{t+1}=j|S_t=i).</p> <p>NOTE: this breaks stochasticity! To get back to a stochastic matrix, we should do: transmat = transmat / np.tile(transmat.sum(axis=1), (hmm.n_components, 1)).T</p> <p>Parameters:</p> Name Type Description Default <code>transmat</code> <code>array of size (n_states, n_states)</code> <p>Transition probability matrix, where A_{ij} = Pr(S_{t+1}=j|S_t=i).</p> required <p>Returns:</p> Name Type Description <code>shuffled</code> <code>array of size (n_states, n_states)</code> <p>Shuffled transition probability matrix.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def shuffle_transmat_Kourosh_breaks_stochasticity(transmat):\n    \"\"\"\n    Shuffle transition probability matrix within each column, leaving self transitions in tact.\n\n    It is assumed that the transmat is stochastic-row-wise, meaning that A_{ij} = Pr(S_{t+1}=j|S_t=i).\n\n    NOTE: this breaks stochasticity! To get back to a stochastic matrix, we should do:\n    transmat = transmat / np.tile(transmat.sum(axis=1), (hmm.n_components, 1)).T\n\n    Parameters\n    ----------\n    transmat : array of size (n_states, n_states)\n        Transition probability matrix, where A_{ij} = Pr(S_{t+1}=j|S_t=i).\n\n    Returns\n    -------\n    shuffled : array of size (n_states, n_states)\n        Shuffled transition probability matrix.\n    \"\"\"\n    shuffled = transmat.copy()\n\n    nrows, ncols = transmat.shape\n    for colidx in range(ncols):\n        all_but_diagonal = np.append(np.arange(colidx), np.arange(colidx + 1, nrows))\n        shuffle_idx = np.random.permutation(all_but_diagonal)\n        shuffle_idx = np.insert(shuffle_idx, colidx, colidx)\n        shuffled[:, colidx] = shuffled[shuffle_idx, colidx]\n\n    return shuffled\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.spike_id_shuffle_bst","title":"<code>spike_id_shuffle_bst(bst, st_flat)</code>","text":"<p>Create a spike ID shuffled surrogate of BinnedSpikeTrainArray.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array.</p> required <code>st_flat</code> <code>ndarray</code> <p>Flattened spike train array for shuffling.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Shuffled spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def spike_id_shuffle_bst(bst, st_flat):\n    \"\"\"\n    Create a spike ID shuffled surrogate of BinnedSpikeTrainArray.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array.\n    st_flat : np.ndarray\n        Flattened spike train array for shuffling.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Shuffled spike train array.\n    \"\"\"\n    all_spiketimes = st_flat.time.squeeze()\n    spike_ids = np.zeros(len(all_spiketimes))\n\n    # determine number of spikes per unit:\n    n_spikes = np.ones(bst.n_units) * np.floor(st_flat.n_spikes[0] / bst.n_units)\n\n    pointer = 0\n    for uu, n_spikes in enumerate(n_spikes):\n        spike_ids[pointer : pointer + int(n_spikes)] = uu\n        pointer += int(n_spikes)\n\n    # permute spike IDs\n    spike_ids = np.random.permutation(spike_ids)\n\n    # now re-assign all spike times according to sampling above\n    spikes = []\n    for unit in range(bst.n_units):\n        spikes.append(all_spiketimes[spike_ids == unit])\n\n    support = bst.support.expand(bst.ds / 2, direction=\"stop\")\n    shuffled_st = SpikeTrainArray(\n        timestamps=spikes, support=support, unit_ids=bst.unit_ids\n    )\n\n    out = shuffled_st.bin(ds=bst.ds)\n    # out = out[bst.support]\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.time_swap_array","title":"<code>time_swap_array(posterior)</code>","text":"<p>Time swap.</p> <p>Note: it is often possible to simply shuffle the time bins, and not the actual data, for computational efficiency. Still, this function works as expected.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>Time-swapped posterior matrix.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def time_swap_array(posterior):\n    \"\"\"\n    Time swap.\n\n    Note: it is often possible to simply shuffle the time bins, and not the actual data, for computational\n    efficiency. Still, this function works as expected.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n\n    Returns\n    -------\n    out : np.ndarray\n        Time-swapped posterior matrix.\n    \"\"\"\n    out = copy.deepcopy(posterior)\n    rows, cols = posterior.shape\n\n    colidx = np.arange(cols)\n    shuffle_cols = np.random.permutation(colidx)\n    out = out[:, shuffle_cols]\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.time_swap_bst","title":"<code>time_swap_bst(bst)</code>","text":"<p>Time swap on BinnedSpikeTrainArray, swapping only within each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array to swap.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Time-swapped spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def time_swap_bst(bst):\n    \"\"\"\n    Time swap on BinnedSpikeTrainArray, swapping only within each epoch.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array to swap.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Time-swapped spike train array.\n    \"\"\"\n    out = copy.deepcopy(bst)  # should this be deep? YES! Oh my goodness, yes!\n    shuffled = np.arange(bst.n_bins)\n    edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n    for ii in range(bst.n_epochs):\n        segment = shuffled[edges[ii] : edges[ii + 1]]\n        shuffled[edges[ii] : edges[ii + 1]] = np.random.permutation(segment)\n\n    out._data = out._data[:, shuffled]\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.trajectory_score_array","title":"<code>trajectory_score_array(posterior, slope=None, intercept=None, w=None, weights=None, normalize=False)</code>","text":"<p>Compute the trajectory score for a given posterior matrix and line parameters.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>Posterior probability matrix (position x time).</p> required <code>slope</code> <code>float</code> <p>Slope of the line. If None, estimated from the data.</p> <code>None</code> <code>intercept</code> <code>float</code> <p>Intercept of the line. If None, estimated from the data.</p> <code>None</code> <code>w</code> <code>int</code> <p>Half band width for calculating the trajectory score. Default is 0.</p> <code>None</code> <code>weights</code> <code>array - like</code> <p>Weights for the band around the line (not yet implemented).</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the score by the number of non-NaN bins.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>Trajectory score for the event.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def trajectory_score_array(\n    posterior, slope=None, intercept=None, w=None, weights=None, normalize=False\n):\n    \"\"\"\n    Compute the trajectory score for a given posterior matrix and line parameters.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        Posterior probability matrix (position x time).\n    slope : float, optional\n        Slope of the line. If None, estimated from the data.\n    intercept : float, optional\n        Intercept of the line. If None, estimated from the data.\n    w : int, optional\n        Half band width for calculating the trajectory score. Default is 0.\n    weights : array-like, optional\n        Weights for the band around the line (not yet implemented).\n    normalize : bool, optional\n        If True, normalize the score by the number of non-NaN bins.\n\n    Returns\n    -------\n    score : float\n        Trajectory score for the event.\n    \"\"\"\n\n    rows, cols = posterior.shape\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n    if slope is None or intercept is None:\n        slope, intercept, _ = linregress_array(posterior=posterior)\n\n    x = np.arange(cols)\n    line_y = np.round((slope * x + intercept))  # in position bin #s\n\n    # idea: cycle each column so that the top w rows are the band surrounding the regression line\n\n    if np.isnan(slope):  # this will happen if we have 0 or only 1 decoded bins\n        return np.nan\n    else:\n        temp = column_cycle_array(posterior, -line_y + w)\n\n    if normalize:\n        num_non_nan_bins = round(np.nansum(posterior))\n    else:\n        num_non_nan_bins = 1\n\n    return np.nansum(temp[: 2 * w + 1, :]) / num_non_nan_bins\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.trajectory_score_bst","title":"<code>trajectory_score_bst(bst, tuningcurve, w=None, n_shuffles=250, weights=None, normalize=False)</code>","text":"<p>Compute the trajectory scores from Davidson et al. for each event in the BinnedSpikeTrainArray.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array containing all candidate events.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve to decode events in bst.</p> required <code>w</code> <code>int</code> <p>Half band width for calculating the trajectory score. Default is 0.</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles for the null distribution. Default is 250.</p> <code>250</code> <code>weights</code> <code>array - like</code> <p>Weights for the band around the line (not yet implemented).</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the score by the number of non-NaN bins.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray</code> <p>Trajectory scores for each event.</p> <code>scores_time_swap</code> <code>ndarray</code> <p>Shuffled scores using time swap.</p> <code>scores_col_cycle</code> <code>ndarray</code> <p>Shuffled scores using column cycle.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def trajectory_score_bst(\n    bst, tuningcurve, w=None, n_shuffles=250, weights=None, normalize=False\n):\n    \"\"\"\n    Compute the trajectory scores from Davidson et al. for each event in the BinnedSpikeTrainArray.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array containing all candidate events.\n    tuningcurve : TuningCurve1D\n        Tuning curve to decode events in bst.\n    w : int, optional\n        Half band width for calculating the trajectory score. Default is 0.\n    n_shuffles : int, optional\n        Number of shuffles for the null distribution. Default is 250.\n    weights : array-like, optional\n        Weights for the band around the line (not yet implemented).\n    normalize : bool, optional\n        If True, normalize the score by the number of non-NaN bins.\n\n    Returns\n    -------\n    scores : np.ndarray\n        Trajectory scores for each event.\n    scores_time_swap : np.ndarray\n        Shuffled scores using time swap.\n    scores_col_cycle : np.ndarray\n        Shuffled scores using column cycle.\n    \"\"\"\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, mode_pth, mean_pth = decode(bst=bst, ratemap=tuningcurve)\n\n    # idea: cycle each column so that the top w rows are the band\n    # surrounding the regression line\n\n    scores = np.zeros(bst.n_epochs)\n    if n_shuffles &gt; 0:\n        scores_time_swap = np.zeros((n_shuffles, bst.n_epochs))\n        scores_col_cycle = np.zeros((n_shuffles, bst.n_epochs))\n\n    for idx in range(bst.n_epochs):\n        posterior_array = posterior[:, bdries[idx] : bdries[idx + 1]]\n        scores[idx] = trajectory_score_array(\n            posterior=posterior_array, w=w, normalize=normalize\n        )\n        for shflidx in range(n_shuffles):\n            # time swap:\n\n            posterior_ts = time_swap_array(posterior_array)\n            posterior_cs = column_cycle_array(posterior_array)\n            scores_time_swap[shflidx, idx] = trajectory_score_array(\n                posterior=posterior_ts, w=w, normalize=normalize\n            )\n            scores_col_cycle[shflidx, idx] = trajectory_score_array(\n                posterior=posterior_cs, w=w, normalize=normalize\n            )\n\n    if n_shuffles &gt; 0:\n        return scores, scores_time_swap, scores_col_cycle\n    return scores\n</code></pre>"},{"location":"reference/nelpy/analysis/replay/#nelpy.analysis.replay.unit_id_shuffle_bst","title":"<code>unit_id_shuffle_bst(bst)</code>","text":"<p>Create a unit ID shuffled surrogate of BinnedSpikeTrainArray.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train array.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>BinnedSpikeTrainArray</code> <p>Shuffled spike train array.</p> Source code in <code>nelpy/analysis/replay.py</code> <pre><code>def unit_id_shuffle_bst(bst):\n    \"\"\"\n    Create a unit ID shuffled surrogate of BinnedSpikeTrainArray.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train array.\n\n    Returns\n    -------\n    out : BinnedSpikeTrainArray\n        Shuffled spike train array.\n    \"\"\"\n    out = copy.deepcopy(bst)  # should this be deep? yes!\n    data = out._data\n    edges = np.insert(np.cumsum(bst.lengths), 0, 0)\n\n    unit_list = np.arange(bst.n_units)\n\n    for ii in range(bst.n_epochs):\n        segment = data[:, edges[ii] : edges[ii + 1]]\n        out._data[:, edges[ii] : edges[ii + 1]] = segment[\n            np.random.permutation(unit_list)\n        ]\n\n    return out\n</code></pre>"},{"location":"reference/nelpy/io/brian/","title":"nelpy.io.brian","text":"<p>This module implements functionailty for converting to and from Brian objects (https://brian2.readthedocs.io/en/stable/)</p> <p>Specifically we may want to convert to and from their trace class, and    perhaps others, too.</p>"},{"location":"reference/nelpy/io/hc11/","title":"nelpy.io.hc11","text":"<p>This file contains nelpy io functions for reading data from hc-11.</p> <p>See https://crcns.org/data-sets/hc/hc-11/about-hc-11.</p> <p>datatype = ['spikes', 'lfp', 'pos', 'waveforms', 'metadata']</p>"},{"location":"reference/nelpy/io/hc11/#nelpy.io.hc11.DataLoaderHC11","title":"<code>DataLoaderHC11</code>","text":"<p>               Bases: <code>DataLoader</code></p> Source code in <code>nelpy/io/hc11.py</code> <pre><code>class DataLoaderHC11(DataLoader):\n    def __init__(self, basedir=None):\n        super().__init__(basedir=basedir)\n\n        self._sessions = None\n        self._session_dirs = None\n        self.data = OrderedDict()\n\n        self.basedir = basedir\n\n    def _get_sessions(self, path=None, cache=True):\n        if path is None:\n            path = self.basedir  # + '/data/'\n        path = os.path.normpath(path)\n        if not os.path.isdir(path):\n            logging.error('\"{}\" should be a directory'.format(path))\n            raise FileNotFoundError('\"{}\" should be a directory'.format(path))\n\n        sessions = next(os.walk(path))[1]\n        if len(sessions) == 0:\n            logging.error('No sessions found at \"{}\"'.format(path))\n            raise FileNotFoundError('No sessions found at \"{}\"'.format(path))\n\n        dirs = [os.path.normpath(path + \"/\" + session + \"/\") for session in sessions]\n\n        sortorder = np.argsort(sessions)\n        sessions = np.array(sessions)[sortorder].tolist()\n        dirs = np.array(dirs)[sortorder].tolist()\n        if cache:\n            self._sessions = sessions\n            self._session_dirs = dirs\n\n        return sessions, dirs\n\n    @property\n    def basedir(self):\n        if self._basedir is None:\n            raise ValueError(\"basedir has not been initialized yet!\")\n        return self._basedir\n\n    @basedir.setter\n    def basedir(self, path):\n        if path is None:\n            return\n        path = os.path.normpath(path)\n        if not os.path.isdir(path):\n            logging.error('\"{}\" should be a directory'.format(path))\n            raise FileNotFoundError('\"{}\" should be a directory'.format(path))\n        self._basedir = path\n        self._load_xml()\n        self._drop_xml()\n        self._get_sessions()\n\n    @property\n    def sessions(self):\n        if self._sessions is None:\n            _, _ = self._get_sessions()\n        return self._sessions\n\n    @property\n    def session_dirs(self):\n        if self._session_dirs is None:\n            _, _ = self._get_sessions()\n        return self._session_dirs\n\n    def _get_num_probes(self, path):\n        \"\"\"Determine the number of probes (silicon probe shanks, tetrodes, ...).\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        dict with {session: num_probes}\n\n        \"\"\"\n\n        num = 0\n        files = [f for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f)))]\n        for ff in files:\n            try:\n                found = re.search(r\"\\.clu\\.[0-9]+$\", ff).group(0)\n                logging.info('Found {} in session at \"{}\"'.format(found, path))\n                num += 1\n            except Exception:\n                pass\n        if num == 0:\n            logging.warning(\n                'Did not find any .clu files for session \"{}\"'.format(self.basedir)\n            )\n\n        return num\n\n    # def _get_num_probes(self, path=None):\n    #     \"\"\"Determine the number of probes (silicon probe shanks, tetrodes, ...).\n\n    #     Parameters\n    #     ----------\n\n    #     Returns\n    #     -------\n    #     dict with {session: num_probes}\n\n    #     \"\"\"\n    #     if path is None:\n    #         if self._sessions is None:\n    #             self._get_sessions()\n    #         sessions, dirs = self._sessions, self._session_dirs\n    #     else:\n    #         sessions, dirs = self._get_sessions(path=path, cache=False)\n\n    #     nums = {}\n    #     for (sdir, session) in zip(dirs, sessions):\n    #         num = 0\n    #         files = [f for f in os.listdir(sdir) if (os.path.isfile(os.path.join(sdir, f)))]\n    #         for ff in files:\n    #             try:\n    #                 found = re.search('\\.clu\\.[0-9]+$', ff).group(0)\n    #                 logging.info('Found {} in session \"{}\"'.format(found, session))\n    #                 num+=1\n    #             except Exception:\n    #                 pass\n    #         if num == 0:\n    #             logging.warning('Did not find any .clu files for session \"{}\"'.format(session))\n    #         nums[session] = num\n    #     return nums\n\n    def _load_xml(self):\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session in sessions:\n            p = self.data.get(session, None)\n            if p is None:\n                self.data[session] = {}\n            p = self.data[session].get(\"params\", None)\n            if p is None:\n                self.data[session][\"params\"] = {}\n\n        for session, sdir in zip(sessions, dirs):\n            with open(sdir + \"/\" + session + \".xml\") as fd:\n                xml = xmltodict.parse(fd.read())\n                self.data[session][\"xml\"] = xml\n                self.data[session][\"params\"][\"fs_wideband\"] = float(\n                    xml[\"parameters\"][\"acquisitionSystem\"][\"samplingRate\"]\n                )\n                self.data[session][\"params\"][\"n_channels\"] = float(\n                    xml[\"parameters\"][\"acquisitionSystem\"][\"nChannels\"]\n                )\n                self.data[session][\"params\"][\"fs_lfp\"] = float(\n                    xml[\"parameters\"][\"fieldPotentials\"][\"lfpSamplingRate\"]\n                )\n                # self.data[session]['params']['fs_video'] = float(xml['parameters']['video']['samplingRate'])\n                # self.data[session]['params']['video_width'] = float(xml['parameters']['video']['width'])\n                # self.data[session]['params']['video_height'] = float(xml['parameters']['video']['height'])\n                self.data[session][\"params\"][\"snippet_len\"] = int(\n                    xml[\"parameters\"][\"neuroscope\"][\"spikes\"][\"nSamples\"]\n                )\n                self.data[session][\"params\"][\"snippet_peak_idx\"] = int(\n                    xml[\"parameters\"][\"neuroscope\"][\"spikes\"][\"peakSampleIndex\"]\n                )\n                self.data[session][\"params\"][\"n_probes\"] = self._get_num_probes(\n                    path=sdir\n                )\n\n    def _drop_xml(self):\n        sessions = self.sessions\n        for session in sessions:\n            self.data[session].pop(\"xml\", None)\n\n    def _load_events(self):\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session, sdir in zip(sessions, dirs):\n            events = OrderedDict()\n            filename = \"{}/{}.cat.evt\".format(sdir, session)\n            with open(filename) as f:\n                for line in f:\n                    line = line.strip()\n                    timestamp, descr = line.split(\" \", maxsplit=1)\n                    descr = descr.split(\"-\", maxsplit=3)[-1]\n                    timestamp = float(timestamp) / 1000\n                    try:\n                        events[descr] = EpochArray((events[descr], timestamp))\n                    except KeyError:\n                        events[descr] = timestamp\n\n            self.data[session][\"events\"] = events\n\n        return\n\n    def _load_pos(self):\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session, sdir in zip(sessions, dirs):\n            fs = self.data[session][\"params\"][\"fs_video\"]\n            width = self.data[session][\"params\"][\"video_width\"]\n            height = self.data[session][\"params\"][\"video_height\"]\n\n            if session in [\"Train-292-20150501\", \"Train-314-20160118\"]:\n                width = 640\n                height = 480\n\n            filename = \"{}/{}.pos\".format(sdir, session)\n\n            logging.info('reading position data from \"{}\"'.format(filename))\n            posdata = np.array(\n                pd.read_table(filename, sep=\"\\t\", skiprows=0, names=[\"x\", \"y\"])\n            )\n            timestamps = np.linspace(0, len(posdata) / fs, len(posdata))\n\n            logging.info(\"dropping samples with missing data\")\n            dropidx = np.where(posdata == -1)[0]\n            posdata = np.delete(posdata, dropidx, 0)\n            timestamps = np.delete(timestamps, dropidx, 0)\n\n            logging.info(\"building nelpy PositionArray\")\n            pos = PositionArray(\n                posdata.T,\n                abscissa_vals=timestamps,\n                fs=fs,\n                xlim=[0, width],\n                ylim=[0, height],\n            )\n            self.data[session][\"pos\"] = pos\n\n    def _load_spikes(self, include_mua=False, includeWaveforms=False):\n        # NOTE: st_array[0] always corresponds to unsortable spikes (i.e., MUA,\n        # not mechanical noise). However, when includeUnsortedSpikes==True, then\n        # it gets populated with spike times; else, it just remains an empty\n        # list []\n\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session, sdir in zip(sessions, dirs):\n            fs = self.data[session][\"params\"][\"fs_wideband\"]\n            fs = 20000\n            # num_probes = self.data[session][\"params\"][\"n_probes\"]\n            snippet_len = self.data[session][\"params\"][\"snippet_len\"]\n            snippet_peak_idx = self.data[session][\"params\"][\"snippet_peak_idx\"]\n            n_channels = int(self.data[session][\"params\"][\"n_channels\"])\n            # n_channels = 10\n            # n_channels = int(self.data[session]['params']['n_channels']/16)\n            # if n_channels == 4:\n            #     print('tetrodes!')\n            # elif n_channels == 8:\n            #     print('octrodes!')\n            # else:\n            #     raise ValueError('unexpected number of channels for hc-11')\n\n            clu_files = ns.humansorted(glob.glob(sdir + \"/{}.clu*\".format(session)))\n\n            if include_mua:\n                st_array = [[]]\n                wf_array = [[]]\n            else:\n                st_array = []\n                wf_array = []\n            wfdt = np.dtype(\n                \"&lt;h\", (snippet_len, n_channels)\n            )  # waveform datatype (.spk files)\n            # note: using pandas.read_table is orders of magnitude faster here than using numpy.loadtxt\n            for clu_file in clu_files:\n                pre, post = clu_file.split(\"clu\")\n                res_file = pre + \"res\" + post\n                spk_file = pre + \"spk\" + post\n                # %time dt1a = np.loadtxt( base_filename1 + '.clu.' + str(probe + 1), skiprows=1,dtype=int)\n                eudf = pd.read_table(\n                    clu_file, header=None, names=\"u\"\n                )  # read unit numbers within electrode\n                tsdf = pd.read_table(\n                    res_file, header=None, names=\"t\"\n                )  # read sample numbers for spikes\n                if includeWaveforms:\n                    waveforms = np.fromfile(spk_file, dtype=wfdt)\n                    waveforms = np.reshape(\n                        waveforms,\n                        (\n                            int(len(waveforms) / (snippet_len * n_channels)),\n                            snippet_len,\n                            n_channels,\n                        ),\n                    )\n                    waveforms = waveforms[:, snippet_peak_idx, :]\n\n                max_units = eudf.u.values[0]\n\n                eu = eudf.u.values[1:]\n                ts = tsdf.t.values\n\n                if includeWaveforms:\n                    noise_idx = np.argwhere(eu == 0).squeeze()\n                    hash_idx = np.argwhere(eu == 1).squeeze()\n                    all_idx = set(np.arange(len(eu)))\n                    discard_idx = set(noise_idx)\n\n                # discard units labeled as '0' or '1', as these correspond to mechanical noise and unsortable units\n                ts = ts[eu != 0]  # always discard mechanical noise\n                eu = eu[eu != 0]  # always discard mechanical noise\n\n                if not include_mua:\n                    ts = ts[eu != 1]  # potentially discard unsortable spikes\n                    eu = eu[eu != 1]  # potentially discard unsortable spikes\n                    if includeWaveforms:\n                        discard_idx = discard_idx.union(set(hash_idx))\n\n                if includeWaveforms:\n                    keep_idx = all_idx - discard_idx\n                    waveforms = waveforms[sorted(list(keep_idx))]\n\n                for uu in np.arange(max_units - 2):\n                    st_array.append(ts[eu == uu + 2])\n                    if includeWaveforms:\n                        wf_array.append(waveforms[eu == uu + 2])\n\n                if include_mua:\n                    st_array[0] = np.append(\n                        st_array[0], ts[eu == 1]\n                    )  # unit 0 now corresponds to unsortable spikes\n                    if includeWaveforms:\n                        if len(wf_array[0]) &gt; 0:\n                            wf_array[0] = np.vstack((wf_array[0], waveforms[eu == 1]))\n                        else:\n                            wf_array[0] = waveforms[eu == 1]\n\n            if include_mua:\n                unit_ids = np.arange(len(st_array))\n            else:\n                unit_ids = np.arange(1, len(st_array) + 1)\n\n            # make sure that spike times are sorted! (this is not true for unit 0 of the hc-3 dataset, for example):\n            for unit, spikes in enumerate(st_array):\n                order = np.argsort(spikes)\n                st_array[unit] = spikes[order] / fs\n                if includeWaveforms:\n                    wf_array[unit] = wf_array[unit][order]\n\n            if includeWaveforms:\n                spikes = SpikeTrainArray(st_array, fs=fs, unit_ids=unit_ids)\n                spikes._marks = wf_array\n            else:\n                spikes = SpikeTrainArray(st_array, fs=fs, unit_ids=unit_ids)\n\n            self.data[session][\"spikes\"] = spikes\n\n    def load(self, datatype=\"all\", data=None):\n        \"\"\"Load (and overwrite!) data into dictionary.\"\"\"\n        if data is None:\n            data = self.data\n        else:\n            logging.error(\"passing in a data dictionary is not fully supported yet!\")\n            raise NotImplementedError\n\n        if datatype == \"pos\":  # load position data\n            self._load_pos()\n\n        if datatype == \"spikes\":  # load sorted spike data\n            self._load_spikes()\n\n        if datatype == \"events\":\n            self._load_events()\n</code></pre>"},{"location":"reference/nelpy/io/hc11/#nelpy.io.hc11.DataLoaderHC11.load","title":"<code>load(datatype='all', data=None)</code>","text":"<p>Load (and overwrite!) data into dictionary.</p> Source code in <code>nelpy/io/hc11.py</code> <pre><code>def load(self, datatype=\"all\", data=None):\n    \"\"\"Load (and overwrite!) data into dictionary.\"\"\"\n    if data is None:\n        data = self.data\n    else:\n        logging.error(\"passing in a data dictionary is not fully supported yet!\")\n        raise NotImplementedError\n\n    if datatype == \"pos\":  # load position data\n        self._load_pos()\n\n    if datatype == \"spikes\":  # load sorted spike data\n        self._load_spikes()\n\n    if datatype == \"events\":\n        self._load_events()\n</code></pre>"},{"location":"reference/nelpy/io/hc18/","title":"nelpy.io.hc18","text":"<p>This file contains nelpy io functions for reading data from hc-18.</p> <p>The data set includes 5 sessions (i.e., 5 rats), each consisting of three travel sessions (passive, active, passive; 2~15 laps each) interspersed with sleep sessions (~90 min each).</p> <p>See https://crcns.org/data-sets/hc/hc-18/about-hc-18.</p> <p>datatype = ['spikes', 'lfp', 'pos', 'waveforms', 'metadata']</p>"},{"location":"reference/nelpy/io/hc18/#nelpy.io.hc18.DataLoaderHC18","title":"<code>DataLoaderHC18</code>","text":"<p>               Bases: <code>DataLoader</code></p> Source code in <code>nelpy/io/hc18.py</code> <pre><code>class DataLoaderHC18(DataLoader):\n    def __init__(self, basedir=None):\n        super().__init__(basedir=basedir)\n\n        self._sessions = None\n        self._session_dirs = None\n        self.data = OrderedDict()\n\n        self.basedir = basedir\n\n    def _get_sessions(self, path=None, cache=True):\n        if path is None:\n            path = self.basedir  # + '/data/'\n        path = os.path.normpath(path)\n        if not os.path.isdir(path):\n            logging.error('\"{}\" should be a directory'.format(path))\n            raise FileNotFoundError('\"{}\" should be a directory'.format(path))\n\n        sessions = next(os.walk(path))[1]\n        if len(sessions) == 0:\n            logging.error('No sessions found at \"{}\"'.format(path))\n            raise FileNotFoundError('No sessions found at \"{}\"'.format(path))\n\n        dirs = [os.path.normpath(path + \"/\" + session + \"/\") for session in sessions]\n\n        sortorder = np.argsort(sessions)\n        sessions = np.array(sessions)[sortorder].tolist()\n        dirs = np.array(dirs)[sortorder].tolist()\n        if cache:\n            self._sessions = sessions\n            self._session_dirs = dirs\n\n        return sessions, dirs\n\n    @property\n    def basedir(self):\n        if self._basedir is None:\n            raise ValueError(\"basedir has not been initialized yet!\")\n        return self._basedir\n\n    @basedir.setter\n    def basedir(self, path):\n        if path is None:\n            return\n        path = os.path.normpath(path)\n        if not os.path.isdir(path):\n            logging.error('\"{}\" should be a directory'.format(path))\n            raise FileNotFoundError('\"{}\" should be a directory'.format(path))\n        self._basedir = path\n        self._load_xml()\n        self._drop_xml()\n        self._get_sessions()\n\n    @property\n    def sessions(self):\n        if self._sessions is None:\n            _, _ = self._get_sessions()\n        return self._sessions\n\n    @property\n    def session_dirs(self):\n        if self._session_dirs is None:\n            _, _ = self._get_sessions()\n        return self._session_dirs\n\n    def _get_num_probes(self, path):\n        \"\"\"Determine the number of probes (silicon probe shanks, tetrodes, ...).\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        dict with {session: num_probes}\n\n        \"\"\"\n\n        num = 0\n        files = [f for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f)))]\n        for ff in files:\n            try:\n                found = re.search(r\"\\.clu\\.[0-9]+$\", ff).group(0)\n                logging.info('Found {} in session at \"{}\"'.format(found, path))\n                num += 1\n            except Exception:\n                pass\n        if num == 0:\n            logging.warning('Did not find any .clu files for session \"{}\"'.format(path))\n\n        return num\n\n    # def _get_num_probes(self, path=None):\n    #     \"\"\"Determine the number of probes (silicon probe shanks, tetrodes, ...).\n\n    #     Parameters\n    #     ----------\n\n    #     Returns\n    #     -------\n    #     dict with {session: num_probes}\n\n    #     \"\"\"\n    #     if path is None:\n    #         if self._sessions is None:\n    #             self._get_sessions()\n    #         sessions, dirs = self._sessions, self._session_dirs\n    #     else:\n    #         sessions, dirs = self._get_sessions(path=path, cache=False)\n\n    #     nums = {}\n    #     for (sdir, session) in zip(dirs, sessions):\n    #         num = 0\n    #         files = [f for f in os.listdir(sdir) if (os.path.isfile(os.path.join(sdir, f)))]\n    #         for ff in files:\n    #             try:\n    #                 found = re.search('\\.clu\\.[0-9]+$', ff).group(0)\n    #                 logging.info('Found {} in session \"{}\"'.format(found, session))\n    #                 num+=1\n    #             except Exception:\n    #                 pass\n    #         if num == 0:\n    #             logging.warning('Did not find any .clu files for session \"{}\"'.format(session))\n    #         nums[session] = num\n    #     return nums\n\n    def _load_xml(self):\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session in sessions:\n            p = self.data.get(session, None)\n            if p is None:\n                self.data[session] = {}\n            p = self.data[session].get(\"params\", None)\n            if p is None:\n                self.data[session][\"params\"] = {}\n\n        for session, sdir in zip(sessions, dirs):\n            with open(sdir + \"/\" + session + \".xml\") as fd:\n                xml = xmltodict.parse(fd.read())\n                self.data[session][\"xml\"] = xml\n                self.data[session][\"params\"][\"fs_wideband\"] = float(\n                    xml[\"parameters\"][\"acquisitionSystem\"][\"samplingRate\"]\n                )\n                self.data[session][\"params\"][\"n_channels\"] = float(\n                    xml[\"parameters\"][\"acquisitionSystem\"][\"nChannels\"]\n                )\n                self.data[session][\"params\"][\"fs_lfp\"] = float(\n                    xml[\"parameters\"][\"fieldPotentials\"][\"lfpSamplingRate\"]\n                )\n                self.data[session][\"params\"][\"fs_video\"] = float(\n                    xml[\"parameters\"][\"video\"][\"samplingRate\"]\n                )\n                self.data[session][\"params\"][\"video_width\"] = float(\n                    xml[\"parameters\"][\"video\"][\"width\"]\n                )\n                self.data[session][\"params\"][\"video_height\"] = float(\n                    xml[\"parameters\"][\"video\"][\"height\"]\n                )\n                self.data[session][\"params\"][\"snippet_len\"] = int(\n                    xml[\"parameters\"][\"neuroscope\"][\"spikes\"][\"nSamples\"]\n                )\n                self.data[session][\"params\"][\"snippet_peak_idx\"] = int(\n                    xml[\"parameters\"][\"neuroscope\"][\"spikes\"][\"peakSampleIndex\"]\n                )\n                self.data[session][\"params\"][\"n_probes\"] = self._get_num_probes(\n                    path=sdir\n                )\n\n    def _drop_xml(self):\n        sessions = self.sessions\n        for session in sessions:\n            self.data[session].pop(\"xml\", None)\n\n    def _load_events(self):\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session, sdir in zip(sessions, dirs):\n            events = OrderedDict()\n            filename = \"{}/{}.cat.evt\".format(sdir, session)\n            with open(filename) as f:\n                for line in f:\n                    line = line.strip()\n                    timestamp, descr = line.split(\" \", maxsplit=1)\n                    descr = descr.split(\"-\", maxsplit=3)[-1]\n                    timestamp = float(timestamp) / 1000\n                    try:\n                        events[descr] = EpochArray((events[descr], timestamp))\n                    except KeyError:\n                        events[descr] = timestamp\n\n            self.data[session][\"events\"] = events\n\n        return\n\n    def _load_pos(self):\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session, sdir in zip(sessions, dirs):\n            fs = self.data[session][\"params\"][\"fs_video\"]\n            width = self.data[session][\"params\"][\"video_width\"]\n            height = self.data[session][\"params\"][\"video_height\"]\n\n            if session in [\"Train-292-20150501\", \"Train-314-20160118\"]:\n                width = 640\n                height = 480\n\n            filename = \"{}/{}.pos\".format(sdir, session)\n\n            logging.info('reading position data from \"{}\"'.format(filename))\n            posdata = np.array(\n                pd.read_table(filename, sep=\"\\t\", skiprows=0, names=[\"x\", \"y\"])\n            )\n            timestamps = np.linspace(0, len(posdata) / fs, len(posdata))\n\n            logging.info(\"dropping samples with missing data\")\n            dropidx = np.where(posdata == -1)[0]\n            posdata = np.delete(posdata, dropidx, 0)\n            timestamps = np.delete(timestamps, dropidx, 0)\n\n            logging.info(\"building nelpy PositionArray\")\n            pos = PositionArray(\n                posdata.T,\n                abscissa_vals=timestamps,\n                fs=fs,\n                xlim=[0, width],\n                ylim=[0, height],\n            )\n            self.data[session][\"pos\"] = pos\n\n    def _load_spikes(self, include_mua=False, includeWaveforms=False):\n        # NOTE: st_array[0] always corresponds to unsortable spikes (i.e., MUA,\n        # not mechanical noise). However, when includeUnsortedSpikes==True, then\n        # it gets populated with spike times; else, it just remains an empty\n        # list []\n\n        sessions, dirs = self.sessions, self.session_dirs\n\n        for session, sdir in zip(sessions, dirs):\n            fs = self.data[session][\"params\"][\"fs_wideband\"]\n            fs = 32552.083\n            # num_probes = self.data[session][\"params\"][\"n_probes\"]\n            snippet_len = self.data[session][\"params\"][\"snippet_len\"]\n            snippet_peak_idx = self.data[session][\"params\"][\"snippet_peak_idx\"]\n            n_channels = int(self.data[session][\"params\"][\"n_channels\"] / 16)\n            if n_channels == 4:\n                print(\"tetrodes!\")\n            elif n_channels == 8:\n                print(\"octrodes!\")\n            else:\n                raise ValueError(\"unexpected number of channels for hc-18\")\n\n            clu_files = ns.humansorted(glob.glob(sdir + \"/{}.clu*\".format(session)))\n\n            if include_mua:\n                st_array = [[]]\n                wf_array = [[]]\n            else:\n                st_array = []\n                wf_array = []\n            wfdt = np.dtype(\n                \"&lt;h\", (snippet_len, n_channels)\n            )  # waveform datatype (.spk files)\n            # note: using pandas.read_table is orders of magnitude faster here than using numpy.loadtxt\n            for clu_file in clu_files:\n                pre, post = clu_file.split(\"clu\")\n                res_file = pre + \"res\" + post\n                spk_file = pre + \"spk\" + post\n                # %time dt1a = np.loadtxt( base_filename1 + '.clu.' + str(probe + 1), skiprows=1,dtype=int)\n                eudf = pd.read_table(\n                    clu_file, header=None, names=\"u\"\n                )  # read unit numbers within electrode\n                tsdf = pd.read_table(\n                    res_file, header=None, names=\"t\"\n                )  # read sample numbers for spikes\n                if includeWaveforms:\n                    waveforms = np.fromfile(spk_file, dtype=wfdt)\n                    waveforms = np.reshape(\n                        waveforms,\n                        (\n                            int(len(waveforms) / (snippet_len * n_channels)),\n                            snippet_len,\n                            n_channels,\n                        ),\n                    )\n                    waveforms = waveforms[:, snippet_peak_idx, :]\n\n                max_units = eudf.u.values[0]\n\n                eu = eudf.u.values[1:]\n                ts = tsdf.t.values\n\n                if includeWaveforms:\n                    noise_idx = np.argwhere(eu == 0).squeeze()\n                    hash_idx = np.argwhere(eu == 1).squeeze()\n                    all_idx = set(np.arange(len(eu)))\n                    discard_idx = set(noise_idx)\n\n                # discard units labeled as '0' or '1', as these correspond to mechanical noise and unsortable units\n                ts = ts[eu != 0]  # always discard mechanical noise\n                eu = eu[eu != 0]  # always discard mechanical noise\n\n                if not include_mua:\n                    ts = ts[eu != 1]  # potentially discard unsortable spikes\n                    eu = eu[eu != 1]  # potentially discard unsortable spikes\n                    if includeWaveforms:\n                        discard_idx = discard_idx.union(set(hash_idx))\n\n                if includeWaveforms:\n                    keep_idx = all_idx - discard_idx\n                    waveforms = waveforms[sorted(list(keep_idx))]\n\n                for uu in np.arange(max_units - 2):\n                    st_array.append(ts[eu == uu + 2])\n                    if includeWaveforms:\n                        wf_array.append(waveforms[eu == uu + 2])\n\n                if include_mua:\n                    st_array[0] = np.append(\n                        st_array[0], ts[eu == 1]\n                    )  # unit 0 now corresponds to unsortable spikes\n                    if includeWaveforms:\n                        if len(wf_array[0]) &gt; 0:\n                            wf_array[0] = np.vstack((wf_array[0], waveforms[eu == 1]))\n                        else:\n                            wf_array[0] = waveforms[eu == 1]\n\n            if include_mua:\n                unit_ids = np.arange(len(st_array))\n            else:\n                unit_ids = np.arange(1, len(st_array) + 1)\n\n            # make sure that spike times are sorted! (this is not true for unit 0 of the hc-3 dataset, for example):\n            for unit, spikes in enumerate(st_array):\n                order = np.argsort(spikes)\n                st_array[unit] = spikes[order] / fs\n                if includeWaveforms:\n                    wf_array[unit] = wf_array[unit][order]\n\n            if includeWaveforms:\n                spikes = SpikeTrainArray(st_array, fs=fs, unit_ids=unit_ids)\n                spikes._marks = wf_array\n            else:\n                spikes = SpikeTrainArray(st_array, fs=fs, unit_ids=unit_ids)\n\n            self.data[session][\"spikes\"] = spikes\n\n    def load(self, datatype=\"all\", data=None):\n        \"\"\"Load (and overwrite!) data into dictionary.\"\"\"\n        if data is None:\n            data = self.data\n        else:\n            logging.error(\"passing in a data dictionary is not fully supported yet!\")\n            raise NotImplementedError\n\n        if datatype == \"pos\":  # load position data\n            self._load_pos()\n\n        if datatype == \"spikes\":  # load sorted spike data\n            self._load_spikes()\n\n        if datatype == \"events\":\n            self._load_events()\n</code></pre>"},{"location":"reference/nelpy/io/hc18/#nelpy.io.hc18.DataLoaderHC18.load","title":"<code>load(datatype='all', data=None)</code>","text":"<p>Load (and overwrite!) data into dictionary.</p> Source code in <code>nelpy/io/hc18.py</code> <pre><code>def load(self, datatype=\"all\", data=None):\n    \"\"\"Load (and overwrite!) data into dictionary.\"\"\"\n    if data is None:\n        data = self.data\n    else:\n        logging.error(\"passing in a data dictionary is not fully supported yet!\")\n        raise NotImplementedError\n\n    if datatype == \"pos\":  # load position data\n        self._load_pos()\n\n    if datatype == \"spikes\":  # load sorted spike data\n        self._load_spikes()\n\n    if datatype == \"events\":\n        self._load_events()\n</code></pre>"},{"location":"reference/nelpy/io/hc3/","title":"nelpy.io.hc3","text":"<p>This file contains the nelpy io functions.</p> <p>This entire module will probably be deprecated soon, so don't rely on any of this to keep working!</p> <p>Examples:</p> <p>datadirs = ['/home/etienne/Dropbox/neoReader/Data',             'C:/etienne/Dropbox/neoReader/Data',             'C:/Users/etien/Dropbox/neoReader/Data',             '/Users/etienne/Dropbox/neoReader/Data',             'D:/Dropbox/neoReader/Data']</p> <p>fileroot = next( (dir for dir in datadirs if os.path.isdir(dir)), None)</p> <p>if fileroot is None:     raise FileNotFoundError('datadir not found')</p> <p>exp_data = dict() myhmm = dict() data = dict() sessiontime = dict()</p> <p>sessions = ['session1', 'session2']</p> <p>animal = 'gor01'; month,day = (6,7); sessiontime['session1'] = '11-26-53'; sessiontime['session2'] = '16-40-19' # 91 units, but session one has missing position data</p>"},{"location":"reference/nelpy/io/hc3/#nelpy.io.hc3--animal-gor01-monthday-612-sessiontimesession1-15-55-31-sessiontimesession2-16-53-46-55-units","title":"animal = 'gor01'; month,day = (6,12); sessiontime['session1'] = '15-55-31'; sessiontime['session2'] = '16-53-46' # 55 units","text":""},{"location":"reference/nelpy/io/hc3/#nelpy.io.hc3--animal-gor01-monthday-613-sessiontimesession1-14-42-6-sessiontimesession2-15-22-3","title":"animal = 'gor01'; month,day = (6,13); sessiontime['session1'] = '14-42-6'; sessiontime['session2'] = '15-22-3'","text":"<p>for session in sessions:</p> <pre><code>exp_data[session] = dict()\n\nexp_kws = dict(fileroot = fileroot,\n           animal = animal,\n           session = sessiontime[session],\n           month = month,\n           day = day,\n           includeUnsortedSpikes=False, # should be True for MUA analysis!\n           verbose = False)\n\nexp_data[session]['spikes'] = nel.load_hc3_data(datatype='spikes', fs=32552, **exp_kws)\n</code></pre>"},{"location":"reference/nelpy/io/hc3/#nelpy.io.hc3--exp_datasession-load_datadatatypeeeg-channels012-fs1252-starttime0-exp_kws","title":"exp_data[session]['eeg'] = load_data(datatype='eeg', channels=[0,1,2], fs=1252, starttime=0, **exp_kws)","text":""},{"location":"reference/nelpy/io/hc3/#nelpy.io.hc3--exp_datasession-load_datadatatypeposexp_kws","title":"exp_data[session]['posdf'] = load_data(datatype='pos',**exp_kws)","text":""},{"location":"reference/nelpy/io/hc3/#nelpy.io.hc3--exp_datasession-klabget_smooth_speedexp_datasessionfs60th8cutoff05showfigfalseverbosefalse","title":"exp_data[session]['speed'] = klab.get_smooth_speed(exp_data[session]['posdf'],fs=60,th=8,cutoff=0.5,showfig=False,verbose=False)","text":""},{"location":"reference/nelpy/io/hc3/#nelpy.io.hc3--make-st1-and-st2-explicitly-available","title":"make st1 and st2 explicitly available:","text":"<p>st1 = exp_data['session1']['spikes'] st2 = exp_data['session2']['spikes']</p>"},{"location":"reference/nelpy/io/hc3/#nelpy.io.hc3.get_hc3_dataframe","title":"<code>get_hc3_dataframe()</code>","text":"<p>this is not for hc-3, but rather for the same (but modified) dataset from Kamran Diba. Essentially Kamran just has a different (much better) unit sorting than what's available on CRCNS. However, with the waveform snippets available on CRCNS, we can do clusterless or our own sorting and get the same performance as Kamran's private dataset.</p> <p>This dataframe is useful in giving a summary of what data is availalbe, as well as demarcations for long and short segments on the track, etc.</p> <p>The data was manually compiled by E Ackermann.</p> Source code in <code>nelpy/io/hc3.py</code> <pre><code>def get_hc3_dataframe():\n    \"\"\"this is not for hc-3, but rather for the same (but modified) dataset\n    from Kamran Diba. Essentially Kamran just has a different (much better) unit\n    sorting than what's available on CRCNS. However, with the waveform snippets\n    available on CRCNS, we can do clusterless or our own sorting and get the\n    same performance as Kamran's private dataset.\n\n    This dataframe is useful in giving a summary of what data is availalbe, as\n    well as demarcations for long and short segments on the track, etc.\n\n    The data was manually compiled by E Ackermann.\n    \"\"\"\n    df = pd.DataFrame(\n        columns=(\n            \"animal\",\n            \"month\",\n            \"day\",\n            \"time\",\n            \"track\",\n            \"segments\",\n            \"segment_labels\",\n            \"whl\",\n            \"n_cells\",\n            \"Notes\",\n            \"has_waveforms\",\n        )\n    )\n\n    df = df.append(\n        {\n            \"animal\": \"pin01\",\n            \"month\": 11,\n            \"day\": 1,\n            \"time\": \"12-58-54\",\n            \"whl\": True,\n            \"track\": \"unknown\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 1670), (2100, 3025)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"pin01\",\n            \"month\": 11,\n            \"day\": 3,\n            \"time\": \"11-0-53\",\n            \"whl\": False,\n            \"track\": \"unknown\",\n            \"segment_labels\": (),\n            \"segments\": [],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"pin01\",\n            \"month\": 11,\n            \"day\": 3,\n            \"time\": \"20-28-3\",\n            \"whl\": True,\n            \"track\": \"unknown\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 700), (720, 1080)],\n        },\n        ignore_index=True,\n    )\n\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 9,\n            \"time\": \"17-29-30\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\", \"long2\"),\n            \"segments\": [(0, 800), (905, 1395), (1445, 1660)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 10,\n            \"time\": \"12-25-50\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 870), (970, 1390)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 10,\n            \"time\": \"21-2-40\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\", \"long2\"),\n            \"segments\": [(0, 590), (625, 937), (989, 1081)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 11,\n            \"time\": \"15-16-59\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 667), (734, 975)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 12,\n            \"time\": \"14-39-31\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 581), (620, 887)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 12,\n            \"time\": \"17-53-55\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\", \"long2\"),\n            \"segments\": [(0, 466), (534, 840), (888, 1178)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 16,\n            \"time\": \"15-12-23\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(22, 528), (650, 997)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 17,\n            \"time\": \"12-33-47\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(9, 438), (459, 865)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 18,\n            \"time\": \"13-6-1\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(14, 496), (519, 795)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 18,\n            \"time\": \"15-23-32\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(8, 283), (295, 499)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 19,\n            \"time\": \"13-34-40\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(10, 394), (413, 657)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 19,\n            \"time\": \"16-48-9\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 271), (359, 601)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 21,\n            \"time\": \"10-24-35\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(14, 465), (490, 777)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 25,\n            \"time\": \"14-28-51\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(9, 394), (405, 616)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 25,\n            \"time\": \"17-17-6\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(7, 316), (330, 520)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 26,\n            \"time\": \"13-22-13\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(20, 375), (415, 614)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 27,\n            \"time\": \"14-43-12\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"top\"),\n            \"segments\": [(9, 611), (640, 908)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 28,\n            \"time\": \"12-17-27\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"bottom\"),\n            \"segments\": [(11, 433), (446, 677)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 28,\n            \"time\": \"16-48-29\",\n            \"whl\": True,\n            \"track\": \"one\",\n            \"segment_labels\": (\"long\", \"bottom\"),\n            \"segments\": [(6, 347), (363, 600)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 9,\n            \"time\": \"16-40-54\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(289, 1150), (1224, 1709)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 10,\n            \"time\": \"12-58-3\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(21, 923), (984, 1450)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 10,\n            \"time\": \"19-11-57\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(25, 916), (1050, 1477)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 11,\n            \"time\": \"12-48-38\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(8, 705), (851, 1284)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 11,\n            \"time\": \"16-2-46\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(10, 578), (614, 886)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 12,\n            \"time\": \"14-59-23\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(7, 284)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 12,\n            \"time\": \"15-25-59\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(13, 462), (498, 855)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 16,\n            \"time\": \"14-49-24\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(14, 749), (773, 1035)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 16,\n            \"time\": \"18-47-52\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(13, 433), (444, 752)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 17,\n            \"time\": \"12-52-15\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(13, 464), (473, 801)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 18,\n            \"time\": \"13-28-57\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 396), (404, 619)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 18,\n            \"time\": \"15-38-2\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(13, 307), (316, 510)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 19,\n            \"time\": \"13-50-7\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(9, 297), (304, 505)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 19,\n            \"time\": \"16-37-40\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(17, 279), (289, 467)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 21,\n            \"time\": \"11-19-2\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(6, 358), (363, 577)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 25,\n            \"time\": \"13-20-55\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(30, 334), (348, 569)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 25,\n            \"time\": \"17-33-28\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(10, 277), (286, 456)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 26,\n            \"time\": \"13-51-50\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"short\", \"long2\"),\n            \"segments\": [(9, 317), (324, 506), (515, 766)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 27,\n            \"time\": \"18-21-57\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"top\"),\n            \"segments\": [(13, 279), (292, 493)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 28,\n            \"time\": \"12-38-13\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"bottom\"),\n            \"segments\": [(5, 286), (291, 526)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"vvp01\",\n            \"month\": 4,\n            \"day\": 28,\n            \"time\": \"17-6-14\",\n            \"whl\": True,\n            \"track\": \"two\",\n            \"segment_labels\": (\"long\", \"bottom\", \"short\"),\n            \"segments\": [(8, 343), (350, 593), (617, 791)],\n        },\n        ignore_index=True,\n    )\n\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 7,\n            \"time\": \"11-26-53\",\n            \"track\": \"one\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\"),\n            \"segments\": [(0, 1730)],\n            \"Notes\": \"missing position data for short segment\",\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 7,\n            \"time\": \"16-40-19\",\n            \"track\": \"two\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 1180), (1250, 2580)],\n            \"Notes\": \"there is a .whl_back file---what is this?\",\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 12,\n            \"time\": \"15-55-31\",\n            \"track\": \"one\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 660), (710, 1120)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 12,\n            \"time\": \"16-53-46\",\n            \"track\": \"two\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 470), (490, 796)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 13,\n            \"time\": \"14-42-6\",\n            \"track\": \"one\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 520), (540, 845)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 13,\n            \"time\": \"15-22-3\",\n            \"track\": \"two\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 530), (540, 865)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 8,\n            \"time\": \"15-46-47\",\n            \"track\": \"two\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\"),\n            \"segments\": [(0, 2400)],\n            \"Notes\": \"short segment seems bad\",\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 8,\n            \"time\": \"21-16-25\",\n            \"track\": \"two\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 720), (750, 1207)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 9,\n            \"time\": \"22-24-40\",\n            \"track\": \"two\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 912), (920, 2540)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 8,\n            \"time\": \"14-26-15\",\n            \"track\": \"one\",\n            \"whl\": False,\n            \"segment_labels\": (\"\"),\n            \"segments\": [],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 9,\n            \"time\": \"1-22-43\",\n            \"track\": \"one\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\", \"short\"),\n            \"segments\": [(0, 1012), (1035, 1652)],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 9,\n            \"time\": \"3-23-37\",\n            \"track\": \"one\",\n            \"whl\": True,\n            \"segment_labels\": (\"long\"),\n            \"segments\": [(28, 530)],\n            \"Notes\": \"no short segment?\",\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 8,\n            \"time\": \"10-43-37\",\n            \"track\": \"sleep\",\n            \"whl\": False,\n            \"segment_labels\": (\"sleep\"),\n            \"segments\": [],\n        },\n        ignore_index=True,\n    )\n    df = df.append(\n        {\n            \"animal\": \"gor01\",\n            \"month\": 6,\n            \"day\": 8,\n            \"time\": \"17-26-16\",\n            \"track\": \"sleep\",\n            \"whl\": False,\n            \"segment_labels\": (\"sleep\"),\n            \"segments\": [],\n        },\n        ignore_index=True,\n    )\n\n    df.month = df.month.astype(np.int64)\n    df.day = df.day.astype(np.int64)\n\n    df[(df.animal == \"gor01\") &amp; df.whl]\n\n    return df\n</code></pre>"},{"location":"reference/nelpy/io/matlab/","title":"nelpy.io.matlab","text":"<p>This module implements functionailty for loading Matlab .mat files</p>"},{"location":"reference/nelpy/io/matlab/#nelpy.io.matlab.load","title":"<code>load(filename, squeeze_me=True, name_space=None)</code>","text":"<p>Load all variables from .mat file.</p> <p>NOTE: You will need an HDF5 python library to read matlab 7.3 or     later format mat files. NOTE: To inject into global namespace, must be called with     name_Space=globals()</p> Source code in <code>nelpy/io/matlab.py</code> <pre><code>def load(filename, squeeze_me=True, name_space=None):\n    \"\"\"Load all variables from .mat file.\n\n    NOTE: You will need an HDF5 python library to read matlab 7.3 or\n        later format mat files.\n    NOTE: To inject into global namespace, must be called with\n        name_Space=globals()\n    \"\"\"\n\n    mat = scipy.io.loadmat(filename, squeeze_me=squeeze_me)\n\n    # optionally inject variables into desired name_space\n    if name_space is not None:\n        vars = [key for key in mat.keys() if \"__\" not in key]\n        for var in vars:\n            insert_into_namespace(var, mat[var], name_space)\n            # Global.var = mat[var]\n\n    return mat\n</code></pre>"},{"location":"reference/nelpy/io/miniscopy/","title":"nelpy.io.miniscopy","text":"<p>This module implements functionailty for converting to and from miniscoPy objects (https://github.com/PeyracheLab/miniscoPy).</p> <p>Specifically we may want to convert to and from their trace class, and perhaps others, too.</p>"},{"location":"reference/nelpy/io/neo/","title":"nelpy.io.neo","text":"<p>This module implements functionailty for converting to and from neo objects</p>"},{"location":"reference/nelpy/io/neuralynx/","title":"nelpy.io.neuralynx","text":"<p>Loads data stored in the formats used by the Neuralynx recording systems.</p>"},{"location":"reference/nelpy/io/neuralynx/#nelpy.io.neuralynx.load_ntt","title":"<code>load_ntt(filename, should_d2a=True)</code>","text":"<p>Loads a neuralynx .ntt tetrode spike file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> required <code>should_d2a</code> <code>True</code> <p>Returns:</p> Name Type Description <code>spiketimes</code> <code>array</code> <p>Spike times as floats (seconds)</p> <code>waveforms</code> <code>array</code> <p>Spike waveforms as (num_spikes, length_waveform, num_channels)</p> <code>fs</code> <code>float</code> <p>Sampling frequency (Hz)</p> <code>cellnums</code> <code>cell numbers</code> <code>Usage</code> <code>spiketimes, waveforms, frequency, cellnums = load_ntt('TT13.ntt')</code> Source code in <code>nelpy/io/neuralynx.py</code> <pre><code>def load_ntt(filename, should_d2a=True):\n    \"\"\"Loads a neuralynx .ntt tetrode spike file.\n\n    Parameters\n    ----------\n    filename: str\n    should_d2a: convert from integer to microVolt units (default True)\n\n    Returns\n    -------\n    spiketimes: np.array\n        Spike times as floats (seconds)\n    waveforms: np.array\n        Spike waveforms as (num_spikes, length_waveform, num_channels)\n    fs: float\n        Sampling frequency (Hz)\n    cellnums: cell numbers\n\n    Usage:\n    spiketimes, waveforms, frequency, cellnums = load_ntt('TT13.ntt')\n    \"\"\"\n\n    with open(filename, \"rb\") as f:\n        # A tetrode spike record is as folows:\n        # uint64 - timestamp                    bytes 0:8\n        # uint32 - acquisition entity number    bytes 8:12\n        # uint32 - classified cell number       bytes 12:16\n        # 8 * uint32- params                    bytes 16:48\n        # 32 * 4 * int16 - waveform points\n        # hence total record size is 2432 bits, 304 bytes\n\n        # header is 16kbyte, i.e. 16 * 2^10 = 2^14\n        header = f.read(2**14).strip(b\"\\x00\")\n\n        # Read the header and find the conversion factors / sampling frequency\n        analog_to_digital = None\n        fs = None\n\n        for line in header.split(b\"\\n\"):\n            if line.strip().startswith(b\"-ADBitVolts\"):\n                analog_to_digital = np.array(float(line.split(b\" \")[1].decode()))\n            if line.strip().startswith(b\"-SamplingFrequency\"):\n                fs = float(line.split(b\" \")[1].decode())\n\n        f.seek(2**14)  # start of the spike, records\n        # Neuralynx writes little endian for some reason\n        dt = np.dtype(\n            [\n                (\"spiketimes\", \"&lt;Q\"),\n                (\"acq\", \"&lt;i\", 1),\n                (\"cellnums\", \"&lt;i\", 1),\n                (\"params\", \"&lt;i\", 8),\n                (\"waveforms\", np.dtype(\"&lt;h\"), (32, 4)),\n            ]\n        )\n        data = np.fromfile(f, dt)\n\n        if analog_to_digital is None:\n            raise IOError(\"ADBitVolts not found in .ntt header for \" + filename)\n        if fs is None:\n            raise IOError(\"Frequency not found in .ntt header for \" + filename)\n\n    return (\n        data[\"spiketimes\"] / 1e6,\n        data[\"waveforms\"] * analog_to_digital,\n        fs,\n        data[\"cellnums\"],\n    )\n</code></pre>"},{"location":"reference/nelpy/io/neuralynx/#nelpy.io.neuralynx.load_nvt","title":"<code>load_nvt(filename)</code>","text":"<p>Loads a neuralynx .nvt file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> required <p>Returns:</p> Name Type Description <code>nvt_data</code> <code>dict</code> <p>With time, x, and y as keys.</p> Source code in <code>nelpy/io/neuralynx.py</code> <pre><code>def load_nvt(filename):\n    \"\"\"Loads a neuralynx .nvt file.\n    Parameters\n    ----------\n    filename: str\n    Returns\n    -------\n    nvt_data: dict\n        With time, x, and y as keys.\n    \"\"\"\n    with open(filename, \"rb\") as f:\n        # Neuralynx files have a 16kbyte header\n        # header = f.read(2**14).strip(b\"\\x00\")\n\n        # The format for .nvt files according the the neuralynx docs is\n        # uint16 - beginning of the record\n        # uint16 - ID for the system\n        # uint16 - size of videorec in bytes\n        # uint64 - timestamp in microseconds\n        # uint32 x 400 - points with the color bitfield values\n        # int16 - unused\n        # int32 - extracted X location of target\n        # int32 - extracted Y location of target\n        # int32 - calculated head angle in degrees clockwise from the positive Y axis\n        # int32 x 50 - colored targets using the same bitfield format used to extract colors earlier\n        dt = np.dtype(\n            [\n                (\"filler1\", \"&lt;h\", 3),\n                (\"time\", \"&lt;Q\"),\n                (\"points\", \"&lt;i\", 400),\n                (\"filler2\", \"&lt;h\"),\n                (\"x\", \"&lt;i\"),\n                (\"y\", \"&lt;i\"),\n                (\"head_angle\", \"&lt;i\"),\n                (\"targets\", \"&lt;i\", 50),\n            ]\n        )\n        data = np.fromfile(f, dt)\n\n    nvt_data = dict()\n    nvt_data[\"time\"] = data[\"time\"] * 1e-6\n    nvt_data[\"x\"] = np.array(data[\"x\"], dtype=float)\n    nvt_data[\"y\"] = np.array(data[\"y\"], dtype=float)\n    nvt_data[\"head_angle\"] = np.array(data[\"head_angle\"], dtype=float)\n    nvt_data[\"targets\"] = np.array(data[\"targets\"], dtype=float)\n\n    empty_idx = (data[\"x\"] == 0) &amp; (data[\"y\"] == 0)\n    for key in nvt_data:\n        nvt_data[key] = nvt_data[key][~empty_idx]\n\n    return nvt_data\n</code></pre>"},{"location":"reference/nelpy/io/neuralynx/#nelpy.io.neuralynx.load_position","title":"<code>load_position(filename, pixels_per_cm=None)</code>","text":"<p>Loads videotracking position as a nelpy PositionArray</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> required <code>pixel_per_cm</code> <p>With (x, y) conversion factors</p> required <p>Returns:</p> Name Type Description <code>position</code> <code>PositionArray</code> Source code in <code>nelpy/io/neuralynx.py</code> <pre><code>def load_position(filename, pixels_per_cm=None):\n    \"\"\"Loads videotracking position as a nelpy PositionArray\n\n    Parameters\n    ----------\n    filename: str\n    pixel_per_cm: tuple\n        With (x, y) conversion factors\n    Returns\n    -------\n    position: nelpy.PositionArray\n    \"\"\"\n\n    if not pixels_per_cm:\n        pixels_per_cm = (1, 1)\n\n    nvt_data = load_nvt(filename)\n\n    xydata = np.vstack(\n        (nvt_data[\"x\"] / pixels_per_cm[0], nvt_data[\"y\"] / pixels_per_cm[1])\n    )\n    timestamps = nvt_data[\"time\"]\n    pos = auxiliary.PositionArray(xydata, timestamps=timestamps)\n    return pos\n</code></pre>"},{"location":"reference/nelpy/plotting/colormaps/","title":"nelpy.plotting.colormaps","text":""},{"location":"reference/nelpy/plotting/colors/","title":"nelpy.plotting.colors","text":""},{"location":"reference/nelpy/plotting/colors/#nelpy.plotting.colors--nelpyplottingcolors","title":"nelpy.plotting.colors","text":"<p>This module provides named color palettes and the ColorGroup class for use in nelpy plotting.</p>"},{"location":"reference/nelpy/plotting/colors/#nelpy.plotting.colors.ColorGroup","title":"<code>ColorGroup</code>","text":"<p>An unordered, named color group for managing and plotting color palettes.</p> <p>Attributes:</p> Name Type Description <code>n_colors</code> <code>int</code> <p>Number of colors in the group.</p> <code>color_names</code> <code>list of str</code> <p>List of color names, ordered by hue.</p> <code>colors</code> <code>list of str</code> <p>List of color hex codes, ordered by hue.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cg = ColorGroup(green=\"#00CF97\", red=\"#F05340\")\n&gt;&gt;&gt; cg.plot()\n</code></pre> Source code in <code>nelpy/plotting/colors.py</code> <pre><code>class ColorGroup:\n    \"\"\"\n    An unordered, named color group for managing and plotting color palettes.\n\n    Attributes\n    ----------\n    n_colors : int\n        Number of colors in the group.\n    color_names : list of str\n        List of color names, ordered by hue.\n    colors : list of str\n        List of color hex codes, ordered by hue.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cg = ColorGroup(green=\"#00CF97\", red=\"#F05340\")\n    &gt;&gt;&gt; cg.plot()\n    \"\"\"\n\n    def __init__(self, *args, label=None, **kwargs):\n        \"\"\"\n        Initialize a ColorGroup with named colors.\n\n        Parameters\n        ----------\n        *args : dict\n            Dictionaries of color names to hex codes.\n        label : str, optional\n            Optional label for the color group.\n        **kwargs : dict\n            Additional color names and hex codes.\n        \"\"\"\n        for arg in args:\n            if isinstance(arg, dict):\n                for k, v in arg.items():\n                    self[k] = v\n\n        if kwargs:\n            for k, v in kwargs.items():\n                self[k] = v\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the ColorGroup.\n        \"\"\"\n        self.plot(size=0.5)\n        address_str = \" at \" + str(hex(id(self)))\n        return \"&lt;ColorGroup\" + address_str + \" with \" + str(self.n_colors) + \" colors&gt;\"\n\n    @property\n    def n_colors(self):\n        \"\"\"\n        Return the number of colors in the group.\n\n        Returns\n        -------\n        int\n            Number of colors in the group.\n        \"\"\"\n        return len(self._colors)\n\n    def __getattr__(self, attr):\n        if attr not in self.__dict__:\n            raise AttributeError(\"{} is not a valid attribute\".format(str(attr)))\n        return self.get(attr)\n\n    def __setattr__(self, key, value):\n        self.__setitem__(key, value)\n\n    def __setitem__(self, key, value):\n        self.__dict__.update({key: value})\n\n    def __delattr__(self, item):\n        self.__delitem__(item)\n\n    def __delitem__(self, key):\n        del self.__dict__[key]\n\n    @property\n    def color_names(self):\n        \"\"\"\n        Return the list of color names, ordered by hue.\n\n        Returns\n        -------\n        list of str\n            Color names ordered by hue.\n        \"\"\"\n        hue_ordered_names = [list(self.__dict__.keys())[i] for i in self._hue_order]\n        return hue_ordered_names\n\n    @property\n    def _colors(self):\n        \"\"\"\n        Return the list of unordered colors.\n\n        Returns\n        -------\n        list of str\n            Unordered color hex codes.\n        \"\"\"\n        return list(self.__dict__.values())\n\n    @property\n    def colors(self):\n        \"\"\"\n        Return the list of colors, ordered by hue.\n\n        Returns\n        -------\n        list of str\n            Color hex codes ordered by hue.\n        \"\"\"\n        hue_ordered_colors = [list(self.__dict__.values())[i] for i in self._hue_order]\n        return hue_ordered_colors\n\n    @property\n    def _hue_order(self):\n        \"\"\"\n        Return the order of colors by hue.\n\n        Returns\n        -------\n        list of int\n            Indices of colors ordered by hue.\n        \"\"\"\n        return sorted(range(self.n_colors), key=lambda k: _get_hsv(self._colors[k]))\n\n    def plot(self, size=0.5):\n        \"\"\"\n        Plot all the colors in the ColorGroup.\n\n        Parameters\n        ----------\n        size : float, optional\n            Size of the color swatches (default is 0.5).\n        \"\"\"\n        _palplot(self.colors, size=size)\n</code></pre>"},{"location":"reference/nelpy/plotting/colors/#nelpy.plotting.colors.ColorGroup.color_names","title":"<code>color_names</code>  <code>property</code>","text":"<p>Return the list of color names, ordered by hue.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>Color names ordered by hue.</p>"},{"location":"reference/nelpy/plotting/colors/#nelpy.plotting.colors.ColorGroup.colors","title":"<code>colors</code>  <code>property</code>","text":"<p>Return the list of colors, ordered by hue.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>Color hex codes ordered by hue.</p>"},{"location":"reference/nelpy/plotting/colors/#nelpy.plotting.colors.ColorGroup.n_colors","title":"<code>n_colors</code>  <code>property</code>","text":"<p>Return the number of colors in the group.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of colors in the group.</p>"},{"location":"reference/nelpy/plotting/colors/#nelpy.plotting.colors.ColorGroup.plot","title":"<code>plot(size=0.5)</code>","text":"<p>Plot all the colors in the ColorGroup.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>float</code> <p>Size of the color swatches (default is 0.5).</p> <code>0.5</code> Source code in <code>nelpy/plotting/colors.py</code> <pre><code>def plot(self, size=0.5):\n    \"\"\"\n    Plot all the colors in the ColorGroup.\n\n    Parameters\n    ----------\n    size : float, optional\n        Size of the color swatches (default is 0.5).\n    \"\"\"\n    _palplot(self.colors, size=size)\n</code></pre>"},{"location":"reference/nelpy/plotting/core/","title":"nelpy.plotting.core","text":""},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.colorline","title":"<code>colorline(x, y, cmap=None, cm_range=(0, 0.7), **kwargs)</code>","text":"<p>Plot a trajectory of (x, y) points with a colormap along the path.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>X coordinates of the trajectory.</p> required <code>y</code> <code>array - like</code> <p>Y coordinates of the trajectory.</p> required <code>cmap</code> <code>Colormap</code> <p>Colormap to use for coloring the line. Defaults to plt.cm.Blues_r.</p> <code>None</code> <code>cm_range</code> <code>tuple of float</code> <p>Range of the colormap to use (min, max). Defaults to (0, 0.7).</p> <code>(0, 0.7)</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the plot (e.g., ax, lw).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>lc</code> <code>LineCollection</code> <p>The colored line collection added to the axis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; colorline(x, y, cmap=plt.cm.viridis)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def colorline(x, y, cmap=None, cm_range=(0, 0.7), **kwargs):\n    \"\"\"\n    Plot a trajectory of (x, y) points with a colormap along the path.\n\n    Parameters\n    ----------\n    x : array-like\n        X coordinates of the trajectory.\n    y : array-like\n        Y coordinates of the trajectory.\n    cmap : matplotlib.colors.Colormap, optional\n        Colormap to use for coloring the line. Defaults to plt.cm.Blues_r.\n    cm_range : tuple of float, optional\n        Range of the colormap to use (min, max). Defaults to (0, 0.7).\n    **kwargs : dict\n        Additional keyword arguments passed to the plot (e.g., ax, lw).\n\n    Returns\n    -------\n    lc : matplotlib.collections.LineCollection\n        The colored line collection added to the axis.\n\n    Examples\n    --------\n    &gt;&gt;&gt; colorline(x, y, cmap=plt.cm.viridis)\n    \"\"\"\n\n    # plt.plot(x, y, '-k', zorder=1)\n    # plt.scatter(x, y, s=40, c=plt.cm.RdBu(np.linspace(0,1,40)), zorder=2, edgecolor='k')\n\n    assert len(cm_range) == 2, \"cm_range must have (min, max)\"\n    assert len(x) == len(y), \"x and y must have the same number of elements!\"\n\n    ax = kwargs.get(\"ax\", plt.gca())\n    lw = kwargs.get(\"lw\", 2)\n    if cmap is None:\n        cmap = plt.cm.Blues_r\n\n    t = np.linspace(cm_range[0], cm_range[1], len(x))\n\n    points = np.array([x, y]).T.reshape(-1, 1, 2)\n    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n\n    lc = LineCollection(segments, cmap=cmap, norm=plt.Normalize(0, 1), zorder=50)\n    lc.set_array(t)\n    lc.set_linewidth(lw)\n\n    ax.add_collection(lc)\n\n    return lc\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.comboplot","title":"<code>comboplot(*, ax=None, raster=None, analog=None, events=None)</code>","text":"<p>Create a combo plot showing spike/state raster, analog signals, and events.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>raster</code> <code>array - like</code> <p>Raster data to plot.</p> <code>None</code> <code>analog</code> <code>array - like</code> <p>Analog signal data to plot.</p> <code>None</code> <code>events</code> <code>array - like</code> <p>Event data to plot.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the combo plot.</p> Notes <p>This function is not yet implemented.</p> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def comboplot(*, ax=None, raster=None, analog=None, events=None):\n    \"\"\"\n    Create a combo plot showing spike/state raster, analog signals, and events.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    raster : array-like, optional\n        Raster data to plot.\n    analog : array-like, optional\n        Analog signal data to plot.\n    events : array-like, optional\n        Event data to plot.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the combo plot.\n\n    Notes\n    -----\n    This function is not yet implemented.\n    \"\"\"\n\n    # Sort out default values for the parameters\n    if ax is None:\n        ax = plt.gca()\n\n    raise NotImplementedError(\"comboplot() not implemented yet\")\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.epochplot","title":"<code>epochplot(epochs, data=None, *, ax=None, height=None, fc='0.5', ec='0.5', alpha=0.5, hatch='', label=None, hc=None, **kwargs)</code>","text":"<p>Plot an EpochArray as horizontal bars (intervals) on a timeline.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>EpochArray</code> <p>The epochs to plot.</p> required <code>data</code> <code>array - like</code> <p>Data to plot on y axis; must be of size (epochs.n_epochs,).</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>height</code> <code>float</code> <p>Height of the bars. If None, uses default.</p> <code>None</code> <code>fc</code> <code>color</code> <p>Face color of the bars. Default is '0.5'.</p> <code>'0.5'</code> <code>ec</code> <code>color</code> <p>Edge color of the bars. Default is '0.5'.</p> <code>'0.5'</code> <code>alpha</code> <code>float</code> <p>Transparency of the bars. Default is 0.5.</p> <code>0.5</code> <code>hatch</code> <code>str</code> <p>Hatching pattern for the bars.</p> <code>''</code> <code>label</code> <code>str</code> <p>Label for the bars.</p> <code>None</code> <code>hc</code> <code>color</code> <p>Highlight color for the bars.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's barh.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the epoch plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import EpochArray\n&gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n&gt;&gt;&gt; epochplot(epochs)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def epochplot(\n    epochs,\n    data=None,\n    *,\n    ax=None,\n    height=None,\n    fc=\"0.5\",\n    ec=\"0.5\",\n    alpha=0.5,\n    hatch=\"\",\n    label=None,\n    hc=None,\n    **kwargs,\n):\n    \"\"\"\n    Plot an EpochArray as horizontal bars (intervals) on a timeline.\n\n    Parameters\n    ----------\n    epochs : nelpy.EpochArray\n        The epochs to plot.\n    data : array-like, optional\n        Data to plot on y axis; must be of size (epochs.n_epochs,).\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    height : float, optional\n        Height of the bars. If None, uses default.\n    fc : color, optional\n        Face color of the bars. Default is '0.5'.\n    ec : color, optional\n        Edge color of the bars. Default is '0.5'.\n    alpha : float, optional\n        Transparency of the bars. Default is 0.5.\n    hatch : str, optional\n        Hatching pattern for the bars.\n    label : str, optional\n        Label for the bars.\n    hc : color, optional\n        Highlight color for the bars.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's barh.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the epoch plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import EpochArray\n    &gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n    &gt;&gt;&gt; epochplot(epochs)\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    # do fixed-value-on-epoch plot if data is not None\n    if data is not None:\n        if epochs.n_intervals != len(data):\n            raise ValueError(\"epocharray and data must have the same length\")\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for epoch, val in zip(epochs, data):\n                ax.plot([epoch.start, epoch.stop], [val, val], \"-o\", **kwargs)\n        return ax\n\n    ymin, ymax = ax.get_ylim()\n    xmin, xmax = ax.get_xlim()\n    if height is None:\n        height = ymax - ymin\n\n    if hc is not None:\n        try:\n            hc_before = mpl.rcParams[\"hatch.color\"]\n            mpl.rcParams[\"hatch.color\"] = hc\n        except KeyError:\n            warnings.warn(\"Hatch color not supported for matplotlib &lt;2.0\")\n\n    for ii, (start, stop) in enumerate(zip(epochs.starts, epochs.stops)):\n        ax.axvspan(\n            start,\n            stop,\n            hatch=hatch,\n            facecolor=fc,\n            edgecolor=ec,\n            alpha=alpha,\n            label=label if ii == 0 else \"_nolegend_\",\n            **kwargs,\n        )\n\n    if epochs.start &lt; xmin:\n        xmin = epochs.start\n    if epochs.stop &gt; xmax:\n        xmax = epochs.stop\n    ax.set_xlim([xmin, xmax])\n\n    if hc is not None:\n        try:\n            mpl.rcParams[\"hatch.color\"] = hc_before\n        except UnboundLocalError:\n            pass\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.imagesc","title":"<code>imagesc(x=None, y=None, data=None, *, ax=None, large=False, **kwargs)</code>","text":"<p>Plot a 2D matrix or image similar to Matlab's imagesc.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>X values (columns).</p> <code>None</code> <code>y</code> <code>array - like</code> <p>Y values (rows).</p> <code>None</code> <code>data</code> <code>ndarray of shape (Nrows, Ncols)</code> <p>Matrix to visualize.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Plot in given axis; if None creates a new figure.</p> <code>None</code> <code>large</code> <code>bool</code> <p>If True, optimize for large matrices. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to imshow.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>im</code> <code>AxesImage</code> <p>The image object.</p> <p>Examples:</p> <p>Plot a simple matrix using imagesc:</p> <pre><code>&gt;&gt;&gt; x = np.linspace(-100, -10, 10)\n&gt;&gt;&gt; y = np.array([-8, -3.0])\n&gt;&gt;&gt; data = np.random.randn(y.size, x.size)\n&gt;&gt;&gt; imagesc(x, y, data)\nor\n&gt;&gt;&gt; imagesc(data)\n</code></pre> <p>Adding a colorbar:</p> <pre><code>&gt;&gt;&gt; ax, img = imagesc(data)\n&gt;&gt;&gt; from mpl_toolkits.axes_grid1 import make_axes_locatable\n&gt;&gt;&gt; divider = make_axes_locatable(ax)\n&gt;&gt;&gt; cax = divider.append_axes(\"right\", size=\"3.5%\", pad=0.1)\n&gt;&gt;&gt; cb = plt.colorbar(img, cax=cax)\n&gt;&gt;&gt; npl.utils.no_yticks(cax)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def imagesc(x=None, y=None, data=None, *, ax=None, large=False, **kwargs):\n    \"\"\"\n    Plot a 2D matrix or image similar to Matlab's imagesc.\n\n    Parameters\n    ----------\n    x : array-like, optional\n        X values (columns).\n    y : array-like, optional\n        Y values (rows).\n    data : ndarray of shape (Nrows, Ncols)\n        Matrix to visualize.\n    ax : matplotlib.axes.Axes, optional\n        Plot in given axis; if None creates a new figure.\n    large : bool, optional\n        If True, optimize for large matrices. Default is False.\n    **kwargs : dict\n        Additional keyword arguments passed to imshow.\n\n    Returns\n    -------\n    im : matplotlib.image.AxesImage\n        The image object.\n\n    Examples\n    --------\n    Plot a simple matrix using imagesc:\n\n    &gt;&gt;&gt; x = np.linspace(-100, -10, 10)\n    &gt;&gt;&gt; y = np.array([-8, -3.0])\n    &gt;&gt;&gt; data = np.random.randn(y.size, x.size)\n    &gt;&gt;&gt; imagesc(x, y, data)\n    or\n    &gt;&gt;&gt; imagesc(data)\n\n    Adding a colorbar:\n\n    &gt;&gt;&gt; ax, img = imagesc(data)\n    &gt;&gt;&gt; from mpl_toolkits.axes_grid1 import make_axes_locatable\n    &gt;&gt;&gt; divider = make_axes_locatable(ax)\n    &gt;&gt;&gt; cax = divider.append_axes(\"right\", size=\"3.5%\", pad=0.1)\n    &gt;&gt;&gt; cb = plt.colorbar(img, cax=cax)\n    &gt;&gt;&gt; npl.utils.no_yticks(cax)\n    \"\"\"\n\n    def extents(f):\n        if len(f) &gt; 1:\n            delta = f[1] - f[0]\n        else:\n            delta = 1\n        return [f[0] - delta / 2, f[-1] + delta / 2]\n\n    if ax is None:\n        ax = plt.gca()\n    if data is None:\n        if x is None:  # no args\n            raise ValueError(\n                \"Unknown input. Usage imagesc(x, y, data) or imagesc(data).\"\n            )\n        elif y is None:  # only one arg, so assume it to be data\n            data = x\n            x = np.arange(data.shape[1])\n            y = np.arange(data.shape[0])\n        else:  # x and y, but no data\n            raise ValueError(\n                \"Unknown input. Usage imagesc(x, y, data) or imagesc(data).\"\n            )\n\n    if data.ndim != 2:\n        raise TypeError(\"data must be 2 dimensional\")\n\n    if not large:\n        # Matplotlib imshow\n        image = ax.imshow(\n            data,\n            aspect=\"auto\",\n            interpolation=\"none\",\n            extent=extents(x) + extents(y),\n            origin=\"lower\",\n            **kwargs,\n        )\n    else:\n        # ModestImage imshow for large images, but 'extent' is still not working well\n        image = utils.imshow(\n            axes=ax,\n            X=data,\n            aspect=\"auto\",\n            interpolation=\"none\",\n            extent=extents(x) + extents(y),\n            origin=\"lower\",\n            **kwargs,\n        )\n\n    return ax, image\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.imshow","title":"<code>imshow(data, *, ax=None, interpolation=None, **kwargs)</code>","text":"<p>Display an image (matrix) using matplotlib's imshow.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>The image data to display.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>interpolation</code> <code>str</code> <p>Interpolation method. Default is 'none'.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's imshow.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>im</code> <code>AxesImage</code> <p>The image object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; img = np.random.rand(10, 10)\n&gt;&gt;&gt; imshow(img)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def imshow(data, *, ax=None, interpolation=None, **kwargs):\n    \"\"\"\n    Display an image (matrix) using matplotlib's imshow.\n\n    Parameters\n    ----------\n    data : array-like\n        The image data to display.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    interpolation : str, optional\n        Interpolation method. Default is 'none'.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's imshow.\n\n    Returns\n    -------\n    im : matplotlib.image.AxesImage\n        The image object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; img = np.random.rand(10, 10)\n    &gt;&gt;&gt; imshow(img)\n    \"\"\"\n\n    # set default interpolation mode to 'none'\n    if interpolation is None:\n        interpolation = \"none\"\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.matshow","title":"<code>matshow(data, *, ax=None, **kwargs)</code>","text":"<p>Display a matrix in a new figure window using matplotlib's matshow.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like or BinnedSpikeTrainArray</code> <p>The matrix or nelpy object to display.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's matshow.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mat = np.random.rand(5, 5)\n&gt;&gt;&gt; matshow(mat)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def matshow(data, *, ax=None, **kwargs):\n    \"\"\"\n    Display a matrix in a new figure window using matplotlib's matshow.\n\n    Parameters\n    ----------\n    data : array-like or nelpy.BinnedSpikeTrainArray\n        The matrix or nelpy object to display.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's matshow.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; mat = np.random.rand(5, 5)\n    &gt;&gt;&gt; matshow(mat)\n    \"\"\"\n\n    # Sort out default values for the parameters\n    if ax is None:\n        ax = plt.gca()\n\n    # Handle different types of input data\n    if isinstance(data, core.BinnedSpikeTrainArray):\n        # TODO: split by epoch, and plot matshows in same row, but with\n        # a small gap to indicate discontinuities. How about slicing\n        # then? Or slicing within an epoch?\n        ax.matshow(data.data, **kwargs)\n        ax.set_xlabel(\"time\")\n        ax.set_ylabel(\"unit\")\n        # warnings.warn(\"Automatic x-axis formatting not yet implemented\")\n    else:\n        raise NotImplementedError(\n            \"matshow({}) not yet supported\".format(str(type(data)))\n        )\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.occupancy","title":"<code>occupancy()</code>","text":"<p>Compute and/or plot the occupancy (time spent) in each spatial bin.</p> <p>Returns:</p> Name Type Description <code>occupancy</code> <code>ndarray</code> <p>Array of occupancy values for each bin.</p> Notes <p>This function is not yet implemented.</p> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def occupancy():\n    \"\"\"\n    Compute and/or plot the occupancy (time spent) in each spatial bin.\n\n    Returns\n    -------\n    occupancy : np.ndarray\n        Array of occupancy values for each bin.\n\n    Notes\n    -----\n    This function is not yet implemented.\n    \"\"\"\n    raise NotImplementedError(\"occupancy() not implemented yet\")\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.overviewstrip","title":"<code>overviewstrip(epochs, *, ax=None, lw=5, solid_capstyle='butt', label=None, **kwargs)</code>","text":"<p>Plot an epoch array as a strip (like a scrollbar) to show gaps in e.g. matshow plots.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>EpochArray</code> <p>The epochs to plot as a strip.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width for the strip. Default is 5.</p> <code>5</code> <code>solid_capstyle</code> <code>str</code> <p>Cap style for the strip. Default is 'butt'.</p> <code>'butt'</code> <code>label</code> <code>str</code> <p>Label for the strip.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's plot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the overview strip.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import EpochArray\n&gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n&gt;&gt;&gt; overviewstrip(epochs)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def overviewstrip(\n    epochs, *, ax=None, lw=5, solid_capstyle=\"butt\", label=None, **kwargs\n):\n    \"\"\"\n    Plot an epoch array as a strip (like a scrollbar) to show gaps in e.g. matshow plots.\n\n    Parameters\n    ----------\n    epochs : nelpy.EpochArray\n        The epochs to plot as a strip.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    lw : float, optional\n        Line width for the strip. Default is 5.\n    solid_capstyle : str, optional\n        Cap style for the strip. Default is 'butt'.\n    label : str, optional\n        Label for the strip.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's plot.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the overview strip.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import EpochArray\n    &gt;&gt;&gt; epochs = EpochArray([[0, 1], [2, 3], [5, 6]])\n    &gt;&gt;&gt; overviewstrip(epochs)\n    \"\"\"\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    if ax is None:\n        ax = plt.gca()\n\n    divider = make_axes_locatable(ax)\n    ax_ = divider.append_axes(\"top\", size=0.2, pad=0.05)\n\n    for epoch in epochs:\n        ax_.plot(\n            [epoch.start, epoch.stop],\n            [1, 1],\n            lw=lw,\n            solid_capstyle=solid_capstyle,\n            **kwargs,\n        )\n\n    if label is not None:\n        ax_.set_yticks([1])\n        ax_.set_yticklabels([label])\n    else:\n        ax_.set_yticks([])\n\n    utils.no_yticks(ax_)\n    utils.clear_left(ax_)\n    utils.clear_right(ax_)\n    utils.clear_top_bottom(ax_)\n\n    ax_.set_xlim(ax.get_xlim())\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.plot","title":"<code>plot(obj, *args, **kwargs)</code>","text":"<p>Plot a nelpy object or array-like data using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>nelpy object or array-like</code> <p>The object or data to plot. Can be a nelpy RegularlySampledAnalogSignalArray or array-like.</p> required <code>*args</code> <code>tuple</code> <p>Additional positional arguments passed to matplotlib's plot.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's plot. Special keys:     ax : matplotlib.axes.Axes, optional         Axis to plot on. If None, uses current axis.     autoscale : bool, optional         Whether to autoscale the axis. Default is True.     xlabel : str, optional         X-axis label.     ylabel : str, optional         Y-axis label.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.core import RegularlySampledAnalogSignalArray\n&gt;&gt;&gt; obj = RegularlySampledAnalogSignalArray(...)  # your data here\n&gt;&gt;&gt; plot(obj)\n&gt;&gt;&gt; plot([1, 2, 3, 4])\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def plot(obj, *args, **kwargs):\n    \"\"\"\n    Plot a nelpy object or array-like data using matplotlib.\n\n    Parameters\n    ----------\n    obj : nelpy object or array-like\n        The object or data to plot. Can be a nelpy RegularlySampledAnalogSignalArray or array-like.\n    *args : tuple\n        Additional positional arguments passed to matplotlib's plot.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's plot. Special keys:\n            ax : matplotlib.axes.Axes, optional\n                Axis to plot on. If None, uses current axis.\n            autoscale : bool, optional\n                Whether to autoscale the axis. Default is True.\n            xlabel : str, optional\n                X-axis label.\n            ylabel : str, optional\n                Y-axis label.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.core import RegularlySampledAnalogSignalArray\n    &gt;&gt;&gt; obj = RegularlySampledAnalogSignalArray(...)  # your data here\n    &gt;&gt;&gt; plot(obj)\n    &gt;&gt;&gt; plot([1, 2, 3, 4])\n    \"\"\"\n\n    ax = kwargs.pop(\"ax\", None)\n    autoscale = kwargs.pop(\"autoscale\", True)\n    xlabel = kwargs.pop(\"xlabel\", None)\n    ylabel = kwargs.pop(\"ylabel\", None)\n\n    if ax is None:\n        ax = plt.gca()\n\n    # Patch: If obj is an EpochArray, delegate to plot_old\n    if isinstance(obj, core.EpochArray):\n        return plot_old(obj, *args, **kwargs)\n\n    if isinstance(obj, core.RegularlySampledAnalogSignalArray):\n        if obj.n_signals == 1:\n            label = kwargs.pop(\"label\", None)\n            for ii, (abscissa_vals, data) in enumerate(\n                zip(\n                    obj._intervaltime.plot_generator(),\n                    obj._intervaldata.plot_generator(),\n                )\n            ):\n                ax.plot(\n                    abscissa_vals,\n                    data.T,\n                    label=label if ii == 0 else \"_nolegend_\",\n                    *args,\n                    **kwargs,\n                )\n        elif obj.n_signals &gt; 1:\n            # TODO: intercept when any color is requested. This could happen\n            # multiple ways, such as plt.plot(x, '-r') or plt.plot(x, c='0.7')\n            # or plt.plot(x, color='red'), and maybe some others? Probably have\n            # dig into the matplotlib code to see how they parse this and do\n            # conflict resolution... Update: they use the last specified color.\n            # but I still need to know how to detect a color that was passed in\n            # the *args part, e.g., '-r'\n\n            color = kwargs.pop(\"color\", None)\n            carg = kwargs.pop(\"c\", None)\n\n            if color is not None and carg is not None:\n                # TODO: fix this so that a warning is issued, not raised\n                raise ValueError(\"saw kwargs ['c', 'color']\")\n                # raise UserWarning(\"saw kwargs ['c', 'color'] which are all aliases for 'color'.  Kept value from 'color'\")\n            if carg:\n                color = carg\n\n            if not color:\n                colors = []\n                for ii in range(obj.n_signals):\n                    (line,) = ax.plot(0, 0.5)\n                    colors.append(line.get_color())\n                    line.remove()\n\n                for ee, (abscissa_vals, data) in enumerate(\n                    zip(\n                        obj._intervaltime.plot_generator(),\n                        obj._intervaldata.plot_generator(),\n                    )\n                ):\n                    if ee &gt; 0:\n                        kwargs[\"label\"] = \"_nolegend_\"\n                    for ii, snippet in enumerate(data):\n                        ax.plot(\n                            abscissa_vals, snippet, *args, color=colors[ii], **kwargs\n                        )\n            else:\n                kwargs[\"color\"] = color\n                for ee, (abscissa_vals, data) in enumerate(\n                    zip(\n                        obj._intervaltime.plot_generator(),\n                        obj._intervaldata.plot_generator(),\n                    )\n                ):\n                    if ee &gt; 0:\n                        kwargs[\"label\"] = \"_nolegend_\"\n                    for ii, snippet in enumerate(data):\n                        ax.plot(abscissa_vals, snippet, *args, **kwargs)\n\n        if xlabel is None:\n            xlabel = obj._abscissa.label\n        if ylabel is None:\n            ylabel = obj._ordinate.label\n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n    else:  # if we didn't handle it yet, just pass it through to matplotlib...\n        ax.plot(obj, *args, **kwargs)\n\n    if autoscale:\n        xmin = np.inf\n        xmax = -np.inf\n        ymin = np.inf\n        ymax = -np.inf\n        for child in ax.get_children():\n            try:\n                cxmin, cymin = np.min(child.get_xydata(), axis=0)\n                cxmax, cymax = np.max(child.get_xydata(), axis=0)\n                if cxmin &lt; xmin:\n                    xmin = cxmin\n                if cymin &lt; ymin:\n                    ymin = cymin\n                if cxmax &gt; xmax:\n                    xmax = cxmax\n                if cymax &gt; ymax:\n                    ymax = cymax\n            except Exception:\n                pass\n        ax.set_xlim(xmin, xmax)\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.plot2d","title":"<code>plot2d(npl_obj, data=None, *, ax=None, mew=None, color=None, mec=None, markerfacecolor=None, **kwargs)</code>","text":"<p>Plot 2D data for nelpy objects or array-like input.</p> <p>Parameters:</p> Name Type Description Default <code>npl_obj</code> <code>nelpy object or array-like</code> <p>The object or data to plot in 2D.</p> required <code>data</code> <code>array - like</code> <p>Data to plot. If None, uses npl_obj's data.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>mew</code> <code>float</code> <p>Marker edge width.</p> <code>None</code> <code>color</code> <code>matplotlib color</code> <p>Trace color.</p> <code>None</code> <code>mec</code> <code>matplotlib color</code> <p>Marker edge color.</p> <code>None</code> <code>markerfacecolor</code> <code>matplotlib color</code> <p>Marker face color.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's plot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot2d([[0, 1], [1, 2], [2, 3]])\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def plot2d(\n    npl_obj,\n    data=None,\n    *,\n    ax=None,\n    mew=None,\n    color=None,\n    mec=None,\n    markerfacecolor=None,\n    **kwargs,\n):\n    \"\"\"\n    Plot 2D data for nelpy objects or array-like input.\n\n    Parameters\n    ----------\n    npl_obj : nelpy object or array-like\n        The object or data to plot in 2D.\n    data : array-like, optional\n        Data to plot. If None, uses npl_obj's data.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    mew : float, optional\n        Marker edge width.\n    color : matplotlib color, optional\n        Trace color.\n    mec : matplotlib color, optional\n        Marker edge color.\n    markerfacecolor : matplotlib color, optional\n        Marker face color.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's plot.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot2d([[0, 1], [1, 2], [2, 3]])\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    if mec is None:\n        mec = color\n    if markerfacecolor is None:\n        markerfacecolor = \"w\"\n\n    if isinstance(npl_obj, np.ndarray):\n        ax.plot(npl_obj, mec=mec, markerfacecolor=markerfacecolor, **kwargs)\n\n    # TODO: better solution for this? we could just iterate over the epochs and\n    # plot them but that might take up too much time since a copy is being made\n    # each iteration?\n    if isinstance(npl_obj, core.RegularlySampledAnalogSignalArray):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for segment in npl_obj:\n                if color is not None:\n                    ax.plot(\n                        segment[:, 0]._data_colsig,\n                        segment[:, 1]._data_colsig,\n                        color=color,\n                        mec=mec,\n                        markerfacecolor=\"w\",\n                        **kwargs,\n                    )\n                else:\n                    ax.plot(\n                        segment[:, 0]._data_colsig,\n                        segment[:, 1]._data_colsig,\n                        # color=color,\n                        mec=mec,\n                        markerfacecolor=\"w\",\n                        **kwargs,\n                    )\n\n    if isinstance(npl_obj, core.PositionArray):\n        xlim, ylim = npl_obj.xlim, npl_obj.ylim\n        if xlim is not None:\n            ax.set_xlim(xlim)\n        if ylim is not None:\n            ax.set_ylim(ylim)\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.plot_old","title":"<code>plot_old(npl_obj, data=None, *, ax=None, mew=None, color=None, mec=None, markerfacecolor=None, **kwargs)</code>","text":"<p>Plot an array-like object on an EpochArray or AnalogSignal.</p> <p>Parameters:</p> Name Type Description Default <code>npl_obj</code> <code>nelpy.EpochArray, nelpy.AnalogSignal, or np.ndarray</code> <p>EpochArray on which the data is defined, AnalogSignal, or array.</p> required <code>data</code> <code>array - like</code> <p>Data to plot on y axis; must be of size (epocharray.n_epochs,).</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Plot in given axis; if None creates a new figure.</p> <code>None</code> <code>mew</code> <code>float</code> <p>Marker edge width, default is equal to lw.</p> <code>None</code> <code>color</code> <code>matplotlib color</code> <p>Trace color.</p> <code>None</code> <code>mec</code> <code>matplotlib color</code> <p>Marker edge color, default is equal to color.</p> <code>None</code> <code>markerfacecolor</code> <code>matplotlib color</code> <p>Marker face color, default is 'w'.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Other keyword arguments are passed to main plot() call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis object with plot data.</p> <p>Examples:</p> <p>Plot a simple 5-element list on an EpochArray:</p> <pre><code>&gt;&gt;&gt; ep = EpochArray([[3, 4], [5, 8], [10, 12], [16, 20], [22, 23]])\n&gt;&gt;&gt; data = [3, 4, 2, 5, 2]\n&gt;&gt;&gt; plot_old(ep, data)\n</code></pre> <p>Hide the markers and change the linewidth:</p> <pre><code>&gt;&gt;&gt; plot_old(ep, data, ms=0, lw=3)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def plot_old(\n    npl_obj,\n    data=None,\n    *,\n    ax=None,\n    mew=None,\n    color=None,\n    mec=None,\n    markerfacecolor=None,\n    **kwargs,\n):\n    \"\"\"\n    Plot an array-like object on an EpochArray or AnalogSignal.\n\n    Parameters\n    ----------\n    npl_obj : nelpy.EpochArray, nelpy.AnalogSignal, or np.ndarray\n        EpochArray on which the data is defined, AnalogSignal, or array.\n    data : array-like, optional\n        Data to plot on y axis; must be of size (epocharray.n_epochs,).\n    ax : matplotlib.axes.Axes, optional\n        Plot in given axis; if None creates a new figure.\n    mew : float, optional\n        Marker edge width, default is equal to lw.\n    color : matplotlib color, optional\n        Trace color.\n    mec : matplotlib color, optional\n        Marker edge color, default is equal to color.\n    markerfacecolor : matplotlib color, optional\n        Marker face color, default is 'w'.\n    **kwargs : dict\n        Other keyword arguments are passed to main plot() call.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        Axis object with plot data.\n\n    Examples\n    --------\n    Plot a simple 5-element list on an EpochArray:\n\n    &gt;&gt;&gt; ep = EpochArray([[3, 4], [5, 8], [10, 12], [16, 20], [22, 23]])\n    &gt;&gt;&gt; data = [3, 4, 2, 5, 2]\n    &gt;&gt;&gt; plot_old(ep, data)\n\n    Hide the markers and change the linewidth:\n\n    &gt;&gt;&gt; plot_old(ep, data, ms=0, lw=3)\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    if mec is None:\n        mec = color\n    if markerfacecolor is None:\n        markerfacecolor = \"w\"\n\n    if isinstance(npl_obj, np.ndarray):\n        ax.plot(npl_obj, mec=mec, markerfacecolor=markerfacecolor, **kwargs)\n\n    # TODO: better solution for this? we could just iterate over the epochs and\n    # plot them but that might take up too much time since a copy is being made\n    # each iteration?\n    if isinstance(npl_obj, core.RegularlySampledAnalogSignalArray):\n        # Get the colors from the current color cycle\n        if npl_obj.n_signals &gt; 1:\n            colors = []\n            lines = []\n            for ii in range(npl_obj.n_signals):\n                (line,) = ax.plot(0, 0.5)\n                lines.append(line)\n                colors.append(line.get_color())\n                # line.remove()\n            for line in lines:\n                line.remove()\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            if not npl_obj.labels:\n                for segment in npl_obj:\n                    if color is not None:\n                        ax.plot(\n                            segment._time,\n                            segment._data_colsig,\n                            color=color,\n                            mec=mec,\n                            markerfacecolor=markerfacecolor,\n                            **kwargs,\n                        )\n                    else:\n                        ax.plot(\n                            segment._time,\n                            segment._data_colsig,\n                            # color=colors[ii],\n                            mec=mec,\n                            markerfacecolor=markerfacecolor,\n                            **kwargs,\n                        )\n            else:  # there are labels\n                if npl_obj.n_signals &gt; 1:\n                    for ii, segment in enumerate(npl_obj):\n                        for signal, label in zip(segment._data_rowsig, npl_obj.labels):\n                            if color is not None:\n                                ax.plot(\n                                    segment._time,\n                                    signal,\n                                    color=color,\n                                    mec=mec,\n                                    markerfacecolor=markerfacecolor,\n                                    label=label if ii == 0 else \"_nolegend_\",\n                                    **kwargs,\n                                )\n                            else:  # color(s) have not been specified, use color cycler\n                                ax.plot(\n                                    segment._time,\n                                    signal,\n                                    # color=colors[ii],\n                                    mec=mec,\n                                    markerfacecolor=markerfacecolor,\n                                    label=label if ii == 0 else \"_nolegend_\",\n                                    **kwargs,\n                                )\n                else:  # only one signal\n                    for ii, segment in enumerate(npl_obj):\n                        if not npl_obj.labels:\n                            label = None\n                        else:\n                            label = npl_obj.labels\n                        if color is not None:\n                            ax.plot(\n                                segment._time,\n                                segment._data_colsig,\n                                color=color,\n                                mec=mec,\n                                markerfacecolor=markerfacecolor,\n                                label=label if ii == 0 else \"_nolegend_\",\n                                **kwargs,\n                            )\n                        else:\n                            ax.plot(\n                                segment._time,\n                                segment._data_colsig,\n                                # color=color,\n                                mec=mec,\n                                markerfacecolor=markerfacecolor,\n                                label=label if ii == 0 else \"_nolegend_\",\n                                **kwargs,\n                            )\n\n    if isinstance(npl_obj, core.IntervalArray):\n        epocharray = npl_obj\n        if epocharray.n_intervals != len(data):\n            raise ValueError(\"epocharray and data must have the same length\")\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            for epoch, val in zip(epocharray, data):\n                ax.plot(\n                    [epoch.start, epoch.stop],\n                    [val, val],\n                    \"-o\",\n                    color=color,\n                    mec=mec,\n                    markerfacecolor=markerfacecolor,\n                    # lw=lw,\n                    # mew=mew,\n                    **kwargs,\n                )\n\n        # ax.set_ylim([np.array(data).min()-0.5, np.array(data).max()+0.5])\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.plot_tuning_curves1D","title":"<code>plot_tuning_curves1D(ratemap, ax=None, normalize=False, pad=None, unit_labels=None, fill=True, color=None, alpha=0.3)</code>","text":"<p>Plot 1D tuning curves for multiple units.</p> <p>Parameters:</p> Name Type Description Default <code>ratemap</code> <code>TuningCurve1D or similar</code> <p>Object with .ratemap (2D array: n_units x n_ext), .bins, .bin_centers, and .unit_labels.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize each curve to its peak value.</p> <code>False</code> <code>pad</code> <code>float</code> <p>Vertical offset between curves. If None, uses mean of ratemap / 2.</p> <code>None</code> <code>unit_labels</code> <code>list</code> <p>Labels for each unit. If None, uses ratemap.unit_labels.</p> <code>None</code> <code>fill</code> <code>bool</code> <p>Whether to fill under each curve. Default is True.</p> <code>True</code> <code>color</code> <code>color or None</code> <p>Color for all curves. If None, uses default color cycle.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Transparency for the fill. Default is 0.3.</p> <code>0.3</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted tuning curves.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot_tuning_curves1D(tc)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def plot_tuning_curves1D(\n    ratemap,\n    ax=None,\n    normalize=False,\n    pad=None,\n    unit_labels=None,\n    fill=True,\n    color=None,\n    alpha=0.3,\n):\n    \"\"\"\n    Plot 1D tuning curves for multiple units.\n\n    Parameters\n    ----------\n    ratemap : auxiliary.TuningCurve1D or similar\n        Object with .ratemap (2D array: n_units x n_ext), .bins, .bin_centers, and .unit_labels.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    normalize : bool, optional\n        If True, normalize each curve to its peak value.\n    pad : float, optional\n        Vertical offset between curves. If None, uses mean of ratemap / 2.\n    unit_labels : list, optional\n        Labels for each unit. If None, uses ratemap.unit_labels.\n    fill : bool, optional\n        Whether to fill under each curve. Default is True.\n    color : color or None, optional\n        Color for all curves. If None, uses default color cycle.\n    alpha : float, optional\n        Transparency for the fill. Default is 0.3.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted tuning curves.\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot_tuning_curves1D(tc)\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    if isinstance(ratemap, auxiliary.TuningCurve1D) | isinstance(\n        ratemap, auxiliary._tuningcurve.TuningCurve1D\n    ):\n        xmin = ratemap.bins[0]\n        xmax = ratemap.bins[-1]\n        xvals = ratemap.bin_centers\n        if unit_labels is None:\n            unit_labels = ratemap.unit_labels\n        ratemap = ratemap.ratemap\n    else:\n        raise NotImplementedError\n\n    if pad is None:\n        pad = ratemap.mean() / 2\n\n    n_units, n_ext = ratemap.shape\n\n    if normalize:\n        peak_firing_rates = ratemap.max(axis=1)\n        ratemap = (ratemap.T / peak_firing_rates).T\n\n    # determine max firing rate\n    # max_firing_rate = ratemap.max()\n\n    if xvals is None:\n        xvals = np.arange(n_ext)\n    if xmin is None:\n        xmin = xvals[0]\n    if xmax is None:\n        xmax = xvals[-1]\n\n    for unit, curve in enumerate(ratemap):\n        if color is None:\n            line = ax.plot(\n                xvals, unit * pad + curve, zorder=int(10 + 2 * n_units - 2 * unit)\n            )\n        else:\n            line = ax.plot(\n                xvals,\n                unit * pad + curve,\n                zorder=int(10 + 2 * n_units - 2 * unit),\n                color=color,\n            )\n        if fill:\n            # Get the color from the current curve\n            fillcolor = line[0].get_color()\n            ax.fill_between(\n                xvals,\n                unit * pad,\n                unit * pad + curve,\n                alpha=alpha,\n                color=fillcolor,\n                zorder=int(10 + 2 * n_units - 2 * unit - 1),\n            )\n\n    ax.set_xlim(xmin, xmax)\n    if pad != 0:\n        yticks = np.arange(n_units) * pad + 0.5 * pad\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(unit_labels)\n        ax.set_xlabel(\"external variable\")\n        ax.set_ylabel(\"unit\")\n        utils.no_yticks(ax)\n        utils.clear_left(ax)\n    else:\n        if normalize:\n            ax.set_ylabel(\"normalized firing rate\")\n        else:\n            ax.set_ylabel(\"firing rate [Hz]\")\n        ax.set_ylim(0)\n\n    utils.clear_top(ax)\n    utils.clear_right(ax)\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.psdplot","title":"<code>psdplot(data, *, fs=None, window=None, nfft=None, detrend='constant', return_onesided=True, scaling='density', ax=None)</code>","text":"<p>Plot the power spectrum of a regularly-sampled time-domain signal.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>RegularlySampledAnalogSignalArray</code> <p>The input signal to analyze. Must be a 1D regularly sampled signal.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency of the time series in Hz. Defaults to data.fs if available.</p> <code>None</code> <code>window</code> <code>str or tuple or array_like</code> <p>Desired window to use. See scipy.signal.get_window for options. If an array, used directly as the window. Defaults to None ('boxcar').</p> <code>None</code> <code>nfft</code> <code>int</code> <p>Length of the FFT used. If None, the length of data will be used.</p> <code>None</code> <code>detrend</code> <code>str or function</code> <p>Specifies how to detrend data prior to computing the spectrum. If a string, passed as the type argument to detrend. If a function, should return a detrended array. Defaults to 'constant'.</p> <code>'constant'</code> <code>return_onesided</code> <code>bool</code> <p>If True, return a one-sided spectrum for real data. If False, return a two-sided spectrum. For complex data, always returns two-sided spectrum.</p> <code>True</code> <code>scaling</code> <code>(density, spectrum)</code> <p>Selects between computing the power spectral density ('density', units V2/Hz) and the power spectrum ('spectrum', units V2). Defaults to 'density'.</p> <code>'density'</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, creates a new figure and axis.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted power spectrum.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.plotting.core import psdplot\n&gt;&gt;&gt; ax = psdplot(my_signal)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def psdplot(\n    data,\n    *,\n    fs=None,\n    window=None,\n    nfft=None,\n    detrend=\"constant\",\n    return_onesided=True,\n    scaling=\"density\",\n    ax=None,\n):\n    \"\"\"\n    Plot the power spectrum of a regularly-sampled time-domain signal.\n\n    Parameters\n    ----------\n    data : RegularlySampledAnalogSignalArray\n        The input signal to analyze. Must be a 1D regularly sampled signal.\n    fs : float, optional\n        Sampling frequency of the time series in Hz. Defaults to data.fs if available.\n    window : str or tuple or array_like, optional\n        Desired window to use. See scipy.signal.get_window for options. If an array, used directly as the window. Defaults to None ('boxcar').\n    nfft : int, optional\n        Length of the FFT used. If None, the length of data will be used.\n    detrend : str or function, optional\n        Specifies how to detrend data prior to computing the spectrum. If a string, passed as the type argument to detrend. If a function, should return a detrended array. Defaults to 'constant'.\n    return_onesided : bool, optional\n        If True, return a one-sided spectrum for real data. If False, return a two-sided spectrum. For complex data, always returns two-sided spectrum.\n    scaling : {'density', 'spectrum'}, optional\n        Selects between computing the power spectral density ('density', units V**2/Hz) and the power spectrum ('spectrum', units V**2). Defaults to 'density'.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, creates a new figure and axis.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the plotted power spectrum.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.plotting.core import psdplot\n    &gt;&gt;&gt; ax = psdplot(my_signal)\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if isinstance(data, core.RegularlySampledAnalogSignalArray):\n        if fs is None:\n            fs = data.fs\n        if fs is None:\n            raise ValueError(\n                \"The sampling rate fs cannot be inferred, and must be specified manually!\"\n            )\n        if data.n_signals &gt; 1:\n            raise NotImplementedError(\n                \"more than one signal is not yet supported for psdplot!\"\n            )\n        else:\n            data = data.data.squeeze()\n    else:\n        raise NotImplementedError(\n            \"datatype {} not yet supported by psdplot!\".format(str(type(data)))\n        )\n\n    kwargs = {\n        \"x\": data,\n        \"fs\": fs,\n        \"window\": window,\n        \"nfft\": nfft,\n        \"detrend\": detrend,\n        \"return_onesided\": return_onesided,\n        \"scaling\": scaling,\n    }\n\n    f, Pxx_den = signal.periodogram(**kwargs)\n\n    if scaling == \"density\":\n        ax.semilogy(f, np.sqrt(Pxx_den))\n        ax.set_ylabel(\"PSD [V**2/Hz]\")\n    elif scaling == \"spectrum\":\n        ax.semilogy(f, np.sqrt(Pxx_den))\n        ax.set_ylabel(\"Linear spectrum [V RMS]\")\n    ax.set_xlabel(\"frequency [Hz]\")\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.rastercountplot","title":"<code>rastercountplot(spiketrain, nbins=50, **kwargs)</code>","text":"<p>Plot a raster plot and spike count histogram for a SpikeTrainArray.</p> <p>Parameters:</p> Name Type Description Default <code>spiketrain</code> <code>SpikeTrainArray</code> <p>The spike train data to plot.</p> required <code>nbins</code> <code>int</code> <p>Number of bins for the histogram. Default is 50.</p> <code>50</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to rasterplot.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax1</code> <code>Axes</code> <p>The axis with the histogram plot.</p> <code>ax2</code> <code>Axes</code> <p>The axis with the raster plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import SpikeTrainArray\n&gt;&gt;&gt; sta = SpikeTrainArray([[1, 2, 3], [2, 4, 6]], fs=10)\n&gt;&gt;&gt; rastercountplot(sta, nbins=20)\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def rastercountplot(spiketrain, nbins=50, **kwargs):\n    \"\"\"\n    Plot a raster plot and spike count histogram for a SpikeTrainArray.\n\n    Parameters\n    ----------\n    spiketrain : nelpy.SpikeTrainArray\n        The spike train data to plot.\n    nbins : int, optional\n        Number of bins for the histogram. Default is 50.\n    **kwargs : dict\n        Additional keyword arguments passed to rasterplot.\n\n    Returns\n    -------\n    ax1 : matplotlib.axes.Axes\n        The axis with the histogram plot.\n    ax2 : matplotlib.axes.Axes\n        The axis with the raster plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import SpikeTrainArray\n    &gt;&gt;&gt; sta = SpikeTrainArray([[1, 2, 3], [2, 4, 6]], fs=10)\n    &gt;&gt;&gt; rastercountplot(sta, nbins=20)\n    \"\"\"\n    plt.figure(figsize=(14, 6))\n    gs = gridspec.GridSpec(2, 1, hspace=0.01, height_ratios=[0.2, 0.8])\n    ax1 = plt.subplot(gs[0])\n    ax2 = plt.subplot(gs[1])\n\n    color = kwargs.get(\"color\", None)\n    if color is None:\n        color = \"0.4\"\n\n    ds = (spiketrain.support.stop - spiketrain.support.start) / nbins\n    flattened = spiketrain.bin(ds=ds).flatten()\n    steps = np.squeeze(flattened.data)\n    stepsx = np.linspace(\n        spiketrain.support.start, spiketrain.support.stop, num=flattened.n_bins\n    )\n\n    #     ax1.plot(stepsx, steps, drawstyle='steps-mid', color='none');\n    ax1.set_ylim([-0.5, np.max(steps) + 1])\n    rasterplot(spiketrain, ax=ax2, **kwargs)\n\n    utils.clear_left_right(ax1)\n    utils.clear_top_bottom(ax1)\n    utils.clear_top(ax2)\n\n    ax1.fill_between(stepsx, steps, step=\"mid\", color=color)\n\n    utils.sync_xlims(ax1, ax2)\n\n    return ax1, ax2\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.rasterplot","title":"<code>rasterplot(data, *, cmap=None, color=None, ax=None, lw=None, lh=None, vertstack=None, labels=None, cmap_lo=0.25, cmap_hi=0.75, **kwargs)</code>","text":"<p>Make a raster plot from a SpikeTrainArray or EventArray object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>SpikeTrainArray or EventArray</code> <p>The spike/event data to plot.</p> required <code>cmap</code> <code>matplotlib colormap</code> <p>Colormap to use for the raster lines.</p> <code>None</code> <code>color</code> <code>matplotlib color</code> <p>Plot color; default is '0.25'.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Plot in given axis. If None, plots on current axes.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Linewidth, default is 1.5.</p> <code>None</code> <code>lh</code> <code>float</code> <p>Line height, default is 0.95.</p> <code>None</code> <code>vertstack</code> <code>bool</code> <p>If True, stack units in vertically adjacent positions. Default is False.</p> <code>None</code> <code>labels</code> <code>list</code> <p>Labels for input data units. If not specified, uses unit_labels from the input.</p> <code>None</code> <code>cmap_lo</code> <code>float</code> <p>Lower bound for colormap normalization. Default is 0.25.</p> <code>0.25</code> <code>cmap_hi</code> <code>float</code> <p>Upper bound for colormap normalization. Default is 0.75.</p> <code>0.75</code> <code>**kwargs</code> <code>dict</code> <p>Other keyword arguments are passed to main vlines() call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis object with plot data.</p> <p>Examples:</p> <p>Instantiate a SpikeTrainArray and create a raster plot:</p> <pre><code>&gt;&gt;&gt; stdata1 = [1, 2, 4, 5, 6, 10, 20]\n&gt;&gt;&gt; stdata2 = [3, 4, 4.5, 5, 5.5, 19]\n&gt;&gt;&gt; stdata3 = [5, 12, 14, 15, 16, 18, 22, 23, 24]\n&gt;&gt;&gt; stdata4 = [5, 12, 14, 15, 16, 18, 23, 25, 32]\n\n&gt;&gt;&gt; sta1 = nelpy.SpikeTrainArray([stdata1, stdata2, stdata3,\n                                  stdata4, stdata1+stdata4],\n                                  fs=5, unit_ids=[1,2,3,4,6])\n&gt;&gt;&gt; ax = rasterplot(sta1, color=\"cyan\", lw=2, lh=2)\n</code></pre> <p>Instantiate another SpikeTrain Array, stack units, and specify labels. Note that the user-specified labels in the call to raster() will be shown instead of the unit_labels associated with the input data:</p> <pre><code>&gt;&gt;&gt; sta3 = nelpy.SpikeTrainArray([stdata1, stdata4, stdata2+stdata3],\n                                 support=ep1, fs=5, unit_ids=[10,5,12],\n                                 unit_labels=['some', 'more', 'cells'])\n&gt;&gt;&gt; rasterplot(sta3, color=plt.cm.Blues, lw=2, lh=2, vertstack=True,\n           labels=['units', 'of', 'interest'])\n</code></pre> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def rasterplot(\n    data,\n    *,\n    cmap=None,\n    color=None,\n    ax=None,\n    lw=None,\n    lh=None,\n    vertstack=None,\n    labels=None,\n    cmap_lo=0.25,\n    cmap_hi=0.75,\n    **kwargs,\n):\n    \"\"\"\n    Make a raster plot from a SpikeTrainArray or EventArray object.\n\n    Parameters\n    ----------\n    data : nelpy.SpikeTrainArray or nelpy.EventArray\n        The spike/event data to plot.\n    cmap : matplotlib colormap, optional\n        Colormap to use for the raster lines.\n    color : matplotlib color, optional\n        Plot color; default is '0.25'.\n    ax : matplotlib.axes.Axes, optional\n        Plot in given axis. If None, plots on current axes.\n    lw : float, optional\n        Linewidth, default is 1.5.\n    lh : float, optional\n        Line height, default is 0.95.\n    vertstack : bool, optional\n        If True, stack units in vertically adjacent positions. Default is False.\n    labels : list, optional\n        Labels for input data units. If not specified, uses unit_labels from the input.\n    cmap_lo : float, optional\n        Lower bound for colormap normalization. Default is 0.25.\n    cmap_hi : float, optional\n        Upper bound for colormap normalization. Default is 0.75.\n    **kwargs : dict\n        Other keyword arguments are passed to main vlines() call.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        Axis object with plot data.\n\n    Examples\n    --------\n    Instantiate a SpikeTrainArray and create a raster plot:\n\n        &gt;&gt;&gt; stdata1 = [1, 2, 4, 5, 6, 10, 20]\n        &gt;&gt;&gt; stdata2 = [3, 4, 4.5, 5, 5.5, 19]\n        &gt;&gt;&gt; stdata3 = [5, 12, 14, 15, 16, 18, 22, 23, 24]\n        &gt;&gt;&gt; stdata4 = [5, 12, 14, 15, 16, 18, 23, 25, 32]\n\n        &gt;&gt;&gt; sta1 = nelpy.SpikeTrainArray([stdata1, stdata2, stdata3,\n                                          stdata4, stdata1+stdata4],\n                                          fs=5, unit_ids=[1,2,3,4,6])\n        &gt;&gt;&gt; ax = rasterplot(sta1, color=\"cyan\", lw=2, lh=2)\n\n    Instantiate another SpikeTrain Array, stack units, and specify labels.\n    Note that the user-specified labels in the call to raster() will be\n    shown instead of the unit_labels associated with the input data:\n\n        &gt;&gt;&gt; sta3 = nelpy.SpikeTrainArray([stdata1, stdata4, stdata2+stdata3],\n                                         support=ep1, fs=5, unit_ids=[10,5,12],\n                                         unit_labels=['some', 'more', 'cells'])\n        &gt;&gt;&gt; rasterplot(sta3, color=plt.cm.Blues, lw=2, lh=2, vertstack=True,\n                   labels=['units', 'of', 'interest'])\n    \"\"\"\n\n    # Sort out default values for the parameters\n    if ax is None:\n        ax = plt.gca()\n    if cmap is None and color is None:\n        color = \"0.25\"\n    if lw is None:\n        lw = 1.5\n    if lh is None:\n        lh = 0.95\n    if vertstack is None:\n        vertstack = False\n\n    firstplot = False\n    if not ax.findobj(match=RasterLabelData):\n        firstplot = True\n        ax.add_artist(RasterLabelData())\n\n    # override labels\n    if labels is not None:\n        series_labels = labels\n    else:\n        series_labels = []\n\n    hh = lh / 2.0  # half the line height\n\n    # Handle different types of input data\n    if isinstance(data, core.EventArray):\n        label_data = ax.findobj(match=RasterLabelData)[0].label_data\n        serieslist = [-np.inf for element in data.series_ids]\n        # no override labels so use unit_labels from input\n        if not series_labels:\n            series_labels = data.series_labels\n\n        if firstplot:\n            if vertstack:\n                minunit = 1\n                maxunit = data.n_series\n                serieslist = range(1, data.n_series + 1)\n            else:\n                minunit = np.array(data.series_ids).min()\n                maxunit = np.array(data.series_ids).max()\n                serieslist = data.series_ids\n        # see if any of the series_ids has already been plotted. If so,\n        # then merge\n        else:\n            for idx, series_id in enumerate(data.series_ids):\n                if series_id in label_data.keys():\n                    position, _ = label_data[series_id]\n                    serieslist[idx] = position\n                else:  # unit not yet plotted\n                    if vertstack:\n                        serieslist[idx] = 1 + max(\n                            int(ax.get_yticks()[-1]), max(serieslist)\n                        )\n                    else:\n                        warnings.warn(\n                            \"Spike trains may be plotted in \"\n                            \"the same vertical position as \"\n                            \"another unit\"\n                        )\n                        serieslist[idx] = data.series_ids[idx]\n\n        if firstplot:\n            minunit = int(minunit)\n            maxunit = int(maxunit)\n        else:\n            (prev_ymin, prev_ymax) = ax.findobj(match=RasterLabelData)[0].yrange\n            minunit = int(np.min([np.ceil(prev_ymin), np.min(serieslist)]))\n            maxunit = int(np.max([np.floor(prev_ymax), np.max(serieslist)]))\n\n        yrange = (minunit - 0.5, maxunit + 0.5)\n\n        if cmap is not None:\n            color_range = range(data.n_series)\n            # TODO: if we go from 0 then most colormaps are invisible at one end of the spectrum\n            colors = cmap(np.linspace(cmap_lo, cmap_hi, data.n_series))\n            for series_ii, series, color_idx in zip(serieslist, data.data, color_range):\n                ax.vlines(\n                    series,\n                    series_ii - hh,\n                    series_ii + hh,\n                    colors=colors[color_idx],\n                    lw=lw,\n                    **kwargs,\n                )\n        else:  # use a constant color:\n            for series_ii, series in zip(serieslist, data.data):\n                ax.vlines(\n                    series,\n                    series_ii - hh,\n                    series_ii + hh,\n                    colors=color,\n                    lw=lw,\n                    **kwargs,\n                )\n\n        # get existing label data so we can set some attributes\n        rld = ax.findobj(match=RasterLabelData)[0]\n\n        ax.set_ylim(yrange)\n        rld.yrange = yrange\n\n        for series_id, loc, label in zip(data.series_ids, serieslist, series_labels):\n            rld.label_data[series_id] = (loc, label)\n        serieslocs = []\n        serieslabels = []\n        for loc, label in label_data.values():\n            serieslocs.append(loc)\n            serieslabels.append(label)\n        ax.set_yticks(serieslocs)\n        ax.set_yticklabels(serieslabels)\n\n    else:\n        raise NotImplementedError(\n            \"plotting {} not yet supported\".format(str(type(data)))\n        )\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/core/#nelpy.plotting.core.spectrogram","title":"<code>spectrogram(data, *, h)</code>","text":"<p>Compute a spectrogram with consecutive Fourier transforms.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array_like</code> <p>Time series of measurement values.</p> required <code>h</code> <code>object</code> <p>Additional parameter (not yet implemented).</p> required <p>Returns:</p> Name Type Description <code>f</code> <code>ndarray</code> <p>Array of sample frequencies.</p> <code>t</code> <code>ndarray</code> <p>Array of segment times.</p> <code>Sxx</code> <code>ndarray</code> <p>Spectrogram of x. By default, the last axis of Sxx corresponds to the segment times.</p> Notes <p>This function is not yet implemented. See scipy.signal.spectrogram for details.</p> Source code in <code>nelpy/plotting/core.py</code> <pre><code>def spectrogram(data, *, h):\n    \"\"\"\n    Compute a spectrogram with consecutive Fourier transforms.\n\n    Parameters\n    ----------\n    data : array_like\n        Time series of measurement values.\n    h : object\n        Additional parameter (not yet implemented).\n\n    Returns\n    -------\n    f : ndarray\n        Array of sample frequencies.\n    t : ndarray\n        Array of segment times.\n    Sxx : ndarray\n        Spectrogram of x. By default, the last axis of Sxx corresponds to the segment times.\n\n    Notes\n    -----\n    This function is not yet implemented. See scipy.signal.spectrogram for details.\n    \"\"\"\n    raise NotImplementedError(\"plotting.spectrogram() does not exist yet!\")\n</code></pre>"},{"location":"reference/nelpy/plotting/decoding/","title":"nelpy.plotting.decoding","text":""},{"location":"reference/nelpy/plotting/decoding/#nelpy.plotting.decoding.decode_and_plot_events1D","title":"<code>decode_and_plot_events1D(*, bst, tc, raster=True, st=None, st_order='track', evt_subset=None, **kwargs)</code>","text":"<p>Decode and plot 1D events with optional raster plot overlay.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>The binned spike train array to decode.</p> required <code>tc</code> <code>TuningCurve1D</code> <p>The tuning curve used for decoding.</p> required <code>raster</code> <code>bool</code> <p>Whether to include a raster plot (default is True).</p> <code>True</code> <code>st</code> <code>SpikeTrainArray</code> <p>The spike train array for raster plotting.</p> <code>None</code> <code>st_order</code> <code>str or array - like</code> <p>Order of units for raster plot. Options: 'track', 'first', 'random', or array of unit ids.</p> <code>'track'</code> <code>evt_subset</code> <code>list</code> <p>List of integer indices for event selection. If not sorted, will be sorted.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for plotting.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>The figure containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig = decode_and_plot_events1D(bst=bst, tc=tc, st=st)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/decoding.py</code> <pre><code>def decode_and_plot_events1D(\n    *, bst, tc, raster=True, st=None, st_order=\"track\", evt_subset=None, **kwargs\n):\n    \"\"\"\n    Decode and plot 1D events with optional raster plot overlay.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        The binned spike train array to decode.\n    tc : TuningCurve1D\n        The tuning curve used for decoding.\n    raster : bool, optional\n        Whether to include a raster plot (default is True).\n    st : SpikeTrainArray, optional\n        The spike train array for raster plotting.\n    st_order : str or array-like, optional\n        Order of units for raster plot. Options: 'track', 'first', 'random', or array of unit ids.\n    evt_subset : list, optional\n        List of integer indices for event selection. If not sorted, will be sorted.\n    **kwargs\n        Additional keyword arguments for plotting.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The figure containing the plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig = decode_and_plot_events1D(bst=bst, tc=tc, st=st)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # TODO: add **kwargs\n    #   fig size, cmap, raster lw, raster color, other axes props, ...\n\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\n\n    unit_ids = set(bst.unit_ids)\n    unit_ids = unit_ids.intersection(st.unit_ids)\n    unit_ids = unit_ids.intersection(tc.unit_ids)\n\n    bst = bst._unit_subset(unit_ids)\n    st = st._unit_subset(unit_ids)\n    tc = tc._unit_subset(unit_ids)\n\n    if evt_subset is None:\n        evt_subset = np.arange(bst.n_epochs)\n    evt_subset = list(evt_subset)\n    if not is_sorted(evt_subset):\n        evt_subset.sort()\n    bst = bst[evt_subset]\n\n    # now that the bst has potentially been restricted by evt_subset, we trim down the spike train as well:\n    st = st[bst.support]\n    st = collapse_time(st)\n\n    if st_order == \"track\":\n        new_order = tc.get_peak_firing_order_ids()\n    elif st_order == \"first\":\n        new_order = st.get_spike_firing_order()\n    elif st_order == \"random\":\n        new_order = np.random.permutation(st.unit_ids)\n    else:\n        new_order = st_order\n    st.reorder_units_by_ids(new_order, inplace=True)\n\n    # now decode events in bst:\n    posterior, bdries, mode_pth, mean_pth = decoding.decode1D(\n        bst=bst, ratemap=tc, xmax=tc.bins[-1]\n    )\n\n    fig, ax = plt.subplots(figsize=(bst.n_bins / 5, 4))\n\n    pixel_width = 0.5\n\n    imagesc(\n        x=np.arange(bst.n_bins),\n        y=np.arange(311),\n        data=posterior,\n        cmap=plt.cm.Spectral_r,\n        ax=ax,\n    )\n    plotutils.yticks_interval(310)\n    plotutils.no_yticks(ax)\n\n    ax.vlines(\n        np.arange(bst.lengths.sum()) - pixel_width,\n        *ax.get_ylim(),\n        lw=1,\n        linestyle=\":\",\n        color=\"0.8\",\n    )\n    ax.vlines(np.cumsum(bst.lengths) - pixel_width, *ax.get_ylim(), lw=1)\n\n    ax.set_xlim(-pixel_width, bst.lengths.sum() - pixel_width)\n\n    event_centers = np.insert(np.cumsum(bst.lengths), 0, 0)\n    event_centers = event_centers[:-1] + bst.lengths / 2 - 0.5\n\n    #     ax.set_xticks([0, bst.n_bins-1])\n    #     ax.set_xticklabels([1, bst.n_bins])\n\n    ax.set_xticks(event_centers)\n    ax.set_xticklabels(evt_subset)\n    #     ax.xaxis.tick_top()\n    #     ax.xaxis.set_label_position('top')\n\n    plotutils.no_xticks(ax)\n\n    divider = make_axes_locatable(ax)\n    axRaster = divider.append_axes(\"top\", size=1.5, pad=0)\n\n    rasterplot(st, vertstack=True, ax=axRaster, lh=1.25, lw=2.5, color=\"0.1\")\n\n    axRaster.set_xlim(st.support.time.squeeze())\n    bin_edges = np.linspace(\n        st.support.time[0, 0], st.support.time[0, 1], bst.n_bins + 1\n    )\n    axRaster.vlines(bin_edges, *axRaster.get_ylim(), lw=1, linestyle=\":\", color=\"0.8\")\n    axRaster.vlines(\n        bin_edges[np.cumsum(bst.lengths)], *axRaster.get_ylim(), lw=1, color=\"0.2\"\n    )\n    plotutils.no_xticks(axRaster)\n    plotutils.no_xticklabels(axRaster)\n    plotutils.no_yticklabels(axRaster)\n    plotutils.no_yticks(axRaster)\n    ax.set_ylabel(\"position\")\n    axRaster.set_ylabel(\"units\")\n    ax.set_xlabel(\"time bins\")\n    plotutils.clear_left_right(axRaster)\n    plotutils.clear_top_bottom(axRaster)\n\n    plotutils.align_ylabels(0, ax, axRaster)\n    return fig\n</code></pre>"},{"location":"reference/nelpy/plotting/decoding/#nelpy.plotting.decoding.plot_cum_error_dist","title":"<code>plot_cum_error_dist(*, cumhist=None, bincenters=None, bst=None, extern=None, decodefunc=None, k=None, transfunc=None, n_extern=None, n_bins=None, extmin=None, extmax=None, sigma=None, lw=None, ax=None, inset=True, inset_ax=None, color=None, **kwargs)</code>","text":"<p>Plot (and optionally compute) the cumulative distribution of decoding errors.</p> <p>Evaluated using a cross-validation procedure. See Fig 3.(b) of \"Analysis of Hippocampal Memory Replay Using Neural Population Decoding\", Fabian Kloosterman, 2012.</p> <p>Parameters:</p> Name Type Description Default <code>cumhist</code> <code>array - like</code> <p>Precomputed cumulative histogram of errors. If None, will be computed.</p> <code>None</code> <code>bincenters</code> <code>array - like</code> <p>Bin centers for the cumulative histogram. If None, will be computed.</p> <code>None</code> <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Required if cumhist and bincenters are not provided. Used for error computation.</p> <code>None</code> <code>extern</code> <code>array - like</code> <p>External variable (e.g., position) for decoding. Required if cumhist and bincenters are not provided.</p> <code>None</code> <code>decodefunc</code> <code>callable</code> <p>Decoding function to use. Defaults to decoding.decode1D.</p> <code>None</code> <code>k</code> <code>int</code> <p>Number of cross-validation folds. Default is 5.</p> <code>None</code> <code>transfunc</code> <code>callable</code> <p>Optional transformation function for the external variable.</p> <code>None</code> <code>n_extern</code> <code>int</code> <p>Number of external variable samples. Default is 100.</p> <code>None</code> <code>n_bins</code> <code>int</code> <p>Number of bins for the error histogram. Default is 200.</p> <code>None</code> <code>extmin</code> <code>float</code> <p>Minimum value of the external variable. Default is 0.</p> <code>None</code> <code>extmax</code> <code>float</code> <p>Maximum value of the external variable. Default is 100.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Smoothing parameter. Default is 3.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width for the plot. Default is 1.5.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>inset</code> <code>bool</code> <p>Whether to include an inset plot. Default is True.</p> <code>True</code> <code>inset_ax</code> <code>Axes</code> <p>Axis for the inset plot. If None, one will be created.</p> <code>None</code> <code>color</code> <code>color</code> <p>Line color. If None, uses next color in cycle.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for plotting.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the cumulative error plot.</p> <code>inset_ax</code> <code>(Axes, optional)</code> <p>The axis with the inset plot (if inset=True).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ax, inset_ax = plot_cum_error_dist(bst=bst, extern=pos)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/decoding.py</code> <pre><code>def plot_cum_error_dist(\n    *,\n    cumhist=None,\n    bincenters=None,\n    bst=None,\n    extern=None,\n    decodefunc=None,\n    k=None,\n    transfunc=None,\n    n_extern=None,\n    n_bins=None,\n    extmin=None,\n    extmax=None,\n    sigma=None,\n    lw=None,\n    ax=None,\n    inset=True,\n    inset_ax=None,\n    color=None,\n    **kwargs,\n):\n    \"\"\"\n    Plot (and optionally compute) the cumulative distribution of decoding errors.\n\n    Evaluated using a cross-validation procedure. See Fig 3.(b) of \"Analysis of Hippocampal Memory Replay Using Neural Population Decoding\", Fabian Kloosterman, 2012.\n\n    Parameters\n    ----------\n    cumhist : array-like, optional\n        Precomputed cumulative histogram of errors. If None, will be computed.\n    bincenters : array-like, optional\n        Bin centers for the cumulative histogram. If None, will be computed.\n    bst : BinnedSpikeTrainArray, optional\n        Required if cumhist and bincenters are not provided. Used for error computation.\n    extern : array-like, optional\n        External variable (e.g., position) for decoding. Required if cumhist and bincenters are not provided.\n    decodefunc : callable, optional\n        Decoding function to use. Defaults to decoding.decode1D.\n    k : int, optional\n        Number of cross-validation folds. Default is 5.\n    transfunc : callable, optional\n        Optional transformation function for the external variable.\n    n_extern : int, optional\n        Number of external variable samples. Default is 100.\n    n_bins : int, optional\n        Number of bins for the error histogram. Default is 200.\n    extmin : float, optional\n        Minimum value of the external variable. Default is 0.\n    extmax : float, optional\n        Maximum value of the external variable. Default is 100.\n    sigma : float, optional\n        Smoothing parameter. Default is 3.\n    lw : float, optional\n        Line width for the plot. Default is 1.5.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    inset : bool, optional\n        Whether to include an inset plot. Default is True.\n    inset_ax : matplotlib.axes.Axes, optional\n        Axis for the inset plot. If None, one will be created.\n    color : color, optional\n        Line color. If None, uses next color in cycle.\n    **kwargs\n        Additional keyword arguments for plotting.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the cumulative error plot.\n    inset_ax : matplotlib.axes.Axes, optional\n        The axis with the inset plot (if inset=True).\n\n    Examples\n    --------\n    &gt;&gt;&gt; ax, inset_ax = plot_cum_error_dist(bst=bst, extern=pos)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    if lw is None:\n        lw = 1.5\n    if decodefunc is None:\n        decodefunc = decoding.decode1D\n    if k is None:\n        k = 5\n    if n_extern is None:\n        n_extern = 100\n    if n_bins is None:\n        n_bins = 200\n    if extmin is None:\n        extmin = 0\n    if extmax is None:\n        extmax = 100\n    if sigma is None:\n        sigma = 3\n\n    # Get the color from the current color cycle\n    if color is None:\n        (line,) = ax.plot(0, 0.5)\n        color = line.get_color()\n        line.remove()\n\n    # if cumhist or bincenters are NOT provided, then compute them\n    if cumhist is None or bincenters is None:\n        assert bst is not None, (\n            \"if cumhist and bincenters are not given, then bst must be provided to recompute them!\"\n        )\n        assert extern is not None, (\n            \"if cumhist and bincenters are not given, then extern must be provided to recompute them!\"\n        )\n        cumhist, bincenters = decoding.cumulative_dist_decoding_error_using_xval(\n            bst=bst,\n            extern=extern,\n            decodefunc=decoding.decode1D,\n            k=k,\n            transfunc=transfunc,\n            n_extern=n_extern,\n            extmin=extmin,\n            extmax=extmax,\n            sigma=sigma,\n            n_bins=n_bins,\n        )\n    # now plot results\n    ax.plot(bincenters, cumhist, lw=lw, color=color, **kwargs)\n    ax.set_xlim(bincenters[0], bincenters[-1])\n    ax.set_xlabel(\"error [cm]\")\n    ax.set_ylabel(\"cumulative probability\")\n\n    ax.set_ylim(0)\n\n    if inset:\n        if inset_ax is None:\n            inset_ax = inset_axes(\n                parent_axes=ax, width=\"60%\", height=\"50%\", loc=4, borderpad=2\n            )\n\n        inset_ax.plot(bincenters, cumhist, lw=lw, color=color, **kwargs)\n\n        # annotate inset\n        thresh1 = 0.7\n        inset_ax.hlines(\n            thresh1, 0, cumhist(thresh1), color=color, alpha=0.9, lw=lw, linestyle=\"--\"\n        )\n        inset_ax.vlines(\n            cumhist(thresh1), 0, thresh1, color=color, alpha=0.9, lw=lw, linestyle=\"--\"\n        )\n        inset_ax.set_xlim(0, 12 * np.ceil(cumhist(thresh1) / 10))\n\n        thresh2 = 0.5\n        inset_ax.hlines(\n            thresh2, 0, cumhist(thresh2), color=color, alpha=0.6, lw=lw, linestyle=\"--\"\n        )\n        inset_ax.vlines(\n            cumhist(thresh2), 0, thresh2, color=color, alpha=0.6, lw=lw, linestyle=\"--\"\n        )\n\n        inset_ax.set_yticks((0, thresh1, thresh2, 1))\n        inset_ax.set_ylim(0)\n\n        return ax, inset_ax\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/decoding/#nelpy.plotting.decoding.plot_posteriors","title":"<code>plot_posteriors(bst, tuningcurve, idx=None, w=1, bin_px_size=0.08)</code>","text":"<p>Plot posterior probabilities for decoded neural activity.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>The binned spike train array to decode.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>The tuning curve used for decoding.</p> required <code>idx</code> <code>array - like</code> <p>Indices of events to plot. If None, all events are plotted.</p> <code>None</code> <code>w</code> <code>int</code> <p>Window size for decoding (default is 1).</p> <code>1</code> <code>bin_px_size</code> <code>float</code> <p>Size of each bin in pixels for the plot (default is 0.08).</p> <code>0.08</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the posterior plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ax = plot_posteriors(bst, tc)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/decoding.py</code> <pre><code>def plot_posteriors(bst, tuningcurve, idx=None, w=1, bin_px_size=0.08):\n    \"\"\"\n    Plot posterior probabilities for decoded neural activity.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        The binned spike train array to decode.\n    tuningcurve : TuningCurve1D\n        The tuning curve used for decoding.\n    idx : array-like, optional\n        Indices of events to plot. If None, all events are plotted.\n    w : int, optional\n        Window size for decoding (default is 1).\n    bin_px_size : float, optional\n        Size of each bin in pixels for the plot (default is 0.08).\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the posterior plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; ax = plot_posteriors(bst, tc)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    if idx is not None:\n        bst = bst[idx]\n    tc = tuningcurve\n\n    # decode neural activity\n    posterior, bdries, mode_pth, mean_pth = decoding.decode1D(\n        bst=bst, ratemap=tc, xmin=tc.bins[0], xmax=tc.bins[-1], w=w\n    )\n\n    pixel_width = 0.5\n\n    n_ext, n_bins = posterior.shape\n    lengths = np.diff(bdries)\n\n    plt.figure(figsize=(bin_px_size * n_bins, 2))\n    ax = plt.gca()\n\n    imagesc(\n        x=np.arange(n_bins),\n        y=np.arange(int(tc.bins[-1] + 1)),\n        data=posterior,\n        cmap=plt.cm.Spectral_r,\n        ax=ax,\n    )\n    plotutils.yticks_interval(tc.bins[-1])\n    plotutils.no_yticks(ax)\n    # plt.imshow(posterior, cmap=plt.cm.Spectral_r, interpolation='none', aspect='auto')\n    ax.vlines(\n        np.arange(lengths.sum()) - pixel_width,\n        *ax.get_ylim(),\n        lw=1,\n        linestyle=\":\",\n        color=\"0.8\",\n    )\n    ax.vlines(np.cumsum(lengths) - pixel_width, *ax.get_ylim(), lw=1)\n\n    ax.set_xlim(-pixel_width, lengths.sum() - pixel_width)\n\n    event_centers = np.insert(np.cumsum(lengths), 0, 0)\n    event_centers = event_centers[:-1] + lengths / 2 - 0.5\n\n    ax.set_xticks(event_centers)\n    if idx is not None:\n        ax.set_xticklabels(idx)\n    else:\n        ax.set_xticklabels(np.arange(bst.n_intervals))\n\n    plotutils.no_xticks(ax)\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/","title":"nelpy.plotting.graph","text":""},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.circular_layout","title":"<code>circular_layout(G, scale=1, center=None, dim=2, direction='CCW')</code>","text":"<p>Position nodes on a circle.</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph or list</code> <p>The graph or list of nodes.</p> required <code>scale</code> <code>float</code> <p>Scale factor for positions (default is 1).</p> <code>1</code> <code>center</code> <code>array - like or None</code> <p>Coordinate pair around which to center the layout (default is None).</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of layout, currently only dim=2 is supported (default is 2).</p> <code>2</code> <code>direction</code> <code>(CCW, CW)</code> <p>Direction of node placement (default is 'CCW').</p> <code>'CCW'</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>dict</code> <p>A dictionary of positions keyed by node.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; G = nx.path_graph(4)\n&gt;&gt;&gt; pos = circular_layout(G)\n</code></pre> Notes <p>This algorithm currently only works in two dimensions and does not try to minimize edge crossings.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def circular_layout(G, scale=1, center=None, dim=2, direction=\"CCW\"):\n    \"\"\"\n    Position nodes on a circle.\n\n    Parameters\n    ----------\n    G : networkx.Graph or list\n        The graph or list of nodes.\n    scale : float, optional\n        Scale factor for positions (default is 1).\n    center : array-like or None, optional\n        Coordinate pair around which to center the layout (default is None).\n    dim : int, optional\n        Dimension of layout, currently only dim=2 is supported (default is 2).\n    direction : {'CCW', 'CW'}, optional\n        Direction of node placement (default is 'CCW').\n\n    Returns\n    -------\n    pos : dict\n        A dictionary of positions keyed by node.\n\n    Examples\n    --------\n    &gt;&gt;&gt; G = nx.path_graph(4)\n    &gt;&gt;&gt; pos = circular_layout(G)\n\n    Notes\n    -----\n    This algorithm currently only works in two dimensions and does not\n    try to minimize edge crossings.\n    \"\"\"\n\n    G, center = _process_params(G, center, dim)\n\n    if len(G) == 0:\n        pos = {}\n    elif len(G) == 1:\n        pos = {nx.utils.arbitrary_element(G): center}\n    else:\n        # Discard the extra angle since it matches 0 radians.\n        theta = np.linspace(0, 1, len(G) + 1)[:-1] * 2 * np.pi\n        theta = theta.astype(np.float32)\n        if direction == \"CCW\":\n            pos = np.column_stack([np.cos(theta), np.sin(theta)])\n        else:\n            pos = np.column_stack([np.sin(theta), np.cos(theta)])\n        pos = rescale_layout(pos, scale=scale) + center\n        pos = dict(zip(G, pos))\n\n    return pos\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.double_circular_layout","title":"<code>double_circular_layout(Gi, scale=1, center=None, dim=2, direction='CCW')</code>","text":"<p>Create a double circular layout for a graph, with inner and outer circles.</p> <p>Parameters:</p> Name Type Description Default <code>Gi</code> <code>Graph</code> <p>The graph to layout.</p> required <code>scale</code> <code>float</code> <p>Scale factor for the inner circle (default is 1).</p> <code>1</code> <code>center</code> <code>array - like or None</code> <p>Center of the layout (default is None).</p> <code>None</code> <code>dim</code> <code>int</code> <p>Dimension of layout (default is 2).</p> <code>2</code> <code>direction</code> <code>(CCW, CW)</code> <p>Direction of node placement (default is 'CCW').</p> <code>'CCW'</code> <p>Returns:</p> Name Type Description <code>npos</code> <code>dict</code> <p>Dictionary of node positions keyed by node.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def double_circular_layout(Gi, scale=1, center=None, dim=2, direction=\"CCW\"):\n    \"\"\"\n    Create a double circular layout for a graph, with inner and outer circles.\n\n    Parameters\n    ----------\n    Gi : networkx.Graph\n        The graph to layout.\n    scale : float, optional\n        Scale factor for the inner circle (default is 1).\n    center : array-like or None, optional\n        Center of the layout (default is None).\n    dim : int, optional\n        Dimension of layout (default is 2).\n    direction : {'CCW', 'CW'}, optional\n        Direction of node placement (default is 'CCW').\n\n    Returns\n    -------\n    npos : dict\n        Dictionary of node positions keyed by node.\n    \"\"\"\n    inner = circular_layout(\n        Gi, center=center, dim=dim, scale=scale, direction=direction\n    )\n    outer = circular_layout(\n        Gi, center=center, dim=dim, scale=scale * 1.3, direction=direction\n    )\n\n    num_states = Gi.number_of_nodes()\n\n    npos = {}\n    for k in outer.keys():\n        npos[k + num_states] = outer[k]\n\n    npos.update(inner)\n\n    return npos\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.draw_transmat_graph","title":"<code>draw_transmat_graph(G, edge_threshold=0, lw=1, ec='0.2', node_size=15)</code>","text":"<p>Draw a transition matrix as a circular graph.</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>The graph to draw, with edge weights as transition probabilities.</p> required <code>edge_threshold</code> <code>float</code> <p>Edges with weights below this value are not drawn (default is 0).</p> <code>0</code> <code>lw</code> <code>float</code> <p>Line width scaling for edges (default is 1).</p> <code>1</code> <code>ec</code> <code>color</code> <p>Edge color (default is '0.2').</p> <code>'0.2'</code> <code>node_size</code> <code>int</code> <p>Size of the nodes (default is 15).</p> <code>15</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the graph plot.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def draw_transmat_graph(G, edge_threshold=0, lw=1, ec=\"0.2\", node_size=15):\n    \"\"\"\n    Draw a transition matrix as a circular graph.\n\n    Parameters\n    ----------\n    G : networkx.Graph\n        The graph to draw, with edge weights as transition probabilities.\n    edge_threshold : float, optional\n        Edges with weights below this value are not drawn (default is 0).\n    lw : float, optional\n        Line width scaling for edges (default is 1).\n    ec : color, optional\n        Edge color (default is '0.2').\n    node_size : int, optional\n        Size of the nodes (default is 15).\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the graph plot.\n    \"\"\"\n    num_states = G.number_of_nodes()\n\n    edgewidth = [d[\"weight\"] for (u, v, d) in G.edges(data=True)]\n    edgewidth = np.array(edgewidth)\n    edgewidth[edgewidth &lt; edge_threshold] = 0\n\n    labels = {}\n    labels[0] = \"1\"\n    labels[1] = \"2\"\n    labels[2] = \"3\"\n    labels[num_states - 1] = str(num_states)\n\n    npos = circular_layout(G, scale=1, direction=\"CW\")\n    lpos = circular_layout(G, scale=1.15, direction=\"CW\")\n\n    nx.draw_networkx_edges(G, npos, alpha=0.8, width=edgewidth * lw, edge_color=ec)\n\n    nx.draw_networkx_nodes(G, npos, node_size=node_size, node_color=\"k\", alpha=0.8)\n    ax = plt.gca()\n    nx.draw_networkx_labels(G, lpos, labels, fontsize=18, ax=ax)\n    # fontsize does not seem to work :/\n\n    ax.set_aspect(\"equal\")\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.draw_transmat_graph_inner","title":"<code>draw_transmat_graph_inner(G, edge_threshold=0, lw=1, ec='0.2', node_size=15)</code>","text":"<p>Draw the inner part of a transition matrix as a circular graph.</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>The graph to draw, with edge weights as transition probabilities.</p> required <code>edge_threshold</code> <code>float</code> <p>Edges with weights below this value are not drawn (default is 0).</p> <code>0</code> <code>lw</code> <code>float</code> <p>Line width scaling for edges (default is 1).</p> <code>1</code> <code>ec</code> <code>color</code> <p>Edge color (default is '0.2').</p> <code>'0.2'</code> <code>node_size</code> <code>int</code> <p>Size of the nodes (default is 15).</p> <code>15</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the graph plot.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def draw_transmat_graph_inner(G, edge_threshold=0, lw=1, ec=\"0.2\", node_size=15):\n    \"\"\"\n    Draw the inner part of a transition matrix as a circular graph.\n\n    Parameters\n    ----------\n    G : networkx.Graph\n        The graph to draw, with edge weights as transition probabilities.\n    edge_threshold : float, optional\n        Edges with weights below this value are not drawn (default is 0).\n    lw : float, optional\n        Line width scaling for edges (default is 1).\n    ec : color, optional\n        Edge color (default is '0.2').\n    node_size : int, optional\n        Size of the nodes (default is 15).\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the graph plot.\n    \"\"\"\n    # num_states = G.number_of_nodes()\n\n    edgewidth = [d[\"weight\"] for (u, v, d) in G.edges(data=True)]\n    edgewidth = np.array(edgewidth)\n    edgewidth[edgewidth &lt; edge_threshold] = 0\n\n    npos = circular_layout(G, scale=1, direction=\"CW\")\n\n    nx.draw_networkx_edges(G, npos, alpha=1.0, width=edgewidth * lw, edge_color=ec)\n\n    nx.draw_networkx_nodes(G, npos, node_size=node_size, node_color=\"k\", alpha=1.0)\n    ax = plt.gca()\n    ax.set_aspect(\"equal\")\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.draw_transmat_graph_outer","title":"<code>draw_transmat_graph_outer(Go, Gi, edge_threshold=0, lw=1, ec='0.2', nc='k', node_size=15)</code>","text":"<p>Draw the outer part of a transition matrix as a double circular graph.</p> <p>Parameters:</p> Name Type Description Default <code>Go</code> <code>Graph</code> <p>The outer graph to draw.</p> required <code>Gi</code> <code>Graph</code> <p>The inner graph for layout reference.</p> required <code>edge_threshold</code> <code>float</code> <p>Edges with weights below this value are not drawn (default is 0).</p> <code>0</code> <code>lw</code> <code>float</code> <p>Line width scaling for edges (default is 1).</p> <code>1</code> <code>ec</code> <code>color</code> <p>Edge color (default is '0.2').</p> <code>'0.2'</code> <code>nc</code> <code>color</code> <p>Node color (default is 'k').</p> <code>'k'</code> <code>node_size</code> <code>int</code> <p>Size of the nodes (default is 15).</p> <code>15</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the graph plot.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def draw_transmat_graph_outer(\n    Go, Gi, edge_threshold=0, lw=1, ec=\"0.2\", nc=\"k\", node_size=15\n):\n    \"\"\"\n    Draw the outer part of a transition matrix as a double circular graph.\n\n    Parameters\n    ----------\n    Go : networkx.Graph\n        The outer graph to draw.\n    Gi : networkx.Graph\n        The inner graph for layout reference.\n    edge_threshold : float, optional\n        Edges with weights below this value are not drawn (default is 0).\n    lw : float, optional\n        Line width scaling for edges (default is 1).\n    ec : color, optional\n        Edge color (default is '0.2').\n    nc : color, optional\n        Node color (default is 'k').\n    node_size : int, optional\n        Size of the nodes (default is 15).\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the graph plot.\n    \"\"\"\n    # num_states = Go.number_of_nodes()\n\n    edgewidth = [d[\"weight\"] for (u, v, d) in Go.edges(data=True)]\n    edgewidth = np.array(edgewidth)\n    edgewidth[edgewidth &lt; edge_threshold] = 0\n\n    npos = double_circular_layout(Gi, scale=1, direction=\"CW\")\n\n    nx.draw_networkx_edges(Go, npos, alpha=1.0, width=edgewidth * lw, edge_color=ec)\n\n    nx.draw_networkx_nodes(Go, npos, node_size=node_size, node_color=nc, alpha=1.0)\n\n    ax = plt.gca()\n    ax.set_aspect(\"equal\")\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.graph_from_transmat","title":"<code>graph_from_transmat(transmat)</code>","text":"<p>Create a symmetric undirected graph from a transition matrix.</p> <p>Parameters:</p> Name Type Description Default <code>transmat</code> <code>array - like</code> <p>Transition matrix (2D square array).</p> required <p>Returns:</p> Name Type Description <code>G</code> <code>Graph</code> <p>The resulting symmetric graph.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def graph_from_transmat(transmat):\n    \"\"\"\n    Create a symmetric undirected graph from a transition matrix.\n\n    Parameters\n    ----------\n    transmat : array-like\n        Transition matrix (2D square array).\n\n    Returns\n    -------\n    G : networkx.Graph\n        The resulting symmetric graph.\n    \"\"\"\n    G = nx.Graph()\n    num_states = transmat.shape[1]\n    # make symmetric\n    tmat = (transmat + transmat.T) / 2\n\n    for s1 in range(num_states):\n        for s2 in range(num_states):\n            G.add_edge(s1, s2, weight=tmat[s1, s2])\n\n    return G\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.inner_graph_from_transmat","title":"<code>inner_graph_from_transmat(transmat)</code>","text":"<p>Create an inner graph from a transition matrix, clearing the super diagonal.</p> <p>Parameters:</p> Name Type Description Default <code>transmat</code> <code>array - like</code> <p>Transition matrix (2D square array).</p> required <p>Returns:</p> Name Type Description <code>G</code> <code>Graph</code> <p>The resulting inner graph.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def inner_graph_from_transmat(transmat):\n    \"\"\"\n    Create an inner graph from a transition matrix, clearing the super diagonal.\n\n    Parameters\n    ----------\n    transmat : array-like\n        Transition matrix (2D square array).\n\n    Returns\n    -------\n    G : networkx.Graph\n        The resulting inner graph.\n    \"\"\"\n    G = nx.Graph()\n    num_states = transmat.shape[1]\n    # make symmetric\n    tmat = (transmat + transmat.T) / 2\n\n    # clear super diagonal\n    for ii in range(num_states - 1):\n        tmat[ii, ii + 1] = 0\n\n    for s1 in range(num_states):\n        for s2 in range(num_states):\n            G.add_edge(s1, s2, weight=tmat[s1, s2])\n\n    return G\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.outer_graph_from_transmat","title":"<code>outer_graph_from_transmat(transmat)</code>","text":"<p>Create an outer graph from a transition matrix, connecting states to outer nodes.</p> <p>Parameters:</p> Name Type Description Default <code>transmat</code> <code>array - like</code> <p>Transition matrix (2D square array).</p> required <p>Returns:</p> Name Type Description <code>G</code> <code>Graph</code> <p>The resulting outer graph.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def outer_graph_from_transmat(transmat):\n    \"\"\"\n    Create an outer graph from a transition matrix, connecting states to outer nodes.\n\n    Parameters\n    ----------\n    transmat : array-like\n        Transition matrix (2D square array).\n\n    Returns\n    -------\n    G : networkx.Graph\n        The resulting outer graph.\n    \"\"\"\n    G = nx.Graph()\n    num_states = transmat.shape[1]\n    # make symmetric\n    tmat = (transmat + transmat.T) / 2\n\n    for s1 in range(num_states):\n        G.add_edge(s1, s1 + num_states, weight=tmat[s1, s1])  # self transitions\n\n    for s1 in range(num_states - 1):\n        G.add_edge(\n            s1, s1 + num_states + 1, weight=tmat[s1, s1 + 1]\n        )  # forward neighbor transitions\n\n    return G\n</code></pre>"},{"location":"reference/nelpy/plotting/graph/#nelpy.plotting.graph.rescale_layout","title":"<code>rescale_layout(pos, scale=1)</code>","text":"<p>Return scaled position array to (-scale, scale) in all axes.</p> <p>The function acts on NumPy arrays which hold position information. Each position is one row of the array. The dimension of the space equals the number of columns. Each coordinate in one column.</p> <p>To rescale, the mean (center) is subtracted from each axis separately. Then all values are scaled so that the largest magnitude value from all axes equals <code>scale</code> (thus, the aspect ratio is preserved). The resulting NumPy Array is returned (order of rows unchanged).</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>ndarray</code> <p>Positions to be scaled. Each row is a position.</p> required <code>scale</code> <code>float</code> <p>The size of the resulting extent in all directions (default is 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>ndarray</code> <p>Scaled positions. Each row is a position.</p> Source code in <code>nelpy/plotting/graph.py</code> <pre><code>def rescale_layout(pos, scale=1):\n    \"\"\"\n    Return scaled position array to (-scale, scale) in all axes.\n\n    The function acts on NumPy arrays which hold position information.\n    Each position is one row of the array. The dimension of the space\n    equals the number of columns. Each coordinate in one column.\n\n    To rescale, the mean (center) is subtracted from each axis separately.\n    Then all values are scaled so that the largest magnitude value\n    from all axes equals `scale` (thus, the aspect ratio is preserved).\n    The resulting NumPy Array is returned (order of rows unchanged).\n\n    Parameters\n    ----------\n    pos : numpy.ndarray\n        Positions to be scaled. Each row is a position.\n    scale : float, optional\n        The size of the resulting extent in all directions (default is 1).\n\n    Returns\n    -------\n    pos : numpy.ndarray\n        Scaled positions. Each row is a position.\n    \"\"\"\n    # Find max length over all dimensions\n    lim = 0  # max coordinate for all axes\n    for i in range(pos.shape[1]):\n        pos[:, i] -= pos[:, i].mean()\n        lim = max(pos[:, i].max(), lim)\n    # rescale to (-scale, scale) in all directions, preserves aspect\n    if lim &gt; 0:\n        for i in range(pos.shape[1]):\n            pos[:, i] *= scale / lim\n    return pos\n</code></pre>"},{"location":"reference/nelpy/plotting/helpers/","title":"nelpy.plotting.helpers","text":"<p>This file contains helper classes and functions for the nelpy plotting package.</p>"},{"location":"reference/nelpy/plotting/helpers/#nelpy.plotting.helpers.NelpyAxes","title":"<code>NelpyAxes</code>","text":"<p>               Bases: <code>Axes</code></p> <p>Custom Axes class for nelpy plotting extensions.</p> Source code in <code>nelpy/plotting/helpers.py</code> <pre><code>class NelpyAxes(Axes):\n    \"\"\"\n    Custom Axes class for nelpy plotting extensions.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        Initialize a NelpyAxes object.\n        \"\"\"\n        Axes.__init__(self, **kwargs)\n        self._empty = None\n\n    @property\n    def isempty(self):\n        \"\"\"\n        Check if the axes is empty.\n\n        Returns\n        -------\n        bool or None\n            True if empty, False otherwise, or None if not set.\n        \"\"\"\n        return self._empty\n\n    def _as_mpl_axes(self):\n        \"\"\"\n        Not implemented: Convert back to pure matplotlib.axes.Axes.\n\n        Raises\n        ------\n        NotImplementedError\n            Always raised, as this is not yet supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"converting back to pure matplotlib.axes.Axes not yet supported!\"\n        )\n</code></pre>"},{"location":"reference/nelpy/plotting/helpers/#nelpy.plotting.helpers.NelpyAxes.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>Check if the axes is empty.</p> <p>Returns:</p> Type Description <code>bool or None</code> <p>True if empty, False otherwise, or None if not set.</p>"},{"location":"reference/nelpy/plotting/helpers/#nelpy.plotting.helpers.RasterLabelData","title":"<code>RasterLabelData</code>","text":"<p>               Bases: <code>Artist</code></p> <p>Helper class for storing label data for raster plots in nelpy.</p> <p>Attributes:</p> Name Type Description <code>label_data</code> <code>dict</code> <p>Dictionary mapping unit_id to (unit_loc, unit_label).</p> <code>yrange</code> <code>list</code> <p>List representing the y-range for the raster labels.</p> Source code in <code>nelpy/plotting/helpers.py</code> <pre><code>class RasterLabelData(artist.Artist):\n    \"\"\"\n    Helper class for storing label data for raster plots in nelpy.\n\n    Attributes\n    ----------\n    label_data : dict\n        Dictionary mapping unit_id to (unit_loc, unit_label).\n    yrange : list\n        List representing the y-range for the raster labels.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize a RasterLabelData object.\n        \"\"\"\n        self.label_data = {}  # (k, v) = (unit_id, (unit_loc, unit_label))\n        artist.Artist.__init__(self)\n        self.yrange = []\n\n    def __repr__(self):\n        \"\"\"\n        Return a string representation of the RasterLabelData object.\n        \"\"\"\n        return \"&lt;nelpy.RasterLabelData at \" + str(hex(id(self))) + \"&gt;\"\n\n    @property\n    def label_data(self):\n        \"\"\"\n        Get the label data dictionary.\n\n        Returns\n        -------\n        dict\n            Dictionary mapping unit_id to (unit_loc, unit_label).\n        \"\"\"\n        return self._label_data\n\n    @label_data.setter\n    def label_data(self, val):\n        \"\"\"\n        Set the label data dictionary.\n\n        Parameters\n        ----------\n        val : dict\n            Dictionary mapping unit_id to (unit_loc, unit_label).\n        \"\"\"\n        self._label_data = val\n\n    @property\n    def yrange(self):\n        \"\"\"\n        Get the y-range for the raster labels.\n\n        Returns\n        -------\n        list\n            The y-range list.\n        \"\"\"\n        return self._yrange\n\n    @yrange.setter\n    def yrange(self, val):\n        \"\"\"\n        Set the y-range for the raster labels.\n\n        Parameters\n        ----------\n        val : list\n            The y-range list.\n        \"\"\"\n        self._yrange = val\n</code></pre>"},{"location":"reference/nelpy/plotting/helpers/#nelpy.plotting.helpers.RasterLabelData.label_data","title":"<code>label_data</code>  <code>property</code> <code>writable</code>","text":"<p>Get the label data dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping unit_id to (unit_loc, unit_label).</p>"},{"location":"reference/nelpy/plotting/helpers/#nelpy.plotting.helpers.RasterLabelData.yrange","title":"<code>yrange</code>  <code>property</code> <code>writable</code>","text":"<p>Get the y-range for the raster labels.</p> <p>Returns:</p> Type Description <code>list</code> <p>The y-range list.</p>"},{"location":"reference/nelpy/plotting/miscplot/","title":"nelpy.plotting.miscplot","text":"<p>Miscellaneous support plots for nelpy</p> <p>'palplot' Copyright (c) 2012-2016, Michael L. Waskom</p>"},{"location":"reference/nelpy/plotting/miscplot/#nelpy.plotting.miscplot.palplot","title":"<code>palplot(pal, size=1)</code>","text":"<p>Plot the values in a color palette as a horizontal array.</p> <p>Parameters:</p> Name Type Description Default <code>pal</code> <code>sequence of matplotlib colors</code> <p>Colors, i.e. as returned by nelpy.color_palette().</p> required <code>size</code> <code>float</code> <p>Scaling factor for size of plot. Default is 1.</p> <code>1</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.plotting.miscplot import palplot\n&gt;&gt;&gt; pal = [\"#FF0000\", \"#00FF00\", \"#0000FF\"]\n&gt;&gt;&gt; palplot(pal)\n</code></pre> Source code in <code>nelpy/plotting/miscplot.py</code> <pre><code>def palplot(pal, size=1):\n    \"\"\"\n    Plot the values in a color palette as a horizontal array.\n\n    Parameters\n    ----------\n    pal : sequence of matplotlib colors\n        Colors, i.e. as returned by nelpy.color_palette().\n    size : float, optional\n        Scaling factor for size of plot. Default is 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.plotting.miscplot import palplot\n    &gt;&gt;&gt; pal = [\"#FF0000\", \"#00FF00\", \"#0000FF\"]\n    &gt;&gt;&gt; palplot(pal)\n    \"\"\"\n    n = len(pal)\n    f, ax = plt.subplots(1, 1, figsize=(n * size, size))\n    ax.imshow(\n        np.arange(n).reshape(1, n),\n        cmap=mpl.colors.ListedColormap(list(pal)),\n        interpolation=\"nearest\",\n        aspect=\"auto\",\n    )\n    ax.set_xticks(np.arange(n) - 0.5)\n    ax.set_yticks([-0.5, 0.5])\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n</code></pre>"},{"location":"reference/nelpy/plotting/miscplot/#nelpy.plotting.miscplot.stripplot","title":"<code>stripplot(*eps, voffset=None, lw=None, labels=None)</code>","text":"<p>Plot epochs as segments on a line.</p> <p>Parameters:</p> Name Type Description Default <code>*eps</code> <code>EpochArray</code> <p>One or more EpochArray objects to plot.</p> <code>()</code> <code>voffset</code> <code>float</code> <p>Vertical offset between lines.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width.</p> <code>None</code> <code>labels</code> <code>array-like of str</code> <p>Labels for each EpochArray.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the strip plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy import EpochArray\n&gt;&gt;&gt; ep1 = EpochArray([[0, 1], [2, 3]])\n&gt;&gt;&gt; ep2 = EpochArray([[4, 5], [6, 7]])\n&gt;&gt;&gt; stripplot(ep1, ep2, labels=[\"A\", \"B\"])\n</code></pre> Source code in <code>nelpy/plotting/miscplot.py</code> <pre><code>def stripplot(*eps, voffset=None, lw=None, labels=None):\n    \"\"\"\n    Plot epochs as segments on a line.\n\n    Parameters\n    ----------\n    *eps : nelpy.EpochArray\n        One or more EpochArray objects to plot.\n    voffset : float, optional\n        Vertical offset between lines.\n    lw : float, optional\n        Line width.\n    labels : array-like of str, optional\n        Labels for each EpochArray.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the strip plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy import EpochArray\n    &gt;&gt;&gt; ep1 = EpochArray([[0, 1], [2, 3]])\n    &gt;&gt;&gt; ep2 = EpochArray([[4, 5], [6, 7]])\n    &gt;&gt;&gt; stripplot(ep1, ep2, labels=[\"A\", \"B\"])\n    \"\"\"\n\n    # TODO: this plot is in alpha mode; i.e., needs lots of work...\n    # TODO: list unpacking if eps is a list of EpochArrays...\n\n    fig = plt.figure(figsize=(10, 2))\n    ax0 = fig.add_subplot(111)\n\n    prop_cycle = plt.rcParams[\"axes.prop_cycle\"]\n    colors = prop_cycle.by_key()[\"color\"]\n\n    epmin = np.inf\n    epmax = -np.inf\n\n    for ii, epa in enumerate(eps):\n        epmin = np.min((epa.start, epmin))\n        epmax = np.max((epa.stop, epmax))\n\n    # WARNING TODO: this does not yet wrap the color cycler, but it's easy to do with mod arith\n    y = 0.2\n    for ii, epa in enumerate(eps):\n        ax0.hlines(y, epmin, epmax, \"0.7\")\n        for ep in epa:\n            ax0.plot(\n                [ep.start, ep.stop],\n                [y, y],\n                lw=6,\n                color=colors[ii],\n                solid_capstyle=\"round\",\n            )\n        y += 0.2\n\n    utils.clear_top(ax0)\n    #     npl.utils.clear_bottom(ax0)\n\n    if labels is None:\n        # try to get labels from epoch arrays\n        labels = [\"\"]\n        labels.extend([epa.label for epa in eps])\n    else:\n        labels.insert(0, \"\")\n\n    ax0.set_yticklabels(labels)\n\n    ax0.set_xlim(epmin - 10, epmax + 10)\n    ax0.set_ylim(0, 0.2 * (ii + 2))\n\n    utils.no_yticks(ax0)\n    utils.clear_left(ax0)\n    utils.clear_right(ax0)\n\n    return ax0\n</code></pre>"},{"location":"reference/nelpy/plotting/miscplot/#nelpy.plotting.miscplot.veva_scatter","title":"<code>veva_scatter(data, *, cmap=None, color=None, ax=None, lw=None, lh=None, **kwargs)</code>","text":"<p>Scatter plot for ValueEventArray objects, colored by value.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ValueEventArray</code> <p>The value event data to plot.</p> required <code>cmap</code> <code>matplotlib colormap</code> <p>Colormap to use for the event values.</p> <code>None</code> <code>color</code> <code>matplotlib color</code> <p>Color for the events if cmap is not specified.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on. If None, uses current axis.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width for the event markers.</p> <code>None</code> <code>lh</code> <code>float</code> <p>Line height for the event markers.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to vlines.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the scatter plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from nelpy.core import ValueEventArray\n&gt;&gt;&gt; vea = ValueEventArray(\n...     [[1, 2, 3], [4, 5, 6]], values=[[10, 20, 30], [40, 50, 60]]\n... )\n&gt;&gt;&gt; veva_scatter(vea)\n</code></pre> Source code in <code>nelpy/plotting/miscplot.py</code> <pre><code>def veva_scatter(data, *, cmap=None, color=None, ax=None, lw=None, lh=None, **kwargs):\n    \"\"\"\n    Scatter plot for ValueEventArray objects, colored by value.\n\n    Parameters\n    ----------\n    data : nelpy.ValueEventArray\n        The value event data to plot.\n    cmap : matplotlib colormap, optional\n        Colormap to use for the event values.\n    color : matplotlib color, optional\n        Color for the events if cmap is not specified.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on. If None, uses current axis.\n    lw : float, optional\n        Line width for the event markers.\n    lh : float, optional\n        Line height for the event markers.\n    **kwargs : dict\n        Additional keyword arguments passed to vlines.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis with the scatter plot.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from nelpy.core import ValueEventArray\n    &gt;&gt;&gt; vea = ValueEventArray(\n    ...     [[1, 2, 3], [4, 5, 6]], values=[[10, 20, 30], [40, 50, 60]]\n    ... )\n    &gt;&gt;&gt; veva_scatter(vea)\n    \"\"\"\n    # Sort out default values for the parameters\n    if ax is None:\n        ax = plt.gca()\n    if cmap is None and color is None:\n        color = \"0.25\"\n    if lw is None:\n        lw = 1.5\n    if lh is None:\n        lh = 0.95\n\n    hh = lh / 2.0  # half the line height\n\n    # Handle different types of input data\n    if isinstance(data, core.ValueEventArray):\n        vmin = (\n            np.min([np.min(x) for x in data.values]) - 1\n        )  # TODO: -1 because white is invisible... fix this properly\n        vmax = np.max([np.max(x) for x in data.values])\n        norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n\n        for ii, (events, values) in enumerate(zip(data.events, data.values)):\n            if cmap is not None:\n                colors = cmap(norm(values))\n            else:\n                colors = color\n            ax.vlines(events, ii - hh, ii + hh, colors=colors, lw=lw, **kwargs)\n\n    else:\n        raise NotImplementedError(\n            \"plotting {} not yet supported\".format(str(type(data)))\n        )\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/palettes/","title":"nelpy.plotting.palettes","text":"<p>Color palettes for use with nelpy.</p> <p>Some of these functions are Copyright (c) 2012-2016, Michael L. Waskom</p>"},{"location":"reference/nelpy/plotting/palettes/#nelpy.plotting.palettes.blend_palette","title":"<code>blend_palette(colors, n_colors=6, as_cmap=False, input='rgb')</code>","text":"<p>Make a palette that blends between a list of colors.</p> <p>Parameters:</p> Name Type Description Default <code>colors</code> <code>sequence</code> <p>Sequence of colors in various formats interpreted by <code>input</code>.</p> required <code>n_colors</code> <code>int</code> <p>Number of colors in the palette.</p> <code>6</code> <code>as_cmap</code> <code>bool</code> <p>If True, return as a matplotlib colormap instead of list.</p> <code>False</code> <code>input</code> <code>(rgb, hls, husl, xkcd)</code> <p>Color space to interpret the input colors.</p> <code>'rgb'</code> <p>Returns:</p> Name Type Description <code>palette</code> <code>_ColorPalette or matplotlib colormap</code> <p>List-like object of colors as RGB tuples, or colormap object that can map continuous values to colors, depending on the value of the <code>as_cmap</code> parameter.</p> Source code in <code>nelpy/plotting/palettes.py</code> <pre><code>def blend_palette(colors, n_colors=6, as_cmap=False, input=\"rgb\"):\n    \"\"\"\n    Make a palette that blends between a list of colors.\n\n    Parameters\n    ----------\n    colors : sequence\n        Sequence of colors in various formats interpreted by ``input``.\n    n_colors : int, optional\n        Number of colors in the palette.\n    as_cmap : bool, optional\n        If True, return as a matplotlib colormap instead of list.\n    input : {'rgb', 'hls', 'husl', 'xkcd'}\n        Color space to interpret the input colors.\n\n    Returns\n    -------\n    palette : _ColorPalette or matplotlib colormap\n        List-like object of colors as RGB tuples, or colormap object that\n        can map continuous values to colors, depending on the value of the\n        ``as_cmap`` parameter.\n    \"\"\"\n    colors = [_color_to_rgb(color, input) for color in colors]\n    name = \"blend\"\n    pal = mpl.colors.LinearSegmentedColormap.from_list(name, colors)\n    if not as_cmap:\n        pal = _ColorPalette(pal(np.linspace(0, 1, n_colors)))\n    return pal\n</code></pre>"},{"location":"reference/nelpy/plotting/palettes/#nelpy.plotting.palettes.color_palette","title":"<code>color_palette(palette=None, n_colors=None, desat=None)</code>","text":"<p>Return a list of colors defining a color palette.</p> <p>Available seaborn palette names:     deep, muted, bright, pastel, dark, colorblind</p> <p>Other options:     hls, husl, any named matplotlib palette, list of colors</p> <p>Calling this function with <code>palette=None</code> will return the current matplotlib color cycle.</p> <p>Matplotlib palettes can be specified as reversed palettes by appending \"_r\" to the name or as dark palettes by appending \"_d\" to the name. (These options are mutually exclusive, but the resulting list of colors can also be reversed).</p> <p>This function can also be used in a <code>with</code> statement to temporarily set the color cycle for a plot or set of plots.</p> <p>Parameters:</p> Name Type Description Default <code>palette</code> <code>None, str, or sequence</code> <p>Name of palette or None to return current palette. If a sequence, input colors are used but possibly cycled and desaturated.</p> <code>None</code> <code>n_colors</code> <code>int</code> <p>Number of colors in the palette. If <code>None</code>, the default will depend on how <code>palette</code> is specified. Named palettes default to 6 colors, but grabbing the current palette or passing in a list of colors will not change the number of colors unless this is specified. Asking for more colors than exist in the palette will cause it to cycle.</p> <code>None</code> <code>desat</code> <code>float</code> <p>Proportion to desaturate each color by.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>palette</code> <code>_ColorPalette</code> <p>Color palette. Behaves like a list, but can be used as a context manager and possesses an <code>as_hex</code> method to convert to hex color codes.</p> See Also <p>set_palette : Set the default color cycle for all plots.</p> <p>Examples:</p> <p>Show one of the \"seaborn palettes\", which have the same basic order of hues as the default matplotlib color cycle but more attractive colors.</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; import seaborn as sns\n...\n... sns.set()\n&gt;&gt;&gt; sns.palplot(sns.color_palette(\"muted\"))\n</code></pre> <p>Use discrete values from one of the built-in matplotlib colormaps.</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; sns.palplot(sns.color_palette(\"RdBu\", n_colors=7))\n</code></pre> <p>Make a \"dark\" matplotlib sequential palette variant. (This can be good when coloring multiple lines or points that correspond to an ordered variable, where you don't want the lightest lines to be invisible).</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; sns.palplot(sns.color_palette(\"Blues_d\"))\n</code></pre> <p>Use a categorical matplotlib palette, add some desaturation. (This can be good when making plots with large patches, which look best with dimmer colors).</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; sns.palplot(sns.color_palette(\"Set1\", n_colors=8, desat=0.5))\n</code></pre> <p>Use as a context manager:</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; import numpy as np, matplotlib.pyplot as plt\n&gt;&gt;&gt; with sns.color_palette(\"husl\", 8):\n...     _ = plt.plot(np.c_[np.zeros(8), np.arange(8)].T)\n</code></pre> Source code in <code>nelpy/plotting/palettes.py</code> <pre><code>def color_palette(palette=None, n_colors=None, desat=None):\n    \"\"\"\n    Return a list of colors defining a color palette.\n\n    Available seaborn palette names:\n        deep, muted, bright, pastel, dark, colorblind\n\n    Other options:\n        hls, husl, any named matplotlib palette, list of colors\n\n    Calling this function with ``palette=None`` will return the current\n    matplotlib color cycle.\n\n    Matplotlib palettes can be specified as reversed palettes by appending\n    \"_r\" to the name or as dark palettes by appending \"_d\" to the name.\n    (These options are mutually exclusive, but the resulting list of colors\n    can also be reversed).\n\n    This function can also be used in a ``with`` statement to temporarily\n    set the color cycle for a plot or set of plots.\n\n    Parameters\n    ----------\n    palette : None, str, or sequence, optional\n        Name of palette or None to return current palette. If a sequence, input\n        colors are used but possibly cycled and desaturated.\n    n_colors : int, optional\n        Number of colors in the palette. If ``None``, the default will depend\n        on how ``palette`` is specified. Named palettes default to 6 colors,\n        but grabbing the current palette or passing in a list of colors will\n        not change the number of colors unless this is specified. Asking for\n        more colors than exist in the palette will cause it to cycle.\n    desat : float, optional\n        Proportion to desaturate each color by.\n\n    Returns\n    -------\n    palette : _ColorPalette\n        Color palette. Behaves like a list, but can be used as a context\n        manager and possesses an ``as_hex`` method to convert to hex color\n        codes.\n\n    See Also\n    --------\n    set_palette : Set the default color cycle for all plots.\n\n    Examples\n    --------\n    Show one of the \"seaborn palettes\", which have the same basic order of hues\n    as the default matplotlib color cycle but more attractive colors.\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; import seaborn as sns\n        ...\n        ... sns.set()\n        &gt;&gt;&gt; sns.palplot(sns.color_palette(\"muted\"))\n\n    Use discrete values from one of the built-in matplotlib colormaps.\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; sns.palplot(sns.color_palette(\"RdBu\", n_colors=7))\n\n    Make a \"dark\" matplotlib sequential palette variant. (This can be good\n    when coloring multiple lines or points that correspond to an ordered\n    variable, where you don't want the lightest lines to be invisible).\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; sns.palplot(sns.color_palette(\"Blues_d\"))\n\n    Use a categorical matplotlib palette, add some desaturation. (This can be\n    good when making plots with large patches, which look best with dimmer\n    colors).\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; sns.palplot(sns.color_palette(\"Set1\", n_colors=8, desat=0.5))\n\n    Use as a context manager:\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; import numpy as np, matplotlib.pyplot as plt\n        &gt;&gt;&gt; with sns.color_palette(\"husl\", 8):\n        ...     _ = plt.plot(np.c_[np.zeros(8), np.arange(8)].T)\n    \"\"\"\n    if palette is None:\n        palette = utils.get_color_cycle()\n        if n_colors is None:\n            n_colors = len(palette)\n    elif isinstance(palette, colors.ColorGroup):\n        palette = palette.colors\n        if n_colors is None:\n            n_colors = len(palette)\n    elif not isinstance(palette, str):\n        palette = palette\n        if n_colors is None:\n            n_colors = len(palette)\n    else:\n        if n_colors is None:\n            n_colors = 6\n\n        # if isinstance(palette, list):\n        #     pass\n        if palette in NELPY_PALETTES:\n            palette = NELPY_PALETTES[palette]\n        elif palette.lower() == \"jet\":\n            raise ValueError(\"No.\")\n        elif palette in SEABORN_PALETTES:\n            palette = SEABORN_PALETTES[palette]\n        elif palette in dir(mpl.cm):\n            palette = mpl_palette(palette, n_colors)\n        elif palette[:-2] in dir(mpl.cm):\n            palette = mpl_palette(palette, n_colors)\n        else:\n            raise ValueError(\"%s is not a valid palette name\" % palette)\n\n    if desat is not None:\n        palette = [utils.desaturate(c, desat) for c in palette]\n\n    # Always return as many colors as we asked for\n    pal_cycle = cycle(palette)\n    palette = [next(pal_cycle) for _ in range(n_colors)]\n\n    # Always return in r, g, b tuple format\n    try:\n        palette = map(mpl.colors.colorConverter.to_rgb, palette)\n        palette = _ColorPalette(palette)\n    except ValueError:\n        raise ValueError(\"Could not generate a palette for %s\" % str(palette))\n\n    return palette\n</code></pre>"},{"location":"reference/nelpy/plotting/palettes/#nelpy.plotting.palettes.dark_palette","title":"<code>dark_palette(color, n_colors=6, reverse=False, as_cmap=False, input='rgb')</code>","text":"<p>Make a sequential palette that blends from dark to <code>color</code>.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>base color for high values</code> <p>hex, rgb-tuple, or html color name</p> required <code>n_colors</code> <code>int</code> <p>Number of colors in the palette.</p> <code>6</code> <code>reverse</code> <code>bool</code> <p>If True, reverse the direction of the blend.</p> <code>False</code> <code>as_cmap</code> <code>bool</code> <p>If True, return as a matplotlib colormap instead of list.</p> <code>False</code> <code>input</code> <code>(rgb, hls, husl, xkcd)</code> <p>Color space to interpret the input color. The first three options apply to tuple inputs and the latter applies to string inputs.</p> <code>'rgb'</code> <p>Returns:</p> Name Type Description <code>palette</code> <code>_ColorPalette or matplotlib colormap</code> <p>List-like object of colors as RGB tuples, or colormap object that can map continuous values to colors, depending on the value of the <code>as_cmap</code> parameter.</p> Source code in <code>nelpy/plotting/palettes.py</code> <pre><code>def dark_palette(color, n_colors=6, reverse=False, as_cmap=False, input=\"rgb\"):\n    \"\"\"\n    Make a sequential palette that blends from dark to ``color``.\n\n    Parameters\n    ----------\n    color : base color for high values\n        hex, rgb-tuple, or html color name\n    n_colors : int, optional\n        Number of colors in the palette.\n    reverse : bool, optional\n        If True, reverse the direction of the blend.\n    as_cmap : bool, optional\n        If True, return as a matplotlib colormap instead of list.\n    input : {'rgb', 'hls', 'husl', 'xkcd'}\n        Color space to interpret the input color. The first three options\n        apply to tuple inputs and the latter applies to string inputs.\n\n    Returns\n    -------\n    palette : _ColorPalette or matplotlib colormap\n        List-like object of colors as RGB tuples, or colormap object that\n        can map continuous values to colors, depending on the value of the\n        ``as_cmap`` parameter.\n    \"\"\"\n    color = _color_to_rgb(color, input)\n    gray = \"#222222\"\n    colors = [color, gray] if reverse else [gray, color]\n    return blend_palette(colors, n_colors, as_cmap)\n</code></pre>"},{"location":"reference/nelpy/plotting/palettes/#nelpy.plotting.palettes.diverging_palette","title":"<code>diverging_palette(h_neg, h_pos, s=75, lightness=50, sep=10, n=6, center='light', as_cmap=False)</code>","text":"<p>Make a diverging palette between two HUSL colors.</p> <p>Parameters:</p> Name Type Description Default <code>h_neg</code> <code>float</code> <p>Anchor hues for negative and positive extents of the map (in [0, 359]).</p> required <code>h_pos</code> <code>float</code> <p>Anchor hues for negative and positive extents of the map (in [0, 359]).</p> required <code>s</code> <code>float</code> <p>Anchor saturation for both extents of the map (in [0, 100]).</p> <code>75</code> <code>lightness</code> <code>float</code> <p>Anchor lightness for both extents of the map (in [0, 100]).</p> <code>50</code> <code>n</code> <code>int</code> <p>Number of colors in the palette (if not returning a cmap).</p> <code>6</code> <code>center</code> <code>(light, dark)</code> <p>Whether the center of the palette is light or dark.</p> <code>\"light\"</code> <code>as_cmap</code> <code>bool</code> <p>If True, return a matplotlib colormap object rather than a list of colors.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>palette</code> <code>_ColorPalette or matplotlib colormap</code> <p>List-like object of colors as RGB tuples, or colormap object that can map continuous values to colors, depending on the value of the <code>as_cmap</code> parameter.</p> Source code in <code>nelpy/plotting/palettes.py</code> <pre><code>def diverging_palette(\n    h_neg, h_pos, s=75, lightness=50, sep=10, n=6, center=\"light\", as_cmap=False\n):\n    \"\"\"\n    Make a diverging palette between two HUSL colors.\n\n    Parameters\n    ----------\n    h_neg, h_pos : float\n        Anchor hues for negative and positive extents of the map (in [0, 359]).\n    s : float, optional\n        Anchor saturation for both extents of the map (in [0, 100]).\n    lightness : float, optional\n        Anchor lightness for both extents of the map (in [0, 100]).\n    n : int, optional\n        Number of colors in the palette (if not returning a cmap).\n    center : {\"light\", \"dark\"}, optional\n        Whether the center of the palette is light or dark.\n    as_cmap : bool, optional\n        If True, return a matplotlib colormap object rather than a list of colors.\n\n    Returns\n    -------\n    palette : _ColorPalette or matplotlib colormap\n        List-like object of colors as RGB tuples, or colormap object that\n        can map continuous values to colors, depending on the value of the\n        ``as_cmap`` parameter.\n    \"\"\"\n    palfunc = dark_palette if center == \"dark\" else light_palette\n    neg = palfunc((h_neg, s, lightness), 128 - (sep / 2), reverse=True, input=\"husl\")\n    pos = palfunc((h_pos, s, lightness), 128 - (sep / 2), input=\"husl\")\n    midpoint = dict(light=[(0.95, 0.95, 0.95, 1.0)], dark=[(0.133, 0.133, 0.133, 1.0)])[\n        center\n    ]\n    mid = midpoint * sep\n    pal = blend_palette(np.concatenate([neg, mid, pos]), n, as_cmap=as_cmap)\n    return pal\n</code></pre>"},{"location":"reference/nelpy/plotting/palettes/#nelpy.plotting.palettes.light_palette","title":"<code>light_palette(color, n_colors=6, reverse=False, as_cmap=False, input='rgb')</code>","text":"<p>Make a sequential palette that blends from light to <code>color</code>.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>base color for high values</code> <p>Hex code, html color name, or tuple in <code>input</code> space.</p> required <code>n_colors</code> <code>int</code> <p>Number of colors in the palette.</p> <code>6</code> <code>reverse</code> <code>bool</code> <p>If True, reverse the direction of the blend.</p> <code>False</code> <code>as_cmap</code> <code>bool</code> <p>If True, return as a matplotlib colormap instead of list.</p> <code>False</code> <code>input</code> <code>(rgb, hls, husl, xkcd)</code> <p>Color space to interpret the input color. The first three options apply to tuple inputs and the latter applies to string inputs.</p> <code>'rgb'</code> <p>Returns:</p> Name Type Description <code>palette</code> <code>_ColorPalette or matplotlib colormap</code> <p>List-like object of colors as RGB tuples, or colormap object that can map continuous values to colors, depending on the value of the <code>as_cmap</code> parameter.</p> Source code in <code>nelpy/plotting/palettes.py</code> <pre><code>def light_palette(color, n_colors=6, reverse=False, as_cmap=False, input=\"rgb\"):\n    \"\"\"\n    Make a sequential palette that blends from light to ``color``.\n\n    Parameters\n    ----------\n    color : base color for high values\n        Hex code, html color name, or tuple in ``input`` space.\n    n_colors : int, optional\n        Number of colors in the palette.\n    reverse : bool, optional\n        If True, reverse the direction of the blend.\n    as_cmap : bool, optional\n        If True, return as a matplotlib colormap instead of list.\n    input : {'rgb', 'hls', 'husl', 'xkcd'}\n        Color space to interpret the input color. The first three options\n        apply to tuple inputs and the latter applies to string inputs.\n\n    Returns\n    -------\n    palette : _ColorPalette or matplotlib colormap\n        List-like object of colors as RGB tuples, or colormap object that\n        can map continuous values to colors, depending on the value of the\n        ``as_cmap`` parameter.\n    \"\"\"\n    color = _color_to_rgb(color, input)\n    light = set_hls_values(color, l=0.95)\n    colors = [color, light] if reverse else [light, color]\n    return blend_palette(colors, n_colors, as_cmap)\n</code></pre>"},{"location":"reference/nelpy/plotting/palettes/#nelpy.plotting.palettes.mpl_palette","title":"<code>mpl_palette(name, n_colors=6)</code>","text":"<p>Return discrete colors from a matplotlib palette.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the palette. This should be a named matplotlib colormap.</p> required <code>n_colors</code> <code>int</code> <p>Number of discrete colors in the palette.</p> <code>6</code> <p>Returns:</p> Name Type Description <code>palette</code> <code>_ColorPalette</code> <p>List-like object of colors as RGB tuples.</p> Notes <p>This handles the qualitative colorbrewer palettes properly, although if you ask for more colors than a particular qualitative palette can provide you will get fewer than you are expecting. In contrast, asking for qualitative color brewer palettes using :func:<code>color_palette</code> will return the expected number of colors, but they will cycle.</p> <p>If you are using the IPython notebook, you can also use the function :func:<code>choose_colorbrewer_palette</code> to interactively select palettes.</p> <p>Examples:</p> <p>Create a qualitative colorbrewer palette with 8 colors:</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; import seaborn as sns\n...\n... sns.set()\n&gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"Set2\", 8))\n</code></pre> <p>Create a sequential colorbrewer palette:</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"Blues\"))\n</code></pre> <p>Create a diverging palette:</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"seismic\", 8))\n</code></pre> <p>Create a \"dark\" sequential palette:</p> <p>.. plot::     :context: close-figs</p> <pre><code>&gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"GnBu_d\"))\n</code></pre> Source code in <code>nelpy/plotting/palettes.py</code> <pre><code>def mpl_palette(name, n_colors=6):\n    \"\"\"\n    Return discrete colors from a matplotlib palette.\n\n    Parameters\n    ----------\n    name : str\n        Name of the palette. This should be a named matplotlib colormap.\n    n_colors : int\n        Number of discrete colors in the palette.\n\n    Returns\n    -------\n    palette : _ColorPalette\n        List-like object of colors as RGB tuples.\n\n    Notes\n    -----\n    This handles the qualitative colorbrewer palettes properly, although if you ask for more colors than a particular qualitative palette can provide you will get fewer than you are expecting. In contrast, asking for qualitative color brewer palettes using :func:`color_palette` will return the expected number of colors, but they will cycle.\n\n    If you are using the IPython notebook, you can also use the function :func:`choose_colorbrewer_palette` to interactively select palettes.\n\n    Examples\n    --------\n    Create a qualitative colorbrewer palette with 8 colors:\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; import seaborn as sns\n        ...\n        ... sns.set()\n        &gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"Set2\", 8))\n\n    Create a sequential colorbrewer palette:\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"Blues\"))\n\n    Create a diverging palette:\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"seismic\", 8))\n\n    Create a \"dark\" sequential palette:\n\n    .. plot::\n        :context: close-figs\n\n        &gt;&gt;&gt; sns.palplot(sns.mpl_palette(\"GnBu_d\"))\n    \"\"\"\n    brewer_qual_pals = {\n        \"Accent\": 8,\n        \"Dark2\": 8,\n        \"Paired\": 12,\n        \"Pastel1\": 9,\n        \"Pastel2\": 8,\n        \"Set1\": 9,\n        \"Set2\": 8,\n        \"Set3\": 12,\n    }\n\n    if name.endswith(\"_d\"):\n        pal = [\"#333333\"]\n        pal.extend(color_palette(name.replace(\"_d\", \"_r\"), 2))\n        cmap = blend_palette(pal, n_colors, as_cmap=True)\n    else:\n        cmap = getattr(mpl.cm, name)\n    if name in brewer_qual_pals:\n        bins = np.linspace(0, 1, brewer_qual_pals[name])[:n_colors]\n    else:\n        bins = np.linspace(0, 1, n_colors + 2)[1:-1]\n    palette = list(map(tuple, cmap(bins)[:, :3]))\n\n    return _ColorPalette(palette)\n</code></pre>"},{"location":"reference/nelpy/plotting/palettes/#nelpy.plotting.palettes.set_hls_values","title":"<code>set_hls_values(color, h=None, l=None, s=None)</code>","text":"<p>Independently manipulate the h, l, or s channels of a color.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>matplotlib color</code> <p>hex, rgb-tuple, or html color name</p> required <code>h</code> <code>float or None</code> <p>New values for each channel in hls space (between 0 and 1).</p> <code>None</code> <code>l</code> <code>float or None</code> <p>New values for each channel in hls space (between 0 and 1).</p> <code>None</code> <code>s</code> <code>float or None</code> <p>New values for each channel in hls space (between 0 and 1).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>new_color</code> <code>tuple</code> <p>New color code in RGB tuple representation.</p> Source code in <code>nelpy/plotting/palettes.py</code> <pre><code>def set_hls_values(color, h=None, l=None, s=None):  # noqa\n    \"\"\"\n    Independently manipulate the h, l, or s channels of a color.\n\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    h, l, s : float or None\n        New values for each channel in hls space (between 0 and 1).\n\n    Returns\n    -------\n    new_color : tuple\n        New color code in RGB tuple representation.\n    \"\"\"\n    # Get an RGB tuple representation\n    rgb = to_rgb(color)\n    vals = list(colorsys.rgb_to_hls(*rgb))\n    for i, val in enumerate([h, l, s]):\n        if val is not None:\n            vals[i] = val\n\n    rgb = colorsys.hls_to_rgb(*vals)\n    return rgb\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/","title":"nelpy.plotting.rcmod","text":"<p>Functions that alter the matplotlib rc dictionary on the fly. Most of these are Copyright (c) 2012-2016, Michael L. Waskom</p>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.axes_style","title":"<code>axes_style(style=None, rc=None)</code>","text":"<p>Return a parameter dict for the aesthetic style of the plots.</p> <p>This affects things like the color of the axes, whether a grid is enabled by default, and other aesthetic elements.</p> <p>This function returns an object that can be used in a <code>with</code> statement to temporarily change the style parameters.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn style dictionaries. This only updates parameters that are considered part of the style definition.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>style_object</code> <code>_AxesStyle</code> <p>An object that can be used as a context manager to temporarily set style.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; st = axes_style(\"whitegrid\")\n&gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; with axes_style(\"white\"):\n...     f, ax = plt.subplots()\n...     ax.plot([0, 1], [0, 1])\n</code></pre> See Also <p>set_style : set the matplotlib parameters for a seaborn theme plotting_context : return a parameter dict to scale plot elements color_palette : define the color palette for a plot</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def axes_style(style=None, rc=None):\n    \"\"\"\n    Return a parameter dict for the aesthetic style of the plots.\n\n    This affects things like the color of the axes, whether a grid is\n    enabled by default, and other aesthetic elements.\n\n    This function returns an object that can be used in a ``with`` statement\n    to temporarily change the style parameters.\n\n    Parameters\n    ----------\n    style : dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}\n        A dictionary of parameters or the name of a preconfigured set.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        style dictionaries. This only updates parameters that are\n        considered part of the style definition.\n\n    Returns\n    -------\n    style_object : _AxesStyle\n        An object that can be used as a context manager to temporarily set style.\n\n    Examples\n    --------\n    &gt;&gt;&gt; st = axes_style(\"whitegrid\")\n    &gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; with axes_style(\"white\"):\n    ...     f, ax = plt.subplots()\n    ...     ax.plot([0, 1], [0, 1])\n\n    See Also\n    --------\n    set_style : set the matplotlib parameters for a seaborn theme\n    plotting_context : return a parameter dict to scale plot elements\n    color_palette : define the color palette for a plot\n    \"\"\"\n    if style is None:\n        style_dict = {k: mpl.rcParams[k] for k in _style_keys}\n\n    elif isinstance(style, dict):\n        style_dict = style\n\n    else:\n        styles = [\"white\", \"dark\", \"whitegrid\", \"darkgrid\", \"ticks\"]\n        if style not in styles:\n            raise ValueError(\"style must be one of %s\" % \", \".join(styles))\n\n        # Define colors here\n        dark_gray = \".15\"\n        light_gray = \".8\"\n\n        # Common parameters\n        style_dict = {\n            \"figure.facecolor\": \"white\",\n            \"text.color\": dark_gray,\n            \"axes.labelcolor\": dark_gray,\n            \"legend.frameon\": False,\n            \"legend.numpoints\": 1,\n            \"legend.scatterpoints\": 1,\n            \"xtick.direction\": \"out\",\n            \"ytick.direction\": \"out\",\n            \"xtick.color\": dark_gray,\n            \"ytick.color\": dark_gray,\n            \"axes.axisbelow\": True,\n            \"lines.linewidth\": 1.75,\n            \"image.cmap\": \"Greys\",\n            \"font.family\": [\"sans-serif\"],\n            \"font.sans-serif\": [\n                \"DejaVu Sans\",\n                \"Arial\",\n                \"Liberation Sans\",\n                \"Bitstream Vera Sans\",\n                \"sans-serif\",\n            ],\n            \"grid.linestyle\": \"-\",\n            \"lines.solid_capstyle\": \"round\",\n        }\n\n        # Set grid on or off\n        if \"grid\" in style:\n            style_dict.update(\n                {\n                    \"axes.grid\": True,\n                }\n            )\n        else:\n            style_dict.update(\n                {\n                    \"axes.grid\": False,\n                }\n            )\n\n        # Set the color of the background, spines, and grids\n        if style.startswith(\"dark\"):\n            style_dict.update(\n                {\n                    \"axes.facecolor\": \"#EAEAF2\",\n                    \"axes.edgecolor\": \"white\",\n                    \"axes.linewidth\": 0,\n                    \"grid.color\": \"white\",\n                }\n            )\n\n        elif style == \"whitegrid\":\n            style_dict.update(\n                {\n                    \"axes.facecolor\": \"white\",\n                    \"axes.edgecolor\": light_gray,\n                    \"axes.linewidth\": 1,\n                    \"grid.color\": light_gray,\n                }\n            )\n\n        elif style in [\"white\", \"ticks\"]:\n            style_dict.update(\n                {\n                    \"axes.facecolor\": \"white\",\n                    \"axes.edgecolor\": dark_gray,\n                    \"axes.linewidth\": 1.25,\n                    \"grid.color\": light_gray,\n                }\n            )\n\n        # Show or hide the axes ticks\n        if style == \"ticks\":\n            style_dict.update(\n                {\n                    \"xtick.major.size\": 6,\n                    \"ytick.major.size\": 6,\n                    \"xtick.minor.size\": 3,\n                    \"ytick.minor.size\": 3,\n                }\n            )\n        else:\n            style_dict.update(\n                {\n                    \"xtick.major.size\": 0,\n                    \"ytick.major.size\": 0,\n                    \"xtick.minor.size\": 0,\n                    \"ytick.minor.size\": 0,\n                }\n            )\n\n    # Override these settings with the provided rc dictionary\n    if rc is not None:\n        rc = {k: v for k, v in rc.items() if k in _style_keys}\n        style_dict.update(rc)\n\n    # Wrap in an _AxesStyle object so this can be used in a with statement\n    style_object = _AxesStyle(style_dict)\n\n    return style_object\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.plotting_context","title":"<code>plotting_context(context=None, font_scale=1, rc=None)</code>","text":"<p>Return a parameter dict to scale elements of the figure.</p> <p>This affects things like the size of the labels, lines, and other elements of the plot, but not the overall style. The base context is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\", which are versions of the notebook parameters scaled by .8, 1.3, and 1.6, respectively.</p> <p>This function returns an object that can be used in a <code>with</code> statement to temporarily change the context parameters.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict, None, or one of {paper, notebook, talk, poster}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>font_scale</code> <code>float</code> <p>Separate scaling factor to independently scale the size of the font elements.</p> <code>1</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn context dictionaries. This only updates parameters that are considered part of the context definition.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>context_object</code> <code>_PlottingContext</code> <p>An object that can be used as a context manager to temporarily set context.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c = plotting_context(\"poster\")\n&gt;&gt;&gt; c = plotting_context(\"notebook\", font_scale=1.5)\n&gt;&gt;&gt; c = plotting_context(\"talk\", rc={\"lines.linewidth\": 2})\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; with plotting_context(\"paper\"):\n...     f, ax = plt.subplots()\n...     ax.plot([0, 1], [0, 1])\n</code></pre> See Also <p>set_context : set the matplotlib parameters to scale plot elements axes_style : return a dict of parameters defining a figure style color_palette : define the color palette for a plot</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def plotting_context(context=None, font_scale=1, rc=None):\n    \"\"\"\n    Return a parameter dict to scale elements of the figure.\n\n    This affects things like the size of the labels, lines, and other\n    elements of the plot, but not the overall style. The base context\n    is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\",\n    which are versions of the notebook parameters scaled by .8, 1.3, and 1.6,\n    respectively.\n\n    This function returns an object that can be used in a ``with`` statement\n    to temporarily change the context parameters.\n\n    Parameters\n    ----------\n    context : dict, None, or one of {paper, notebook, talk, poster}\n        A dictionary of parameters or the name of a preconfigured set.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        context dictionaries. This only updates parameters that are\n        considered part of the context definition.\n\n    Returns\n    -------\n    context_object : _PlottingContext\n        An object that can be used as a context manager to temporarily set context.\n\n    Examples\n    --------\n    &gt;&gt;&gt; c = plotting_context(\"poster\")\n    &gt;&gt;&gt; c = plotting_context(\"notebook\", font_scale=1.5)\n    &gt;&gt;&gt; c = plotting_context(\"talk\", rc={\"lines.linewidth\": 2})\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; with plotting_context(\"paper\"):\n    ...     f, ax = plt.subplots()\n    ...     ax.plot([0, 1], [0, 1])\n\n    See Also\n    --------\n    set_context : set the matplotlib parameters to scale plot elements\n    axes_style : return a dict of parameters defining a figure style\n    color_palette : define the color palette for a plot\n    \"\"\"\n    if context is None:\n        context_dict = {k: mpl.rcParams[k] for k in _context_keys}\n\n    elif isinstance(context, dict):\n        context_dict = context\n\n    else:\n        contexts = [\"paper\", \"notebook\", \"talk\", \"poster\"]\n        if context not in contexts:\n            raise ValueError(\"context must be in %s\" % \", \".join(contexts))\n\n        # Set up dictionary of default parameters\n        base_context = {\n            \"figure.figsize\": np.array([8, 5.5]),\n            \"font.size\": 12,\n            \"axes.labelsize\": 11,\n            \"axes.titlesize\": 12,\n            \"xtick.labelsize\": 10,\n            \"ytick.labelsize\": 10,\n            \"legend.fontsize\": 10,\n            \"grid.linewidth\": 1,\n            \"lines.linewidth\": 1.75,\n            \"patch.linewidth\": 0.3,\n            \"lines.markersize\": 7,\n            \"lines.markeredgewidth\": 0,\n            \"xtick.major.width\": 1,\n            \"ytick.major.width\": 1,\n            \"xtick.minor.width\": 0.5,\n            \"ytick.minor.width\": 0.5,\n            \"xtick.major.pad\": 7,\n            \"ytick.major.pad\": 7,\n        }\n\n        # Scale all the parameters by the same factor depending on the context\n        scaling = dict(paper=0.8, notebook=1, talk=1.3, poster=1.6)[context]\n        context_dict = {k: v * scaling for k, v in base_context.items()}\n\n        # Now independently scale the fonts\n        font_keys = [\n            \"axes.labelsize\",\n            \"axes.titlesize\",\n            \"legend.fontsize\",\n            \"xtick.labelsize\",\n            \"ytick.labelsize\",\n            \"font.size\",\n        ]\n        font_dict = {k: context_dict[k] * font_scale for k in font_keys}\n        context_dict.update(font_dict)\n\n    # Implement hack workaround for matplotlib bug\n    # See https://github.com/mwaskom/seaborn/issues/344\n    # There is a bug in matplotlib 1.4.2 that makes points invisible when\n    # they don't have an edgewidth. It will supposedly be fixed in 1.4.3.\n    if mpl.__version__ == \"1.4.2\":\n        context_dict[\"lines.markeredgewidth\"] = 0.01\n\n    # Override these settings with the provided rc dictionary\n    if rc is not None:\n        rc = {k: v for k, v in rc.items() if k in _context_keys}\n        context_dict.update(rc)\n\n    # Wrap in a _PlottingContext object so this can be used in a with statement\n    context_object = _PlottingContext(context_dict)\n\n    return context_object\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.reset_defaults","title":"<code>reset_defaults()</code>","text":"<p>Restore all matplotlib RC params to default settings.</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def reset_defaults():\n    \"\"\"\n    Restore all matplotlib RC params to default settings.\n    \"\"\"\n    mpl.rcParams.update(mpl.rcParamsDefault)\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.reset_orig","title":"<code>reset_orig()</code>","text":"<p>Restore all matplotlib RC params to original settings (respects custom rc).</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def reset_orig():\n    \"\"\"\n    Restore all matplotlib RC params to original settings (respects custom rc).\n    \"\"\"\n    mpl.rcParams.update(_orig_rc_params)\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.set_context","title":"<code>set_context(context=None, font_scale=1, rc=None)</code>","text":"<p>Set the plotting context parameters.</p> <p>This affects things like the size of the labels, lines, and other elements of the plot, but not the overall style. The base context is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\", which are versions of the notebook parameters scaled by .8, 1.3, and 1.6, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict, None, or one of {paper, notebook, talk, poster}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>font_scale</code> <code>float</code> <p>Separate scaling factor to independently scale the size of the font elements.</p> <code>1</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn context dictionaries. This only updates parameters that are considered part of the context definition.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_context(\"paper\")\n&gt;&gt;&gt; set_context(\"talk\", font_scale=1.4)\n&gt;&gt;&gt; set_context(\"talk\", rc={\"lines.linewidth\": 2})\n</code></pre> See Also <p>plotting_context : return a dictionary of rc parameters, or use in                    a <code>with</code> statement to temporarily set the context. set_style : set the default parameters for figure style set_palette : set the default color palette for figures</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def set_context(context=None, font_scale=1, rc=None):\n    \"\"\"\n    Set the plotting context parameters.\n\n    This affects things like the size of the labels, lines, and other\n    elements of the plot, but not the overall style. The base context\n    is \"notebook\", and the other contexts are \"paper\", \"talk\", and \"poster\",\n    which are versions of the notebook parameters scaled by .8, 1.3, and 1.6,\n    respectively.\n\n    Parameters\n    ----------\n    context : dict, None, or one of {paper, notebook, talk, poster}\n        A dictionary of parameters or the name of a preconfigured set.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        context dictionaries. This only updates parameters that are\n        considered part of the context definition.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set_context(\"paper\")\n    &gt;&gt;&gt; set_context(\"talk\", font_scale=1.4)\n    &gt;&gt;&gt; set_context(\"talk\", rc={\"lines.linewidth\": 2})\n\n    See Also\n    --------\n    plotting_context : return a dictionary of rc parameters, or use in\n                       a ``with`` statement to temporarily set the context.\n    set_style : set the default parameters for figure style\n    set_palette : set the default color palette for figures\n    \"\"\"\n    context_object = plotting_context(context, font_scale, rc)\n    mpl.rcParams.update(context_object)\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.set_fontsize","title":"<code>set_fontsize(fontsize=14)</code>","text":"<p>Set the fontsize for most plot elements.</p> <p>Parameters:</p> Name Type Description Default <code>fontsize</code> <code>int or float</code> <p>The font size to set for most plot elements (default is 14).</p> <code>14</code> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def set_fontsize(fontsize=14):\n    \"\"\"\n    Set the fontsize for most plot elements.\n\n    Parameters\n    ----------\n    fontsize : int or float, optional\n        The font size to set for most plot elements (default is 14).\n    \"\"\"\n\n    rc = {\n        \"font.size\": fontsize,\n        \"axes.titlesize\": fontsize,\n        \"axes.labelsize\": fontsize,\n        \"xtick.labelsize\": fontsize,\n        \"ytick.labelsize\": fontsize,\n        \"legend.fontsize\": fontsize,\n    }\n\n    mpl.rcParams.update(rc)\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.set_palette","title":"<code>set_palette(palette, n_colors=None, desat=None)</code>","text":"<p>Set the matplotlib color cycle using a seaborn palette.</p> <p>Parameters:</p> Name Type Description Default <code>palette</code> <code>hls | husl | matplotlib colormap | seaborn color palette</code> <p>Palette definition. Should be something that :func:<code>color_palette</code> can process.</p> required <code>n_colors</code> <code>int</code> <p>Number of colors in the cycle. The default number of colors will depend on the format of <code>palette</code>, see the :func:<code>color_palette</code> documentation for more information.</p> <code>None</code> <code>desat</code> <code>float</code> <p>Proportion to desaturate each color by.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_palette(\"Reds\")\n&gt;&gt;&gt; set_palette(\"Set1\", 8, 0.75)\n</code></pre> See Also <p>color_palette : build a color palette or set the color cycle temporarily                 in a <code>with</code> statement. set_context : set parameters to scale plot elements set_style : set the default parameters for figure style</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def set_palette(palette, n_colors=None, desat=None):\n    \"\"\"\n    Set the matplotlib color cycle using a seaborn palette.\n\n    Parameters\n    ----------\n    palette : hls | husl | matplotlib colormap | seaborn color palette\n        Palette definition. Should be something that :func:`color_palette` can process.\n    n_colors : int, optional\n        Number of colors in the cycle. The default number of colors will depend\n        on the format of ``palette``, see the :func:`color_palette`\n        documentation for more information.\n    desat : float, optional\n        Proportion to desaturate each color by.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set_palette(\"Reds\")\n    &gt;&gt;&gt; set_palette(\"Set1\", 8, 0.75)\n\n    See Also\n    --------\n    color_palette : build a color palette or set the color cycle temporarily\n                    in a ``with`` statement.\n    set_context : set parameters to scale plot elements\n    set_style : set the default parameters for figure style\n    \"\"\"\n    colors = palettes.color_palette(palette, n_colors, desat)\n    if mpl_ge_150:\n        from cycler import cycler\n\n        cyl = cycler(\"color\", colors)\n        mpl.rcParams[\"axes.prop_cycle\"] = cyl\n    else:\n        mpl.rcParams[\"axes.color_cycle\"] = list(colors)\n    mpl.rcParams[\"patch.facecolor\"] = colors[0]\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.set_style","title":"<code>set_style(style=None, rc=None)</code>","text":"<p>Set the aesthetic style of the plots.</p> <p>This affects things like the color of the axes, whether a grid is enabled by default, and other aesthetic elements.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}</code> <p>A dictionary of parameters or the name of a preconfigured set.</p> <code>None</code> <code>rc</code> <code>dict</code> <p>Parameter mappings to override the values in the preset seaborn style dictionaries. This only updates parameters that are considered part of the style definition.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_style(\"whitegrid\")\n&gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n</code></pre> See Also <p>axes_style : return a dict of parameters or use in a <code>with</code> statement              to temporarily set the style. set_context : set parameters to scale plot elements set_palette : set the default color palette for figures</p> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def set_style(style=None, rc=None):\n    \"\"\"\n    Set the aesthetic style of the plots.\n\n    This affects things like the color of the axes, whether a grid is\n    enabled by default, and other aesthetic elements.\n\n    Parameters\n    ----------\n    style : dict, None, or one of {darkgrid, whitegrid, dark, white, ticks}\n        A dictionary of parameters or the name of a preconfigured set.\n    rc : dict, optional\n        Parameter mappings to override the values in the preset seaborn\n        style dictionaries. This only updates parameters that are\n        considered part of the style definition.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set_style(\"whitegrid\")\n    &gt;&gt;&gt; set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n\n    See Also\n    --------\n    axes_style : return a dict of parameters or use in a ``with`` statement\n                 to temporarily set the style.\n    set_context : set parameters to scale plot elements\n    set_palette : set the default color palette for figures\n    \"\"\"\n    style_object = axes_style(style, rc)\n    mpl.rcParams.update(style_object)\n</code></pre>"},{"location":"reference/nelpy/plotting/rcmod/#nelpy.plotting.rcmod.setup","title":"<code>setup(context='notebook', style='ticks', palette='sweet', font='sans-serif', font_scale=1, rc=None)</code>","text":"<p>Set aesthetic figure parameters for matplotlib plots.</p> <p>Each set of parameters can be set directly or temporarily. See the referenced functions below for more information.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str or dict</code> <p>Plotting context parameters, see :func:<code>plotting_context</code>.</p> <code>'notebook'</code> <code>style</code> <code>str or dict</code> <p>Axes style parameters, see :func:<code>axes_style</code>.</p> <code>'ticks'</code> <code>palette</code> <code>str or sequence</code> <p>Color palette, see :func:<code>color_palette</code>.</p> <code>'sweet'</code> <code>font</code> <code>str</code> <p>Font family, see matplotlib font manager.</p> <code>'sans-serif'</code> <code>font_scale</code> <code>float</code> <p>Separate scaling factor to independently scale the size of the font elements.</p> <code>1</code> <code>rc</code> <code>dict or None</code> <p>Dictionary of rc parameter mappings to override the above.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; setup(\n...     context=\"talk\",\n...     style=\"whitegrid\",\n...     palette=\"muted\",\n...     font=\"Arial\",\n...     font_scale=1.2,\n... )\n</code></pre> Source code in <code>nelpy/plotting/rcmod.py</code> <pre><code>def setup(\n    context=\"notebook\",\n    style=\"ticks\",\n    palette=\"sweet\",\n    font=\"sans-serif\",\n    font_scale=1,\n    rc=None,\n):\n    \"\"\"\n    Set aesthetic figure parameters for matplotlib plots.\n\n    Each set of parameters can be set directly or temporarily. See the\n    referenced functions below for more information.\n\n    Parameters\n    ----------\n    context : str or dict, optional\n        Plotting context parameters, see :func:`plotting_context`.\n    style : str or dict, optional\n        Axes style parameters, see :func:`axes_style`.\n    palette : str or sequence, optional\n        Color palette, see :func:`color_palette`.\n    font : str, optional\n        Font family, see matplotlib font manager.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    rc : dict or None, optional\n        Dictionary of rc parameter mappings to override the above.\n\n    Examples\n    --------\n    &gt;&gt;&gt; setup(\n    ...     context=\"talk\",\n    ...     style=\"whitegrid\",\n    ...     palette=\"muted\",\n    ...     font=\"Arial\",\n    ...     font_scale=1.2,\n    ... )\n    \"\"\"\n    set_context(context, font_scale)\n    set_style(style, rc={\"font.family\": font})\n    set_palette(palette=palette)\n    if rc is not None:\n        mpl.rcParams.update(rc)\n</code></pre>"},{"location":"reference/nelpy/plotting/scalebar/","title":"nelpy.plotting.scalebar","text":""},{"location":"reference/nelpy/plotting/scalebar/#nelpy.plotting.scalebar.AnchoredScaleBar","title":"<code>AnchoredScaleBar</code>","text":"<p>               Bases: <code>AnchoredOffsetbox</code></p> Source code in <code>nelpy/plotting/scalebar.py</code> <pre><code>class AnchoredScaleBar(AnchoredOffsetbox):\n    def __init__(\n        self,\n        transform,\n        *,\n        sizex=0,\n        sizey=0,\n        labelx=None,\n        labely=None,\n        loc=4,\n        pad=0.5,\n        borderpad=0.1,\n        sep=2,\n        prop=None,\n        ec=\"k\",\n        fc=\"k\",\n        fontsize=None,\n        lw=1.5,\n        capstyle=\"projecting\",\n        xfirst=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Create an anchored scale bar for matplotlib axes.\n\n        Parameters\n        ----------\n        transform : matplotlib transform\n            The coordinate frame (typically axes.transData).\n        sizex : float, optional\n            Width of the x bar, in data units. 0 to omit. Default is 0.\n        sizey : float, optional\n            Height of the y bar, in data units. 0 to omit. Default is 0.\n        labelx : str, optional\n            Label for the x bar. None to omit.\n        labely : str, optional\n            Label for the y bar. None to omit.\n        loc : int, optional\n            Location in containing axes (see matplotlib legend locations). Default is 4 (lower right).\n        pad : float, optional\n            Padding, in fraction of the legend font size. Default is 0.5.\n        borderpad : float, optional\n            Border padding, in fraction of the legend font size. Default is 0.1.\n        sep : float, optional\n            Separation between labels and bars in points. Default is 2.\n        prop : font properties, optional\n            Font properties for the labels.\n        ec : color, optional\n            Edge color of the scalebar. Default is 'k'.\n        fc : color, optional\n            Font color / face color of labels. Default is 'k'.\n        fontsize : float, optional\n            Font size of labels. If None, uses matplotlib default.\n        lw : float, optional\n            Line width of the scalebar. Default is 1.5.\n        capstyle : {'round', 'butt', 'projecting'}, optional\n            Cap style of bars. Default is 'projecting'.\n        xfirst : bool, optional\n            If True, draw x bar and label first. Default is True.\n        **kwargs : dict\n            Additional arguments passed to base constructor.\n\n        Notes\n        -----\n        Adapted from https://gist.github.com/dmeliza/3251476\n\n        Examples\n        --------\n        &gt;&gt;&gt; from nelpy.plotting.scalebar import AnchoredScaleBar\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; fig, ax = plt.subplots()\n        &gt;&gt;&gt; scalebar = AnchoredScaleBar(ax.transData, sizex=1, labelx=\"1 s\")\n        &gt;&gt;&gt; ax.add_artist(scalebar)\n        \"\"\"\n        import matplotlib.patches as mpatches\n        from matplotlib.offsetbox import AuxTransformBox, HPacker, TextArea, VPacker\n\n        if fontsize is None:\n            fontsize = mpl.rcParams[\"font.size\"]\n\n        bars = AuxTransformBox(transform)\n\n        if sizex and sizey:  # both horizontal and vertical scalebar\n            # hacky fix for possible misalignment errors that may occur\n            #  on small figures\n            if ec is None:\n                lw = 0\n            endpt = (sizex, 0)\n            art = mpatches.FancyArrowPatch(\n                (0, 0),\n                endpt,\n                color=ec,\n                linewidth=lw,\n                capstyle=capstyle,\n                arrowstyle=mpatches.ArrowStyle.BarAB(widthA=0, widthB=lw * 2),\n            )\n            barsx = bars\n            barsx.add_artist(art)\n            endpt = (0, sizey)\n            art = mpatches.FancyArrowPatch(\n                (0, 0),\n                endpt,\n                color=ec,\n                linewidth=lw,\n                capstyle=capstyle,\n                arrowstyle=mpatches.ArrowStyle.BarAB(widthA=0, widthB=lw * 2),\n            )\n            barsy = bars\n            barsy.add_artist(art)\n        else:\n            if sizex:\n                endpt = (sizex, 0)\n                art = mpatches.FancyArrowPatch(\n                    (0, 0),\n                    endpt,\n                    color=ec,\n                    linewidth=lw,\n                    arrowstyle=mpatches.ArrowStyle.BarAB(widthA=lw * 2, widthB=lw * 2),\n                )\n                bars.add_artist(art)\n\n            if sizey:\n                endpt = (0, sizey)\n                art = mpatches.FancyArrowPatch(\n                    (0, 0),\n                    endpt,\n                    color=ec,\n                    linewidth=lw,\n                    arrowstyle=mpatches.ArrowStyle.BarAB(widthA=lw * 2, widthB=lw * 2),\n                )\n                bars.add_artist(art)\n\n        if xfirst:\n            if sizex and labelx:\n                bars = VPacker(\n                    children=[\n                        bars,\n                        TextArea(\n                            labelx,\n                            textprops=dict(color=fc, size=fontsize),\n                        ),\n                    ],\n                    align=\"center\",\n                    pad=pad,\n                    sep=sep,\n                )\n            if sizey and labely:\n                bars = HPacker(\n                    children=[\n                        TextArea(labely, textprops=dict(color=fc, size=fontsize)),\n                        bars,\n                    ],\n                    align=\"center\",\n                    pad=pad,\n                    sep=sep,\n                )\n        else:\n            if sizey and labely:\n                bars = HPacker(\n                    children=[\n                        TextArea(labely, textprops=dict(color=fc, size=fontsize)),\n                        bars,\n                    ],\n                    align=\"center\",\n                    pad=pad,\n                    sep=sep,\n                )\n            if sizex and labelx:\n                bars = VPacker(\n                    children=[\n                        bars,\n                        TextArea(\n                            labelx,\n                            textprops=dict(color=fc, size=fontsize),\n                        ),\n                    ],\n                    align=\"center\",\n                    pad=pad,\n                    sep=sep,\n                )\n\n        AnchoredOffsetbox.__init__(\n            self,\n            loc,\n            pad=pad,\n            borderpad=borderpad,\n            child=bars,\n            prop=prop,\n            frameon=False,\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/nelpy/plotting/scalebar/#nelpy.plotting.scalebar.add_scalebar","title":"<code>add_scalebar(ax, *, matchx=False, matchy=False, sizex=None, sizey=None, labelx=None, labely=None, hidex=True, hidey=True, ec='k', **kwargs)</code>","text":"<p>Add scalebars to axes, matching the size to the ticks of the plot and optionally hiding the x and y axes.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axis to attach scalebars to.</p> required <code>matchx</code> <code>bool</code> <p>If True, set size of x scalebar to spacing between ticks. Default is False.</p> <code>False</code> <code>matchy</code> <code>bool</code> <p>If True, set size of y scalebar to spacing between ticks. Default is False.</p> <code>False</code> <code>sizex</code> <code>float</code> <p>Size of x scalebar. Used if matchx is False.</p> <code>None</code> <code>sizey</code> <code>float</code> <p>Size of y scalebar. Used if matchy is False.</p> <code>None</code> <code>labelx</code> <code>str</code> <p>Label for x scalebar.</p> <code>None</code> <code>labely</code> <code>str</code> <p>Label for y scalebar.</p> <code>None</code> <code>hidex</code> <code>bool</code> <p>If True, hide x-axis of parent. Default is True.</p> <code>True</code> <code>hidey</code> <code>bool</code> <p>If True, hide y-axis of parent. Default is True.</p> <code>True</code> <code>ec</code> <code>color</code> <p>Edge color of the scalebar. Default is 'k'.</p> <code>'k'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to AnchoredScaleBar.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis containing the scalebar object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; add_scalebar(ax, sizex=1, labelx=\"1 s\")\n</code></pre> Source code in <code>nelpy/plotting/scalebar.py</code> <pre><code>def add_scalebar(\n    ax,\n    *,\n    matchx=False,\n    matchy=False,\n    sizex=None,\n    sizey=None,\n    labelx=None,\n    labely=None,\n    hidex=True,\n    hidey=True,\n    ec=\"k\",\n    **kwargs,\n):\n    \"\"\"\n    Add scalebars to axes, matching the size to the ticks of the plot and optionally hiding the x and y axes.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        The axis to attach scalebars to.\n    matchx : bool, optional\n        If True, set size of x scalebar to spacing between ticks. Default is False.\n    matchy : bool, optional\n        If True, set size of y scalebar to spacing between ticks. Default is False.\n    sizex : float, optional\n        Size of x scalebar. Used if matchx is False.\n    sizey : float, optional\n        Size of y scalebar. Used if matchy is False.\n    labelx : str, optional\n        Label for x scalebar.\n    labely : str, optional\n        Label for y scalebar.\n    hidex : bool, optional\n        If True, hide x-axis of parent. Default is True.\n    hidey : bool, optional\n        If True, hide y-axis of parent. Default is True.\n    ec : color, optional\n        Edge color of the scalebar. Default is 'k'.\n    **kwargs : dict\n        Additional arguments passed to AnchoredScaleBar.\n\n    Returns\n    -------\n    ax : matplotlib.axes.Axes\n        The axis containing the scalebar object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; add_scalebar(ax, sizex=1, labelx=\"1 s\")\n    \"\"\"\n\n    # determine which type op scalebar to plot:\n    # [(horizontal, vertical, both), (matchx, matchy), (labelx, labely)]\n    #\n    # matchx AND sizex ==&gt; error\n    # matchy AND sizey ==&gt; error\n    #\n    # matchx == True ==&gt; determine sizex\n    # matchy == True ==&gt; determine sizey\n    #\n    # if sizex ==&gt; horizontal\n    # if sizey ==&gt; vertical\n    # if sizex and sizey ==&gt; both\n    #\n    # at this point we fully know which type the scalebar is\n    #\n    # labelx is None ==&gt; determine from size\n    # labely is None ==&gt; determine from size\n    #\n    # NOTE: to force label empty, use labelx = ' '\n    #\n\n    # TODO: add logic for inverted axes:\n    # yinverted = ax.yaxis_inverted()\n    # xinverted = ax.xaxis_inverted()\n\n    def f(axis):\n        tick_locations = axis.get_majorticklocs()\n        return len(tick_locations) &gt; 1 and (tick_locations[1] - tick_locations[0])\n\n    if matchx and sizex:\n        raise ValueError(\"matchx and sizex cannot both be specified\")\n    if matchy and sizey:\n        raise ValueError(\"matchy and sizey cannot both be specified\")\n\n    if matchx:\n        sizex = f(ax.xaxis)\n    if matchy:\n        sizey = f(ax.yaxis)\n\n    if not sizex and not sizey:\n        raise ValueError(\"sizex and sizey cannot both be zero\")\n\n    kwargs[\"sizex\"] = sizex\n    kwargs[\"sizey\"] = sizey\n\n    if sizex:\n        sbtype = \"horizontal\"\n        if labelx is None:\n            labelx = str(sizex)\n    if sizey:\n        sbtype = \"vertical\"\n        if labely is None:\n            labely = str(sizey)\n    if sizex and sizey:\n        sbtype = \"both\"\n\n    kwargs[\"labelx\"] = labelx\n    kwargs[\"labely\"] = labely\n    kwargs[\"ec\"] = ec\n\n    if sbtype == \"both\":\n        # draw horizontal component:\n        kwargs[\"labely\"] = \" \"  # necessary to correct center alignment\n        kwargs[\"ec\"] = None  # necessary to correct possible artifact\n        sbx = AnchoredScaleBar(ax.transData, xfirst=True, **kwargs)\n\n        # draw vertical component:\n        kwargs[\"ec\"] = ec\n        kwargs[\"labelx\"] = \" \"\n        kwargs[\"labely\"] = labely\n        sby = AnchoredScaleBar(ax.transData, xfirst=False, **kwargs)\n        ax.add_artist(sbx)\n        ax.add_artist(sby)\n    else:\n        sb = AnchoredScaleBar(ax.transData, **kwargs)\n        ax.add_artist(sb)\n\n    if hidex:\n        ax.xaxis.set_visible(False)\n    if hidey:\n        ax.yaxis.set_visible(False)\n\n    return ax\n</code></pre>"},{"location":"reference/nelpy/plotting/scalebar/#nelpy.plotting.scalebar.add_simple_scalebar","title":"<code>add_simple_scalebar(text, ax=None, xy=None, length=None, orientation='v', rotation_text=None, xytext=None, **kwargs)</code>","text":"<p>Add a simple horizontal or vertical scalebar with a label to an axis.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The label for the scalebar.</p> required <code>ax</code> <code>Axes</code> <p>Axis to add the scalebar to. If None, uses current axis.</p> <code>None</code> <code>xy</code> <code>tuple of float</code> <p>Starting (x, y) position for the scalebar.</p> <code>None</code> <code>length</code> <code>float</code> <p>Length of the scalebar. Default is 10.</p> <code>None</code> <code>orientation</code> <code>(v, h, vert, horz)</code> <p>Orientation of the scalebar. 'v' or 'vert' for vertical, 'h' or 'horz' for horizontal. Default is 'v'.</p> <code>'v'</code> <code>rotation_text</code> <code>int or str</code> <p>Rotation of the label text. Default is 0.</p> <code>None</code> <code>xytext</code> <code>tuple of float</code> <p>Position for the label text. If None, automatically determined.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to matplotlib's annotate.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; add_simple_scalebar(\"10 s\", ax=ax, xy=(0, 0), length=10, orientation=\"h\")\n</code></pre> Source code in <code>nelpy/plotting/scalebar.py</code> <pre><code>def add_simple_scalebar(\n    text,\n    ax=None,\n    xy=None,\n    length=None,\n    orientation=\"v\",\n    rotation_text=None,\n    xytext=None,\n    **kwargs,\n):\n    \"\"\"\n    Add a simple horizontal or vertical scalebar with a label to an axis.\n\n    Parameters\n    ----------\n    text : str\n        The label for the scalebar.\n    ax : matplotlib.axes.Axes, optional\n        Axis to add the scalebar to. If None, uses current axis.\n    xy : tuple of float\n        Starting (x, y) position for the scalebar.\n    length : float, optional\n        Length of the scalebar. Default is 10.\n    orientation : {'v', 'h', 'vert', 'horz'}, optional\n        Orientation of the scalebar. 'v' or 'vert' for vertical, 'h' or 'horz' for horizontal. Default is 'v'.\n    rotation_text : int or str, optional\n        Rotation of the label text. Default is 0.\n    xytext : tuple of float, optional\n        Position for the label text. If None, automatically determined.\n    **kwargs : dict\n        Additional keyword arguments passed to matplotlib's annotate.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; add_simple_scalebar(\"10 s\", ax=ax, xy=(0, 0), length=10, orientation=\"h\")\n    \"\"\"\n    if rotation_text is None:\n        rotation_text = 0\n    if rotation_text == \"vert\" or rotation_text == \"v\":\n        rotation_text = 90\n    if rotation_text == \"horz\" or rotation_text == \"h\":\n        rotation_text = 0\n    if orientation is None:\n        orientation = 0\n    if orientation == \"vert\" or orientation == \"v\":\n        orientation = 90\n    if orientation == \"horz\" or orientation == \"h\":\n        orientation = 0\n\n    if length is None:\n        length = 10\n\n    if ax is None:\n        ax = plt.gca()\n\n    #     if va is None:\n    #         if rotation_text == 90:\n    #             va = 'bottom'\n    #         else:\n    #             va = 'baseline'\n\n    if orientation == 0:\n        ax.hlines(xy[1], xy[0], xy[0] + length, lw=2, zorder=1000)\n    else:\n        ax.vlines(xy[0], xy[1], xy[1] + length, lw=2, zorder=1000)\n        xytext = (xy[0] + 3, xy[1] + length / 2)\n        ax.annotate(\n            text, xy=xytext, rotation=rotation_text, va=\"center\", zorder=1000, **kwargs\n        )\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/","title":"nelpy.plotting.utils","text":"<p>This file contains the nelpy plotting functions and utilities.</p> <p>Some functions Copyright (c) 2016, Etienne R. Ackermann Some functions are modified from Jessica B. Hamrick, Copyright (c) 2013 'get_color_cycle', 'set_palette', and 'desaturate' are Copyright (c) 2012-2016, Michael L. Waskom 'FigureManager' modified from Camille Scott, Copyright (C) 2015 camille.scott.w@gmail.com</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.FigureManager","title":"<code>FigureManager</code>","text":"<p>               Bases: <code>object</code></p> <p>Figure context manager for creating, displaying, and saving figures.</p> <p>See http://stackoverflow.com/questions/12594148/skipping-execution-of-with-block but I was unable to get a solution so far...</p> <p>See http://stackoverflow.com/questions/11195140/break-or-exit-out-of-with-statement for additional inspiration for making nested context managers...</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename without an extension. If an extension is present, AND if formats is empty, then the filename extension will be used.</p> <code>None</code> <code>save</code> <code>bool</code> <p>If True, figure will be saved to disk.</p> <code>False</code> <code>show</code> <code>bool</code> <p>If True, figure will be shown.</p> <code>False</code> <code>nrows</code> <code>int</code> <p>Number of subplot rows.</p> <code>1</code> <code>ncols</code> <code>int</code> <p>Number of subplot columns.</p> <code>1</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches (width, height).</p> <code>(8, 3)</code> <code>tight_layout</code> <code>bool</code> <p>If True, use tight layout.</p> <code>False</code> <code>formats</code> <code>list</code> <p>List of formats to export. Defaults to ['pdf', 'png']</p> <code>None</code> <code>dpi</code> <code>float</code> <p>Resolution of the figure in dots per inch (DPI).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print additional output to screen.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>If True, file will be overwritten.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to plt.figure().</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with FigureManager(filename=\"myfig\", save=True, show=False) as (fig, ax):\n...     ax.plot([1, 2, 3], [4, 5, 6])\n</code></pre> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>class FigureManager(object):\n    \"\"\"\n    Figure context manager for creating, displaying, and saving figures.\n\n    See http://stackoverflow.com/questions/12594148/skipping-execution-of-with-block\n    but I was unable to get a solution so far...\n\n    See http://stackoverflow.com/questions/11195140/break-or-exit-out-of-with-statement\n    for additional inspiration for making nested context managers...\n\n    Parameters\n    ----------\n    filename : str, optional\n        Filename without an extension. If an extension is present,\n        AND if formats is empty, then the filename extension will be used.\n    save : bool, optional\n        If True, figure will be saved to disk.\n    show : bool, optional\n        If True, figure will be shown.\n    nrows : int, optional\n        Number of subplot rows.\n    ncols : int, optional\n        Number of subplot columns.\n    figsize : tuple, optional\n        Figure size in inches (width, height).\n    tight_layout : bool, optional\n        If True, use tight layout.\n    formats : list, optional\n        List of formats to export. Defaults to ['pdf', 'png']\n    dpi : float, optional\n        Resolution of the figure in dots per inch (DPI).\n    verbose : bool, optional\n        If True, print additional output to screen.\n    overwrite : bool, optional\n        If True, file will be overwritten.\n    **kwargs : dict\n        Additional keyword arguments passed to plt.figure().\n\n    Examples\n    --------\n    &gt;&gt;&gt; with FigureManager(filename=\"myfig\", save=True, show=False) as (fig, ax):\n    ...     ax.plot([1, 2, 3], [4, 5, 6])\n    \"\"\"\n\n    class Break(Exception):\n        \"\"\"Exception to break out of the context manager block.\"\"\"\n\n        pass\n\n    def __init__(\n        self,\n        *,\n        filename=None,\n        save=False,\n        show=False,\n        nrows=1,\n        ncols=1,\n        figsize=(8, 3),\n        tight_layout=False,\n        formats=None,\n        dpi=None,\n        verbose=True,\n        overwrite=False,\n        **kwargs,\n    ):\n        self.nrows = nrows\n        self.ncols = ncols\n        self.figsize = figsize\n        self.tight_layout = tight_layout\n        self.dpi = dpi\n        self.kwargs = kwargs\n\n        self.filename = filename\n        self.show = show\n        self.save = save\n        self.formats = formats\n        self.dpi = dpi\n        self.verbose = verbose\n        self.overwrite = overwrite\n\n        if self.show or self.save:\n            self.skip = False\n        else:\n            self.skip = True\n\n    def __enter__(self):\n        \"\"\"\n        Enter the context manager, creating the figure and axes.\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            The created figure.\n        ax : matplotlib.axes.Axes or numpy.ndarray\n            The created axes (single or array, depending on nrows/ncols).\n        \"\"\"\n        if not self.skip:\n            self.fig = plt.figure(figsize=self.figsize, dpi=self.dpi, **self.kwargs)\n            self.fig.npl_gs = gridspec.GridSpec(nrows=self.nrows, ncols=self.ncols)\n\n            self.ax = np.array([self.fig.add_subplot(ss) for ss in self.fig.npl_gs])\n            # self.fig, self.ax = plt.subplots(nrows=self.nrows,\n            #                                  ncols=self.ncols,\n            #                                  figsize=self.figsize,\n            #                                  tight_layout=self.tight_layout,\n            #                                  dpi=self.dpi,\n            #                                  **self.kwargs)\n            if len(self.ax) == 1:\n                self.ax = self.ax[0]\n\n            if self.tight_layout:\n                self.fig.npl_gs.tight_layout(self.fig)\n\n            # gs1.tight_layout(fig, rect=[0, 0.03, 1, 0.95])\n            if self.fig != plt.gcf():\n                self.clear()\n                raise RuntimeError(\"Figure does not match active mpl figure\")\n            return self.fig, self.ax\n        return -1, -1\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"\n        Exit the context manager, saving and/or showing the figure if requested.\n\n        Parameters\n        ----------\n        exc_type : type\n            Exception type, if any.\n        exc_value : Exception\n            Exception value, if any.\n        traceback : traceback\n            Traceback object, if any.\n        \"\"\"\n        if self.skip:\n            return True\n        if not exc_type:\n            if self.save:\n                assert self.filename is not None, \"filename has to be specified!\"\n                savefig(\n                    name=self.filename,\n                    fig=self.fig,\n                    formats=self.formats,\n                    dpi=self.dpi,\n                    verbose=self.verbose,\n                    overwrite=self.overwrite,\n                )\n\n            if self.show:\n                plt.show(self.fig)\n            self.clear()\n        else:\n            self.clear()\n            return False\n\n    def clear(self):\n        \"\"\"\n        Close the figure and clean up references.\n        \"\"\"\n        plt.close(self.fig)\n        del self.ax\n        del self.fig\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.FigureManager.Break","title":"<code>Break</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception to break out of the context manager block.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>class Break(Exception):\n    \"\"\"Exception to break out of the context manager block.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.FigureManager.clear","title":"<code>clear()</code>","text":"<p>Close the figure and clean up references.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Close the figure and clean up references.\n    \"\"\"\n    plt.close(self.fig)\n    del self.ax\n    del self.fig\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.FixedOrderFormatter","title":"<code>FixedOrderFormatter</code>","text":"<p>               Bases: <code>ScalarFormatter</code></p> <p>Formats axis ticks using scientific notation with a constant order of magnitude.</p> <p>Parameters:</p> Name Type Description Default <code>order_of_mag</code> <code>int</code> <p>Order of magnitude for the exponent.</p> <code>0</code> <code>useOffset</code> <code>bool</code> <p>If True includes an offset. Default is True.</p> <code>None</code> <code>useMathText</code> <code>bool</code> <p>If True use 1x10^exp; otherwise use 1e-exp. Default is True.</p> <code>None</code> <p>Examples:</p> <p>Force the y-axis ticks to use 1e+2 as a base exponent:</p> <pre><code>&gt;&gt;&gt; ax.yaxis.set_major_formatter(npl.FixedOrderFormatter(+2))\n</code></pre> <p>Make the x-axis ticks formatted to 0 decimal places:</p> <pre><code>&gt;&gt;&gt; from matplotlib.ticker import FormatStrFormatter\n&gt;&gt;&gt; ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n</code></pre> <p>Turn off offset on x-axis:</p> <pre><code>&gt;&gt;&gt; ax.xaxis.get_major_formatter().set_useOffset(False)\n</code></pre> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>class FixedOrderFormatter(ScalarFormatter):\n    \"\"\"\n    Formats axis ticks using scientific notation with a constant order of magnitude.\n\n    Parameters\n    ----------\n    order_of_mag : int\n        Order of magnitude for the exponent.\n    useOffset : bool, optional\n        If True includes an offset. Default is True.\n    useMathText : bool, optional\n        If True use 1x10^exp; otherwise use 1e-exp. Default is True.\n\n    Examples\n    --------\n    Force the y-axis ticks to use 1e+2 as a base exponent:\n    &gt;&gt;&gt; ax.yaxis.set_major_formatter(npl.FixedOrderFormatter(+2))\n\n    Make the x-axis ticks formatted to 0 decimal places:\n    &gt;&gt;&gt; from matplotlib.ticker import FormatStrFormatter\n    &gt;&gt;&gt; ax.xaxis.set_major_formatter(FormatStrFormatter(\"%0.0f\"))\n\n    Turn off offset on x-axis:\n    &gt;&gt;&gt; ax.xaxis.get_major_formatter().set_useOffset(False)\n    \"\"\"\n\n    def __init__(self, order_of_mag=0, *, useOffset=None, useMathText=None):\n        # set parameter defaults:\n        if useOffset is None:\n            useOffset = True\n        if useMathText is None:\n            useMathText = True\n\n        self._order_of_mag = order_of_mag\n        ScalarFormatter.__init__(self, useOffset=useOffset, useMathText=useMathText)\n\n    def _set_orderOfMagnitude(self, range):\n        \"\"\"Override to prevent order_of_mag being reset elsewhere.\"\"\"\n        self.orderOfMagnitude = self._order_of_mag\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.ModestImage","title":"<code>ModestImage</code>","text":"<p>               Bases: <code>AxesImage</code></p> <p>Computationally modest image class.</p> <p>Customization of https://github.com/ChrisBeaumont/ModestImage to allow extent support.</p> <p>ModestImage is an extension of the Matplotlib AxesImage class better suited for the interactive display of larger images. Before drawing, ModestImage resamples the data array based on the screen resolution and view window. This has very little affect on the appearance of the image, but can substantially cut down on computation since calculations of unresolved or clipped pixels are skipped.</p> <p>The interface of ModestImage is the same as AxesImage. However, it does not currently support setting the 'extent' property. There may also be weird coordinate warping operations for images that I'm not aware of. Don't expect those to work either.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>class ModestImage(AxesImage):\n    \"\"\"Computationally modest image class.\n\n    Customization of https://github.com/ChrisBeaumont/ModestImage to allow\n    extent support.\n\n    ModestImage is an extension of the Matplotlib AxesImage class\n    better suited for the interactive display of larger images. Before\n    drawing, ModestImage resamples the data array based on the screen\n    resolution and view window. This has very little affect on the\n    appearance of the image, but can substantially cut down on\n    computation since calculations of unresolved or clipped pixels\n    are skipped.\n\n    The interface of ModestImage is the same as AxesImage. However, it\n    does not currently support setting the 'extent' property. There\n    may also be weird coordinate warping operations for images that\n    I'm not aware of. Don't expect those to work either.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._full_res = None\n        self._sx, self._sy = None, None\n        self._bounds = (None, None, None, None)\n        self._origExtent = None\n        super(ModestImage, self).__init__(*args, **kwargs)\n        if \"extent\" in kwargs and kwargs[\"extent\"] is not None:\n            self.set_extent(kwargs[\"extent\"])\n\n    def set_extent(self, extent):\n        super(ModestImage, self).set_extent(extent)\n        if self._origExtent is None:\n            self._origExtent = self.get_extent()\n\n    def get_image_extent(self):\n        \"\"\"Returns the extent of the whole image.\n\n        get_extent returns the extent of the drawn area and not of the full\n        image.\n\n        :return: Bounds of the image (x0, x1, y0, y1).\n        :rtype: Tuple of 4 floats.\n        \"\"\"\n        if self._origExtent is not None:\n            return self._origExtent\n        else:\n            return self.get_extent()\n\n    def set_data(self, A):\n        \"\"\"\n        Set the image array\n\n        ACCEPTS: numpy/PIL Image A\n        \"\"\"\n\n        self._full_res = A\n        self._A = A\n\n        if self._A.dtype != np.uint8 and not np.can_cast(self._A.dtype, np.float):\n            raise TypeError(\"Image data can not convert to float\")\n\n        if self._A.ndim not in (2, 3) or (\n            self._A.ndim == 3 and self._A.shape[-1] not in (3, 4)\n        ):\n            raise TypeError(\"Invalid dimensions for image data\")\n\n        self._imcache = None\n        self._rgbacache = None\n        self._oldxslice = None\n        self._oldyslice = None\n        self._sx, self._sy = None, None\n\n    def get_array(self):\n        \"\"\"Override to return the full-resolution array\"\"\"\n        return self._full_res\n\n    def _scale_to_res(self):\n        \"\"\"Change self._A and _extent to render an image whose\n        resolution is matched to the eventual rendering.\"\"\"\n        # extent has to be set BEFORE set_data\n        if self._origExtent is None:\n            if self.origin == \"upper\":\n                self._origExtent = (\n                    0,\n                    self._full_res.shape[1],\n                    self._full_res.shape[0],\n                    0,\n                )\n            else:\n                self._origExtent = (\n                    0,\n                    self._full_res.shape[1],\n                    0,\n                    self._full_res.shape[0],\n                )\n\n        if self.origin == \"upper\":\n            origXMin, origXMax, origYMax, origYMin = self._origExtent[0:4]\n        else:\n            origXMin, origXMax, origYMin, origYMax = self._origExtent[0:4]\n        ax = self.axes\n        ext = ax.transAxes.transform([1, 1]) - ax.transAxes.transform([0, 0])\n        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n        xlim = max(xlim[0], origXMin), min(xlim[1], origXMax)\n        if ylim[0] &gt; ylim[1]:\n            ylim = max(ylim[1], origYMin), min(ylim[0], origYMax)\n        else:\n            ylim = max(ylim[0], origYMin), min(ylim[1], origYMax)\n        # print(\"THOSE LIMITS ARE TO BE COMPARED WITH THE EXTENT\")\n        # print(\"IN ORDER TO KNOW WHAT IT IS LIMITING THE DISPLAY\")\n        # print(\"IF THE AXES OR THE EXTENT\")\n        dx, dy = xlim[1] - xlim[0], ylim[1] - ylim[0]\n\n        y0 = max(0, ylim[0] - 5)\n        y1 = min(self._full_res.shape[0], ylim[1] + 5)\n        x0 = max(0, xlim[0] - 5)\n        x1 = min(self._full_res.shape[1], xlim[1] + 5)\n        y0, y1, x0, x1 = [int(a) for a in [y0, y1, x0, x1]]\n\n        sy = int(max(1, min((y1 - y0) / 5.0, np.ceil(dy / ext[1]))))\n        sx = int(max(1, min((x1 - x0) / 5.0, np.ceil(dx / ext[0]))))\n\n        # have we already calculated what we need?\n        if (self._sx is not None) and (self._sy is not None):\n            if (\n                sx &gt;= self._sx\n                and sy &gt;= self._sy\n                and x0 &gt;= self._bounds[0]\n                and x1 &lt;= self._bounds[1]\n                and y0 &gt;= self._bounds[2]\n                and y1 &lt;= self._bounds[3]\n            ):\n                return\n\n        self._A = self._full_res[y0:y1:sy, x0:x1:sx]\n        self._A = cbook.safe_masked_invalid(self._A)\n        x1 = x0 + self._A.shape[1] * sx\n        y1 = y0 + self._A.shape[0] * sy\n\n        if self.origin == \"upper\":\n            self.set_extent([x0, x1, y1, y0])\n        else:\n            self.set_extent([x0, x1, y0, y1])\n        self._sx = sx\n        self._sy = sy\n        self._bounds = (x0, x1, y0, y1)\n        self.changed()\n\n    def draw(self, renderer, *args, **kwargs):\n        self._scale_to_res()\n        super(ModestImage, self).draw(renderer, *args, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.ModestImage.get_array","title":"<code>get_array()</code>","text":"<p>Override to return the full-resolution array</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def get_array(self):\n    \"\"\"Override to return the full-resolution array\"\"\"\n    return self._full_res\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.ModestImage.get_image_extent","title":"<code>get_image_extent()</code>","text":"<p>Returns the extent of the whole image.</p> <p>get_extent returns the extent of the drawn area and not of the full image.</p> <p>:return: Bounds of the image (x0, x1, y0, y1). :rtype: Tuple of 4 floats.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def get_image_extent(self):\n    \"\"\"Returns the extent of the whole image.\n\n    get_extent returns the extent of the drawn area and not of the full\n    image.\n\n    :return: Bounds of the image (x0, x1, y0, y1).\n    :rtype: Tuple of 4 floats.\n    \"\"\"\n    if self._origExtent is not None:\n        return self._origExtent\n    else:\n        return self.get_extent()\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.ModestImage.set_data","title":"<code>set_data(A)</code>","text":"<p>Set the image array</p> <p>ACCEPTS: numpy/PIL Image A</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def set_data(self, A):\n    \"\"\"\n    Set the image array\n\n    ACCEPTS: numpy/PIL Image A\n    \"\"\"\n\n    self._full_res = A\n    self._A = A\n\n    if self._A.dtype != np.uint8 and not np.can_cast(self._A.dtype, np.float):\n        raise TypeError(\"Image data can not convert to float\")\n\n    if self._A.ndim not in (2, 3) or (\n        self._A.ndim == 3 and self._A.shape[-1] not in (3, 4)\n    ):\n        raise TypeError(\"Invalid dimensions for image data\")\n\n    self._imcache = None\n    self._rgbacache = None\n    self._oldxslice = None\n    self._oldyslice = None\n    self._sx, self._sy = None, None\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.add_colorbar","title":"<code>add_colorbar(img, ax=None)</code>","text":"<p>Add a colorbar to the given axis for the provided image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>AxesImage</code> <p>The image to which the colorbar applies.</p> required <code>ax</code> <code>Axes</code> <p>The axis to which the colorbar will be added. If None, uses current axis.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>cb</code> <code>Colorbar</code> <p>The colorbar object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from nelpy.plotting.utils import add_colorbar\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; im = ax.imshow(np.random.rand(10, 10))\n&gt;&gt;&gt; cb = add_colorbar(im, ax=ax)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def add_colorbar(img, ax=None):\n    \"\"\"\n    Add a colorbar to the given axis for the provided image.\n\n    Parameters\n    ----------\n    img : matplotlib.image.AxesImage\n        The image to which the colorbar applies.\n    ax : matplotlib.axes.Axes, optional\n        The axis to which the colorbar will be added. If None, uses current axis.\n\n    Returns\n    -------\n    cb : matplotlib.colorbar.Colorbar\n        The colorbar object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from nelpy.plotting.utils import add_colorbar\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; im = ax.imshow(np.random.rand(10, 10))\n    &gt;&gt;&gt; cb = add_colorbar(im, ax=ax)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.15)\n    cb = plt.colorbar(img, cax=cax, orientation=\"vertical\")\n    # cb.set_label('probability', labelpad=-10)\n    # cb.set_ticks([0,1])\n    return cb\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.align_xlabels","title":"<code>align_xlabels(ycoord, *axes)</code>","text":"<p>Align the x-axis labels of multiple axes</p> <p>Parameters:</p> Name Type Description Default <code>ycoord</code> <code>float</code> <p>y-coordinate of the x-axis labels</p> required <code>*axes</code> <code>axis objects</code> <p>The matplotlib axis objects to format</p> <code>()</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def align_xlabels(ycoord, *axes):\n    \"\"\"Align the x-axis labels of multiple axes\n\n    Parameters\n    ----------\n    ycoord : float\n        y-coordinate of the x-axis labels\n    *axes : axis objects\n        The matplotlib axis objects to format\n\n    \"\"\"\n    set_xlabel_coords(ycoord, *axes)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.align_ylabels","title":"<code>align_ylabels(xcoord, *axes)</code>","text":"<p>Align the y-axis labels of multiple axes.</p> <p>Parameters:</p> Name Type Description Default <code>xcoord</code> <code>float</code> <p>x-coordinate of the y-axis labels</p> required <code>*axes</code> <code>axis objects</code> <p>The matplotlib axis objects to format</p> <code>()</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def align_ylabels(xcoord, *axes):\n    \"\"\"Align the y-axis labels of multiple axes.\n\n    Parameters\n    ----------\n    xcoord : float\n        x-coordinate of the y-axis labels\n    *axes : axis objects\n        The matplotlib axis objects to format\n\n    \"\"\"\n    set_ylabel_coords(xcoord, *axes)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.annotate","title":"<code>annotate(text, ax=None, xy=None, rotation=None, va=None, **kwargs)</code>","text":"<p>Annotate the given axis with text at a specified location and rotation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The annotation text.</p> required <code>ax</code> <code>Axes</code> <p>The axis to annotate. If None, uses current axis.</p> <code>None</code> <code>xy</code> <code>tuple</code> <p>The (x, y) location for the annotation. Defaults to (0.5, 0.5).</p> <code>None</code> <code>rotation</code> <code>float or str</code> <p>The rotation angle in degrees, or 'vert'/'v' for 90, 'horz'/'h' for 0.</p> <code>None</code> <code>va</code> <code>str</code> <p>Vertical alignment. If None, chosen based on rotation.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to ax.annotate().</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; annotate(\"Hello\", ax=ax, xy=(0.2, 0.8), rotation=45)\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def annotate(text, ax=None, xy=None, rotation=None, va=None, **kwargs):\n    \"\"\"\n    Annotate the given axis with text at a specified location and rotation.\n\n    Parameters\n    ----------\n    text : str\n        The annotation text.\n    ax : matplotlib.axes.Axes, optional\n        The axis to annotate. If None, uses current axis.\n    xy : tuple, optional\n        The (x, y) location for the annotation. Defaults to (0.5, 0.5).\n    rotation : float or str, optional\n        The rotation angle in degrees, or 'vert'/'v' for 90, 'horz'/'h' for 0.\n    va : str, optional\n        Vertical alignment. If None, chosen based on rotation.\n    **kwargs : dict\n        Additional keyword arguments passed to ax.annotate().\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; annotate(\"Hello\", ax=ax, xy=(0.2, 0.8), rotation=45)\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n    if xy is None:\n        xy = (0.5, 0.5)\n    if rotation is None:\n        rotation = 0\n    if rotation == \"vert\" or rotation == \"v\":\n        rotation = 90\n    if rotation == \"horz\" or rotation == \"h\":\n        rotation = 0\n    if va is None:\n        if rotation == 90:\n            va = \"bottom\"\n        else:\n            va = \"baseline\"\n\n    ax.annotate(text, xy=xy, rotation=rotation, va=va, **kwargs)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.clear_bottom","title":"<code>clear_bottom(*axes)</code>","text":"<p>Remove the bottom edge of the axis bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear_bottom(*axes):\n    \"\"\"Remove the bottom edge of the axis bounding box.\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.spines[\"bottom\"].set_color(\"none\")\n        ax.xaxis.set_ticks_position(\"top\")\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.clear_left","title":"<code>clear_left(*axes)</code>","text":"<p>Remove the left edge of the axis bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear_left(*axes):\n    \"\"\"Remove the left edge of the axis bounding box.\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.spines[\"left\"].set_color(\"none\")\n        ax.yaxis.set_ticks_position(\"right\")\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.clear_left_right","title":"<code>clear_left_right(*axes)</code>","text":"<p>Remove the left and right edges of the axis bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear_left_right(*axes):\n    \"\"\"Remove the left and right edges of the axis bounding box.\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.spines[\"left\"].set_color(\"none\")\n        ax.spines[\"right\"].set_color(\"none\")\n        ax.yaxis.set_ticks([])\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.clear_right","title":"<code>clear_right(*axes)</code>","text":"<p>Remove the right edge of the axis bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear_right(*axes):\n    \"\"\"Remove the right edge of the axis bounding box.\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.spines[\"right\"].set_color(\"none\")\n        ax.yaxis.set_ticks_position(\"left\")\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.clear_top","title":"<code>clear_top(*axes)</code>","text":"<p>Remove the top edge of the axis bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear_top(*axes):\n    \"\"\"Remove the top edge of the axis bounding box.\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.spines[\"top\"].set_color(\"none\")\n        ax.xaxis.set_ticks_position(\"bottom\")\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.clear_top_bottom","title":"<code>clear_top_bottom(*axes)</code>","text":"<p>Remove the top and bottom edges of the axis bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def clear_top_bottom(*axes):\n    \"\"\"Remove the top and bottom edges of the axis bounding box.\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.spines[\"top\"].set_color(\"none\")\n        ax.spines[\"bottom\"].set_color(\"none\")\n        ax.xaxis.set_ticks([])\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.desaturate","title":"<code>desaturate(color, prop)</code>","text":"<p>Decrease the saturation channel of a color by some percent.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>matplotlib color</code> <p>hex, rgb-tuple, or html color name</p> required <code>prop</code> <code>float</code> <p>saturation channel of color will be multiplied by this value</p> required <p>Returns:</p> Name Type Description <code>new_color</code> <code>rgb tuple</code> <p>desaturated color code in RGB tuple representation</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def desaturate(color, prop):\n    \"\"\"Decrease the saturation channel of a color by some percent.\n    Parameters\n    ----------\n    color : matplotlib color\n        hex, rgb-tuple, or html color name\n    prop : float\n        saturation channel of color will be multiplied by this value\n    Returns\n    -------\n    new_color : rgb tuple\n        desaturated color code in RGB tuple representation\n    \"\"\"\n    # Check inputs\n    if not 0 &lt;= prop &lt;= 1:\n        raise ValueError(\"prop must be between 0 and 1\")\n\n    # Get rgb tuple rep\n    rgb = mplcolors.colorConverter.to_rgb(color)\n\n    # Convert to hls\n    h, lightness, s = colorsys.rgb_to_hls(*rgb)\n\n    # Desaturate the saturation channel\n    s *= prop\n\n    # Convert back to rgb\n    new_color = colorsys.hls_to_rgb(h, lightness, s)\n\n    return new_color\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.figure_grid","title":"<code>figure_grid(b=True, fig=None)</code>","text":"<p>Draw a figure grid over an entire figure to facilitate annotation placement.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>bool</code> <p>Whether to draw the grid (default True).</p> <code>True</code> <code>fig</code> <code>Figure</code> <p>The figure to draw on. If None, uses current figure.</p> <code>None</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def figure_grid(b=True, fig=None):\n    \"\"\"\n    Draw a figure grid over an entire figure to facilitate annotation placement.\n\n    Parameters\n    ----------\n    b : bool, optional\n        Whether to draw the grid (default True).\n    fig : matplotlib.figure.Figure, optional\n        The figure to draw on. If None, uses current figure.\n    \"\"\"\n\n    if fig is None:\n        fig = plt.gcf()\n\n    if b:\n        # new clear axis overlay with 0-1 limits\n        ax = fig.add_axes([0, 0, 1, 1], axisbg=(1, 1, 1, 0.7))\n        ax.minorticks_on()\n        ax.grid(b=True, which=\"major\", color=\"k\")\n        ax.grid(b=True, which=\"minor\", color=\"0.4\", linestyle=\":\")\n    else:\n        pass\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.get_extension_from_filename","title":"<code>get_extension_from_filename(name)</code>","text":"<p>Extract the extension from a filename string.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The filename string.</p> required <p>Returns:</p> Name Type Description <code>nameOnly</code> <code>str</code> <p>The filename without extension.</p> <code>ext</code> <code>str or None</code> <p>The extension, or None if not present.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def get_extension_from_filename(name):\n    \"\"\"\n    Extract the extension from a filename string.\n\n    Parameters\n    ----------\n    name : str\n        The filename string.\n\n    Returns\n    -------\n    nameOnly : str\n        The filename without extension.\n    ext : str or None\n        The extension, or None if not present.\n    \"\"\"\n    name = name.strip()\n    ext = ((name.split(\"\\\\\")[-1]).split(\"/\")[-1]).split(\".\")\n    if len(ext) &gt; 1 and ext[-1] != \"\":\n        nameOnly = \".\".join(name.split(\".\")[:-1])\n        ext = ext[-1]\n    else:\n        nameOnly = name\n        ext = None\n    return nameOnly, ext\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.imshow","title":"<code>imshow(axes, X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, shape=None, filternorm=1, filterrad=4.0, imlim=None, resample=None, url=None, clearaxes=True, **kwargs)</code>","text":"<p>Similar to matplotlib's imshow command, but produces a ModestImage</p> <p>Unlike matplotlib version, must explicitly specify axes</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def imshow(\n    axes,\n    X,\n    cmap=None,\n    norm=None,\n    aspect=None,\n    interpolation=None,\n    alpha=None,\n    vmin=None,\n    vmax=None,\n    origin=None,\n    extent=None,\n    shape=None,\n    filternorm=1,\n    filterrad=4.0,\n    imlim=None,\n    resample=None,\n    url=None,\n    clearaxes=True,\n    **kwargs,\n):\n    \"\"\"Similar to matplotlib's imshow command, but produces a ModestImage\n\n    Unlike matplotlib version, must explicitly specify axes\n    \"\"\"\n\n    if clearaxes:\n        axes.cla()\n    if norm is not None:\n        assert isinstance(norm, mcolors.Normalize)\n    if aspect is None:\n        aspect = rcParams[\"image.aspect\"]\n    axes.set_aspect(aspect)\n    im = ModestImage(\n        axes,\n        cmap,\n        norm,\n        interpolation,\n        origin,\n        extent,\n        filternorm=filternorm,\n        filterrad=filterrad,\n        resample=resample,\n        **kwargs,\n    )\n\n    im.set_data(X)\n    im.set_alpha(alpha)\n    axes._set_artist_props(im)\n\n    if im.get_clip_path() is None:\n        # image does not already have clipping set, clip to axes patch\n        im.set_clip_path(axes.patch)\n\n    # if norm is None and shape is None:\n    #    im.set_clim(vmin, vmax)\n    if vmin is not None or vmax is not None:\n        im.set_clim(vmin, vmax)\n    elif norm is None:\n        im.autoscale_None()\n\n    im.set_url(url)\n\n    # update ax.dataLim, and, if autoscaling, set viewLim\n    # to tightly fit the image, regardless of dataLim.\n    im.set_extent(im.get_extent())\n\n    axes.images.append(im)\n    im._remove_method = lambda h: axes.images.remove(h)\n\n    return im\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.no_ticks","title":"<code>no_ticks(*axes, where=None)</code>","text":"<p>Remove the tick marks on the desired axes (but leave the labels).</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> <code>where</code> <code>string, optional (default 'all') or list</code> <p>Where to remove ticks ['left', 'right', 'top', 'bottom', 'all']</p> <code>None</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def no_ticks(*axes, where=None):\n    \"\"\"Remove the tick marks on the desired axes (but leave the labels).\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n    where : string, optional (default 'all') or list\n        Where to remove ticks ['left', 'right', 'top', 'bottom', 'all']\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    if where is None:\n        where = [\"all\"]\n\n    if isinstance(where, str):\n        where = [where]\n    for ax in axes:\n        if \"left\" in where:\n            ax.tick_params(axis=\"y\", which=\"both\", left=False)\n        if \"right\" in where:\n            ax.tick_params(axis=\"y\", which=\"both\", right=False)\n        if \"top\" in where:\n            ax.tick_params(axis=\"x\", which=\"both\", top=False)\n        if \"bottom\" in where:\n            ax.tick_params(axis=\"x\", which=\"both\", bottom=False)\n        if \"all\" in where:\n            ax.tick_params(axis=\"y\", which=\"both\", left=False)\n            ax.tick_params(axis=\"y\", which=\"both\", right=False)\n            ax.tick_params(axis=\"x\", which=\"both\", top=False)\n            ax.tick_params(axis=\"x\", which=\"both\", bottom=False)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.no_xticklabels","title":"<code>no_xticklabels(*axes)</code>","text":"<p>Remove the tick labels on the x-axis (but leave the tick marks).</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def no_xticklabels(*axes):\n    \"\"\"Remove the tick labels on the x-axis (but leave the tick marks).\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.set_xticklabels([])\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.no_xticks","title":"<code>no_xticks(*axes)</code>","text":"<p>Remove the tick marks on the x-axis (but leave the labels).</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def no_xticks(*axes):\n    \"\"\"Remove the tick marks on the x-axis (but leave the labels).\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.tick_params(axis=\"x\", which=\"both\", length=0)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.no_yticklabels","title":"<code>no_yticklabels(*axes)</code>","text":"<p>Remove the tick labels on the y-axis (but leave the tick marks).</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def no_yticklabels(*axes):\n    \"\"\"Remove the tick labels on the y-axis (but leave the tick marks).\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.set_yticklabels([])\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.no_yticks","title":"<code>no_yticks(*axes)</code>","text":"<p>Remove the tick marks on the y-axis (but leave the labels).</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def no_yticks(*axes):\n    \"\"\"Remove the tick marks on the y-axis (but leave the labels).\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.tick_params(axis=\"y\", which=\"both\", length=0)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.outward_ticks","title":"<code>outward_ticks(*axes, axis='both')</code>","text":"<p>Make axis ticks face outwards rather than inwards (which is the default).</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> <code>axis</code> <code>string(default=both)</code> <p>The axis (either 'x', 'y', or 'both') for which to set the tick direction.</p> <code>'both'</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def outward_ticks(*axes, axis=\"both\"):\n    \"\"\"Make axis ticks face outwards rather than inwards (which is the\n    default).\n\n    Parameters\n    ----------\n    axes : axis object (default=pyplot.gca())\n    axis : string (default='both')\n        The axis (either 'x', 'y', or 'both') for which to set the tick\n        direction.\n\n    \"\"\"\n\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        if axis == \"both\":\n            ax.tick_params(direction=\"out\")\n        else:\n            ax.tick_params(axis=axis, direction=\"out\")\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.savefig","title":"<code>savefig(name, fig=None, formats=None, dpi=None, verbose=True, overwrite=False)</code>","text":"<p>Save a figure in one or multiple formats.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename without an extension. If an extension is present, AND if formats is empty, then the filename extension will be used.</p> required <code>fig</code> <code>Figure</code> <p>Figure to save, default uses current figure.</p> <code>None</code> <code>formats</code> <code>list</code> <p>List of formats to export. Defaults to ['pdf', 'png']</p> <code>None</code> <code>dpi</code> <code>float</code> <p>Resolution of the figure in dots per inch (DPI).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print additional output to screen.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>If True, file will be overwritten.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; ax.plot([1, 2, 3], [4, 5, 6])\n&gt;&gt;&gt; savefig(\"myplot\", fig=fig, formats=[\"png\"], overwrite=True)\n</code></pre> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def savefig(name, fig=None, formats=None, dpi=None, verbose=True, overwrite=False):\n    \"\"\"\n    Save a figure in one or multiple formats.\n\n    Parameters\n    ----------\n    name : str\n        Filename without an extension. If an extension is present,\n        AND if formats is empty, then the filename extension will be used.\n    fig : matplotlib.figure.Figure, optional\n        Figure to save, default uses current figure.\n    formats : list, optional\n        List of formats to export. Defaults to ['pdf', 'png']\n    dpi : float, optional\n        Resolution of the figure in dots per inch (DPI).\n    verbose : bool, optional\n        If True, print additional output to screen.\n    overwrite : bool, optional\n        If True, file will be overwritten.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; ax.plot([1, 2, 3], [4, 5, 6])\n    &gt;&gt;&gt; savefig(\"myplot\", fig=fig, formats=[\"png\"], overwrite=True)\n    \"\"\"\n    # Check inputs\n    # if not 0 &lt;= prop &lt;= 1:\n    #     raise ValueError(\"prop must be between 0 and 1\")\n\n    if dpi is None:\n        dpi = 300\n\n    supportedFormats = [\n        \"eps\",\n        \"jpeg\",\n        \"jpg\",\n        \"pdf\",\n        \"pgf\",\n        \"png\",\n        \"ps\",\n        \"raw\",\n        \"rgba\",\n        \"svg\",\n        \"svgz\",\n        \"tif\",\n        \"tiff\",\n    ]\n\n    name, ext = get_extension_from_filename(name)\n\n    # if no list of formats is given, use defaults\n    if formats is None and ext is None:\n        formats = [\"pdf\", \"png\"]\n    # if the filename has an extension, AND a list of extensions is given, then use only the list\n    elif formats is not None and ext is not None:\n        if not isinstance(formats, list):\n            formats = [formats]\n        print(\"WARNING! Extension in filename ignored in favor of formats list.\")\n    # if no list of extensions is given, use the extension from the filename\n    elif formats is None and ext is not None:\n        formats = [ext]\n    else:\n        pass\n\n    if fig is None:\n        fig = plt.gcf()\n\n    for extension in formats:\n        if extension not in supportedFormats:\n            print(\"WARNING! Format '{}' not supported. Aborting...\".format(extension))\n        else:\n            my_file = \"figures/{}.{}\".format(name, extension)\n\n            if os.path.isfile(my_file):\n                # file exists\n                print(\"{} already exists!\".format(my_file))\n\n                if overwrite:\n                    fig.savefig(my_file, dpi=dpi, bbox_inches=\"tight\")\n\n                    if verbose:\n                        print(\n                            \"{} saved successfully... [using overwrite]\".format(\n                                extension\n                            )\n                        )\n            else:\n                fig.savefig(my_file, dpi=dpi, bbox_inches=\"tight\")\n\n                if verbose:\n                    print(\"{} saved successfully...\".format(extension))\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.set_figsize","title":"<code>set_figsize(width, height, fig=None)</code>","text":"<p>Set the figure width and height.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>float</code> <p>Figure width</p> required <code>height</code> <code>float</code> <p>Figure height</p> required <code>fig</code> <code>figure object (default=pyplot.gcf())</code> <code>None</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def set_figsize(width, height, fig=None):\n    \"\"\"Set the figure width and height.\n\n    Parameters\n    ----------\n    width : float\n        Figure width\n    height : float\n        Figure height\n    fig : figure object (default=pyplot.gcf())\n\n    \"\"\"\n\n    if fig is None:\n        fig = plt.gcf()\n    fig.set_figwidth(width)\n    fig.set_figheight(height)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.set_scientific","title":"<code>set_scientific(low, high, axis=None, *axes)</code>","text":"<p>Set the axes or axis specified by <code>axis</code> to use scientific notation for ticklabels, if the value is &lt;10low or &gt;10high.</p> <p>Parameters:</p> Name Type Description Default <code>low</code> <code>int</code> <p>Lower exponent bound for non-scientific notation</p> required <code>high</code> <code>int</code> <p>Upper exponent bound for non-scientific notation</p> required <code>axis</code> <code>str(default=None)</code> <p>Which axis to format ('x', 'y', or None for both)</p> <code>None</code> <code>axes</code> <code>axis object (default=pyplot.gca())</code> <p>The matplotlib axis object to use</p> <code>()</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def set_scientific(low, high, axis=None, *axes):\n    \"\"\"Set the axes or axis specified by `axis` to use scientific notation for\n    ticklabels, if the value is &lt;10**low or &gt;10**high.\n\n    Parameters\n    ----------\n    low : int\n        Lower exponent bound for non-scientific notation\n    high : int\n        Upper exponent bound for non-scientific notation\n    axis : str (default=None)\n        Which axis to format ('x', 'y', or None for both)\n    axes : axis object (default=pyplot.gca())\n        The matplotlib axis object to use\n\n    \"\"\"\n    # get the axis\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    # create the tick label formatter\n    fmt = plt.ScalarFormatter()\n    fmt.set_scientific(True)\n    fmt.set_powerlimits((low, high))\n    # format the axis/axes\n    for ax in axes:\n        if axis is None or axis == \"x\":\n            ax.get_yaxis().set_major_formatter(fmt)\n        if axis is None or axis == \"y\":\n            ax.get_yaxis().set_major_formatter(fmt)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.set_xlabel_coords","title":"<code>set_xlabel_coords(y, *axes, x=0.5)</code>","text":"<p>Set the y-coordinate (and optionally the x-coordinate) of the x-axis label.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>float</code> <p>y-coordinate for the label</p> required <code>x</code> <code>float(default=0.5)</code> <p>x-coordinate for the label</p> <code>0.5</code> <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/faq/howto_faq.html#align-my-ylabels-across-multiple-subplots</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def set_xlabel_coords(y, *axes, x=0.5):\n    \"\"\"Set the y-coordinate (and optionally the x-coordinate) of the x-axis\n    label.\n\n    Parameters\n    ----------\n    y : float\n        y-coordinate for the label\n    x : float (default=0.5)\n        x-coordinate for the label\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/faq/howto_faq.html#align-my-ylabels-across-multiple-subplots\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.xaxis.set_label_coords(x, y)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.set_xlim","title":"<code>set_xlim(xlims, *axes)</code>","text":"<p>Sets the xlims for all axes.</p> <p>Parameters:</p> Name Type Description Default <code>xlims</code> <code>tuple? list?</code> required <code>*axes</code> <code>axis objects</code> <p>List of matplotlib axis objects to format</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def set_xlim(xlims, *axes):\n    \"\"\"Sets the xlims for all axes.\n\n    Parameters\n    ----------\n    xlims : tuple? list?\n    *axes : axis objects\n        List of matplotlib axis objects to format\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    for ax in axes:\n        ax.set_xlim(xlims[0], xlims[1])\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.set_ylabel_coords","title":"<code>set_ylabel_coords(x, *axes, y=0.5)</code>","text":"<p>Set the x-coordinate (and optionally the y-coordinate) of the y-axis label.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>x-coordinate for the label</p> required <code>y</code> <code>float(default=0.5)</code> <p>y-coordinate for the label</p> <code>0.5</code> <code>axes</code> <code>axis object (default=pyplot.gca())</code> <code>()</code> References <p>http://matplotlib.org/faq/howto_faq.html#align-my-ylabels-across-multiple-subplots</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def set_ylabel_coords(x, *axes, y=0.5):\n    \"\"\"Set the x-coordinate (and optionally the y-coordinate) of the y-axis\n    label.\n\n    Parameters\n    ----------\n    x : float\n        x-coordinate for the label\n    y : float (default=0.5)\n        y-coordinate for the label\n    axes : axis object (default=pyplot.gca())\n\n    References\n    ----------\n    http://matplotlib.org/faq/howto_faq.html#align-my-ylabels-across-multiple-subplots\n\n    \"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    for ax in axes:\n        ax.yaxis.set_label_coords(x, y)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.set_ylim","title":"<code>set_ylim(ylims, *axes)</code>","text":"<p>Sets the ylims for all axes.</p> <p>Parameters:</p> Name Type Description Default <code>ylims</code> <code>tuple? list?</code> required <code>*axes</code> <code>axis objects</code> <p>List of matplotlib axis objects to format</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def set_ylim(ylims, *axes):\n    \"\"\"Sets the ylims for all axes.\n\n    Parameters\n    ----------\n    ylims : tuple? list?\n    *axes : axis objects\n        List of matplotlib axis objects to format\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    for ax in axes:\n        ax.set_ylim(ylims[0], ylims[1])\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.skip_if_no_output","title":"<code>skip_if_no_output(fig)</code>","text":"<p>Raise FigureManager.Break if the figure is not to be output.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure or int</code> <p>The figure object or -1 if not outputting.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if output should continue, otherwise raises exception.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def skip_if_no_output(fig):\n    \"\"\"\n    Raise FigureManager.Break if the figure is not to be output.\n\n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure or int\n        The figure object or -1 if not outputting.\n\n    Returns\n    -------\n    bool\n        True if output should continue, otherwise raises exception.\n    \"\"\"\n    if fig == -1:\n        raise FigureManager.Break\n    return True\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.suptitle","title":"<code>suptitle(t, gs=None, rect=(0, 0, 1, 0.95), **kwargs)</code>","text":"<p>Add a suptitle to a figure with an embedded gridspec.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>str</code> <p>The suptitle text.</p> required <code>gs</code> <code>GridSpec</code> <p>The gridspec to use. If None, uses fig.npl_gs.</p> <code>None</code> <code>rect</code> <code>tuple</code> <p>Rectangle in figure coordinates (x1, y1, x2, y2).</p> <code>(0, 0, 1, 0.95)</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to fig.suptitle().</p> <code>{}</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If no gridspec is found in the figure.</p> See Also <p>https://matplotlib.org/users/tight_layout_guide.html</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def suptitle(t, gs=None, rect=(0, 0, 1, 0.95), **kwargs):\n    \"\"\"\n    Add a suptitle to a figure with an embedded gridspec.\n\n    Parameters\n    ----------\n    t : str\n        The suptitle text.\n    gs : matplotlib.gridspec.GridSpec, optional\n        The gridspec to use. If None, uses fig.npl_gs.\n    rect : tuple, optional\n        Rectangle in figure coordinates (x1, y1, x2, y2).\n    **kwargs : dict\n        Additional keyword arguments passed to fig.suptitle().\n\n    Raises\n    ------\n    AttributeError\n        If no gridspec is found in the figure.\n\n    See Also\n    --------\n    https://matplotlib.org/users/tight_layout_guide.html\n    \"\"\"\n    fig = plt.gcf()\n    if gs is None:\n        try:\n            gs = fig.npl_gs\n        except AttributeError:\n            raise AttributeError(\n                \"nelpy suptitle requires an embedded gridspec! Use the nelpy FigureManager.\"\n            )\n\n    fig.suptitle(t, **kwargs)\n    gs.tight_layout(fig, rect=rect)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.sync_xlims","title":"<code>sync_xlims(*axes)</code>","text":"<p>Synchronize the x-axis data limits for multiple axes. Uses the maximum upper limit and minimum lower limit across all given axes.</p> <p>Parameters:</p> Name Type Description Default <code>*axes</code> <code>axis objects</code> <p>List of matplotlib axis objects to format</p> <code>()</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(yxin, xmax)</code> <p>The computed bounds</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def sync_xlims(*axes):\n    \"\"\"Synchronize the x-axis data limits for multiple axes. Uses the maximum\n    upper limit and minimum lower limit across all given axes.\n\n    Parameters\n    ----------\n    *axes : axis objects\n        List of matplotlib axis objects to format\n\n    Returns\n    -------\n    out : yxin, xmax\n        The computed bounds\n\n    \"\"\"\n    xmins, xmaxs = zip(*[ax.get_xlim() for ax in axes])\n    xmin = min(xmins)\n    xmax = max(xmaxs)\n    for ax in axes:\n        ax.set_xlim(xmin, xmax)\n    return xmin, xmax\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.sync_ylims","title":"<code>sync_ylims(*axes)</code>","text":"<p>Synchronize the y-axis data limits for multiple axes. Uses the maximum upper limit and minimum lower limit across all given axes.</p> <p>Parameters:</p> Name Type Description Default <code>*axes</code> <code>axis objects</code> <p>List of matplotlib axis objects to format</p> <code>()</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(ymin, ymax)</code> <p>The computed bounds</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def sync_ylims(*axes):\n    \"\"\"Synchronize the y-axis data limits for multiple axes. Uses the maximum\n    upper limit and minimum lower limit across all given axes.\n\n    Parameters\n    ----------\n    *axes : axis objects\n        List of matplotlib axis objects to format\n\n    Returns\n    -------\n    out : ymin, ymax\n        The computed bounds\n\n    \"\"\"\n    ymins, ymaxs = zip(*[ax.get_ylim() for ax in axes])\n    ymin = min(ymins)\n    ymax = max(ymaxs)\n    for ax in axes:\n        ax.set_ylim(ymin, ymax)\n    return ymin, ymax\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.xticks_interval","title":"<code>xticks_interval(step=10, *axes)</code>","text":"<p>Set xticks interval.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def xticks_interval(step=10, *axes):\n    \"\"\"Set xticks interval.\"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    loc = mpl.ticker.MultipleLocator(\n        base=step\n    )  # this locator puts ticks at regular intervals\n    for ax in axes:\n        ax.xaxis.set_major_locator(loc)\n</code></pre>"},{"location":"reference/nelpy/plotting/utils/#nelpy.plotting.utils.yticks_interval","title":"<code>yticks_interval(step=10, *axes)</code>","text":"<p>Set yticks interval.</p> Source code in <code>nelpy/plotting/utils.py</code> <pre><code>def yticks_interval(step=10, *axes):\n    \"\"\"Set yticks interval.\"\"\"\n    if len(axes) == 0:\n        axes = [plt.gca()]\n    loc = mpl.ticker.MultipleLocator(\n        base=step\n    )  # this locator puts ticks at regular intervals\n    for ax in axes:\n        ax.yaxis.set_major_locator(loc)\n</code></pre>"},{"location":"reference/nelpy/synthesis/poisson/","title":"nelpy.synthesis.poisson","text":""},{"location":"reference/nelpy/utils_/decorators/","title":"nelpy.utils_.decorators","text":""},{"location":"reference/nelpy/utils_/decorators/#nelpy.utils_.decorators.add_method_to_class","title":"<code>add_method_to_class(cls)</code>","text":"<p>working for both class and instance inputs</p> Source code in <code>nelpy/utils_/decorators.py</code> <pre><code>def add_method_to_class(cls):\n    \"\"\"working for both class and instance inputs\"\"\"\n    if not inspect.isclass(cls):\n        cls = type(cls)\n\n    def decorator(f):\n        if not hasattr(cls, \"__perinstance\"):\n            cls.__perinstance = True\n        setattr(cls, f.__name__, f)\n        return f\n\n    return decorator\n</code></pre>"},{"location":"reference/nelpy/utils_/decorators/#nelpy.utils_.decorators.add_method_to_instance","title":"<code>add_method_to_instance(instance)</code>","text":"<p>Add a method to an object instance.</p> Example <p>class Foo: def init(self):     self.x = 42</p> <p>foo = Foo()</p> <p>@add_method_to_instance(foo) def print_x(self):     \"\"\"hello\"\"\"     print(self.x)</p> Source code in <code>nelpy/utils_/decorators.py</code> <pre><code>def add_method_to_instance(instance):\n    \"\"\"Add a method to an object instance.\n\n    Example\n    -------\n\n    &gt;&gt;&gt; class Foo:\n    &gt;&gt;&gt; def __init__(self):\n    &gt;&gt;&gt;     self.x = 42\n\n    &gt;&gt;&gt; foo = Foo()\n\n    &gt;&gt;&gt; @add_method_to_instance(foo)\n    &gt;&gt;&gt; def print_x(self):\n    &gt;&gt;&gt;     \\\"\"\"hello\\\"\"\"\n    &gt;&gt;&gt;     print(self.x)\n\n    \"\"\"\n    if inspect.isclass(instance):\n        raise TypeError(\"instance expected, class object received\")\n\n    def decorator(f):\n        import types\n\n        f = types.MethodType(f, instance)\n        setattr(instance, f.__name__, f)\n        return f\n\n    return decorator\n</code></pre>"},{"location":"reference/nelpy/utils_/decorators/#nelpy.utils_.decorators.add_prop_to_class","title":"<code>add_prop_to_class(cls)</code>","text":"<p>working</p> Source code in <code>nelpy/utils_/decorators.py</code> <pre><code>def add_prop_to_class(cls):\n    \"\"\"working\"\"\"\n    if not inspect.isclass(cls):\n        raise TypeError(\"class expected!\")\n\n    def decorator(f):\n        if not hasattr(cls, \"__perinstance\"):\n            cls.__perinstance = True\n        setattr(cls, f.__name__, property(f))\n        return f\n\n    return decorator\n</code></pre>"},{"location":"reference/nelpy/utils_/decorators/#nelpy.utils_.decorators.add_prop_to_instance","title":"<code>add_prop_to_instance(instance)</code>","text":"<p>working</p> Source code in <code>nelpy/utils_/decorators.py</code> <pre><code>def add_prop_to_instance(instance):\n    \"\"\"working\"\"\"\n    if inspect.isclass(instance):\n        raise TypeError(\"instance expected, class object received\")\n\n    def decorator(f):\n        cls = type(instance)\n        cls = type(cls.__name__, (cls,), {})\n        if not hasattr(cls, \"__perinstance\"):\n            cls.__perinstance = True\n        instance.__class__ = cls\n        setattr(cls, f.__name__, property(f))\n        return f\n\n    return decorator\n</code></pre>"},{"location":"reference/nelpy/utils_/decorators/#nelpy.utils_.decorators.deprecated","title":"<code>deprecated(func)</code>","text":"<p>This is a decorator which can be used to mark functions as deprecated. It will result in a warning being emitted when the function is used.</p> Source code in <code>nelpy/utils_/decorators.py</code> <pre><code>def deprecated(func):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\"\"\"\n\n    @functools.wraps(func)\n    def new_func(*args, **kwargs):\n        warnings.warn_explicit(\n            \"Call to deprecated function {}.\".format(func.__name__),\n            category=DeprecationWarning,\n            filename=func.__code__.co_filename,\n            lineno=func.__code__.co_firstlineno + 1,\n        )\n        return func(*args, **kwargs)\n\n    new_func.__name__ = func.__name__\n    new_func.__doc__ = func.__doc__\n    new_func.__dict__.update(func.__dict__)\n    return new_func\n</code></pre>"},{"location":"reference/nelpy/utils_/decorators/#nelpy.utils_.decorators.keyword_deprecation","title":"<code>keyword_deprecation(func=None, *, replace_x_with_y=None)</code>","text":"<p>Keyword deprecator.</p> <p>If you have a function with keywords kw1 and kw2 that you want to replace or update to nkw1 and nkw2, then this decorator can be used to support the transition. In particular, you should modify your function to use only the new keywords (both in the function definition and body), and then use this decorator to support calling the function with the previous keywords, kw1 and kw2. This decorator assumes a one-to-one mapping between old and new keywords, so essentially it is used when a keyword is renamed for clarity.</p> <p>Parameters:</p> Name Type Description Default <code>replace_x_with_y</code> <code>dict</code> <p>Dictionary of kwargs to replace, {'old': 'new'}</p> <code>None</code> Example <p>@keyword_deprecation(replace_x_with_y={'old1':'new1', 'old2':'new2'}) def myfunc(arg1, arg2, *, new1=None, new2=5):     pass</p> Source code in <code>nelpy/utils_/decorators.py</code> <pre><code>def keyword_deprecation(func=None, *, replace_x_with_y=None):\n    \"\"\"\n    Keyword deprecator.\n\n    If you have a function with keywords kw1 and kw2 that you want to replace\n    or update to nkw1 and nkw2, then this decorator can be used to support the\n    transition. In particular, you should modify your function to use only the\n    new keywords (both in the function definition and body), and then use this\n    decorator to support calling the function with the previous keywords, kw1\n    and kw2. This decorator assumes a one-to-one mapping between old and new\n    keywords, so essentially it is used when a keyword is renamed for clarity.\n\n    Parameters\n    ----------\n    replace_x_with_y : dict\n        Dictionary of kwargs to replace, {'old': 'new'}\n\n    Example\n    -------\n    @keyword_deprecation(replace_x_with_y={'old1':'new1', 'old2':'new2'})\n    def myfunc(arg1, arg2, *, new1=None, new2=5):\n        pass\n    \"\"\"\n\n    def _decorate(function):\n        @functools.wraps(function)\n        def wrapped_function(*args, **kwargs):\n            if replace_x_with_y is not None:\n                for oldkwarg, newkwarg in replace_x_with_y.items():\n                    newvalue = kwargs.pop(newkwarg, None)\n                    oldvalue = kwargs.pop(oldkwarg, None)\n                    if newvalue is not None and oldvalue is not None:\n                        raise ValueError(\n                            \"Cannot pass both '{}' and '{}'. Use '{}' instead.\".format(\n                                oldkwarg, newkwarg, newkwarg\n                            )\n                        )\n                    if oldvalue is not None:\n                        logging.warn(\n                            \"'{}' has been deprecated, use '{}' instead.\".format(\n                                oldkwarg, newkwarg\n                            )\n                        )\n                        kwargs[newkwarg] = oldvalue\n                    else:\n                        kwargs[newkwarg] = newvalue\n            return function(*args, **kwargs)\n\n        return wrapped_function\n\n    if func:\n        #         print('no args in decorator')\n        return _decorate(func)\n\n    return _decorate\n</code></pre>"},{"location":"reference/nelpy/utils_/decorators/#nelpy.utils_.decorators.keyword_equivalence","title":"<code>keyword_equivalence(func=None, *, this_or_that)</code>","text":"<p>Keyword equivalences decorator.</p> <p>Parameters:</p> Name Type Description Default <code>this_or_that</code> <code>dict</code> <p>Dictionary of equivalent kwargs, {'canonical': ['alt1', 'alt2', 'alt3']}</p> required <p>Examples:</p> <p>@keyword_equivalence(this_or_that={'data':'time', 'series_ids':['unit_ids', 'cell_ids', 'neuron_ids']}) def myfunc(arg1, arg2, *, data=None, unit_ids=5):     ...</p> <p>@keyword_equivalence(this_or_that={'n_intervals':'n_epochs'}) def partition(n_intervals=None, n_samples=None):     ...</p> Source code in <code>nelpy/utils_/decorators.py</code> <pre><code>def keyword_equivalence(func=None, *, this_or_that):\n    \"\"\"\n    Keyword equivalences decorator.\n\n    Parameters\n    ----------\n    this_or_that : dict\n        Dictionary of equivalent kwargs, {'canonical': ['alt1', 'alt2', 'alt3']}\n\n    Examples\n    --------\n    @keyword_equivalence(this_or_that={'data':'time', 'series_ids':['unit_ids', 'cell_ids', 'neuron_ids']})\n    def myfunc(arg1, arg2, *, data=None, unit_ids=5):\n        ...\n\n    @keyword_equivalence(this_or_that={'n_intervals':'n_epochs'})\n    def partition(n_intervals=None, n_samples=None):\n        ...\n\n    \"\"\"\n\n    def _decorate(function):\n        @functools.wraps(function)\n        def wrapped_function(*args, **kwargs):\n            for canonical, equiv in this_or_that.items():\n                canonical_val = kwargs.pop(canonical, None)\n                if isinstance(equiv, list):\n                    equiv_val = None\n                    count = 0\n                    alt = []\n                    for ee in equiv:\n                        temp_val = kwargs.pop(ee, None)\n                        if canonical_val is not None and temp_val is not None:\n                            raise ValueError(\n                                \"Cannot pass both '{}' and '{}'. Use '{}' instead.\".format(\n                                    canonical, ee, canonical\n                                )\n                            )\n                        if temp_val is not None:\n                            equiv_val = temp_val\n                            count += 1\n                            alt.append(ee)\n                        if count &gt; 1:\n                            raise ValueError(\n                                \"Cannot pass both '{}' and '{}'. Use '{}' instead.\".format(\n                                    alt[0], alt[1], canonical\n                                )\n                            )\n                elif isinstance(equiv, str):\n                    equiv_val = kwargs.pop(equiv, None)\n                    if canonical_val is not None and equiv_val is not None:\n                        raise ValueError(\n                            \"Cannot pass both '{}' and '{}'. Use '{}' instead.\".format(\n                                canonical, ee, canonical\n                            )\n                        )\n                else:\n                    raise TypeError(\"unknown equivalence kwarg type\")\n                if equiv_val is not None:\n                    kwargs[canonical] = equiv_val\n                else:\n                    kwargs[canonical] = canonical_val\n\n            return function(*args, **kwargs)\n\n        return wrapped_function\n\n    if func:\n        return _decorate(func)\n\n    return _decorate\n</code></pre>"},{"location":"reference/nelpy/utils_/metrics/","title":"nelpy.utils_.metrics","text":""},{"location":"reference/nelpy/utils_/metrics/#nelpy.utils_.metrics--modmetrics-metrics-and-measures","title":":mod:<code>metrics</code> --- metrics and measures","text":""},{"location":"reference/nelpy/utils_/metrics/#nelpy.utils_.metrics.gini","title":"<code>gini(arr, mode='all')</code>","text":"<p>Calculate the Gini coefficient(s) of a matrix or vector.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>array - like</code> <p>Array or matrix on which to compute the Gini coefficient(s).</p> required <code>mode</code> <code>string</code> <p>One of ['row-wise', 'col-wise', 'all']. Default is 'all'.</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>coeffs</code> <code>array - like</code> <p>Array of Gini coefficients.</p> Note <p>If arr is a transition matrix A, such that Aij = P(S_k=j|S_{k-1}=i), then 'row-wise' is equivalent to 'tmat_departure' and 'col-wise' is equivalent to 'tmat_arrival'.</p> <p>Similarly, if arr is the observation (lambda) matrix of an HMM such that lambda \\in \\mathcal{C}^{n_states \\times n_units}, then 'row-wise' is equivalent to 'lambda_across_units' and 'col-wise' is equivalent to 'lambda_across_units'.</p> <p>If mode = 'all', then the matrix is unwrapped into a numel-dimensional array before computing the Gini coefficient.</p> Source code in <code>nelpy/utils_/metrics.py</code> <pre><code>def gini(arr, mode=\"all\"):\n    \"\"\"Calculate the Gini coefficient(s) of a matrix or vector.\n\n    Parameters\n    ----------\n    arr : array-like\n        Array or matrix on which to compute the Gini coefficient(s).\n    mode : string, optional\n        One of ['row-wise', 'col-wise', 'all']. Default is 'all'.\n\n    Returns\n    -------\n    coeffs : array-like\n        Array of Gini coefficients.\n\n    Note\n    ----\n    If arr is a transition matrix A, such that Aij = P(S_k=j|S_{k-1}=i),\n    then 'row-wise' is equivalent to 'tmat_departure' and 'col-wise' is\n    equivalent to 'tmat_arrival'.\n\n    Similarly, if arr is the observation (lambda) matrix of an HMM such that\n    lambda \\\\in \\\\mathcal{C}^{n_states \\\\times n_units}, then 'row-wise' is\n    equivalent to 'lambda_across_units' and 'col-wise' is equivalent to\n    'lambda_across_units'.\n\n    If mode = 'all', then the matrix is unwrapped into a numel-dimensional\n    array before computing the Gini coefficient.\n\n    \"\"\"\n    if mode is None:\n        mode = \"row-wise\"\n\n    if mode not in [\"row-wise\", \"col-wise\", \"all\"]:\n        raise ValueError(\"mode '{}' not supported!\".format(mode))\n\n    gini_coeffs = None\n\n    if mode == \"all\":\n        arr = np.atleast_1d(arr).astype(float)\n        gini_coeffs = _gini(arr)\n\n    elif mode == \"row-wise\":\n        arr = np.atleast_2d(arr).astype(float)\n        gini_coeffs = []\n        for row in arr:\n            gini_coeffs.append(_gini(row))\n\n    elif mode == \"col-wise\":\n        arr = np.atleast_2d(arr).astype(float)\n        gini_coeffs = []\n        for row in arr.T:\n            gini_coeffs.append(_gini(row))\n\n    return gini_coeffs\n</code></pre>"},{"location":"tutorials/AnalogSignalArrayTutorial/","title":"AnalogSignalArray Tutorial","text":"<p>The <code>AnalogSignalArray</code> is used to store fairly-regularly-sampled temporal signals. Ideally the signals should be sampled regularly, but many of the methods handle irregularly sampled data gracefully, and moreover, the <code>AnalogSignalArray</code> makes it easy to sanitize an irregularly sampled signal into an easier-to-work-with regularly sampled signal.</p> <p>Fundamentally, an <code>AnalogSignalArray</code> contains a <code>.time</code> attribute, and a <code>.data</code> attribute, corresponding to the (n,) sample timestamps, in seconds, and the (m, n) signal values (n samples for each of m signals).</p> <pre># create an AnalogSignalArray with a single signal, with four samples:\nasa = nel.AnalogSignalArray(data=[2, 4, 5, 6])\n\n# create an AnalogSignalArray with a single signal, with four samples:\nasa = nel.AnalogSignalArray(timestamps=[0, 1, 2, 3],\n                            data=[2, 4, 5, 6])\n\n# create an AnalogSignalArray with two signals, each with four samples:\nasa = nel.AnalogSignalArray(timestamps=[0, 1, 2, 3],\n                            data=[[2, 4, 5, 6], [5, 4, 3, 2]])\n\n# create an AnalogSignalArray with a single signal, with four samples:\nasa = nel.AnalogSignalArray(timestamps=[0, 1, 2, 3],\n                            data=[2, 4, 5, 6])\n\n# create an AnalogSignalArray with a single signal, with four samples, \n# and an explicit support:\nasa = nel.AnalogSignalArray(timestamps=[0, 1, 2, 3],\n                            data=[2, 4, 5, 6],\n                            support=nel.EpochArray(0,4))\n\n# create an AnalogSignalArray with a single signal, with four samples, \n# and an explicit sampling rate:\nasa = nel.AnalogSignalArray(timestamps=[0, 1, 2, 3],\n                            data=[2, 4, 5, 6],\n                            fs=1)\n\n# create an AnalogSignalArray with a single signal, with ten samples:\nasa = nel.AnalogSignalArray(timestamps=[0, 1, 2, 3, 10, 11, 12, 13, 14, 15],\n                            data=[1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n\n# npl.plot(asa, marker='.')\n</pre>"},{"location":"tutorials/AnalogSignalArrayTutorial/#analogsignalarray-tutorial","title":"AnalogSignalArray Tutorial\u00b6","text":"<p><code>AnalogSignalArrayTutorial.ipynb</code></p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#overview","title":"Overview\u00b6","text":""},{"location":"tutorials/AnalogSignalArrayTutorial/#core-nelpy-objects","title":"Core Nelpy Objects\u00b6","text":""},{"location":"tutorials/AnalogSignalArrayTutorial/#analogsignalarray-m-d","title":"AnalogSignalArray (m-D)\u00b6","text":"<p>An <code>AnalogSignalArray</code> represents a multi-dimensional analog signal function:</p> <p>$$f: \\mathbb{R} \\to \\mathbb{R}^m \\quad \\bigl(t \\mapsto (y_1, y_2, \\ldots, y_m)\\bigr)$$</p> <p>The default domain is $\\Omega = \\mathbb{R}$, and the support can either be specified or inferred.</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#support-specification","title":"Support Specification\u00b6","text":"<p>If a support is specified as a collection of intervals $\\{s_1, s_2, \\ldots s_k\\}$, then the AnalogSignalArray behaves like:</p> <p>$$f: \\mathbb{S} \\to \\mathbb{R}^m \\quad \\bigl(t \\mapsto (y_1, y_2, \\ldots, y_m)\\bigr)$$</p> <p>where $\\mathbb{S} = \\bigcup_{i=1}^k s_i$, so that the AnalogSignalArray is undefined in $\\Omega \\backslash \\mathbb{S}$.</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#internal-data-organization","title":"Internal Data Organization\u00b6","text":"<p>An <code>AnalogSignalArray</code> contains <code>data</code> and <code>time</code> attributes:</p> <ul> <li><code>data</code> $\\in \\mathbb{R}^{m \\times n}$ where $m$ is the number of signals and $n$ is the number of samples</li> <li><code>time</code> is a numpy vector (1-dimensional array) with shape $(n,)$, containing the sample times in seconds</li> </ul>"},{"location":"tutorials/AnalogSignalArrayTutorial/#working-with-data","title":"Working with Data\u00b6","text":"<p>When looking at the <code>data</code> matrix itself (as a numpy array, not a nelpy object), it's convenient to iterate over signals:</p> <pre>for signal in data:\n    pass\n</pre> <p>However, plotting with matplotlib directly can be inconvenient since matplotlib plots each column of a matrix as a single trace. To plot <code>data</code> with matplotlib:</p> <pre>import matplotlib.pyplot as plt\nplt.plot(data.T)  # plot each signal as a single continuous trace\n</pre> <p>!!! warning \"Discontinuities\" This approach ignores the fact that signals are not always continuous. Nelpy considers each signal to be composed of different segments, each assumed to be continuous.</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#nelpy-plotting","title":"Nelpy Plotting\u00b6","text":"<p>Nelpy provides plotting as an almost drop-in replacement for matplotlib:</p> <pre>import nelpy.plotting as npl\nnpl.plot(asa)  # plot each signal as a single trace, respecting discontinuities\n</pre>"},{"location":"tutorials/AnalogSignalArrayTutorial/#working-with-segments","title":"Working with Segments\u00b6","text":"<p>Here's an example of creating an <code>AnalogSignalArray</code> with multiple segments:</p> <pre>import numpy as np\nimport nelpy as nel\n\nt = np.linspace(0, 10, 100)\n\ny1 = t**3\ny2 = 3*t**2\ny3 = 6*t\ny4 = 6*np.ones(t.shape)\n\nasa = nel.AnalogSignalArray(\n    np.vstack((y1, y2, y3, y4)), \n    timestamps=t, \n    support=nel.EpochArray([[0,3], [5,10]])\n)\n</pre> <p>This creates an <code>AnalogSignalArray</code> with four signals and two snippets/segments/epochs.</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#accessing-epoch-data","title":"Accessing Epoch Data\u00b6","text":"<p>You can access timestamps and signal data using <code>asa.time</code> and <code>asa.data</code>, but these don't directly indicate which samples are contiguous. For better handling of discontinuities, use the <code>_epochtime</code> and <code>_epochdata</code> special objects:</p> <pre>import matplotlib.pyplot as plt\n\n# Plot each snippet separately to preserve discontinuities\nfor timestamps, data in zip(asa._epochtime, asa._epochdata):\n    plt.plot(timestamps, data.T)\n</pre> <p>!!! tip \"Better Plotting\" While this preserves discontinuities, signal snippets will have different colors, making it difficult to see which segments correspond to which signals. Using <code>npl.plot(asa)</code> handles this automatically.</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#iteration","title":"Iteration\u00b6","text":"<p>Iteration over an <code>AnalogSignalArray</code> (and most core temporal nelpy objects) iterates over the continuous epochs:</p> <pre>for snippet in asa:\n    # snippet is an AnalogSignalArray with only a single underlying support epoch\n    timestamps, data = (snippet.time, snippet.data)  # one continuous epoch at a time\n</pre>"},{"location":"tutorials/AnalogSignalArrayTutorial/#indexing-and-restriction","title":"Indexing and Restriction\u00b6","text":"<p><code>AnalogSignalArray</code>s can be indexed/restricted using <code>EpochArray</code>s. The resulting <code>AnalogSignalArray</code> will be defined on the intersection of its own underlying support and the requested <code>EpochArray</code>:</p> <pre>ep = nel.EpochArray([2, 7])\nasa_new = asa[ep]\n</pre> <p>If <code>asa</code> had a support of $[0, 3) \\cup [5, 10)$, then <code>asa_new</code> will be defined on $[2, 3) \\cup [5, 7)$.</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#epoch-indexing","title":"Epoch Indexing\u00b6","text":"<p>You can index epochs with integers:</p> <pre>asa[0]  # asa restricted to first epoch\nasa[1]  # asa restricted to second epoch\nasa[2]  # empty AnalogSignalArray (no third epoch)\n</pre>"},{"location":"tutorials/AnalogSignalArrayTutorial/#signal-indexing","title":"Signal Indexing\u00b6","text":"<p>Extended syntax allows access to individual signals:</p> <pre>asa[0, 0]     # first epoch, first signal\nasa[0, 1]     # first epoch, second signal\nasa[:, 2:]    # all epochs, third and fourth signals\nasa[:, [1,3]] # all epochs, second and fourth signals\n</pre> <p>!!! info \"Future Enhancement\" Eventually, sample indexing will be supported with the form <code>asa[epoch, signal, sample]</code>. This indexing form is consistent across most nelpy objects (e.g., <code>SpikeTrainArray</code> uses <code>sta[epoch, unit, ...]</code>).</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#common-operations","title":"Common Operations\u00b6","text":"<p>Common tricks include:</p> <ul> <li>Resampling</li> <li>Simplifying for plots</li> <li>Joining arrays</li> <li>Casting to <code>BinnedSpikeTrainArray</code>s</li> <li>Changing underlying support</li> <li>Creating copy-like objects with attached metadata without actual data</li> <li>Using <code>__call__</code> and <code>asarray</code></li> </ul> <p>!!! warning \"Advanced Usage\" For advanced users: access data with underscore methods, and use <code>__renew__</code> after modifying objects.</p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#positionarray","title":"PositionArray\u00b6","text":""},{"location":"tutorials/AnalogSignalArrayTutorial/#1d-position","title":"1D Position\u00b6","text":"<p>A 1D <code>PositionArray</code> represents position as a function of time:</p> <p>$$f: \\mathbb{R} \\to \\mathbb{R} \\quad \\bigl(t \\mapsto x\\bigr)$$</p> <p>Special attributes: <code>x</code>, <code>speed</code></p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#2d-position","title":"2D Position\u00b6","text":"<p>A 2D <code>PositionArray</code> represents 2D position as a function of time:</p> <p>$$f: \\mathbb{R} \\to \\mathbb{R}^2 \\quad \\bigl(t \\mapsto (x,y)\\bigr)$$</p> <p>Special attributes: <code>x</code>, <code>y</code>, <code>speed</code></p>"},{"location":"tutorials/AnalogSignalArrayTutorial/#spiketrainarray-n-d","title":"SpikeTrainArray (N-D)\u00b6","text":"<p>A <code>SpikeTrainArray</code> represents spike trains for multiple units:</p> <p>$$f: \\mathbb{Z} \\to \\mathbb{R}^{n_i} \\quad \\bigl(u \\mapsto (t_1, t_2, \\ldots, t_{n_i})\\bigr)$$</p> <p>where $u \\in \\{1,2,\\ldots N\\}$ and unit $u$ has $n_i$ spikes.</p>"},{"location":"tutorials/BackyardBrainsEEG/","title":"Backyard Brains EEG","text":"In\u00a0[1]: Copied! <pre>import os\n\nimport requests\nfrom tqdm import tqdm\n\ndatadir = os.path.join(os.getcwd(), \"example-data/eeg\")\nos.makedirs(datadir, exist_ok=True)\n\nfilename = os.path.join(datadir, \"EEG_Alpha_SampleData.zip\")\nurl = \"https://backyardbrains.com/experiments/files/EEG_Alpha_SampleData.zip\"\n\n\nif os.path.exists(filename):\n    print(\"you already have the example data, skipping download...\")\nelse:\n    print(\"downloading data from {}\".format(url))\n    # Streaming, so we can iterate over the response.\n    r = requests.get(url, stream=True)\n\n    # Total size in bytes.\n    total_size = int(r.headers.get(\"content-length\", 0))\n    chunk_size = 1024  # number of bytes to process at a time (NOTE: progress bar unit only accurate if this is 1 kB)\n\n    with open(filename, \"wb+\") as f:\n        for data in tqdm(\n            r.iter_content(chunk_size), total=int(total_size / chunk_size), unit=\"kB\"\n        ):\n            f.write(data)\n\n    print(\"data saved to local directory {}\".format(filename))\n</pre> import os  import requests from tqdm import tqdm  datadir = os.path.join(os.getcwd(), \"example-data/eeg\") os.makedirs(datadir, exist_ok=True)  filename = os.path.join(datadir, \"EEG_Alpha_SampleData.zip\") url = \"https://backyardbrains.com/experiments/files/EEG_Alpha_SampleData.zip\"   if os.path.exists(filename):     print(\"you already have the example data, skipping download...\") else:     print(\"downloading data from {}\".format(url))     # Streaming, so we can iterate over the response.     r = requests.get(url, stream=True)      # Total size in bytes.     total_size = int(r.headers.get(\"content-length\", 0))     chunk_size = 1024  # number of bytes to process at a time (NOTE: progress bar unit only accurate if this is 1 kB)      with open(filename, \"wb+\") as f:         for data in tqdm(             r.iter_content(chunk_size), total=int(total_size / chunk_size), unit=\"kB\"         ):             f.write(data)      print(\"data saved to local directory {}\".format(filename)) <pre>downloading data from https://backyardbrains.com/experiments/files/EEG_Alpha_SampleData.zip\n</pre> <pre>363kB [00:00, 22721.99kB/s]            </pre> <pre>data saved to local directory d:\\github\\nelpy\\tutorials\\example-data/eeg\\EEG_Alpha_SampleData.zip\n</pre> <pre>\n</pre> In\u00a0[2]: Copied! <pre>import zipfile\n\nwith zipfile.ZipFile(filename, \"r\") as zip_ref:\n    zip_ref.extractall(path=datadir)\n</pre> import zipfile  with zipfile.ZipFile(filename, \"r\") as zip_ref:     zip_ref.extractall(path=datadir) <p>There are two pieces of data that we will load:</p> <p><code>TimBrain_VisualCortex_BYB_Recording-events.txt</code>: This is a file denoting the start and stop times for when Tim's eyes were closed. All times are in seconds.</p> <p><code>TimBrain_VisualCortex_BYB_Recording.wav</code>: The raw EEG recording, sampled at 10 kHz.</p> In\u00a0[3]: Copied! <pre>from scipy.io.wavfile import read as read_wavefile\n\nrate, data = read_wavefile(\n    datadir + \"/EEG_Alpha_SampleData/TimBrain_VisualCortex_BYB_Recording.wav\"\n)\n\nprint(\"Data was recorded at a sampling rate of {} Hz\".format(rate))\n</pre> from scipy.io.wavfile import read as read_wavefile  rate, data = read_wavefile(     datadir + \"/EEG_Alpha_SampleData/TimBrain_VisualCortex_BYB_Recording.wav\" )  print(\"Data was recorded at a sampling rate of {} Hz\".format(rate)) <pre>Data was recorded at a sampling rate of 10000 Hz\n</pre> In\u00a0[4]: Copied! <pre>import numpy as np\n\nbounds = []\nct = 0\nwith open(\n    datadir + \"/EEG_Alpha_SampleData/TimBrain_VisualCortex_BYB_Recording-events.txt\",\n    \"r\",\n) as file:\n    for line in file:\n        y = line.split(\",\")\n        if y[0] in (\"eyes closed\", \"eyes open\"):\n            bounds.append(float(y[1].strip(\"\\t\").strip(\"\\n\")))\n            ct += 1\n\nif ct &amp; 1:\n    raise ValueError(\"File has {} boundaries but expected an even number\".format(ct))\n\nclosed_intervals = np.array(bounds).reshape((ct // 2, 2))\nprint(closed_intervals)\n</pre> import numpy as np  bounds = [] ct = 0 with open(     datadir + \"/EEG_Alpha_SampleData/TimBrain_VisualCortex_BYB_Recording-events.txt\",     \"r\", ) as file:     for line in file:         y = line.split(\",\")         if y[0] in (\"eyes closed\", \"eyes open\"):             bounds.append(float(y[1].strip(\"\\t\").strip(\"\\n\")))             ct += 1  if ct &amp; 1:     raise ValueError(\"File has {} boundaries but expected an even number\".format(ct))  closed_intervals = np.array(bounds).reshape((ct // 2, 2)) print(closed_intervals) <pre>[[ 4.2552 14.9426]\n [23.2801 36.0951]\n [45.4738 59.3751]\n [72.0337 85.0831]]\n</pre> In\u00a0[5]: Copied! <pre>import nelpy as nel\nimport nelpy.plotting as npl\n\neeg = nel.AnalogSignalArray(data=data, fs=rate)\nclosed_epochs = nel.EpochArray(closed_intervals)\n\nprint(\"The 'eeg' object is now an {}\".format(eeg))\nprint(\n    \"During the experiment, eyes were closed for {} out of {}.\".format(\n        closed_epochs.duration, eeg.support.duration\n    )\n)\n</pre> import nelpy as nel import nelpy.plotting as npl  eeg = nel.AnalogSignalArray(data=data, fs=rate) closed_epochs = nel.EpochArray(closed_intervals)  print(\"The 'eeg' object is now an {}\".format(eeg)) print(     \"During the experiment, eyes were closed for {} out of {}.\".format(         closed_epochs.duration, eeg.support.duration     ) ) <pre>WARNING:root:creating support from abscissa_vals and sampling rate, fs!\nWARNING:root:'fs' has been deprecated; use 'step' instead\nWARNING:root:some steps in the data are smaller than the requested step size.\n</pre> <pre>The 'eeg' object is now an &lt;AnalogSignalArray at 0x2a8cf3d7d90: 1 signals&gt; for a total of 1:37:075 minutes\nDuring the experiment, eyes were closed for 50.4531 seconds out of 1:37:075 minutes.\n</pre> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt  %matplotlib inline In\u00a0[7]: Copied! <pre>plt.figure(figsize=(20, 4))\nnpl.plot(eeg, color=\"gray\")\nnpl.plot(eeg[closed_epochs], color=\"orange\")\nnpl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\")\n</pre> plt.figure(figsize=(20, 4)) npl.plot(eeg, color=\"gray\") npl.plot(eeg[closed_epochs], color=\"orange\") npl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\") <pre>WARNING:root:ignoring signal outside of support\n</pre> <p>Now we will further explore the characteristics of the EEG by doing some spectral analysis. This decomposes the recording into frequency components.</p> <p>Though this sounds a little complicated, one way to understand this is through music. The reference tone for an orchestra is an A4, 440 Hz. Suppose we play that note on the piano for 1 second. Now play an A note but one octave higher (880 Hz), also for 1 second. Why do the notes sound different? Though they were played for the same length of time, their frequency content was different.</p> <p>In addition, any note on the piano (or any acoustic instrument for that matter) is composed of not just one but a mixture of frequencies. Suppose for simplicity that we have an evenly distributed two-component mixture, 50% 250 Hz and 50% 500 Hz. If we change this proportion to 75% 250 Hz and 25% 500 Hz, we will get a different sound. The mixture of frequencies and the relative proportion/strength of each frequency component is part of the reason why different instruments sound different from each other, even if they play the exact same note for the exact same amount of time.</p> <p>This is exactl the question we want to explore for the EEG. Namely, (1) what frequencies are present, (2) how strong are they relative to one another, and (3) how long did they last for.</p> <p>We will investigate by generating a spectrogram, allowing us to visualize the EEG over both time and frequency.</p> In\u00a0[8]: Copied! <pre>from scipy import signal\n\nf, t, Sxx = signal.spectrogram(x=eeg.data.squeeze(), fs=eeg.fs, nperseg=2**13)\nplt.figure(figsize=(20, 4))\nplt.pcolormesh(t, f, Sxx, cmap=plt.cm.Spectral_r)\nplt.ylabel(\"Frequency [Hz]\")\nplt.xlabel(\"Time [sec]\")\nplt.ylim(0, 50)\nnpl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\")\n</pre> from scipy import signal  f, t, Sxx = signal.spectrogram(x=eeg.data.squeeze(), fs=eeg.fs, nperseg=2**13) plt.figure(figsize=(20, 4)) plt.pcolormesh(t, f, Sxx, cmap=plt.cm.Spectral_r) plt.ylabel(\"Frequency [Hz]\") plt.xlabel(\"Time [sec]\") plt.ylim(0, 50) npl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\") In\u00a0[9]: Copied! <pre>eeg2 = eeg.downsample(fs_out=150)\n\nf, t, Sxx = signal.spectrogram(x=eeg2.data.squeeze(), fs=eeg2.fs)\nplt.figure(figsize=(20, 4))\nplt.pcolormesh(t, f, Sxx, cmap=plt.cm.Spectral_r)\nplt.ylabel(\"Frequency [Hz]\")\nplt.xlabel(\"Time [sec]\")\nplt.ylim(0, 50)\nnpl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\")\n</pre> eeg2 = eeg.downsample(fs_out=150)  f, t, Sxx = signal.spectrogram(x=eeg2.data.squeeze(), fs=eeg2.fs) plt.figure(figsize=(20, 4)) plt.pcolormesh(t, f, Sxx, cmap=plt.cm.Spectral_r) plt.ylabel(\"Frequency [Hz]\") plt.xlabel(\"Time [sec]\") plt.ylim(0, 50) npl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\") <p>The two spectrogram plots were generated using the Fourier Transform. It is one of the most important transforms you will encounter, but it also has its drawbacks.</p> <p>An alternative way to generate spectrograms is through the wavelet transform. One of the features of wavelets is that they can allow for greater temporal resolution. This means we can see the structure of each \"eyes closed\" event with finer detail. That may sound confusing at first, but hopefully a wavelet-based spectrogram plot will help show this difference.</p> In\u00a0[10]: Copied! <pre>import time\n\nimport ghost.wave as gwave\n\nfig, ax = plt.subplots(figsize=(20, 4))\ncwt = gwave.ContinuousWaveletTransform()\nt0 = time.time()\ncwt.transform(eeg2, freq_limits=[1, 50])\ncwt.plot(logscale=False, ax=ax, cmap=plt.cm.Spectral_r)\nprint(time.time() - t0)\nnpl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\")\n</pre> import time  import ghost.wave as gwave  fig, ax = plt.subplots(figsize=(20, 4)) cwt = gwave.ContinuousWaveletTransform() t0 = time.time() cwt.transform(eeg2, freq_limits=[1, 50]) cwt.plot(logscale=False, ax=ax, cmap=plt.cm.Spectral_r) print(time.time() - t0) npl.overviewstrip(closed_epochs, label=\"eyes closed\", color=\"orange\") <pre>WARNING:root:Module 'pyfftw' not found, using scipy backend\n</pre> <pre>0.4697887897491455\n</pre> <p>Now that we've tried out two different methods, we see an advantage of using the wavelet transform method. Notice that the regions of high 10 Hz activity align more closely to the \"eyes closed\" periods that we obtained from the events file.</p>"},{"location":"tutorials/BackyardBrainsEEG/#spectral-analysis-of-an-eeg-trace-recorded-from-the-human-brain-from-backyard-brains","title":"Spectral analysis of an EEG trace recorded from the human brain (from Backyard Brains)\u00b6","text":"<p><code>BackyardBrainsEEG.ipynb</code></p> <p>Notebook was created by Etienne Ackermann.</p>"},{"location":"tutorials/BackyardBrainsEEG/#overview","title":"Overview\u00b6","text":"<p>Backyard Brains has a very nice experiment where they introduce the \"much sought after, often misunderstood, signal of neuroscience: the Electroencephalogram (EEG)\". In this notebook, we will take the recorded EEG trace from Backyard Brains, and analyze the spectral content in a few different ways, including with the very cool generalized Morse wavelet transform.</p>"},{"location":"tutorials/BackyardBrainsEEG/#preliminaries","title":"Preliminaries\u00b6","text":"<p>The following Python packages will be used in this tutorial:</p> <p>For downloading example data from the web:</p> <pre>1. requests   # used to download data from web\n2. tqdm       # used to show progress of download\n</pre> <p>the rest of this notebook:</p> <pre>3. numpy      # numerical powerhorse for Python (for matrix and vector calculations)\n4. matplotlib # used to make plots and figures\n5. scipy      # signal processing, stats, etc.\n6. sklearn    # machine learning in Python\n</pre> <p>and of course:</p> <pre>7. nelpy      # electrophysiology data containers\n8. ghost      # spectral analysis routines\n</pre>"},{"location":"tutorials/BackyardBrainsEEG/#installation","title":"Installation\u00b6","text":"<p>Packages 1-7 above can be installed either with <code>conda install &lt;pkg&gt;</code> or with <code>pip install &lt;pkg&gt;</code>. Many of these are very common, and it is more than likely that you have at least one of them installed already.</p> <p>It doesn't hurt to type <code>pip install &lt;pkg&gt;</code> if the package is already installed, so don't be nervous to try it out! Example:</p> <pre>pip install nelpy\n</pre> <p>For <code>ghost</code> please type:</p> <pre>pip install git+https://github.com/nelpy/ghost.git\n</pre> <p>Now that we have all the packages we need, let's get started! First, we need to get the sample data...</p>"},{"location":"tutorials/BackyardBrainsEEG/#1-obtain-example-data","title":"1. Obtain example data\u00b6","text":""},{"location":"tutorials/EpochArrayTutorial/","title":"EpochArray Tutorial","text":"In\u00a0[1]: Copied! <pre>import nelpy as nel\n</pre> import nelpy as nel In\u00a0[2]: Copied! <pre>ep = nel.EpochArray([[0, 3], [5, 6], [10, 15]])\n</pre> ep = nel.EpochArray([[0, 3], [5, 6], [10, 15]]) <p>which we can look at either by calling <code>print()</code> on the <code>EpochArray</code> object, or by evaluating <code>ep</code> in the REPL.</p> In\u00a0[3]: Copied! <pre>print(ep)\n</pre> print(ep) <pre>&lt;EpochArray at 0x1be4dae8a10: 3 epochs&gt; of length 9 seconds\n</pre> <p>Here we can see that we received immediate feedback, namely that there are 3 epochs inside the <code>EpochArray</code>, and that the total duration of those epochs is 9 seconds.</p> <p>Nelpy makes it easy to read durations of various magnitudes, and would print \"1:20 minutes\" for a duration of 80 seconds, and \"2:13:20 hours\" for a duration of 8000 seconds, for example.</p> In\u00a0[4]: Copied! <pre>import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ipywidgets import fixed, interact\n\nimport nelpy.plotting as npl\n\n%matplotlib inline\n\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings  import matplotlib.pyplot as plt import numpy as np from ipywidgets import fixed, interact  import nelpy.plotting as npl  %matplotlib inline  warnings.filterwarnings(\"ignore\") In\u00a0[5]: Copied! <pre>times = np.array(\n    [\n        [1.0, 3.0],\n        [4.0, 8.0],\n        [12.0, 13.0],\n        [20.0, 25.0],\n        [1.0, 5.0],\n        [6.0, 7.0],\n        [15.0, 18.0],\n        [30.0, 35.0],\n    ]\n)\nep1 = nel.EpochArray(times, domain=nel.EpochArray([0, 50]))\n</pre> times = np.array(     [         [1.0, 3.0],         [4.0, 8.0],         [12.0, 13.0],         [20.0, 25.0],         [1.0, 5.0],         [6.0, 7.0],         [15.0, 18.0],         [30.0, 35.0],     ] ) ep1 = nel.EpochArray(times, domain=nel.EpochArray([0, 50])) In\u00a0[6]: Copied! <pre>(ep1 - ep).time\n</pre> (ep1 - ep).time Out[6]: <pre>array([[ 3.,  5.],\n       [ 4.,  5.],\n       [ 6.,  8.],\n       [ 6.,  7.],\n       [15., 18.],\n       [20., 25.],\n       [30., 35.]])</pre> In\u00a0[7]: Copied! <pre>(ep1 + 5).domain\n</pre> (ep1 + 5).domain Out[7]: <pre>&lt;EpochArray at 0x1be4f66cdd0: 1 epoch&gt; of length 50 seconds</pre> In\u00a0[8]: Copied! <pre>def epplt(epoch, expand=0, rshift=0, lshift=0, merge=False, complement=False):\n    fig = plt.figure(figsize=(16, 1))\n    ax = fig.add_subplot(111)\n\n    epoch = epoch + expand\n    epoch = epoch &gt;&gt; rshift\n    epoch = epoch &lt;&lt; lshift\n    if merge:\n        epoch = epoch.merge()\n    if complement:\n        epoch = ~epoch\n    ax = npl.epochplot(epoch)\n    ax.set_xlim(epoch.domain.start, epoch.domain.stop)\n    npl.utils.clear_left_right(ax)\n    npl.utils.clear_top(ax)\n</pre> def epplt(epoch, expand=0, rshift=0, lshift=0, merge=False, complement=False):     fig = plt.figure(figsize=(16, 1))     ax = fig.add_subplot(111)      epoch = epoch + expand     epoch = epoch &gt;&gt; rshift     epoch = epoch &lt;&lt; lshift     if merge:         epoch = epoch.merge()     if complement:         epoch = ~epoch     ax = npl.epochplot(epoch)     ax.set_xlim(epoch.domain.start, epoch.domain.stop)     npl.utils.clear_left_right(ax)     npl.utils.clear_top(ax) In\u00a0[9]: Copied! <pre>interact(\n    epplt,\n    epoch=fixed(ep1),\n    expand=(-0.5, 2.5, 0.1),\n    rshift=(0, 12.5, 0.5),\n    lshift=(0, 12, 0.5),\n    merge=False,\n    complement=False,\n);\n</pre> interact(     epplt,     epoch=fixed(ep1),     expand=(-0.5, 2.5, 0.1),     rshift=(0, 12.5, 0.5),     lshift=(0, 12, 0.5),     merge=False,     complement=False, ); <pre>interactive(children=(FloatSlider(value=0.0, description='expand', max=2.5, min=-0.5), FloatSlider(value=0.0, \u2026</pre> In\u00a0[10]: Copied! <pre>ep1.time\n</pre> ep1.time Out[10]: <pre>array([[ 1.,  3.],\n       [ 1.,  5.],\n       [ 4.,  8.],\n       [ 6.,  7.],\n       [12., 13.],\n       [15., 18.],\n       [20., 25.],\n       [30., 35.]])</pre> In\u00a0[11]: Copied! <pre>(ep1 + 2).time\n</pre> (ep1 + 2).time Out[11]: <pre>array([[-1.,  5.],\n       [-1.,  7.],\n       [ 2., 10.],\n       [ 4.,  9.],\n       [10., 15.],\n       [13., 20.],\n       [18., 27.],\n       [28., 37.]])</pre> <p>Recall that the default domain for any <code>nelpy</code> object is $\\Omega = (-\\infty, \\infty)$. Several set-theoretic operations are available in <code>nelpy</code>, namely:</p> <ul> <li>Set difference: $$A - B := \\{x \\in \\Omega : x\\in A, x \\notin B\\}$$ <code>nelpy: A - B</code></li> <li>Set intersection $$A \\cap B := \\{x \\in \\Omega : x\\in A, x\\in B\\}$$ <code>nelpy: A &amp; B</code> or <code>A[B]</code></li> <li>Set union $$A \\cup B := \\{x \\in \\Omega : x\\in A \\text{ or } x\\in B\\}$$ <code>nelpy: A | B</code></li> <li>Set complement $$A^{\\text{c}} := \\{x \\in \\Omega : x \\notin A\\}$$ <code>nelpy: ~A</code></li> <li>Set merger $$\\text{merge}(A) := \\{x \\in \\Omega : x \\in A\\}$$ <code>nelpy: A.merge()</code></li> <li>Set addition $$A + B := \\{\\omega : \\omega \\in A \\text{ or } \\omega \\in B \\}$$ <code>nelpy: A + B</code></li> </ul> <p>where $\\{x\\}$ are samples, and $\\{\\omega\\}$ are epochs.</p>"},{"location":"tutorials/EpochArrayTutorial/#introduction-to-nelpy-epocharrays","title":"Introduction to <code>nelpy</code> EpochArrays\u00b6","text":""},{"location":"tutorials/EpochArrayTutorial/#epocharray","title":"EpochArray\u00b6","text":""},{"location":"tutorials/EpochArrayTutorial/#overview","title":"Overview\u00b6","text":"<p>In <code>nelpy</code>, all core data objects (<code>EpochArray</code>, <code>AnalogSignalArray</code>, <code>SpiketrainArray</code>, <code>BinnedSpiketrainArray</code>, etc.) have an associated and underlying <code>EpochArray</code>, including <code>EpochArray</code> itself.</p>"},{"location":"tutorials/EpochArrayTutorial/#what-is-an-epoch","title":"What is an Epoch?\u00b6","text":"<p>An Epoch is simply a time interval. More specifically, epochs in <code>nelpy</code> are half-open intervals <code>I=[a, b)</code> measured in seconds. The <code>EpochArray</code> object defines a collection of epochs, along with common operations and transformations that can be performed on them.</p>"},{"location":"tutorials/EpochArrayTutorial/#what-are-epocharrays-used-for","title":"What are EpochArrays used for?\u00b6","text":"<p><code>EpochArray</code>s are fundamental to nelpy because they define when data is defined.</p>"},{"location":"tutorials/EpochArrayTutorial/#example-firing-rate-calculation","title":"Example: Firing Rate Calculation\u00b6","text":"<p>Consider recording action potentials (spikes) from a single neuron with spike times:</p> <pre><code>spike_times = 1, 3, 4, 5, 9, 18 (seconds)\n</code></pre> <p>To compute even the simplest estimates like average firing rate, we need to know: how long did we record for?</p> <ul> <li>If we observed these spike times over 1 minute:<ul> <li>Firing rate = 6 spikes / 60 seconds = 0.1 Hz</li> </ul> </li> <li>If we observed the neuron only for <code>I=[0,20)</code>:<ul> <li>Firing rate = 6 spikes / 20 seconds = 0.3 Hz</li> </ul> </li> </ul> <p>!!! info \"Key Insight\" <code>EpochArrays</code> define when the observed data is valid, making rate calculations and other analyses meaningful.</p>"},{"location":"tutorials/EpochArrayTutorial/#example-data-subset-extraction","title":"Example: Data Subset Extraction\u00b6","text":"<p><code>EpochArrays</code> make it easy to extract subsets of data for further analysis.</p> <p>Scenario:</p> <ul> <li>LFP (local field potential) data on <code>I_LFP=[0,60)</code></li> <li>Animal run speed estimates on <code>I_speed = [3,27)</code></li> <li>Goal: Calculate average LFP only during periods when the animal was running faster than some threshold</li> </ul> <p>Solution:</p> <ol> <li>Determine epochs within <code>I_speed</code> that satisfy the speed criteria</li> <li>Store these epochs in a single <code>EpochArray</code></li> <li>Directly index LFP with this new <code>EpochArray</code> to get the restricted subset</li> </ol> <p>!!! note \"Disclaimer\" This example requires additional objects to be fully implemented, but conceptually this type of operation is very common and is made almost trivial with <code>EpochArrays</code>.</p>"},{"location":"tutorials/EpochArrayTutorial/#support-vs-domain","title":"Support vs Domain\u00b6","text":""},{"location":"tutorials/EpochArrayTutorial/#support","title":"Support\u00b6","text":"<p>While technically a mathematical \"support\" of a function refers to the region on its domain where the function is non-zero, we use the term more loosely here:</p> <p>!!! info \"Nelpy Support Definition\" A <code>nelpy</code> object's support is the region where the object (data) is defined.</p>"},{"location":"tutorials/EpochArrayTutorial/#domain","title":"Domain\u00b6","text":"<p>The domain specifies where the object (data) could be defined.</p> <ul> <li>Default domain: All objects have a domain of <code>\u03a9 = (-\u221e, \u221e)</code></li> <li>Purpose: The domain is necessary when talking about complements</li> <li>Example: In the default domain, the complement of <code>[a, b)</code> is <code>(-\u221e, a) \u222a [b, \u221e)</code></li> </ul>"},{"location":"tutorials/EpochArrayTutorial/#key-distinction","title":"Key Distinction\u00b6","text":"Concept Definition Example Support Where data is defined <code>[0, 10) \u222a [20, 30)</code> Domain Where data could be defined <code>(-\u221e, \u221e)</code> Complement Domain minus support <code>(-\u221e, 0) \u222a [10, 20) \u222a [30, \u221e)</code>"},{"location":"tutorials/EpochArrayTutorial/#set-theoretic-operations","title":"Set-Theoretic Operations\u00b6","text":"<p>Recall that the default domain for any <code>nelpy</code> object is $\\Omega = (-\\infty, \\infty)$. Several set-theoretic operations are available in <code>nelpy</code>:</p>"},{"location":"tutorials/EpochArrayTutorial/#set-difference","title":"Set Difference\u00b6","text":"<p>$A - B := \\{x \\in \\Omega : x\\in A, x \\notin B\\}$</p> <pre># nelpy syntax\nresult = A - B\n</pre>"},{"location":"tutorials/EpochArrayTutorial/#set-intersection","title":"Set Intersection\u00b6","text":"<p>$A \\cap B := \\{x \\in \\Omega : x\\in A, x\\in B\\}$</p> <pre># nelpy syntax (two equivalent forms)\nresult = A &amp; B\nresult = A[B]\n</pre>"},{"location":"tutorials/EpochArrayTutorial/#set-union","title":"Set Union\u00b6","text":"<p>$A \\cup B := \\{x \\in \\Omega : x\\in A \\text{ or } x\\in B\\}$</p> <pre># nelpy syntax\nresult = A | B\n</pre>"},{"location":"tutorials/EpochArrayTutorial/#set-complement","title":"Set Complement\u00b6","text":"<p>$A^{\\text{c}} := \\{x \\in \\Omega : x \\notin A\\}$</p> <pre># nelpy syntax\nresult = ~A\n</pre>"},{"location":"tutorials/EpochArrayTutorial/#set-merger","title":"Set Merger\u00b6","text":"<p>$\\text{merge}(A) := \\{x \\in \\Omega : x \\in A\\}$</p> <pre># nelpy syntax\nresult = A.merge()\n</pre>"},{"location":"tutorials/EpochArrayTutorial/#set-addition","title":"Set Addition\u00b6","text":"<p>$A + B := \\{\\omega : \\omega \\in A \\text{ or } \\omega \\in B \\}$</p> <pre># nelpy syntax\nresult = A + B\n</pre> <p>In the above definitions:</p> <p>$\\{x\\}$ are samples (individual time points) $\\{\\omega \\}$ are epochs (time intervals)</p>"},{"location":"tutorials/EpochArrayTutorial/#importing-nelpy","title":"Importing nelpy\u00b6","text":""},{"location":"tutorials/EpochArrayTutorial/#interacting-with-epocharrays","title":"Interacting with EpochArrays\u00b6","text":"<p>Frequently, <code>EpochArrays</code> will underlie other <code>nelpy</code> objects, and you won't need to worry about interacting with them too much. However, there are cases where direct manipulation and / or interaction can be very useful.</p> <p>First of all, all <code>nelpy</code> objects have human-readable <code>__repr__</code> methods that make it convenient to get a quick summary (and insight) of the data object. Let's take a closer look.</p> <p>We start by creating an <code>EpochArray</code> containing three disjoint intervals, namely <code>[0, 3) \u222a [5, 6) \u222a [10, 15)</code>:</p>"},{"location":"tutorials/EpochArrayTutorial/#todo-add-examples-of-operators-union-intersection-iteration-slicing-indexing-class-attributes-plotting","title":"TODO: add examples of operators, union, intersection, iteration, slicing, indexing, class attributes, plotting, ...\u00b6","text":""},{"location":"tutorials/EpochArrayTutorial/#now-we-add-interactive-widgets-to-explore-transformations-of-epocharrays","title":"Now we add interactive widgets to explore transformations of EpochArrays\u00b6","text":""},{"location":"tutorials/GettingStarted/","title":"Getting Started","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nimport nelpy as nel\nimport nelpy.plotting as npl\n\n# assume default aesthetics\nnpl.setup()\n</pre> import matplotlib.pyplot as plt import numpy as np  import nelpy as nel import nelpy.plotting as npl  # assume default aesthetics npl.setup() In\u00a0[2]: Copied! <pre># Simulate spike times for 3 units\nnp.random.seed(42)\nn_units = 3\nduration = 10.0  # seconds\nspike_times = [np.sort(np.random.uniform(0, duration, size=50)) for _ in range(n_units)]\n\n# Simulate 1D position sampled at 100 Hz\nfs = 100\nt = np.arange(0, duration, 1 / fs)\nposition = np.sin(2 * np.pi * t / duration)  # simple oscillatory trajectory\n</pre> # Simulate spike times for 3 units np.random.seed(42) n_units = 3 duration = 10.0  # seconds spike_times = [np.sort(np.random.uniform(0, duration, size=50)) for _ in range(n_units)]  # Simulate 1D position sampled at 100 Hz fs = 100 t = np.arange(0, duration, 1 / fs) position = np.sin(2 * np.pi * t / duration)  # simple oscillatory trajectory In\u00a0[3]: Copied! <pre># Create EventArray for spikes\nspk = nel.SpikeTrainArray(timestamps=spike_times, fs=fs)\n\n# Create AnalogSignalArray for position\nasa = nel.AnalogSignalArray(data=position, fs=fs)\n\nepoch_1 = nel.EpochArray([3, 4])\nprint(spk)\nprint(asa)\nprint(epoch_1)\n</pre> # Create EventArray for spikes spk = nel.SpikeTrainArray(timestamps=spike_times, fs=fs)  # Create AnalogSignalArray for position asa = nel.AnalogSignalArray(data=position, fs=fs)  epoch_1 = nel.EpochArray([3, 4]) print(spk) print(asa) print(epoch_1) <pre>WARNING:root:creating support from abscissa_vals and sampling rate, fs!\nWARNING:root:'fs' has been deprecated; use 'step' instead\nWARNING:root:some steps in the data are smaller than the requested step size.\n</pre> <pre>&lt;SpikeTrainArray at 0x2912ea09750: 3 units&gt; at 100 Hz\n&lt;AnalogSignalArray at 0x2912f34bb10: 1 signals&gt; for a total of 10 seconds\n&lt;EpochArray at 0x2912f4e6890: 1 epoch&gt; of length 1 seconds\n</pre> In\u00a0[4]: Copied! <pre>fig, ax = plt.subplots(2, 1, figsize=(14, 5), sharex=True)\n\n# use nelpy to plot the spike raster\nnpl.rasterplot(spk, ax=ax[0])\nax[0].set_ylabel(\"unit\")\nax[0].set_title(\"Spike Raster\")\n\nnpl.plot(asa, ax=ax[1])\n\nax[1].set_xlabel(\"time [seconds]\")\nax[1].set_ylabel(\"Position\")\nax[1].set_title(\"Position Trace\")\nplt.show()\n</pre> fig, ax = plt.subplots(2, 1, figsize=(14, 5), sharex=True)  # use nelpy to plot the spike raster npl.rasterplot(spk, ax=ax[0]) ax[0].set_ylabel(\"unit\") ax[0].set_title(\"Spike Raster\")  npl.plot(asa, ax=ax[1])  ax[1].set_xlabel(\"time [seconds]\") ax[1].set_ylabel(\"Position\") ax[1].set_title(\"Position Trace\") plt.show() In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots(2, 1, figsize=(14, 5), sharex=True)\n\n# use nelpy to plot the spike raster\nnpl.rasterplot(spk[epoch_1], ax=ax[0])\nax[0].set_ylabel(\"unit\")\nax[0].set_title(\"Spike Raster\")\n\nnpl.plot(asa[epoch_1], ax=ax[1])\n\nax[1].set_xlabel(\"time [seconds]\")\nax[1].set_ylabel(\"Position\")\nax[1].set_title(\"Position Trace\")\nplt.show()\n</pre> fig, ax = plt.subplots(2, 1, figsize=(14, 5), sharex=True)  # use nelpy to plot the spike raster npl.rasterplot(spk[epoch_1], ax=ax[0]) ax[0].set_ylabel(\"unit\") ax[0].set_title(\"Spike Raster\")  npl.plot(asa[epoch_1], ax=ax[1])  ax[1].set_xlabel(\"time [seconds]\") ax[1].set_ylabel(\"Position\") ax[1].set_title(\"Position Trace\") plt.show() <pre>WARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\n</pre> In\u00a0[6]: Copied! <pre># Bin spikes\nbin_size = 0.1  # seconds\nbinned = spk.bin(ds=bin_size)\n\n# Compute firing rate (spikes per bin / bin size)\nfiring_rate = binned.data / bin_size\nbin_centers = binned.bin_centers\n\n# Plot firing rate for first unit\nfig, ax = plt.subplots(2, 1, figsize=(14, 5), sharex=True)\nax[0].plot(bin_centers, firing_rate[0], label=\"Unit 0\")\nax[0].set_ylabel(\"Firing Rate (Hz)\")\nax[0].set_title(\"Binned Firing Rate (Unit 0)\")\n\nnpl.plot(asa, ax=ax[1])\n\nax[1].set_xlabel(\"time [seconds]\")\nax[1].set_ylabel(\"Position\")\nax[1].set_title(\"Position Trace\")\n\nplt.show()\n</pre> # Bin spikes bin_size = 0.1  # seconds binned = spk.bin(ds=bin_size)  # Compute firing rate (spikes per bin / bin size) firing_rate = binned.data / bin_size bin_centers = binned.bin_centers  # Plot firing rate for first unit fig, ax = plt.subplots(2, 1, figsize=(14, 5), sharex=True) ax[0].plot(bin_centers, firing_rate[0], label=\"Unit 0\") ax[0].set_ylabel(\"Firing Rate (Hz)\") ax[0].set_title(\"Binned Firing Rate (Unit 0)\")  npl.plot(asa, ax=ax[1])  ax[1].set_xlabel(\"time [seconds]\") ax[1].set_ylabel(\"Position\") ax[1].set_title(\"Position Trace\")  plt.show()"},{"location":"tutorials/GettingStarted/#getting-started-with-nelpy","title":"Getting Started with Nelpy\u00b6","text":"<p>Welcome to nelpy! This notebook will guide you through the basics of using nelpy for neural data analysis.</p> <p>We'll cover installation, basic data structures, and a minimal workflow.</p>"},{"location":"tutorials/GettingStarted/#installation","title":"Installation\u00b6","text":"<p>If you haven't already, install nelpy (and dependencies) using pip or conda.</p> <pre>pip install nelpy\n</pre> <p>Or, for the latest development version:</p> <pre>pip install git+https://github.com/nelpy/nelpy.git\n</pre>"},{"location":"tutorials/GettingStarted/#importing-nelpy","title":"Importing nelpy\u00b6","text":"<p>Let's import the main nelpy modules.</p>"},{"location":"tutorials/GettingStarted/#simulate-or-load-data","title":"Simulate or Load Data\u00b6","text":"<p>For this demo, we'll simulate some spike times and position data.</p>"},{"location":"tutorials/GettingStarted/#create-nelpy-objects","title":"Create Nelpy Objects\u00b6","text":"<p>Let's create an SpikeTrainArray for spikes and an AnalogSignalArray for position.</p>"},{"location":"tutorials/GettingStarted/#basic-plotting","title":"Basic Plotting\u00b6","text":"<p>Let's plot the spike raster and position trace.</p>"},{"location":"tutorials/GettingStarted/#slice-the-data-with-epocharray","title":"slice the data with EpochArray\u00b6","text":""},{"location":"tutorials/GettingStarted/#minimal-end-to-end-workflow","title":"Minimal End-to-End Workflow\u00b6","text":"<p>Let's bin the spikes, compute a simple firing rate, and plot it alongside position.</p>"},{"location":"tutorials/GettingStarted/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Explore the API Reference for more details on nelpy objects.</li> <li>Try running the other tutorials in the <code>tutorials/</code> folder.</li> <li>Check out the GitHub repo for more info and updates.</li> </ul> <p>Happy analyzing!</p>"},{"location":"tutorials/LinearTrackDemo/","title":"Linear Track Demo","text":"<p>Here we load sorted unit data obtained from MatClust to identify putative place cells. The session has about 15 minutes of linear track running, followed by 15 minutes of rest.</p> <p>Data was recorded by Joshua Chu, with a Spikegadgets wireless headstage, on July 8th, 2017, from the CA1 area of a male Long-Evans rat named <code>install</code>.</p> <p>Notebook was created by Etienne Ackermann.</p> <p>We will look for data in the <code>example-data\\linear-track\\</code> directory inside your current working directory. If the data doesn't exist, we will download it from https://github.com/nelpy/example-data, and save it to your local machine.</p> <p>If you already have the data, it won't be downloaded again.</p> <p>In particular, we will download two files, namely</p> <ol> <li><code>trajectory.videoPositionTracking</code> which is a binary file with (x,y) position coordinate pairs and timestamps, and</li> <li><code>spikes.mat</code> which is a Matlab file containing information about sorted units (cells) obtained by using MatClust (https://bitbucket.org/mkarlsso/matclust).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport requests\n\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm\n\ndatadir = os.path.join(os.getcwd(), \"example-data\\linear-track\")\nos.makedirs(datadir, exist_ok=True)\n\nfilenames = []\nfilenames.append(os.path.join(datadir, \"trajectory.videoPositionTracking\"))\nfilenames.append(os.path.join(datadir, \"spikes.mat\"))\nurls = []\nurls.append(\n    \"https://github.com/nelpy/example-data/raw/master/linear-track/trajectory.videoPositionTracking\"\n)\nurls.append(\"https://github.com/nelpy/example-data/raw/master/linear-track/spikes.mat\")\n\nfor filename, url in zip(filenames, urls):\n    if os.path.exists(filename):\n        print(\"you already have the example data, skipping download...\")\n    else:\n        print(\"downloading data from {}\".format(url))\n        # Streaming, so we can iterate over the response.\n        r = requests.get(url, stream=True)\n\n        # Total size in bytes.\n        total_size = int(r.headers.get(\"content-length\", 0))\n        chunk_size = 1024  # number of bytes to process at a time (NOTE: progress bar unit only accurate if this is 1 kB)\n\n        with open(filename, \"wb+\") as f:\n            for data in tqdm(\n                r.iter_content(chunk_size),\n                total=int(total_size / chunk_size),\n                unit=\"kB\",\n            ):\n                f.write(data)\n\n        print(\"data saved to local directory {}\".format(filename))\n\nfilename_pos = filenames[0]\nfilename_spikes = filenames[1]\n</pre> import os  import requests  # from tqdm import tqdm_notebook as tqdm from tqdm import tqdm  datadir = os.path.join(os.getcwd(), \"example-data\\linear-track\") os.makedirs(datadir, exist_ok=True)  filenames = [] filenames.append(os.path.join(datadir, \"trajectory.videoPositionTracking\")) filenames.append(os.path.join(datadir, \"spikes.mat\")) urls = [] urls.append(     \"https://github.com/nelpy/example-data/raw/master/linear-track/trajectory.videoPositionTracking\" ) urls.append(\"https://github.com/nelpy/example-data/raw/master/linear-track/spikes.mat\")  for filename, url in zip(filenames, urls):     if os.path.exists(filename):         print(\"you already have the example data, skipping download...\")     else:         print(\"downloading data from {}\".format(url))         # Streaming, so we can iterate over the response.         r = requests.get(url, stream=True)          # Total size in bytes.         total_size = int(r.headers.get(\"content-length\", 0))         chunk_size = 1024  # number of bytes to process at a time (NOTE: progress bar unit only accurate if this is 1 kB)          with open(filename, \"wb+\") as f:             for data in tqdm(                 r.iter_content(chunk_size),                 total=int(total_size / chunk_size),                 unit=\"kB\",             ):                 f.write(data)          print(\"data saved to local directory {}\".format(filename))  filename_pos = filenames[0] filename_spikes = filenames[1] <pre>you already have the example data, skipping download...\nyou already have the example data, skipping download...\n</pre> In\u00a0[\u00a0]: Copied! <pre>import struct\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport nelpy as nel\nimport nelpy.plotting as npl\n\n# assume default aesthetics\nnpl.setup()\n\n%matplotlib inline\n</pre> import struct  import matplotlib.pyplot as plt import numpy as np  import nelpy as nel import nelpy.plotting as npl  # assume default aesthetics npl.setup()  %matplotlib inline <pre>C:\\Users\\etien\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:106: MatplotlibDeprecationWarning: The mpl_toolkits.axes_grid module was deprecated in version 2.1. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist provies the same functionality instead.\n</pre> <p>The position data is stored in a binary file, and we already made a reference to it as <code>filename_pos</code>, which should point to <code>./example-data/linear-track/trajectory.videoPositionTracking</code>, inside your current working directory.</p> <p>More information about the format of the file can be obtained at https://github.com/nelpy/example-data/tree/master/linear-track .</p> <p>Here we define a simple function to read the plaintext header from the file, since the header also informs us of the format of the rest of the data.</p> In\u00a0[3]: Copied! <pre>def print_header(filename, timeout=50):\n    \"\"\"Reads header lines from a SpikeGadgets .rec file, and prints it to screen.\"\"\"\n    linecount = 0\n    with open(filename, \"rb\") as fileobj:\n        instr = fileobj.readline()\n        linecount += 1\n        while instr != b\"&lt;End settings&gt;\\n\":\n            print(instr)\n            instr = fileobj.readline()\n            if linecount &gt; timeout:\n                break\n    print(instr)\n</pre> def print_header(filename, timeout=50):     \"\"\"Reads header lines from a SpikeGadgets .rec file, and prints it to screen.\"\"\"     linecount = 0     with open(filename, \"rb\") as fileobj:         instr = fileobj.readline()         linecount += 1         while instr != b\"\\n\":             print(instr)             instr = fileobj.readline()             if linecount &gt; timeout:                 break     print(instr) In\u00a0[4]: Copied! <pre>print_header(filename_pos)\n</pre> print_header(filename_pos) <pre>b'&lt;Start settings&gt;\\n'\nb'threshold: 199\\n'\nb'dark: 0\\n'\nb'clockrate: 30000\\n'\nb'camera resolution: 640x480\\n'\nb'pixel scale: 0 pix/cm\\n'\nb'Fields: &lt;time uint32&gt;&lt;xloc uint16&gt;&lt;yloc uint16&gt;&lt;xloc2 uint16&gt;&lt;yloc2 uint16&gt;\\n'\nb'&lt;End settings&gt;\\n'\n</pre> <p>Armed with this new information, we read over the header, and then extract (1) the 32 bit unsigned <code>timestamp</code>, as well as (2) the 16 bit unsigned position data into lists <code>x1</code> and <code>y1</code> (<code>x2</code> and <code>y2</code> were not used in this recording, and can be ignored).</p> In\u00a0[5]: Copied! <pre>n_packets = 500000\ntimestamps = []\nx1 = []\ny1 = []\nx2 = []\ny2 = []\nii = 0\nwith open(filename_pos, \"rb\") as fileobj:\n    instr = fileobj.readline()\n\n    n_max_header_lines = 50\n    hh = 0\n    # consume all header lines\n    while instr != b\"&lt;End settings&gt;\\n\":\n        hh += 1\n        instr = fileobj.readline()\n        if hh &gt; n_max_header_lines:\n            print(\"End of header not found! Aborting...\")\n            break\n    for packet in iter(lambda: fileobj.read(12), \"\"):\n        if packet:\n            ts_ = struct.unpack(\"&lt;L\", packet[0:4])[0]\n            x1_ = struct.unpack(\"&lt;H\", packet[4:6])[0]\n            y1_ = struct.unpack(\"&lt;H\", packet[6:8])[0]\n            x2_ = struct.unpack(\"&lt;H\", packet[8:10])[0]\n            y2_ = struct.unpack(\"&lt;H\", packet[10:12])[0]\n            timestamps.append(ts_)\n            x1.append(x1_)\n            y1.append(y1_)\n            x2.append(x2_)\n            y2.append(y2_)\n        else:\n            break\n        if ii &gt;= n_packets:\n            print(\"Stopped before reaching end of file\")\n            break\n</pre> n_packets = 500000 timestamps = [] x1 = [] y1 = [] x2 = [] y2 = [] ii = 0 with open(filename_pos, \"rb\") as fileobj:     instr = fileobj.readline()      n_max_header_lines = 50     hh = 0     # consume all header lines     while instr != b\"\\n\":         hh += 1         instr = fileobj.readline()         if hh &gt; n_max_header_lines:             print(\"End of header not found! Aborting...\")             break     for packet in iter(lambda: fileobj.read(12), \"\"):         if packet:             ts_ = struct.unpack(\"= n_packets:             print(\"Stopped before reaching end of file\")             break <p>Remark: up to this point, we have not used <code>nelpy</code> yet. We have our position as two lists <code>x1</code> and <code>y1</code>, along with a list of <code>timestamps</code>. But working with these lists of coordinates and timestamps directly can be tedious and error-prone. Consequently, we will wrap the data into <code>nelpy</code> objects to make our life easier.</p> <p>First, we will estimate epochs during which the animal was supposed to be running on the track, and those for which the animal was in its sleep box. Note that this task can be a little tricky to estimate directly from the data, since the animal is not always running while on the track, and may in fact just be stationary for significant periods.</p> <p>We can (and probably should) of course get the experimantal epochs (that of running on the track, and that of resting in the sleep box) from our lab notes, but it's always good to make sure that your data agrees with your notes, and moreover, it demonstrates how we can use <code>nelpy</code> to estimate these epochs in case our notes were lost, or inaccurate.</p> In\u00a0[6]: Copied! <pre># we estimate large periods of inactivity as periods where the animal's estimated position did not move for at least 10 seconds\nminLength = 600  # 10 seconds @ 60 fps\nbounds, _, _ = nel.utils.get_events_boundaries(\n    np.gradient(x1),\n    PrimaryThreshold=0,\n    SecondaryThreshold=0,\n    mode=\"below\",\n    minLength=minLength,\n    ds=1,\n)\n\n# bounds are now in sample numbers, so we convert them to time using timestamps\nFS = 30000\nbounds_ts = np.zeros(bounds.shape)\nfor row in range(len(bounds)):\n    for col in range(2):\n        bounds_ts[row, col] = timestamps[bounds[row, col]]\n</pre> # we estimate large periods of inactivity as periods where the animal's estimated position did not move for at least 10 seconds minLength = 600  # 10 seconds @ 60 fps bounds, _, _ = nel.utils.get_events_boundaries(     np.gradient(x1),     PrimaryThreshold=0,     SecondaryThreshold=0,     mode=\"below\",     minLength=minLength,     ds=1, )  # bounds are now in sample numbers, so we convert them to time using timestamps FS = 30000 bounds_ts = np.zeros(bounds.shape) for row in range(len(bounds)):     for col in range(2):         bounds_ts[row, col] = timestamps[bounds[row, col]] <p>Here we've used <code>nelpy</code> to get event boundaries, in sample numbers, where the animal spent at least 10 seconds with zero change in its <code>x1</code> coordinate. We could do this, because the tracker does not track the animal when it is in the sleep box (it is outside of the camera's view).</p> <p>Next, we create our first <code>nelpy</code> object, namely an <code>EpochArray</code> containing all the epochs defined by the bounds returned above.</p> <p>In general it is not necessary to sort epochs like we do below, but we have to do that here because the way we build our <code>session_epochs</code> is a little convoluted: we build it by first estimating sleep box epochs, and then combine the sleep box epochs with the complement of the sleep box epochs. When we take a union of epochs, they are no longer guaranteed to be in sorted order, and so we sort them for good measure.</p> In\u00a0[7]: Copied! <pre>sleep_box = nel.EpochArray(\n    bounds_ts / FS, domain=nel.EpochArray((timestamps[0] / FS, timestamps[-1] / FS))\n)\nsession_epochs = (\n    sleep_box + ~sleep_box\n)  # the entire session includes when the animal was in its sleep box,\n# and when it was not(~) in its sleep box\nsession_epochs._sort()\n</pre> sleep_box = nel.EpochArray(     bounds_ts / FS, domain=nel.EpochArray((timestamps[0] / FS, timestamps[-1] / FS)) ) session_epochs = (     sleep_box + ~sleep_box )  # the entire session includes when the animal was in its sleep box, # and when it was not(~) in its sleep box session_epochs._sort() <p>We can always inspect <code>nelpy</code> objects by printing them to screen like so:</p> In\u00a0[8]: Copied! <pre>print(session_epochs)\n</pre> print(session_epochs) <pre>&lt;EpochArray at 0x2a5cc455550: 3 epochs&gt; of length 33:02:423 minutes\n</pre> <p>Update: we used to have three epochs as a result of the above construction, but somewhere along the line we changed an implementation detail in nelpy so that when contiguous epochs are added together, they are automatically merged. This is sometimes very useful, and sometimes quite frustrating. It's unclear which approach is superior, but the automatically merged case is better for a majority of underlying epoch array processing functions. If this is an issue, you can always subtract epsilon from the right endpoint of your epochs to prevent them from being viewed as contiguous. This is a hacky-but-effective fix, with very low chances of having any unintended consequences.</p> <p>It used to be that we had three resultant epochs, two in the rest state, and one while running on the track. But since all the epochs are contiguous, we now only have a single epoch. Consequently, to demonstrate what I wanted to demonstrate (before the change in nelpy), I'll artificially reduce the lengths of the epochs such that they are no longer contiguous:</p> In\u00a0[9]: Copied! <pre>session_epochs = sleep_box.shrink(0.000001, direction=\"stop\") + (~sleep_box).shrink(\n    0.000001, direction=\"stop\"\n)  # hacky \"fix\" described above\nprint(session_epochs)\n</pre> session_epochs = sleep_box.shrink(0.000001, direction=\"stop\") + (~sleep_box).shrink(     0.000001, direction=\"stop\" )  # hacky \"fix\" described above print(session_epochs) <pre>&lt;EpochArray at 0x2a5d01934e0: 3 epochs&gt; of length 33:02:423 minutes\n</pre> <p>which tells us that we have 3 epochs, with a total duration of approximately 30 minutes.</p> <p>HOWEVER! We expected to only have two epochs! One 15 minute run, followed by a 15 minute sleep box session. So what happened? Well, we can have a closer look at the epochs, by printing out their durations:</p> In\u00a0[10]: Copied! <pre>for ep in session_epochs:\n    print(ep.duration)\n</pre> for ep in session_epochs:     print(ep.duration) <pre>25.8235 seconds\n15:59:115 minutes\n16:37:485 minutes\n</pre> <p>We see that the first epoch is only about 25 seconds long, and this corresponds to when we moved the animal onto the track, so we're happy to ignore that particular epoch, and focus on the other two.</p> <p>Note also that <code>nelpy</code> returned a nice human readable duration for each epoch. Under the hood, the durations are stored (calculated, more accurately) in seconds, and we could have computed it ourselves like so:</p> In\u00a0[11]: Copied! <pre>print(\n    session_epochs.time\n)  # .time returns the bounds defining the epochs, in seconds; each row is [start, stop]\n</pre> print(     session_epochs.time )  # .time returns the bounds defining the epochs, in seconds; each row is [start, stop] <pre>[[4397.0317     4422.85523233]\n [4422.85523333 5381.970299  ]\n [5381.9703     6379.455599  ]]\n</pre> In\u00a0[12]: Copied! <pre>print(\n    session_epochs.time[:, 1] - session_epochs.time[:, 0]\n)  # durations are stops - starts\n</pre> print(     session_epochs.time[:, 1] - session_epochs.time[:, 0] )  # durations are stops - starts <pre>[ 25.82353233 959.11506567 997.485299  ]\n</pre> <p>We see that the durations that we computed above agree with the ones we printed out earlier, but 15:59:115 minutes is arguably easier to parse than 959.11506667 seconds.</p> <p>In fact, we don't even really need to call <code>print()</code> on many of the objects, and we can just invoke them to see a representation of the objects. Let's try that on our <code>sleep_box</code> object:</p> In\u00a0[13]: Copied! <pre>sleep_box\n</pre> sleep_box Out[13]: <pre>&lt;EpochArray at 0x2a5d017e710: 2 epochs&gt; of length 17:03:308 minutes</pre> <p>We see that (as we already know) there are two epochs during which we think the animal was in its sleep box, namely the first 25 seconds, and then the last 16 minutes of the recording session.</p> <p>We can similarly look at the complement of the sleep box epochs, which should then be (naturally) when the animal was on the linear track:</p> In\u00a0[14]: Copied! <pre>~sleep_box\n</pre> ~sleep_box Out[14]: <pre>&lt;EpochArray at 0x2a5d01b3518: 1 epoch&gt; of length 15:59:115 minutes</pre> <p>Next we will use our lists <code>x1</code> and <code>y1</code> to build a <code>nelpy</code> <code>AnalogSignalArray</code>. This is pretty easy: we pass in the position coordinates as a 2 x n_samples array, along with the corresponding timestamps, and we specify that we want this object to be defined on the epoch where the animal was NOT in its sleep box, i.e., when the animal was on the track.</p> <p>Note also that we defined the trajectory object on a slightly smaller epoch, namely we shrunk the epoch by 20 seconds from both directions. We did this in order to be sure that the first and last parts of the trajectory were not those where we were moving the animal onto and off of the track.</p> In\u00a0[15]: Copied! <pre>pos = nel.AnalogSignalArray(\n    np.vstack((x1, y1)),\n    timestamps=np.array(timestamps) / FS,\n    support=(~sleep_box).shrink(20),\n    fs=60,\n)\n</pre> pos = nel.AnalogSignalArray(     np.vstack((x1, y1)),     timestamps=np.array(timestamps) / FS,     support=(~sleep_box).shrink(20),     fs=60, ) <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:789: UserWarning: ignoring signal outside of support\n</pre> In\u00a0[16]: Copied! <pre>pos  # inspect pos object to see if everything looks good\n</pre> pos  # inspect pos object to see if everything looks good Out[16]: <pre>&lt;AnalogSignalArray at 0x2a5d102b3c8: 2 signals&gt; for a total of 15:19:115 minutes</pre> <p>We can plot the position/trajectory object like so:</p> In\u00a0[17]: Copied! <pre>npl.plot2d(pos)  # plot the entire trajectory\nax = plt.gca()\nax.set_aspect(\"equal\")\n</pre> npl.plot2d(pos)  # plot the entire trajectory ax = plt.gca() ax.set_aspect(\"equal\") <p>Recall that the linear track epoch ranged from approcimately 4420 to 5380 seconds (We can always get these by simply inspecting pos again, like blow):</p> In\u00a0[18]: Copied! <pre>pos.support.time  # print the time boundaries on which pos is defined\n</pre> pos.support.time  # print the time boundaries on which pos is defined Out[18]: <pre>array([[4442.85523333, 5361.9703    ]])</pre> <p>and so we can easily plot the trajectory, say, for the first 30 seconds while the animal was on the track. One way to do this, is to define an epoch, and then to restrict the position object to that newly defined epoch. Let's do this now:</p> In\u00a0[19]: Copied! <pre>ep = nel.EpochArray([4442, 4472])\nep\n</pre> ep = nel.EpochArray([4442, 4472]) ep Out[19]: <pre>&lt;EpochArray at 0x2a5d01b3ef0: 1 epoch&gt; of length 30 seconds</pre> In\u00a0[20]: Copied! <pre>npl.plot2d(pos)  # plot the entire trajectory\nnpl.plot2d(pos[ep], color=\"k\")  # plot pos restricted to ep\nax = plt.gca()\nax.set_aspect(\"equal\")\n</pre> npl.plot2d(pos)  # plot the entire trajectory npl.plot2d(pos[ep], color=\"k\")  # plot pos restricted to ep ax = plt.gca() ax.set_aspect(\"equal\") <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:789: UserWarning: ignoring signal outside of support\n</pre> <p>This way of filtering / restricting the data is very powerful, and we will use it later on to get spikes only during times when the animal was running faster than some threshold, and so on.</p> <p>Before we move onto the next phase (linearization), let's take a quick aside to make the plot above a bit nicer. First, we may want to set the aspect ratio to equal, and we may also want to add titles, labels, etc. We may even want to smooth the trajectory.</p> In\u00a0[21]: Copied! <pre>sigmas = [0, 0.1, 0.3]  # smooth trajectory in time, with sigma = 100 ms (=0.1 seconds)\n\nfig, axes = plt.subplots(ncols=3, figsize=(16, 4))\n\nfor sigma, ax in zip(sigmas, axes):\n    npl.plot2d(pos.smooth(sigma=sigma), lw=0.5, color=\"0.3\", ax=ax)\n    ax.set_aspect(\"equal\")\n    ax.set_xlabel(\"x position\")\n    ax.set_ylabel(\"y position\")\n    npl.utils.clear_right(ax)\n    npl.utils.clear_top(ax)\n    ax.set_title(\"Smoothed trajectory, $\\sigma$={} ms\".format(round(sigma * 1000)))\n</pre> sigmas = [0, 0.1, 0.3]  # smooth trajectory in time, with sigma = 100 ms (=0.1 seconds)  fig, axes = plt.subplots(ncols=3, figsize=(16, 4))  for sigma, ax in zip(sigmas, axes):     npl.plot2d(pos.smooth(sigma=sigma), lw=0.5, color=\"0.3\", ax=ax)     ax.set_aspect(\"equal\")     ax.set_xlabel(\"x position\")     ax.set_ylabel(\"y position\")     npl.utils.clear_right(ax)     npl.utils.clear_top(ax)     ax.set_title(\"Smoothed trajectory, $\\sigma$={} ms\".format(round(sigma * 1000))) <p>We can do the rest of our analysis in 2D, but sometimes it's nicer to linearize the position data first. Here, we simply use PCA (a dimensionality reduction technique) to find the direction of maximal variance in our position data, and we use this as our new 1D linear track axis.</p> <p>NOTE: To use PCA from scikit-learn, we need to provide our data as an n_samples x n_features matrix. The internal representation of the <code>nelpy</code> <code>AnalogSignalArray</code> can actually be changed, but we can always explicitly ask to get the underlying data back in a particular format. We do this, by asking for <code>_ydata_colsig</code>, which means that we want each feature (or signal, or component) to be a column of the data matrix. Here you shouldn't be concerned with the favt that we asked for <code>_ydata</code>: this is simply an internal naming convention of the <code>AnalogSignalArray</code>, and has nothing to do with our <code>x</code> and <code>y</code> coordinates. More precisely, <code>_ydata</code> refers to the entire data matrix, in contrast to <code>_tdata</code> which stores the timestamp info. So <code>_ydata</code> contains both <code>x1</code> and <code>y1</code>.</p> In\u00a0[22]: Copied! <pre>from sklearn.decomposition import PCA\n\nX = pos._ydata_colsig  # we access the\npca = PCA(n_components=1)\nXlinear = pca.fit_transform(X)\n</pre> from sklearn.decomposition import PCA  X = pos._ydata_colsig  # we access the pca = PCA(n_components=1) Xlinear = pca.fit_transform(X) In\u00a0[23]: Copied! <pre>Xlinear_ = pca.inverse_transform(Xlinear)\nax = npl.plot2d(pos, lw=0.5, color=\"0.8\", label=\"original trajectory\")\nplt.plot(Xlinear_[:, 0], Xlinear_[:, 1], color=\"0.2\", label=\"linearization\")\nplt.legend()\nax.set_aspect(\"equal\")\nax.set_xlabel(\"x position\")\nax.set_ylabel(\"y position\")\nnpl.utils.clear_right(ax)\nnpl.utils.clear_top(ax)\nax.set_title(\"linearization using PCA\")\n</pre> Xlinear_ = pca.inverse_transform(Xlinear) ax = npl.plot2d(pos, lw=0.5, color=\"0.8\", label=\"original trajectory\") plt.plot(Xlinear_[:, 0], Xlinear_[:, 1], color=\"0.2\", label=\"linearization\") plt.legend() ax.set_aspect(\"equal\") ax.set_xlabel(\"x position\") ax.set_ylabel(\"y position\") npl.utils.clear_right(ax) npl.utils.clear_top(ax) ax.set_title(\"linearization using PCA\") Out[23]: <pre>Text(0.5,1,'linearization using PCA')</pre> <p>We also re-scale our data to range from 0 to 100 (so that we can express movement along the track as a percentage). Typically we would actually scale our data to some physical dimension or unit, such as cm, and not pixels as we have it above, or percentage, as we're transforming it into here.</p> In\u00a0[24]: Copied! <pre>Xlinear = Xlinear - np.min(Xlinear)\nXlinear = (Xlinear / np.max(Xlinear)) * 100\n</pre> Xlinear = Xlinear - np.min(Xlinear) Xlinear = (Xlinear / np.max(Xlinear)) * 100 <p>We now have <code>Xlinear</code> as a scaled (0 to 100) position variable, as a numpy array. We want to put it back into a <code>nelpy</code> <code>AnalogSignalArray</code> container, so that we can slice and interogate it as before:</p> In\u00a0[25]: Copied! <pre>pos1d = nel.AnalogSignalArray(Xlinear, timestamps=pos.time, support=pos.support, fs=60)\nprint(\"pos:  \", pos)\nprint(\"pos1d:\", pos1d)\n</pre> pos1d = nel.AnalogSignalArray(Xlinear, timestamps=pos.time, support=pos.support, fs=60) print(\"pos:  \", pos) print(\"pos1d:\", pos1d) <pre>pos:   &lt;AnalogSignalArray at 0x2a5d102b3c8: 2 signals&gt; for a total of 15:19:115 minutes\npos1d: &lt;AnalogSignalArray at 0x2a5d20df160: 1 signals&gt; for a total of 15:19:115 minutes\n</pre> <p>We see that <code>pos1d</code> now only has 1 signal, as expected, and if we plot it, we can confirm that it ranges between 0 and 100:</p> In\u00a0[26]: Copied! <pre>fig = plt.figure(figsize=(15, 4))\nnpl.plot(pos1d)\nax = plt.gca()\nax.set_xlabel(\"time [s]\")\nax.set_ylabel(\"linearized position (%)\")\nnpl.utils.clear_right(ax)\nnpl.utils.clear_top(ax)\n</pre> fig = plt.figure(figsize=(15, 4)) npl.plot(pos1d) ax = plt.gca() ax.set_xlabel(\"time [s]\") ax.set_ylabel(\"linearized position (%)\") npl.utils.clear_right(ax) npl.utils.clear_top(ax) <p>We recorded extracellular activity in region CA1 of the hippocampus from a male Long-Evans rat, and we already performed spike detection and sorting (clusterting) in a separate program named MatClust (https://bitbucket.org/mkarlsso/matclust). MatClust is Matlab-based, and it gives us a .mat file with containing our sorted units and their associated spike times.</p> <p>Luckily, we can read .mat files directly into Python, but the file format is still a little clunky, and so we need to parse it into a more intuitive format fisrt. For more information on the format of the .mat file, you can refer to https://github.com/nelpy/example-data/tree/master/linear-track.</p> <p>At any rate, we extract spike times below, so that we end up with a list of lists, where the ith list is a list of the spike times associated with the ith unit. For example,</p> <pre><code>spikes = [[1, 2, 5, 10], [3, 5], [2, 9, 20]]\n</code></pre> <p>contains three units (three inner lists), with the spike times for the 1st unit being <code>[1, 2, 5, 10]</code> and those for the last unit being <code>[2, 9, 20]</code>. Spike times are given in seconds, so floating point numbers are used.</p> In\u00a0[27]: Copied! <pre># load matlab file contining sorted spikes\nmat = nel.io.matlab.load(filename_spikes)\n\nunit_id_to_matclust = dict()\n\n# parse mat file contents into list of lists as described above\nspikes = []\nct = 0\nnum_array = 0\nfor ii, array in enumerate(mat[\"spikes\"]):\n    # If empty array, that particular tetrode was not sorted\n    if array.size &gt; 1:\n        for jj, subarray in enumerate(array):\n            if subarray.size != 0:\n                # Exclude tetrodes with no spikes\n                if len(subarray[\"time\"].ravel()[0]) != 0:\n                    spikes.append(subarray[\"time\"].ravel()[0])\n                    unit_id_to_matclust[ct + 1] = ii\n                    ct += 1\n    elif array.size == 1:\n        if len(array[\"time\"].ravel()[0]) != 0:\n            spikes.append(array[\"time\"].ravel()[0])\n            unit_id_to_matclust[ct + 1] = ii\n            ct += 1\nprint(\"Found {} non-empty units total\".format(ct))\n\n# unit_id_to_matclust\n</pre> # load matlab file contining sorted spikes mat = nel.io.matlab.load(filename_spikes)  unit_id_to_matclust = dict()  # parse mat file contents into list of lists as described above spikes = [] ct = 0 num_array = 0 for ii, array in enumerate(mat[\"spikes\"]):     # If empty array, that particular tetrode was not sorted     if array.size &gt; 1:         for jj, subarray in enumerate(array):             if subarray.size != 0:                 # Exclude tetrodes with no spikes                 if len(subarray[\"time\"].ravel()[0]) != 0:                     spikes.append(subarray[\"time\"].ravel()[0])                     unit_id_to_matclust[ct + 1] = ii                     ct += 1     elif array.size == 1:         if len(array[\"time\"].ravel()[0]) != 0:             spikes.append(array[\"time\"].ravel()[0])             unit_id_to_matclust[ct + 1] = ii             ct += 1 print(\"Found {} non-empty units total\".format(ct))  # unit_id_to_matclust <pre>Found 31 non-empty units total\n</pre> <p>As before, we now put the list of spike times into a <code>nelpy</code> container object to make it easier to interact with the data. In particular, we put the spikes into a <code>SpikeTrainArray</code>. But first, let's also get out the time boundaries for which the spikes were recorded. We can approximate these boundaries by looking for the first and last recorded spikes, but the .mat file actually contains this information explicitly, so we'll get it directly from there:</p> In\u00a0[\u00a0]: Copied! <pre>mattdtype = np.dtype([(\"time\", \"O\"), (\"timerange\", \"O\"), (\"meanrate\", \"O\")])\nsingleidx = 0\nmultiidx = 0\nfor nn in range(len(mat[\"spikes\"])):\n    if mat[\"spikes\"][nn].dtype == mattdtype:\n        singleidx = nn\n        start, stop = mat[\"spikes\"][singleidx][\"timerange\"].ravel()[0]\n        break\nif singleidx:\n    print(\"singleidx\")\n    start, stop = mat[\"spikes\"][singleidx][\"timerange\"].ravel()[0]\nelse:\n    for nn in range(len(mat[\"spikes\"])):\n        try:\n            for mm in range(len(mat[\"spikes\"][nn])):\n                if mat[\"spikes\"][nn][mm].dtype == mattdtype:\n                    singleidx = nn\n                    multiidx = mm\n                    start, stop = mat[\"spikes\"][singleidx][multiidx][\n                        \"timerange\"\n                    ].ravel()[0]\n                    break\n        except TypeError:\n            continue\n\nif multiidx:\n    print(\"multiidx\")\n    start, stop = mat[\"spikes\"][singleidx][multiidx][\"timerange\"].ravel()[0]\n</pre> mattdtype = np.dtype([(\"time\", \"O\"), (\"timerange\", \"O\"), (\"meanrate\", \"O\")]) singleidx = 0 multiidx = 0 for nn in range(len(mat[\"spikes\"])):     if mat[\"spikes\"][nn].dtype == mattdtype:         singleidx = nn         start, stop = mat[\"spikes\"][singleidx][\"timerange\"].ravel()[0]         break if singleidx:     print(\"singleidx\")     start, stop = mat[\"spikes\"][singleidx][\"timerange\"].ravel()[0] else:     for nn in range(len(mat[\"spikes\"])):         try:             for mm in range(len(mat[\"spikes\"][nn])):                 if mat[\"spikes\"][nn][mm].dtype == mattdtype:                     singleidx = nn                     multiidx = mm                     start, stop = mat[\"spikes\"][singleidx][multiidx][                         \"timerange\"                     ].ravel()[0]                     break         except TypeError:             continue  if multiidx:     print(\"multiidx\")     start, stop = mat[\"spikes\"][singleidx][multiidx][\"timerange\"].ravel()[0] <pre>multiidx\n</pre> In\u00a0[29]: Copied! <pre># start, stop = mat['spikes'][0][0]['timerange'].ravel()[0]\n\n# Epoch for which spikes were recorded\nsession_bounds = nel.EpochArray([start, stop])\n</pre> # start, stop = mat['spikes'][0][0]['timerange'].ravel()[0]  # Epoch for which spikes were recorded session_bounds = nel.EpochArray([start, stop]) <p>Now we're ready to build our <code>SpikeTrainArray</code>. We simply pass in the list of spike times, the epoch during which spikes were recorded, and the sampling rate (recall that we previously specified the sampling rate, FS=30000 Hz).</p> In\u00a0[30]: Copied! <pre>st = nel.SpikeTrainArray(timestamps=spikes, support=session_bounds, fs=FS)\nprint(st)\n</pre> st = nel.SpikeTrainArray(timestamps=spikes, support=session_bounds, fs=FS) print(st) <pre>&lt;SpikeTrainArray at 0x2a5d218f048: 31 series&gt; at 30000 Hz\n</pre> <p>Printing out our <code>SpikeTrainArray</code> (<code>st</code>) tells us that we have 31 units, sampled at 30,000 Hz.</p> <p>We can ask several things from this <code>SpikeTrainArray</code> object, such as the number of spikes for each unit. To do that, we simply access the <code>.n_spikes</code> property, which returns the number of spikes associated with each unit as an array:</p> In\u00a0[31]: Copied! <pre>st.n_spikes\n</pre> st.n_spikes Out[31]: <pre>array([1748,  106,  352,   88,  875,  305,  145,  113,  408,  557, 1613,\n        491,  270,  984, 1381, 7959,  931,   71,  477, 1183,  487,  816,\n        479,   44, 1065,   92,   41, 2127,  901, 1179, 1541])</pre> <p>Of course, we can also compute these things manually. Let's take a step back, and compute the average firing rate for each unit, using not the <code>nelpy</code> <code>SpikeTrainArray</code>, but the original list of lists:</p> In\u00a0[32]: Copied! <pre>avg_firing_rates = []\nst_duration = stop - start\nfor unit in spikes:\n    n_spikes = len(unit)\n    avg_firing_rate = n_spikes / st_duration\n    avg_firing_rates.append(avg_firing_rate)\n\nprint(avg_firing_rates)\n</pre> avg_firing_rates = [] st_duration = stop - start for unit in spikes:     n_spikes = len(unit)     avg_firing_rate = n_spikes / st_duration     avg_firing_rates.append(avg_firing_rate)  print(avg_firing_rates) <pre>[0.8880880967134036, 0.05385431250092722, 0.17883696226723, 0.0447092405668075, 0.44455210790859734, 0.15495816332813964, 0.07366863502485327, 0.057410729364196, 0.20728829717338024, 0.28298917040581567, 0.8195000572075057, 0.24945723998071004, 0.13717607901179574, 0.49993059906521115, 0.7016302411677404, 4.043645973536601, 0.47300344281474754, 0.036072228184583326, 0.24234440625417247, 0.6010344498924236, 0.24742500177312787, 0.4145765943467605, 0.24336052535796357, 0.02235462028340375, 0.5410834227687499, 0.04674147877438966, 0.020830441627717133, 1.0806426668818132, 0.45776165625788134, 0.5990022116848415, 0.7829197694710268]\n</pre> <p>This wasn't too difficult, but <code>nelpy</code> can make this a little easier.</p> <p>Like all other core <code>nelpy</code> objects, a <code>SpikeTrainArray</code> has a support on which it is defined, and so we can always get timing information by accessing the underlying support object (instead of using <code>start</code> and <code>stop</code> explicitly, as we did above).</p> <p>Let's calculate the average firing rates using <code>nelpy</code> now:</p> In\u00a0[33]: Copied! <pre>avg_firing_rates = st.n_spikes / st.support.duration\n\nprint(avg_firing_rates)\n</pre> avg_firing_rates = st.n_spikes / st.support.duration  print(avg_firing_rates) <pre>[0.8880881  0.05385431 0.17883696 0.04470924 0.44455211 0.15495816\n 0.07366864 0.05741073 0.2072883  0.28298917 0.81950006 0.24945724\n 0.13717608 0.4999306  0.70163024 4.04364597 0.47300344 0.03607223\n 0.24234441 0.60103445 0.247425   0.41457659 0.24336053 0.02235462\n 0.54108342 0.04674148 0.02083044 1.08064267 0.45776166 0.59900221\n 0.78291977]\n</pre> <p>Now wasn't that a little simpler? But the real beauty comes in when the <code>SpikeTrainArray</code> is not simply defined on one continuous epoch, but on many smaller epochs. For example, we might be interested in asking \"what is the average firing rate for each unit, during periods when the animal was running?\"---in that case, the firing rate calculation using the list of lists approach would become exceedingly painful, but we would be able to simply write</p> <pre><code>avg_firing_rates_during_run = st[run_epochs].n_spikes / st[run_epochs].support.duration \n</code></pre> <p>which will work just the same, even though <code>st[run].support</code> now consists of many discontiguous epochs.</p> <p>Nelpy has several built-in plot types (we have already seen <code>npl.plot2d()</code> before), and one that is frequently useful is the <code>rasterplot()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(ncols=1, figsize=(14, 5))\n\n# use nelpy to plot the spike raster\nnpl.rasterplot(st, lw=0.5, ax=ax)\nax.set_xlabel(\"time [seconds]\")\nax.set_ylabel(\"unit\")\nax.set_xlim(*session_bounds.time)\n</pre> fig, ax = plt.subplots(ncols=1, figsize=(14, 5))  # use nelpy to plot the spike raster npl.rasterplot(st, lw=0.5, ax=ax) ax.set_xlabel(\"time [seconds]\") ax.set_ylabel(\"unit\") ax.set_xlim(*session_bounds.time) <p>We notice that there is a clear difference in overall activity between the run session (4400--5400 seconds) and when the animal was in its sleep box (5550--). For example, units 4, 7, 8, 26, 27, and 29 seem to be largely inactive during run, but active during the sleep box. Let's use the <code>rasterplot()</code> to highlight those units:</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(ncols=1, figsize=(14, 5))\n\nunits_in_sleepbox = [2, 4, 6, 7, 8, 26, 27, 29]\nunits_in_run = [11, 15, 17, 21]\n\ntransition_epoch = nel.EpochArray([5382, 5530])\n\n# use nelpy to plot the spike rasters\nnpl.rasterplot(st[:, units_in_sleepbox], lw=1, ax=ax, color=npl.colors.sweet.red)\nnpl.rasterplot(st[:, units_in_run], lw=1, ax=ax, color=npl.colors.sweet.blue)\n\nnpl.epochplot(\n    sleep_box - transition_epoch,\n    alpha=0.2,\n    hatch=\"\",\n    color=npl.colors.sweet.red,\n    label=\"in sleep box\",\n)\nnpl.epochplot(\n    ~sleep_box, alpha=0.2, hatch=\"\", color=npl.colors.sweet.blue, label=\"on track\"\n)\nax.set_xlabel(\"time [seconds]\")\nax.set_ylabel(\"unit\")\nax.set_xlim(*session_bounds.time)\n\nplt.legend(loc=(1.02, 0.9))\n</pre> fig, ax = plt.subplots(ncols=1, figsize=(14, 5))  units_in_sleepbox = [2, 4, 6, 7, 8, 26, 27, 29] units_in_run = [11, 15, 17, 21]  transition_epoch = nel.EpochArray([5382, 5530])  # use nelpy to plot the spike rasters npl.rasterplot(st[:, units_in_sleepbox], lw=1, ax=ax, color=npl.colors.sweet.red) npl.rasterplot(st[:, units_in_run], lw=1, ax=ax, color=npl.colors.sweet.blue)  npl.epochplot(     sleep_box - transition_epoch,     alpha=0.2,     hatch=\"\",     color=npl.colors.sweet.red,     label=\"in sleep box\", ) npl.epochplot(     ~sleep_box, alpha=0.2, hatch=\"\", color=npl.colors.sweet.blue, label=\"on track\" ) ax.set_xlabel(\"time [seconds]\") ax.set_ylabel(\"unit\") ax.set_xlim(*session_bounds.time)  plt.legend(loc=(1.02, 0.9)) <pre>d:\\dropbox\\code\\nelpy\\nelpy\\plotting\\core.py:906: UserWarning: Spike trains may be plotted in the same vertical position as another unit\nC:\\Users\\etien\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py:91: UserWarning: Setting the 'color' property will overridethe edgecolor or facecolor properties. \n</pre> Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x2a5d5d0bd68&gt;</pre> <p>The <code>rasterplot()</code> function has quite a bit of flexibility. As another example, let's plot the same as above, but let's collapse (stack) all the units onto each other:</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(ncols=1, figsize=(14, 2.4))\n\nunits_in_sleepbox = [2, 4, 6, 7, 8, 26, 27, 29]\nunits_in_run = [11, 15, 17, 21]\n\ntransition_epoch = nel.EpochArray([5382, 5530])\n\n# use nelpy to plot the spike rasters\nnpl.rasterplot(\n    st[:, units_in_sleepbox], lw=1, ax=ax, color=npl.colors.sweet.red, vertstack=True\n)\nnpl.rasterplot(\n    st[:, units_in_run], lw=1, ax=ax, color=npl.colors.sweet.blue, vertstack=True\n)\n\nnpl.epochplot(\n    sleep_box - transition_epoch,\n    alpha=0.2,\n    hatch=\"\",\n    color=npl.colors.sweet.red,\n    label=\"in sleep box\",\n)\nnpl.epochplot(\n    ~sleep_box, alpha=0.2, hatch=\"\", color=npl.colors.sweet.blue, label=\"on track\"\n)\nax.set_xlabel(\"time [seconds]\")\nax.set_ylabel(\"unit\")\nax.set_xlim(*session_bounds.time)\n\nplt.legend(loc=(1.02, 0.9))\n</pre> fig, ax = plt.subplots(ncols=1, figsize=(14, 2.4))  units_in_sleepbox = [2, 4, 6, 7, 8, 26, 27, 29] units_in_run = [11, 15, 17, 21]  transition_epoch = nel.EpochArray([5382, 5530])  # use nelpy to plot the spike rasters npl.rasterplot(     st[:, units_in_sleepbox], lw=1, ax=ax, color=npl.colors.sweet.red, vertstack=True ) npl.rasterplot(     st[:, units_in_run], lw=1, ax=ax, color=npl.colors.sweet.blue, vertstack=True )  npl.epochplot(     sleep_box - transition_epoch,     alpha=0.2,     hatch=\"\",     color=npl.colors.sweet.red,     label=\"in sleep box\", ) npl.epochplot(     ~sleep_box, alpha=0.2, hatch=\"\", color=npl.colors.sweet.blue, label=\"on track\" ) ax.set_xlabel(\"time [seconds]\") ax.set_ylabel(\"unit\") ax.set_xlim(*session_bounds.time)  plt.legend(loc=(1.02, 0.9)) <pre>C:\\Users\\etien\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py:91: UserWarning: Setting the 'color' property will overridethe edgecolor or facecolor properties. \n</pre> Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x2a5d4a227f0&gt;</pre> <p>We can even get a histogram with the number of spikes across all units by using the <code>npl.rastercountplot()</code> function. Note that this function is not yet fully developed, and still needs to be modified to increase flexibility.</p> In\u00a0[\u00a0]: Copied! <pre>axc, axr = npl.rastercountplot(st, lw=0.5, nbins=130)\naxes = (axr, axc)\naxr.set_xlabel(\"time [seconds]\")\naxr.set_ylabel(\"unit\")\nfor ax in axes:\n    ax.set_xlim(*session_bounds.time)\n</pre> axc, axr = npl.rastercountplot(st, lw=0.5, nbins=130) axes = (axr, axc) axr.set_xlabel(\"time [seconds]\") axr.set_ylabel(\"unit\") for ax in axes:     ax.set_xlim(*session_bounds.time) <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:2099: UserWarning: _restrict_to_interval_array() not yet implemented for BinnedTypes\n</pre> <p>One final raster example, where we call <code>rastercountplot()</code> on the list of units that seem to be more active in the sleep box than on the track:</p> In\u00a0[\u00a0]: Copied! <pre>axc, axr = npl.rastercountplot(\n    st[:, units_in_sleepbox], nbins=130, color=npl.colors.sweet.red, vertstack=True\n)\naxes = (axr, axc)\naxr.set_xlabel(\"time [seconds]\")\naxr.set_ylabel(\"unit\")\nfor ax in axes:\n    ax.set_xlim(*session_bounds.time)\n\nnpl.epochplot(\n    sleep_box - transition_epoch,\n    ax=axc,\n    alpha=0.2,\n    hatch=\"\",\n    color=npl.colors.sweet.red,\n    label=\"in sleep box\",\n)\nnpl.epochplot(\n    ~sleep_box,\n    ax=axc,\n    alpha=0.2,\n    hatch=\"\",\n    color=npl.colors.sweet.blue,\n    label=\"on track\",\n)\naxc.legend(loc=(1.0, 0.5))\n</pre> axc, axr = npl.rastercountplot(     st[:, units_in_sleepbox], nbins=130, color=npl.colors.sweet.red, vertstack=True ) axes = (axr, axc) axr.set_xlabel(\"time [seconds]\") axr.set_ylabel(\"unit\") for ax in axes:     ax.set_xlim(*session_bounds.time)  npl.epochplot(     sleep_box - transition_epoch,     ax=axc,     alpha=0.2,     hatch=\"\",     color=npl.colors.sweet.red,     label=\"in sleep box\", ) npl.epochplot(     ~sleep_box,     ax=axc,     alpha=0.2,     hatch=\"\",     color=npl.colors.sweet.blue,     label=\"on track\", ) axc.legend(loc=(1.0, 0.5)) <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:2099: UserWarning: _restrict_to_interval_array() not yet implemented for BinnedTypes\nC:\\Users\\etien\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py:91: UserWarning: Setting the 'color' property will overridethe edgecolor or facecolor properties. \n</pre> Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x2a5d0a8c828&gt;</pre> <p>Next, we will use the position and spikes to estimate the spatial tuning curves of our 31 units.</p> <p>Place fields are usually estimated using only those epochs when the animal is running faster than some speed threshold, so we find these epochs first. We smooth our speed estimates with a Guassian kernel with sigma = 100 ms, and then we return those epochs where the animal was running at least 8 percent per second.</p> <p>Note: Usually we would measure the speed in cm/s or similar, but recall that our position has been scaled to be a percentage; the track was about 117 cm long, so that percent/s is not too different from cm/s.</p> In\u00a0[39]: Copied! <pre>sigma_100ms = 0.1\n\n# compute and smooth speed of linearized position\nspeed1 = nel.utils.ddt_asa(pos1d, smooth=True, sigma=sigma_100ms, norm=True)\n\n# find epochs where the animal ran at least as fast as v1=8 percent per second\nrun_epochs = nel.utils.get_run_epochs(speed1, v1=8, v2=8)\n</pre> sigma_100ms = 0.1  # compute and smooth speed of linearized position speed1 = nel.utils.ddt_asa(pos1d, smooth=True, sigma=sigma_100ms, norm=True)  # find epochs where the animal ran at least as fast as v1=8 percent per second run_epochs = nel.utils.get_run_epochs(speed1, v1=8, v2=8) <p>The <code>get_run_epochs()</code> function has two thresholds (both set to 8 above). More specifically, the first threshold (<code>v1</code>) is referred to as the primary threshold, and this must be reached or exceeded for an epoch to be considered a candidate. But then the secondary threshold (<code>v2</code>) is used to determine the epoch boundaries.</p> <p>For example, if we had used <code>v1=8, v2=5</code> then only epochs during which the animal reached a speed of at least 8 u/s would be considered, and the epoch boundaries would be when the animal ran ran at or slower than 5 u/s, starting at the peak speed of &gt;8 u/s.</p> <p>There are other possible arguments, too, so that we can e.g. ask that the animal must run at least 8 u/s for some minimum period of time, and so on.</p> <p>Let's see for how much time the animal actually ran faster than 8 percent per second:</p> In\u00a0[40]: Copied! <pre>print(run_epochs)\n</pre> print(run_epochs) <pre>&lt;EpochArray at 0x2a5d60f64e0: 221 epochs&gt; of length 4:18:131 minutes\n</pre> <p>We see that there are 221! short epochs during the animal ran sufficiently fast, and that the total amount of time spent running at this speed or faster, is about 4 minutes and 18 seconds (out of approximately 16 minutes spent on the track).</p> <p>As promised before, we can now easily (trivially) calculate the firing rates of all the units, only during times when the animal was running:</p> In\u00a0[41]: Copied! <pre>avg_firing_rates_during_run = st[run_epochs].n_spikes / st[run_epochs].support.duration\n\nfor unit, (run_rate, avg_rate) in enumerate(\n    zip(avg_firing_rates_during_run, avg_firing_rates)\n):\n    print(\n        \"unit {}: {:2.3f} Hz (run and rest: {:2.3f} Hz)\".format(\n            unit + 1, run_rate, avg_rate\n        )\n    )\n</pre> avg_firing_rates_during_run = st[run_epochs].n_spikes / st[run_epochs].support.duration  for unit, (run_rate, avg_rate) in enumerate(     zip(avg_firing_rates_during_run, avg_firing_rates) ):     print(         \"unit {}: {:2.3f} Hz (run and rest: {:2.3f} Hz)\".format(             unit + 1, run_rate, avg_rate         )     ) <pre>unit 1: 1.058 Hz (run and rest: 0.888 Hz)\nunit 2: 0.004 Hz (run and rest: 0.054 Hz)\nunit 3: 0.023 Hz (run and rest: 0.179 Hz)\nunit 4: 0.000 Hz (run and rest: 0.045 Hz)\nunit 5: 0.112 Hz (run and rest: 0.445 Hz)\nunit 6: 0.046 Hz (run and rest: 0.155 Hz)\nunit 7: 0.000 Hz (run and rest: 0.074 Hz)\nunit 8: 0.015 Hz (run and rest: 0.057 Hz)\nunit 9: 0.360 Hz (run and rest: 0.207 Hz)\nunit 10: 0.019 Hz (run and rest: 0.283 Hz)\nunit 11: 3.281 Hz (run and rest: 0.820 Hz)\nunit 12: 0.132 Hz (run and rest: 0.249 Hz)\nunit 13: 0.418 Hz (run and rest: 0.137 Hz)\nunit 14: 2.243 Hz (run and rest: 0.500 Hz)\nunit 15: 1.786 Hz (run and rest: 0.702 Hz)\nunit 16: 6.318 Hz (run and rest: 4.044 Hz)\nunit 17: 0.856 Hz (run and rest: 0.473 Hz)\nunit 18: 0.077 Hz (run and rest: 0.036 Hz)\nunit 19: 0.666 Hz (run and rest: 0.242 Hz)\nunit 20: 1.251 Hz (run and rest: 0.601 Hz)\nunit 21: 1.449 Hz (run and rest: 0.247 Hz)\nunit 22: 0.701 Hz (run and rest: 0.415 Hz)\nunit 23: 0.209 Hz (run and rest: 0.243 Hz)\nunit 24: 0.000 Hz (run and rest: 0.022 Hz)\nunit 25: 0.050 Hz (run and rest: 0.541 Hz)\nunit 26: 0.008 Hz (run and rest: 0.047 Hz)\nunit 27: 0.000 Hz (run and rest: 0.021 Hz)\nunit 28: 3.913 Hz (run and rest: 1.081 Hz)\nunit 29: 0.023 Hz (run and rest: 0.458 Hz)\nunit 30: 1.143 Hz (run and rest: 0.599 Hz)\nunit 31: 1.550 Hz (run and rest: 0.783 Hz)\n</pre> <p>We can also plot the linearized trajectory only during those run epochs. Each new color indicates that it is a new epoch. The black on the bottom figure corresponds to epochs when the animal did not meet the 8 percent per second requirement, and so those are when the animal is considered to be at rest.</p> In\u00a0[\u00a0]: Copied! <pre>with npl.FigureManager(show=True, nrows=2, figsize=(16, 4)) as (fig2, axes):\n    npl.utils.skip_if_no_output(fig2)\n    ax0, ax1 = axes\n    for ax in axes:\n        ax.plot(pos1d.time, pos1d.asarray().yvals, lw=1, alpha=0.2, color=\"gray\")\n        ax.set_ylabel(\"position (%)\")\n\n    npl.plot(pos1d[run_epochs], ax=ax0, lw=1, label=\"run\")\n    npl.plot(pos1d[~run_epochs], ax=ax1, lw=1, label=\"run\", color=\"k\")\n\n    npl.utils.no_xticklabels(ax0)\n\n    ax0.set_title(\"run\")\n    ax1.set_title(\"~run (=rest)\")\n</pre> with npl.FigureManager(show=True, nrows=2, figsize=(16, 4)) as (fig2, axes):     npl.utils.skip_if_no_output(fig2)     ax0, ax1 = axes     for ax in axes:         ax.plot(pos1d.time, pos1d.asarray().yvals, lw=1, alpha=0.2, color=\"gray\")         ax.set_ylabel(\"position (%)\")      npl.plot(pos1d[run_epochs], ax=ax0, lw=1, label=\"run\")     npl.plot(pos1d[~run_epochs], ax=ax1, lw=1, label=\"run\", color=\"k\")      npl.utils.no_xticklabels(ax0)      ax0.set_title(\"run\")     ax1.set_title(\"~run (=rest)\") <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:789: UserWarning: ignoring signal outside of support\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:789: UserWarning: ignoring signal outside of support\n</pre> <p>Now that we have the epochs during which the animal was running, we can use it to restrict our <code>SpikeTrainArray</code> to those epochs. Then we will use these spikes, along with the position data, to estimate the spatial tuning curves.</p> In\u00a0[43]: Copied! <pre>st_run = st[\n    run_epochs\n]  # restrict spike trains to those epochs during which the animal was running\n</pre> st_run = st[     run_epochs ]  # restrict spike trains to those epochs during which the animal was running <p>We bin our spikes into 50 ms bins, so that we can count the number of spikes in a small window of time, which we will then later associate with the particular position bin that the animal was at when those spikes occured.</p> <p>We also apply a little bit of spike time smoothing.</p> In\u00a0[44]: Copied! <pre>ds_run = 0.5  # 100 ms\nds_50ms = 0.05\n\n# smooth and re-bin:\nsigma = 0.3  # 300 ms spike smoothing\nbst_run = (\n    st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms)\n)\n</pre> ds_run = 0.5  # 100 ms ds_50ms = 0.05  # smooth and re-bin: sigma = 0.3  # 300 ms spike smoothing bst_run = (     st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms) ) <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:1689: UserWarning: interval duration is less than bin size: ignoring...\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:2099: UserWarning: _restrict_to_interval_array() not yet implemented for BinnedTypes\n</pre> <p>We now estimate the tuning curves using the binned spikes during run (<code>bst_run</code>), and we partition our linear track into <code>n_extern=50</code> equal-sized spatial bins, and we specify that the linearized position ranges from <code>extmin=0</code> to <code>extmax=100</code>. We also smooth the estimated tuning curves with a 0.2 cm smoothing kernel (this should be imperceptable, and a larger value for sigma should really be used, but we can always apply the smoothing later, so it's not a real problem).</p> In\u00a0[45]: Copied! <pre>sigma = 0.2  # smoothing std dev in cm\ntc = nel.TuningCurve1D(\n    bst=bst_run,\n    extern=pos1d,\n    n_extern=50,\n    extmin=0,\n    extmax=100,\n    sigma=sigma,\n    min_duration=1,\n)\n</pre> sigma = 0.2  # smoothing std dev in cm tc = nel.TuningCurve1D(     bst=bst_run,     extern=pos1d,     n_extern=50,     extmin=0,     extmax=100,     sigma=sigma,     min_duration=1, ) <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\n</pre> <p>Finally, we reorder the units by their peak firing positions on the track, simply to make visualization a little nicer:</p> In\u00a0[46]: Copied! <pre>tc = tc.reorder_units()\n</pre> tc = tc.reorder_units() <p>That's it! Let's see what we've got:</p> In\u00a0[\u00a0]: Copied! <pre>npl.set_palette(npl.colors.rainbow)\nwith npl.FigureManager(show=True, figsize=(8, 8)) as (fig_tc, ax):\n    npl.utils.skip_if_no_output(fig_tc)\n    npl.plot_tuning_curves1D(tc.smooth(sigma=2), normalize=False, pad=3)\n</pre> npl.set_palette(npl.colors.rainbow) with npl.FigureManager(show=True, figsize=(8, 8)) as (fig_tc, ax):     npl.utils.skip_if_no_output(fig_tc)     npl.plot_tuning_curves1D(tc.smooth(sigma=2), normalize=False, pad=3) <p>We see that some units have spatially localized tuning curves, and some don't. Also, some units have large firing rates, and some don't. To see the shape of all the tuning curves better, we can normalize the peak firing rates like so:</p> In\u00a0[\u00a0]: Copied! <pre>npl.set_palette(npl.colors.rainbow)\nwith npl.FigureManager(show=True, figsize=(8, 8)) as (fig_tc_norm, ax):\n    npl.utils.skip_if_no_output(fig_tc_norm)\n    npl.plot_tuning_curves1D(tc.smooth(sigma=2), normalize=True, pad=0.8)\n</pre> npl.set_palette(npl.colors.rainbow) with npl.FigureManager(show=True, figsize=(8, 8)) as (fig_tc_norm, ax):     npl.utils.skip_if_no_output(fig_tc_norm)     npl.plot_tuning_curves1D(tc.smooth(sigma=2), normalize=True, pad=0.8) <p>This normalized view allows us to see the shapes of the tuning curves more clearly, and we can see that units 4, 24, and 7 are essentially flat, meaning they have NO spatial localization for their firing rates. This is partly due to there just being almost no spikes during run for those units though.</p> <p>At any rate, let's filter out the units a little further, to try and figure out which ones might actually be place cells.</p> In\u00a0[\u00a0]: Copied! <pre># set criteria for units used in decoding\nmin_peakfiringrate = 1  # Hz\nmax_avgfiringrate = 5  # Hz\npeak_to_mean_ratio_threshold = (\n    3.5  # peak firing rate should be greater than 3.5 times mean firing rate\n)\n\n# unimodal_cells = find_unimodal_tuningcurves1D(smoothed_rate, peakthresh=0.5)\n\n# enforce minimum peak firing rate\nunit_ids_to_keep = set(\n    np.asanyarray(tc.unit_ids)[\n        np.argwhere(tc.ratemap.max(axis=1) &gt; min_peakfiringrate).squeeze().tolist()\n    ]\n)\n# enforce maximum average firing rate\nunit_ids_to_keep = unit_ids_to_keep.intersection(\n    set(\n        np.asanyarray(tc.unit_ids)[\n            np.argwhere(tc.ratemap.mean(axis=1) &lt; max_avgfiringrate).squeeze().tolist()\n        ]\n    )\n)\n\n# enforce peak to mean firing ratio\npeak_firing_rates = tc.max(axis=1)\nmean_firing_rates = tc.mean(axis=1)\nratio = peak_firing_rates / mean_firing_rates\nunit_ids_to_keep = unit_ids_to_keep.intersection(\n    set(\n        np.asanyarray(tc.unit_ids)[\n            np.argwhere(ratio &gt;= peak_to_mean_ratio_threshold).squeeze().tolist()\n        ]\n    )\n)\n\n\n# finally, convert remaining units into a list of indices\nunit_ids_to_keep = list(unit_ids_to_keep)\n\n# modify spike trains and ratemap to only include those units that passed all the criteria\nsta_placecells = st._unit_subset(unit_ids_to_keep)\n\ntc = tc._unit_subset(unit_ids_to_keep)\n\n# reorder cells by peak firing location on track (this is nice for visualization, but doesn't affect decoding)\ntc.reorder_units(inplace=True)\n\n# with plt.xkcd():\nwith npl.palettes.color_palette(npl.colors.rainbow):\n    with npl.FigureManager(show=True, nrows=1, ncols=3, figsize=(16, 4)) as (fig, axes):\n        npl.utils.skip_if_no_output(fig)\n        ax0, ax1, ax2 = axes\n\n        npl.plot_tuning_curves1D(tc.smooth(sigma=3), ax=ax0, pad=5.5)\n        npl.plot_tuning_curves1D(tc.smooth(sigma=3), ax=ax1, normalize=True, pad=0.9)\n        npl.plot_tuning_curves1D(tc.smooth(sigma=3), ax=ax2, pad=0)\n\n        for ax in axes:\n            ax.set_xlabel(\"position [cm]\")\n        npl.utils.xticks_interval(25, *axes)\n        npl.utils.yticks_interval(5, ax2)\n        npl.add_simple_scalebar(\n            \"10 Hz\",\n            ax=ax0,\n            xy=(10, 57),\n            length=10,\n            orientation=\"v\",\n            rotation_text=\"h\",\n            size=14,\n        )\n        #         npl.add_simple_scalebar(\"5 Hz\", ax=ax1, xy=(10, 17.5), length=5, orientation='v', rotation_text='h', size=14)\n        ax0.set_title(\"True firing rates\", size=12)\n        ax1.set_title(\"Normalized firing rates\", size=12)\n        ax2.set_title(\"Collapsed units (pad=0)\", size=12)\n</pre> # set criteria for units used in decoding min_peakfiringrate = 1  # Hz max_avgfiringrate = 5  # Hz peak_to_mean_ratio_threshold = (     3.5  # peak firing rate should be greater than 3.5 times mean firing rate )  # unimodal_cells = find_unimodal_tuningcurves1D(smoothed_rate, peakthresh=0.5)  # enforce minimum peak firing rate unit_ids_to_keep = set(     np.asanyarray(tc.unit_ids)[         np.argwhere(tc.ratemap.max(axis=1) &gt; min_peakfiringrate).squeeze().tolist()     ] ) # enforce maximum average firing rate unit_ids_to_keep = unit_ids_to_keep.intersection(     set(         np.asanyarray(tc.unit_ids)[             np.argwhere(tc.ratemap.mean(axis=1) &lt; max_avgfiringrate).squeeze().tolist()         ]     ) )  # enforce peak to mean firing ratio peak_firing_rates = tc.max(axis=1) mean_firing_rates = tc.mean(axis=1) ratio = peak_firing_rates / mean_firing_rates unit_ids_to_keep = unit_ids_to_keep.intersection(     set(         np.asanyarray(tc.unit_ids)[             np.argwhere(ratio &gt;= peak_to_mean_ratio_threshold).squeeze().tolist()         ]     ) )   # finally, convert remaining units into a list of indices unit_ids_to_keep = list(unit_ids_to_keep)  # modify spike trains and ratemap to only include those units that passed all the criteria sta_placecells = st._unit_subset(unit_ids_to_keep)  tc = tc._unit_subset(unit_ids_to_keep)  # reorder cells by peak firing location on track (this is nice for visualization, but doesn't affect decoding) tc.reorder_units(inplace=True)  # with plt.xkcd(): with npl.palettes.color_palette(npl.colors.rainbow):     with npl.FigureManager(show=True, nrows=1, ncols=3, figsize=(16, 4)) as (fig, axes):         npl.utils.skip_if_no_output(fig)         ax0, ax1, ax2 = axes          npl.plot_tuning_curves1D(tc.smooth(sigma=3), ax=ax0, pad=5.5)         npl.plot_tuning_curves1D(tc.smooth(sigma=3), ax=ax1, normalize=True, pad=0.9)         npl.plot_tuning_curves1D(tc.smooth(sigma=3), ax=ax2, pad=0)          for ax in axes:             ax.set_xlabel(\"position [cm]\")         npl.utils.xticks_interval(25, *axes)         npl.utils.yticks_interval(5, ax2)         npl.add_simple_scalebar(             \"10 Hz\",             ax=ax0,             xy=(10, 57),             length=10,             orientation=\"v\",             rotation_text=\"h\",             size=14,         )         #         npl.add_simple_scalebar(\"5 Hz\", ax=ax1, xy=(10, 17.5), length=5, orientation='v', rotation_text='h', size=14)         ax0.set_title(\"True firing rates\", size=12)         ax1.set_title(\"Normalized firing rates\", size=12)         ax2.set_title(\"Collapsed units (pad=0)\", size=12) <p>Next, having defined a subset of 12 place cells, we may reasonably want to ask how well these cells can represent the animal's location? We can evaluate the expected decoding performance by using a Bayesian decoder, and by evaluating the decoding accuracy on a test set.</p> <p>Here, we use 5-fold cross-validation and the tuning curves of our 12 place cells to evaluate the performance, and the results are summarized in the figure below:</p> In\u00a0[\u00a0]: Copied! <pre>with npl.FigureManager(show=True, figsize=(5, 5)) as (fig, ax):\n    npl.utils.skip_if_no_output(fig)\n\n    ds_run = 0.5  # 100 ms\n    ds_50ms = 0.05\n    #     st_run = st[run_epochs]\n\n    # smooth and re-bin:\n    bst_run = (\n        st_run.bin(ds=ds_50ms)\n        .smooth(sigma=0.15, inplace=True)\n        .rebin(w=ds_run / ds_50ms)\n    )\n\n    bst = bst_run\n\n    npl.plot_cum_error_dist(bst=bst, extern=pos1d, extmin=0, extmax=100, sigma=0.0)\n</pre> with npl.FigureManager(show=True, figsize=(5, 5)) as (fig, ax):     npl.utils.skip_if_no_output(fig)      ds_run = 0.5  # 100 ms     ds_50ms = 0.05     #     st_run = st[run_epochs]      # smooth and re-bin:     bst_run = (         st_run.bin(ds=ds_50ms)         .smooth(sigma=0.15, inplace=True)         .rebin(w=ds_run / ds_50ms)     )      bst = bst_run      npl.plot_cum_error_dist(bst=bst, extern=pos1d, extmin=0, extmax=100, sigma=0.0) <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:1689: UserWarning: interval duration is less than bin size: ignoring...\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:2099: UserWarning: _restrict_to_interval_array() not yet implemented for BinnedTypes\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\n</pre> In\u00a0[\u00a0]: Copied! <pre>cumhist, bincenters = nel.decoding.cumulative_dist_decoding_error_using_xval(\n    bst_run, extern=pos1d, sigma=0.0, extmax=np.ceil(pos1d.max())\n)\nnpl.plot_cum_error_dist(cumhist=cumhist, bincenters=bincenters, label=\"a\")\n</pre> cumhist, bincenters = nel.decoding.cumulative_dist_decoding_error_using_xval(     bst_run, extern=pos1d, sigma=0.0, extmax=np.ceil(pos1d.max()) ) npl.plot_cum_error_dist(cumhist=cumhist, bincenters=bincenters, label=\"a\") <pre>d:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\nd:\\dropbox\\code\\nelpy\\nelpy\\core\\_eventarray.py:385: UserWarning: series tags have not yet been specified\n</pre> <p>We can evaluate <code>cumhist</code> at any value between <code>p=[0,1]</code>, which will return the error at that percentile. For example, to get the median decoding error, we simply evaluate <code>cumhist(0.5)</code>:</p> In\u00a0[52]: Copied! <pre>print(\"median decoding error: {:2.2f}\".format(cumhist(0.5)))\nprint(\"90th percentile decoding error: {:2.2f}\".format(cumhist(0.9)))\n</pre> print(\"median decoding error: {:2.2f}\".format(cumhist(0.5))) print(\"90th percentile decoding error: {:2.2f}\".format(cumhist(0.9))) <pre>median decoding error: 4.47\n90th percentile decoding error: 19.38\n</pre> <p>This tells us that, with probability 0.9, we can decode the animal's position to within 20 percent of the true position. Recall that in our particular example, percent and cm are almost the same. The inset shows a zoomed-in view, from where we can see that with probability 0.7, we have an error of less than 8 percent.</p> <p>It's not the best decoding accuracy imaginable, but overall, not bad!</p> In\u00a0[61]: Copied! <pre>plt.figure(figsize=(16, 4))\n\nposteriors, lengths, mode_pth, mean_pth = nel.decoding.decode1D(\n    bst_run.loc[:, unit_ids_to_keep], tc\n)\nactual_pos = pos1d(bst_run.bin_centers)\nplt.plot(actual_pos, c=\"0.8\", label=\"actual\")\nplt.ylabel(\"position (% along track)\")\nplt.xlabel(\"time bin (concatenated)\")\nplt.plot(mean_pth, c=\"0.2\", label=\"decoded using sorted units\")\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(16, 4))  posteriors, lengths, mode_pth, mean_pth = nel.decoding.decode1D(     bst_run.loc[:, unit_ids_to_keep], tc ) actual_pos = pos1d(bst_run.bin_centers) plt.plot(actual_pos, c=\"0.8\", label=\"actual\") plt.ylabel(\"position (% along track)\") plt.xlabel(\"time bin (concatenated)\") plt.plot(mean_pth, c=\"0.2\", label=\"decoded using sorted units\") plt.legend() plt.show() <pre>C:\\Users\\etien\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n</pre> In\u00a0[114]: Copied! <pre>from mpl_toolkits.axes_grid1 import make_axes_locatable\n\nwith npl.FigureManager(show=True, figsize=(30, 5)) as (fig, ax):\n    npl.utils.skip_if_no_output(fig)\n\n    npl.rasterplot(st, lw=0.5, ax=ax)\n    npl.rasterplot(sta_placecells, lw=0.5, color=npl.colors.sweet.green)\n    npl.epochplot(run_epochs)\n\n    divider = make_axes_locatable(ax)\n\n    axSpeed1d = divider.append_axes(\"bottom\", size=0.6, pad=0.1)\n    npl.plot(pos1d, ax=axSpeed1d)\n    npl.epochplot(run_epochs, ax=axSpeed1d)\n\n    xlims = (4500, 5300)\n\n    ax.set_xlim(xlims)\n    axSpeed1d.set_xlim(xlims)\n</pre> from mpl_toolkits.axes_grid1 import make_axes_locatable  with npl.FigureManager(show=True, figsize=(30, 5)) as (fig, ax):     npl.utils.skip_if_no_output(fig)      npl.rasterplot(st, lw=0.5, ax=ax)     npl.rasterplot(sta_placecells, lw=0.5, color=npl.colors.sweet.green)     npl.epochplot(run_epochs)      divider = make_axes_locatable(ax)      axSpeed1d = divider.append_axes(\"bottom\", size=0.6, pad=0.1)     npl.plot(pos1d, ax=axSpeed1d)     npl.epochplot(run_epochs, ax=axSpeed1d)      xlims = (4500, 5300)      ax.set_xlim(xlims)     axSpeed1d.set_xlim(xlims) <p>Next, we take a look at where on the track spikes from a particular unit occurred:</p> In\u00a0[115]: Copied! <pre>ax = npl.plot2d(pos, lw=0.5, c=\"0.8\")\nunit_id = 9\n_, pos_at_spikes = pos.asarray(at=st[:, unit_id].time)\nax.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\")\nax.set_aspect(\"equal\")\n</pre> ax = npl.plot2d(pos, lw=0.5, c=\"0.8\") unit_id = 9 _, pos_at_spikes = pos.asarray(at=st[:, unit_id].time) ax.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\") ax.set_aspect(\"equal\") In\u00a0[116]: Copied! <pre>fig, axes = plt.subplots(ncols=3, figsize=(16, 4))\n\nax0, ax1, ax2 = axes\n\n# plot trajectory in gray\nfor ax in axes:\n    npl.plot2d(pos, lw=0.5, c=\"0.8\", ax=ax)\n    ax.set_aspect(\"equal\")\n    ax.set_xlabel(\"x position\")\n    ax.set_ylabel(\"y position\")\n    npl.utils.clear_right(ax)\n    npl.utils.clear_top(ax)\n\nunit_id = 19\n\n# all spikes\nat = st.loc[:, unit_id][pos.support].time\n_, pos_at_spikes = pos.asarray(at=at)\nax0.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\")\nax0.set_aspect(\"equal\")\nax0.set_title(\"All spikes, unit {}\".format(unit_id))\n\n# spikes during RUN\nat = st.loc[:, unit_id][run_epochs].time\n_, pos_at_spikes = pos.asarray(at=at)\nax1.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\")\nax1.set_aspect(\"equal\")\nax1.set_title(\"Spikes while running, unit {}\".format(unit_id))\n\n# spikes during REST\nat = st.loc[:, unit_id][~run_epochs].time\n_, pos_at_spikes = pos.asarray(at=at)\nax2.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\")\nax2.set_aspect(\"equal\")\nax2.set_title(\"Spikes while at rest, unit {}\".format(unit_id))\n</pre> fig, axes = plt.subplots(ncols=3, figsize=(16, 4))  ax0, ax1, ax2 = axes  # plot trajectory in gray for ax in axes:     npl.plot2d(pos, lw=0.5, c=\"0.8\", ax=ax)     ax.set_aspect(\"equal\")     ax.set_xlabel(\"x position\")     ax.set_ylabel(\"y position\")     npl.utils.clear_right(ax)     npl.utils.clear_top(ax)  unit_id = 19  # all spikes at = st.loc[:, unit_id][pos.support].time _, pos_at_spikes = pos.asarray(at=at) ax0.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\") ax0.set_aspect(\"equal\") ax0.set_title(\"All spikes, unit {}\".format(unit_id))  # spikes during RUN at = st.loc[:, unit_id][run_epochs].time _, pos_at_spikes = pos.asarray(at=at) ax1.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\") ax1.set_aspect(\"equal\") ax1.set_title(\"Spikes while running, unit {}\".format(unit_id))  # spikes during REST at = st.loc[:, unit_id][~run_epochs].time _, pos_at_spikes = pos.asarray(at=at) ax2.plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\") ax2.set_aspect(\"equal\") ax2.set_title(\"Spikes while at rest, unit {}\".format(unit_id)) Out[116]: <pre>Text(0.5,1,'Spikes while at rest, unit 19')</pre> <p>This demo is still a work in progress, but hopefully it highlights a few ways in which using <code>nelpy</code> might make it easier to combine and analyze ephys data with different sampling rates, and different epochs where they are defined. There's a lot more that we can do with <code>nelpy</code>, so feel free to dig a little deeper to explore!</p> <p>Feedback about this notebook would be appreciated, and can be sent to era3@rice.edu.</p> <p>Nelpy makes use of <code>hmmlearn</code> (https://github.com/hmmlearn/hmmlearn), but modified to include a Poisson emissions model. The modified version can be found  at https://github.com/eackermann/hmmlearn . In addition to the Poisson emissions model, this modified branch also fixed up an issue in calculating the log probabilities, as well as consistency with random seeds. The random seed issue has since been fixed in the main repository, and it is possible that the log probability issue has also been fixed, and if not, I (Etienne) should make a pull request to incorporate the fixes into the main branch. At any rate, for now, nelpy makes use of the fork at https://github.com/eackermann/hmmlearn .</p> <p>Installing <code>hmmlearn</code> should be trivial on Linux, fairly easy on MacOS, and doable on Windows. To get it to compile on Windows, all you really need is to download and install the free version of Microsoft Visual Studio (with the Python extensions), and then <code>hmmlearn</code> should compile with no issues. After installation, you can remove Visual Studio again. However, if you don't want to go through the trouble of compiling it yourself, there is also a 64-bit Windows binary in the nelpy repository.</p> <p>Here we give a quick demo of how we may use nelpy to learn hidden Markov models. Typically, HMMs are used in the absence of any [observable] behavioral correlate, but here we demonstrate the approach on the run data, where we do actually have access to the position data. Nevertheless, we don't use the position data when training the HMM, nor do we need it to decode neural activity to the state space. But it does make it nice for a demo, since we can use the position data to interpret what those hidden states might represent.</p> <p>Briefly, we will use a <code>SpikeTrainArray</code> during bouts of running activity to learn our HMM. The HMM makes use of vectors of spike counts, so we need to bin our <code>SpikeTrainArray</code> into an appropriate bin size. Since we are learning a model on run data, we want to choose a bin size that's small enough to capture the animal's behavioral dynamics, but large enough to capture sufficiently many spikes (lot of empty bins will do us no good). Here, we will use 250 ms bins.</p> <p>We will also apply a little bit of spike smoothing (300 ms standard devitation Gaussian kernel), since this has been shown to improve decoding accuracy (in general, not just for HMMs; the idea is roughly that there is some inherent noise in the signals, and so modeling that uncertainty with spike smoothing actually improves our model robustness).</p> <p>Thereafter, we train our HMM on a subset of the <code>BinnedSpikeTrainArray</code> (the train set), and we can evaluate our model on the test set. We demonstrate how to decode to the state space, and also how to decode back to an actual behavioral correlate (in this cas position, if we augment our HMM with an additional mapping that we've learned between the state space and the external behavioral correlates.</p> <p>There are much more that we can do with HMMs, and this section is by no means meant to be comprehensive, nor definitive, but hopefully it demonstrates how to get started, and how to do basic inference.</p> In\u00a0[117]: Copied! <pre>import nelpy.hmmutils\n</pre> import nelpy.hmmutils In\u00a0[118]: Copied! <pre>ds_run = 0.25  # 100 ms\nds_50ms = 0.05\n\n# smooth and re-bin:\nsigma = 0.3  # 300 ms spike smoothing\nbst_run = (\n    st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms)\n)\n</pre> ds_run = 0.25  # 100 ms ds_50ms = 0.05  # smooth and re-bin: sigma = 0.3  # 300 ms spike smoothing bst_run = (     st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms) ) <pre>/home/etienne/Dropbox/code/nelpy/nelpy/core/_eventarray.py:1672: UserWarning: interval duration is less than bin size: ignoring...\n/home/etienne/Dropbox/code/nelpy/nelpy/core/_eventarray.py:2089: UserWarning: _restrict_to_interval_array() not yet implemented for BinnedTypes\n</pre> In\u00a0[119]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[120]: Copied! <pre>trainidx, testidx = train_test_split(\n    np.arange(bst_run.n_epochs), test_size=0.2, random_state=1\n)\n\ntrainidx.sort()\ntestidx.sort()\n\nbst_train = bst_run[trainidx]\nbst_test = bst_run[testidx]\n\nprint(\"{} train sequences and {} test sequences\".format(len(trainidx), len(testidx)))\n</pre> trainidx, testidx = train_test_split(     np.arange(bst_run.n_epochs), test_size=0.2, random_state=1 )  trainidx.sort() testidx.sort()  bst_train = bst_run[trainidx] bst_test = bst_run[testidx]  print(\"{} train sequences and {} test sequences\".format(len(trainidx), len(testidx))) <pre>138 train sequences and 35 test sequences\n</pre> In\u00a0[121]: Copied! <pre>num_states = 20\nhmm = nel.hmmutils.PoissonHMM(n_components=num_states, random_state=0, verbose=False)\nhmm.fit(bst_train)\n</pre> num_states = 20 hmm = nel.hmmutils.PoissonHMM(n_components=num_states, random_state=0, verbose=False) hmm.fit(bst_train) <pre>/home/etienne/temp/hmmlearn/hmmlearn/utils.py:87: RuntimeWarning: divide by zero encountered in log\n/home/etienne/temp/hmmlearn/hmmlearn/utils.py:87: RuntimeWarning: invalid value encountered in log\n/home/etienne/Dropbox/code/nelpy/nelpy/core/_eventarray.py:385: UserWarning: series tags have not yet been specified\n</pre> Out[121]: <pre>nelpy.PoissonHMM(init_params='stm', n_components=20, n_iter=50, params='stm',\n      random_state=0, verbose=False); fit=True, fit_ext=False</pre> In\u00a0[122]: Copied! <pre>with npl.FigureManager(show=True, nrows=1, ncols=2, figsize=(8, 8)) as (fig, axes):\n    npl.utils.skip_if_no_output(fig)\n    ax0, ax1 = axes\n\n    ax0.matshow(hmm.transmat_, cmap=plt.cm.RdPu)\n    ax0.set_title(\"Before reordering\")\n\n    transmat_order = hmm.get_state_order(\"transmat\")\n    hmm.reorder_states(transmat_order)\n    ax1.matshow(hmm.transmat_, cmap=plt.cm.GnBu)\n    ax1.set_title(\"After reordering\")\n\n    for ax in axes:\n        npl.utils.no_xticks(ax)\n        npl.utils.no_xticklabels(ax)\n        npl.utils.no_yticks(ax)\n        npl.utils.no_yticklabels(ax)\n\n    ys = np.arange(-0.5, num_states + 0.5, step=1)\n    xs = np.arange(-0.5, num_states + 0.5, step=1)\n\n    ax0.hlines(ys[1:-2], xs[2:-1], xs[3:])\n    ax0.hlines(ys[1:-1], xs[:-1], xs[1:])\n    ax0.vlines(xs[:-1], ys[:], ys[1:])\n    ax0.vlines(xs[2:-1], ys[:], ys[1:])\n\n    ax1.hlines(ys[1:-2], xs[2:-1], xs[3:])\n    ax1.hlines(ys[1:-1], xs[:-1], xs[1:])\n    ax1.vlines(xs[:-1], ys[:], ys[1:])\n    ax1.vlines(xs[2:-1], ys[:], ys[1:])\n</pre> with npl.FigureManager(show=True, nrows=1, ncols=2, figsize=(8, 8)) as (fig, axes):     npl.utils.skip_if_no_output(fig)     ax0, ax1 = axes      ax0.matshow(hmm.transmat_, cmap=plt.cm.RdPu)     ax0.set_title(\"Before reordering\")      transmat_order = hmm.get_state_order(\"transmat\")     hmm.reorder_states(transmat_order)     ax1.matshow(hmm.transmat_, cmap=plt.cm.GnBu)     ax1.set_title(\"After reordering\")      for ax in axes:         npl.utils.no_xticks(ax)         npl.utils.no_xticklabels(ax)         npl.utils.no_yticks(ax)         npl.utils.no_yticklabels(ax)      ys = np.arange(-0.5, num_states + 0.5, step=1)     xs = np.arange(-0.5, num_states + 0.5, step=1)      ax0.hlines(ys[1:-2], xs[2:-1], xs[3:])     ax0.hlines(ys[1:-1], xs[:-1], xs[1:])     ax0.vlines(xs[:-1], ys[:], ys[1:])     ax0.vlines(xs[2:-1], ys[:], ys[1:])      ax1.hlines(ys[1:-2], xs[2:-1], xs[3:])     ax1.hlines(ys[1:-1], xs[:-1], xs[1:])     ax1.vlines(xs[:-1], ys[:], ys[1:])     ax1.vlines(xs[2:-1], ys[:], ys[1:]) In\u00a0[123]: Copied! <pre>ds_run = 0.125  # 100 ms\nds_50ms = 0.05\nst_run = st[run_epochs]\n\n# smooth and re-bin:\nsigma = 0.3  # 300 ms spike smoothing\nbst_run = (\n    st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms)\n)\n</pre> ds_run = 0.125  # 100 ms ds_50ms = 0.05 st_run = st[run_epochs]  # smooth and re-bin: sigma = 0.3  # 300 ms spike smoothing bst_run = (     st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms) ) <pre>/home/etienne/Dropbox/code/nelpy/nelpy/core/_eventarray.py:1672: UserWarning: interval duration is less than bin size: ignoring...\n/home/etienne/Dropbox/code/nelpy/nelpy/core/_eventarray.py:2089: UserWarning: _restrict_to_interval_array() not yet implemented for BinnedTypes\n</pre> In\u00a0[\u00a0]: Copied! <pre>sigma_tc = 2\n\nbst = bst_run\nxpos = pos1d.asarray(at=bst.centers).yvals\n\nx0 = pos1d.min()\nxl = pos1d.max()\nn_extern = 50\nxx_left = np.linspace(x0, xl, n_extern + 1)\nxx_mid = np.linspace(x0, xl, n_extern + 1)[:-1]\nxx_mid += (xx_mid[1] - xx_mid[0]) / 2\n\next_x = np.digitize(xpos, xx_left) - 1  # spatial bin numbers\next_x = ext_x.astype(float)\next_x[ext_x == 0] = np.nan\next_x[ext_x &gt;= n_extern] = np.nan\n\nextern = hmm.fit_ext(X=bst_run, ext=ext_x, n_extern=n_extern)\n\nvtc = nel.TuningCurve1D(ratemap=extern, min_duration=0, extmin=x0, extmax=xl)\nvtc = vtc.smooth(sigma=sigma_tc)\n\nstates_in_track_order = np.array(vtc.get_peak_firing_order_ids()) - 1\n\nvtc.reorder_units(inplace=True)\n\nhmm.reorder_states(states_in_track_order)\n</pre> sigma_tc = 2  bst = bst_run xpos = pos1d.asarray(at=bst.centers).yvals  x0 = pos1d.min() xl = pos1d.max() n_extern = 50 xx_left = np.linspace(x0, xl, n_extern + 1) xx_mid = np.linspace(x0, xl, n_extern + 1)[:-1] xx_mid += (xx_mid[1] - xx_mid[0]) / 2  ext_x = np.digitize(xpos, xx_left) - 1  # spatial bin numbers ext_x = ext_x.astype(float) ext_x[ext_x == 0] = np.nan ext_x[ext_x &gt;= n_extern] = np.nan  extern = hmm.fit_ext(X=bst_run, ext=ext_x, n_extern=n_extern)  vtc = nel.TuningCurve1D(ratemap=extern, min_duration=0, extmin=x0, extmax=xl) vtc = vtc.smooth(sigma=sigma_tc)  states_in_track_order = np.array(vtc.get_peak_firing_order_ids()) - 1  vtc.reorder_units(inplace=True)  hmm.reorder_states(states_in_track_order) <pre>/home/etienne/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: RuntimeWarning: invalid value encountered in greater_equal\n/home/etienne/anaconda3/lib/python3.5/site-packages/numpy/lib/function_base.py:780: RuntimeWarning: invalid value encountered in greater_equal\n/home/etienne/anaconda3/lib/python3.5/site-packages/numpy/lib/function_base.py:781: RuntimeWarning: invalid value encountered in less_equal\n</pre> In\u00a0[125]: Copied! <pre>npl.setup(font_scale=1.2)\nwith npl.palettes.color_palette(npl.colors.rainbow):\n    with npl.FigureManager(show=True, figsize=(6, 6)) as (fig, ax):\n        npl.utils.skip_if_no_output(fig)\n\n        ax = npl.plot_tuning_curves1D(vtc, pad=0.08)\n        ax.set_xlabel(\"position [cm]\")\n        ax.set_ylabel(\"state\")\n        npl.utils.xticks_interval(25)\n\n        fig.suptitle(\"virtual tuning curves estimated from run data\")\n</pre> npl.setup(font_scale=1.2) with npl.palettes.color_palette(npl.colors.rainbow):     with npl.FigureManager(show=True, figsize=(6, 6)) as (fig, ax):         npl.utils.skip_if_no_output(fig)          ax = npl.plot_tuning_curves1D(vtc, pad=0.08)         ax.set_xlabel(\"position [cm]\")         ax.set_ylabel(\"state\")         npl.utils.xticks_interval(25)          fig.suptitle(\"virtual tuning curves estimated from run data\") In\u00a0[126]: Copied! <pre>_, posterior_states = hmm.score_samples(bst_test)\n</pre> _, posterior_states = hmm.score_samples(bst_test) In\u00a0[127]: Copied! <pre>bst_test.lengths\n</pre> bst_test.lengths Out[127]: <pre>array([ 1, 15,  1,  1, 12,  2,  1, 14,  1,  1,  1, 15, 15, 15,  1, 14,  1,\n        9,  4, 15,  1,  1,  8,  1,  1,  4,  1, 15,  1,  1,  7,  4,  1,  1,\n        3])</pre> In\u00a0[128]: Copied! <pre>npl.imagesc(posterior_states[1])\n</pre> npl.imagesc(posterior_states[1]) Out[128]: <pre>(&lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffa140d5ef0&gt;,\n &lt;matplotlib.image.AxesImage at 0x7ffa140e7048&gt;)</pre> In\u00a0[129]: Copied! <pre>posteriors, bdries, mode_pth, mean_pth = hmm.decode_ext(\n    X=bst_test[1], ext_shape=(n_extern,)\n)\n\nnpl.imagesc(posteriors)\n</pre> posteriors, bdries, mode_pth, mean_pth = hmm.decode_ext(     X=bst_test[1], ext_shape=(n_extern,) )  npl.imagesc(posteriors) Out[129]: <pre>(&lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffa140bd438&gt;,\n &lt;matplotlib.image.AxesImage at 0x7ffa14088a58&gt;)</pre> In\u00a0[130]: Copied! <pre>posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(\n    X=bst_test, ext_shape=(vtc.n_bins,)\n)\nmean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0])\n</pre> posterior_pos, bdries, mode_pth, mean_pth = hmm.decode_ext(     X=bst_test, ext_shape=(vtc.n_bins,) ) mean_pth = vtc.bins[0] + mean_pth * (vtc.bins[-1] - vtc.bins[0]) In\u00a0[131]: Copied! <pre>plt.plot(\n    pos1d.asarray(at=bst_test.bin_centers).yvals,\n    c=\"0.6\",\n    label=\"True linearized position\",\n)\nplt.plot(mean_pth, label=\"HMM decoded position\")\nplt.legend(loc=(1.025, 0.5))\nplt.ylabel(\"position (cm)\")\n</pre> plt.plot(     pos1d.asarray(at=bst_test.bin_centers).yvals,     c=\"0.6\",     label=\"True linearized position\", ) plt.plot(mean_pth, label=\"HMM decoded position\") plt.legend(loc=(1.025, 0.5)) plt.ylabel(\"position (cm)\") Out[131]: <pre>Text(0,0.5,'position (cm)')</pre> In\u00a0[140]: Copied! <pre>def first_event(st):\n    \"\"\"Returns the [time of the] first event across all series.\"\"\"\n    first = np.inf\n    for series in st.data:\n        if series[0] &lt; first:\n            first = series[0]\n    return first\n\n\ndef last_event(st):\n    \"\"\"Returns the [time of the] last event across all series.\"\"\"\n    last = -np.inf\n    for series in st.data:\n        if series[-1] &gt; last:\n            last = series[-1]\n    return last\n</pre> def first_event(st):     \"\"\"Returns the [time of the] first event across all series.\"\"\"     first = np.inf     for series in st.data:         if series[0] &lt; first:             first = series[0]     return first   def last_event(st):     \"\"\"Returns the [time of the] last event across all series.\"\"\"     last = -np.inf     for series in st.data:         if series[-1] &gt; last:             last = series[-1]     return last In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/LinearTrackDemo/#linear-track-demo","title":"Linear track demo\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#preliminaries","title":"Preliminaries\u00b6","text":"<p>We use the following Python packages; they can be installed either with <code>conda install &lt;pkg&gt;</code> or with <code>pip install &lt;pkg&gt;</code>. Most of these packages are very common, and it is more than likely that you have them installed already.</p> <p>It doesn't hurt to type <code>pip install &lt;pkg&gt;</code> if the package is already installed, so don't be nervous to try it out!</p> <pre>pip install nelpy\n</pre> <p>For downloading example data from the web:</p> <pre>1. requests   # used to download data from web\n2. tqdm       # used to show progress of download\n</pre> <p>the rest of this notebook:</p> <pre>3. numpy      # numerical powerhorse for Python\n4. matplotlib # used to make plots and figures\n5. scipy      # signal processing, stats, etc.; used here to smooth signals\n6. sklearn    # machine learning in Python; used here to do train-test split \n              # (under the hood) when evaluating Bayesian decoding performance\n</pre> <p>and of course:</p> <pre>7. nelpy      # Ephys object models, and analysis routines\n</pre> <p>Now that we have all the packages we nee, let's get started! First, we need to get the sample data...</p>"},{"location":"tutorials/LinearTrackDemo/#1-obtain-example-data","title":"1. Obtain example data\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#2-import-packages","title":"2. Import packages\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#3-extract-position-data","title":"3. Extract position data\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#3a-get-session-boundaries","title":"3.a) Get session boundaries\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#3b-create-2d-trajectory-object","title":"3.b) Create 2D trajectory object\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#3c-linearize-position","title":"3.c) Linearize position\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#4-extract-spike-times","title":"4. Extract spike times\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#visualize-spikes-with-raster-plots","title":"Visualize spikes with raster plots\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#5-estimate-tuning-curves","title":"5. Estimate tuning curves\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#restrict-subset-of-cells-units-to-use-for-subsequent-decoding-andor-analysis","title":"Restrict subset of cells (units) to use for subsequent decoding and/or analysis\u00b6","text":"<p>Here we may request to use</p> <ol> <li>unimodal cells only,</li> <li>pyramidal cells only,</li> <li>active cells only,</li> <li>any combination of the above, and other criteria</li> </ol> <p>Here we only impose a minimum peak firing rate of 1.5 Hz, and we reject putative interneurons by imposing a maximum average firing rate of 5 Hz averaged over the entire track.</p>"},{"location":"tutorials/LinearTrackDemo/#6-evaluate-decoding-performance","title":"6. Evaluate decoding performance\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#some-additional-views","title":"Some additional views\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#supplementary-analysis-hidden-markov-models-hmms","title":"Supplementary analysis: hidden Markov models (HMMs)\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#preliminaries","title":"Preliminaries\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#overview","title":"Overview\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#estimate-virtual-tuning-curves","title":"Estimate virtual tuning curves\u00b6","text":""},{"location":"tutorials/LinearTrackDemo/#decoding-to-the-state-space","title":"Decoding to the state space:\u00b6","text":""},{"location":"tutorials/SpikeTrainSmoothing/","title":"SpikeTrain Smoothing","text":"<p>In particular, given a <code>SpikeTrainArray</code> or a collection of events, we want to compute the exact smoothed version by convolving with a truncated Gaussian kernel. The kernel shape should be passed in as arguments (default trunc = 4 standard deviations, sigma = 100 ms), and from that, we need to determine the appropriate window specification and scale parameter for the Gaussian kernel.</p> <p>Furthermore, we need to think about how we want to deal with edge conditions. We may want to do smoothing (1) within, or (2) across epochs. Furthermore, we may want to do (i) natural smoothing, cutting off any mass outside of the support, or we may choose to (ii) clump at the edge, or (iii) reflect at the edges, or (iv) re-adjust after-the-fact, etc.</p> <p>I personally think natural smoothing with support for either within or across epoch smoothing would be perfect.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nimport nelpy as nel\n\n%matplotlib inline\n</pre> import matplotlib.pyplot as plt import numpy as np  import nelpy as nel  %matplotlib inline <ul> <li>create bins within which to aggregate</li> <li>compute bin centers</li> <li>compute offsets and bin_idx for each spike</li> <li>compute weight vector for each spike</li> <li>aggregate spike weights into the appropriate bins, by using bin_idx from above</li> <li>how about edge conditions? how about different epochs? how about which bins to keep, finally?</li> </ul> In\u00a0[2]: Copied! <pre>bins = np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0])\nspikes = np.array([0.1, 0.3, 0.55, 0.56, 0.57, 0.58, 0.59, 0.62, 0.9])\n</pre> bins = np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0]) spikes = np.array([0.1, 0.3, 0.55, 0.56, 0.57, 0.58, 0.59, 0.62, 0.9]) In\u00a0[3]: Copied! <pre>st = nel.SpikeTrainArray(spikes)\nst.support = [0, 1]\n\nds = 0.2\n\nfig = plt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\n\n# bin without smoothing\nbst = st.bin(ds=ds)\nn_bins = bst.n_bins\n\nplt.bar(range(n_bins), bst.data.squeeze())\nplt.title(\"binning without smoothing\")\n\nplt.subplot(1, 2, 2)\n# smooth and re-bin:\ndss = 0.01\nsigma = 0.1  # 100 ms spike smoothing\nbst = st.bin(ds=dss).smooth(sigma=sigma, inplace=True).rebin(w=ds / dss)\n\nplt.bar(range(n_bins), bst.data.squeeze())\nplt.title(\"smoothing by binning to a fine bin size first\")\n\nassert bst.data.sum() == 9, (\n    \"The sum must equal 9, because we had nine spikes to spread over the bins\"\n)\n</pre> st = nel.SpikeTrainArray(spikes) st.support = [0, 1]  ds = 0.2  fig = plt.figure(figsize=(12, 4)) plt.subplot(1, 2, 1)  # bin without smoothing bst = st.bin(ds=ds) n_bins = bst.n_bins  plt.bar(range(n_bins), bst.data.squeeze()) plt.title(\"binning without smoothing\")  plt.subplot(1, 2, 2) # smooth and re-bin: dss = 0.01 sigma = 0.1  # 100 ms spike smoothing bst = st.bin(ds=dss).smooth(sigma=sigma, inplace=True).rebin(w=ds / dss)  plt.bar(range(n_bins), bst.data.squeeze()) plt.title(\"smoothing by binning to a fine bin size first\")  assert bst.data.sum() == 9, (     \"The sum must equal 9, because we had nine spikes to spread over the bins\" ) <pre>WARNING:root:No sampling rate was specified! Assuming default of 30000 Hz.\n</pre> <ul> <li>We need to relate sigma to the scale and bw, so that we can set the proper scale, and have the proper window</li> <li>We also need to deal elegantly with edge conditions, within or across epochs</li> </ul> In\u00a0[4]: Copied! <pre>import scipy\n\nbin_indices = np.digitize(spikes, bins) - 1\noffsets = (\n    ds / 2 - spikes + bins[bin_indices]\n)  # offset for each spike from the bin start, to the spike event\n\nn_spikes = len(spikes)\nn_bins = len(bins) - 1\n\nbw = 1\nwindow = np.array([[-1, 0, 1]])\nweights = (np.repeat(np.array([[-0.1, 0, 0.1]]), n_spikes, axis=0).T + offsets / 2).T\nweights = scipy.stats.norm.pdf(weights, scale=0.055)\n\n# weights = np.hstack((np.fliplr(weights[:,1:]), weights))\nweights = (weights.T / weights.sum(axis=1)).T\n\ndata = np.zeros(n_bins)\nfor weight_vector, bin_idx in zip(weights, bin_indices):\n    # decide how to deal with edge effects: across epochs? within epochs? reflect? natural?\n    # for now, we ignore any mass outside of the epoch\n    bin_range = (window + bin_idx).squeeze()\n    weight_vector = weight_vector[(bin_range &gt; -1) &amp; (bin_range &lt; n_bins)]\n    bin_range = bin_range[(bin_range &gt; -1) &amp; (bin_range &lt; n_bins)]\n    #     print('bin_range', bin_range)\n    try:\n        data[bin_range] += weight_vector\n    except (IndexError, ValueError):\n        pass\n\nplt.bar(range(n_bins), data)\n</pre> import scipy  bin_indices = np.digitize(spikes, bins) - 1 offsets = (     ds / 2 - spikes + bins[bin_indices] )  # offset for each spike from the bin start, to the spike event  n_spikes = len(spikes) n_bins = len(bins) - 1  bw = 1 window = np.array([[-1, 0, 1]]) weights = (np.repeat(np.array([[-0.1, 0, 0.1]]), n_spikes, axis=0).T + offsets / 2).T weights = scipy.stats.norm.pdf(weights, scale=0.055)  # weights = np.hstack((np.fliplr(weights[:,1:]), weights)) weights = (weights.T / weights.sum(axis=1)).T  data = np.zeros(n_bins) for weight_vector, bin_idx in zip(weights, bin_indices):     # decide how to deal with edge effects: across epochs? within epochs? reflect? natural?     # for now, we ignore any mass outside of the epoch     bin_range = (window + bin_idx).squeeze()     weight_vector = weight_vector[(bin_range &gt; -1) &amp; (bin_range &lt; n_bins)]     bin_range = bin_range[(bin_range &gt; -1) &amp; (bin_range &lt; n_bins)]     #     print('bin_range', bin_range)     try:         data[bin_range] += weight_vector     except (IndexError, ValueError):         pass  plt.bar(range(n_bins), data) Out[4]: <pre>&lt;BarContainer object of 5 artists&gt;</pre>"},{"location":"tutorials/SpikeTrainSmoothing/#spiketrain-smoothing","title":"SpikeTrain Smoothing\u00b6","text":""},{"location":"tutorials/SpikeTrainSmoothing/#goal","title":"Goal:\u00b6","text":"<ul> <li>exact convolution for spike density estimation (as opposed to convolution over a discretized, binned, space)</li> </ul>"},{"location":"tutorials/SpikeTrainSmoothing/#exact-convolution-for-spike-density-estimation","title":"Exact convolution for spike density estimation\u00b6","text":"<p>Usually we have to do something like this to get better decoding performance:</p> <pre>ds_run = 0.125\nds_50ms = 0.05\nst_run = st[run_epochs]\n\n# smooth and re-bin:\nsigma = 0.3 # 300 ms spike smoothing\nbst_run = st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run/ds_50ms)\n\nsigma = 3.1; # smoothing std dev in cm\ntc = nel.TuningCurve1D(bst=bst_run, extern=pos1d, n_extern=100, extmin=0, extmax=180, sigma=sigma, min_duration=0)\n</pre> <p>But we can do better than binning-then-smoothing! We can concolve directly (and exactly), which better preserves the fine temporal dynamics in the spikes.</p>"},{"location":"tutorials/SpikeTrainSmoothing/#example-convolution","title":"example convolution\u00b6","text":"<pre>bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\nspikes = [0.1, 0.3, 0.55, 0.56, 0.57, 0.58, 0.59, 0.62, 0.9]\n</pre> <p>In the above example, if we binned first, we would obtain</p> <pre>binned = [1, 1, 5, 1, 1]\n</pre> <p>so that the 5 spikes in the 3rd bin would be symmetrically smoothed to both the 2nd and 4th bins (and, depending on the size of the smoothing kernel, also to the 1st and 5th bins).</p> <p>However, closer inspection of the spike times reveals that they are heavily concentrated close to the right edge of the 2nd bin. In addition, the single spike in the 4th bin is close to the left edge of bin 4, so that the TRUE spike density should not have the same value at both the 2nd and 4th bins.</p>"},{"location":"tutorials/SpikeTrainSmoothing/#work-in-progress","title":"Work in progress\u00b6","text":""},{"location":"tutorials/SpikeTrainSmoothing/#discussion","title":"Discussion\u00b6","text":"<p>This notebook is not meant to be an efficient, nor a guaranteed-to-be-correct implementation, but rather a proof-of-concept starting point for thinking about the smoothing-before-binning approach. Savva has made some good progress in making this more useful and efficient, so this is only for quick reference.</p>"},{"location":"tutorials/WMazeDemo/","title":"WMaze Demo","text":"<p>Here we load sorted unit data obtained from MatClust to identify putative place cells. The recording session has two runs on a w-maze, each one followed by a period of rest. Each segment (both run and rest segments) is about 15 minutes in duration.</p> <p>Data was recorded by Joshua Chu, with a Spikegadgets wireless headstage, on July 8th, 2017, from the CA! area of a male Long-Evans rat named <code>install</code>.</p> <p>Notebook was created by Etienne Ackermann.</p> <p>We will look for data in the <code>example-data\\w-maze\\</code> directory inside your current working directory. If the data doesn't exist, we will download it from https://github.com/nelpy/example-data, and save it to your local machine.</p> <p>If you already have the data, it won't be downloaded again.</p> <p>In particular, we will download two files, namely</p> <ol> <li><code>trajectory.videoPositionTracking</code> which is a binary file with (x,y) position coordinate pairs and timestamps, and</li> <li><code>spikes.mat</code> which is a Matlab file containing information about sorted units (cells) obtained by using MatClust (https://bitbucket.org/mkarlsso/matclust).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport requests\n\n# from tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm\n\ndatadir = os.path.join(os.getcwd(), \"example-data\\w-maze\")\nos.makedirs(datadir, exist_ok=True)\n\nfilenames = []\nfilenames.append(os.path.join(datadir, \"trajectory.videoPositionTracking\"))\nfilenames.append(os.path.join(datadir, \"spikes.mat\"))\nurls = []\nurls.append(\n    \"https://github.com/nelpy/example-data/raw/master/w-maze/trajectory.videoPositionTracking\"\n)\nurls.append(\"https://github.com/nelpy/example-data/raw/master/w-maze/spikes.mat\")\n\nfor filename, url in zip(filenames, urls):\n    if os.path.exists(filename):\n        print(\"you already have the example data, skipping download...\")\n    else:\n        print(\"downloading data from {}\".format(url))\n        # Streaming, so we can iterate over the response.\n        r = requests.get(url, stream=True)\n\n        # Total size in bytes.\n        total_size = int(r.headers.get(\"content-length\", 0))\n        chunk_size = 1024  # number of bytes to process at a time (NOTE: progress bar unit only accurate if this is 1 kB)\n\n        with open(filename, \"wb+\") as f:\n            for data in tqdm(\n                r.iter_content(chunk_size),\n                total=int(total_size / chunk_size),\n                unit=\"kB\",\n            ):\n                f.write(data)\n\n        print(\"data saved to local directory {}\".format(filename))\n\nfilename_pos = filenames[0]\nfilename_spikes = filenames[1]\n</pre> import os  import requests  # from tqdm import tqdm_notebook as tqdm from tqdm import tqdm  datadir = os.path.join(os.getcwd(), \"example-data\\w-maze\") os.makedirs(datadir, exist_ok=True)  filenames = [] filenames.append(os.path.join(datadir, \"trajectory.videoPositionTracking\")) filenames.append(os.path.join(datadir, \"spikes.mat\")) urls = [] urls.append(     \"https://github.com/nelpy/example-data/raw/master/w-maze/trajectory.videoPositionTracking\" ) urls.append(\"https://github.com/nelpy/example-data/raw/master/w-maze/spikes.mat\")  for filename, url in zip(filenames, urls):     if os.path.exists(filename):         print(\"you already have the example data, skipping download...\")     else:         print(\"downloading data from {}\".format(url))         # Streaming, so we can iterate over the response.         r = requests.get(url, stream=True)          # Total size in bytes.         total_size = int(r.headers.get(\"content-length\", 0))         chunk_size = 1024  # number of bytes to process at a time (NOTE: progress bar unit only accurate if this is 1 kB)          with open(filename, \"wb+\") as f:             for data in tqdm(                 r.iter_content(chunk_size),                 total=int(total_size / chunk_size),                 unit=\"kB\",             ):                 f.write(data)          print(\"data saved to local directory {}\".format(filename))  filename_pos = filenames[0] filename_spikes = filenames[1] <pre>you already have the example data, skipping download...\nyou already have the example data, skipping download...\n</pre> In\u00a0[\u00a0]: Copied! <pre>import struct\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport nelpy as nel\nimport nelpy.plotting as npl\n\n# assume default aesthetics\nnpl.setup()\n\n%matplotlib inline\n</pre> import struct  import matplotlib.pyplot as plt import numpy as np  import nelpy as nel import nelpy.plotting as npl  # assume default aesthetics npl.setup()  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>def print_header(filename, timeout=50):\n    \"\"\"Reads header lines from a SpikeGadgets .rec file, and prints it to screen.\"\"\"\n    linecount = 0\n    with open(filename, \"rb\") as fileobj:\n        instr = fileobj.readline()\n        linecount += 1\n        while instr != b\"&lt;End settings&gt;\\n\":\n            print(instr)\n            instr = fileobj.readline()\n            if linecount &gt; timeout:\n                break\n    print(instr)\n</pre> def print_header(filename, timeout=50):     \"\"\"Reads header lines from a SpikeGadgets .rec file, and prints it to screen.\"\"\"     linecount = 0     with open(filename, \"rb\") as fileobj:         instr = fileobj.readline()         linecount += 1         while instr != b\"\\n\":             print(instr)             instr = fileobj.readline()             if linecount &gt; timeout:                 break     print(instr) In\u00a0[11]: Copied! <pre>print_header(filename_pos)\n</pre> print_header(filename_pos) <pre>b'&lt;Start settings&gt;\\n'\nb'threshold: 199\\n'\nb'dark: 0\\n'\nb'clockrate: 30000\\n'\nb'camera resolution: 640x480\\n'\nb'pixel scale: 0 pix/cm\\n'\nb'Fields: &lt;time uint32&gt;&lt;xloc uint16&gt;&lt;yloc uint16&gt;&lt;xloc2 uint16&gt;&lt;yloc2 uint16&gt;\\n'\nb'&lt;End settings&gt;\\n'\n</pre> In\u00a0[\u00a0]: Copied! <pre>n_packets = 500000\ntimestamps = []\nx1 = []\ny1 = []\nx2 = []\ny2 = []\nii = 0\nwith open(filename_pos, \"rb\") as fileobj:\n    instr = fileobj.readline()\n    while instr != b\"&lt;End settings&gt;\\n\":\n        print(instr)\n        instr = fileobj.readline()\n    for packet in iter(lambda: fileobj.read(12), \"\"):\n        if packet:\n            ts_ = struct.unpack(\"&lt;L\", packet[0:4])[0]\n            x1_ = struct.unpack(\"&lt;H\", packet[4:6])[0]\n            y1_ = struct.unpack(\"&lt;H\", packet[6:8])[0]\n            x2_ = struct.unpack(\"&lt;H\", packet[8:10])[0]\n            y2_ = struct.unpack(\"&lt;H\", packet[10:12])[0]\n            timestamps.append(ts_)\n            x1.append(x1_)\n            y1.append(y1_)\n            x2.append(x2_)\n            y2.append(y2_)\n        else:\n            break\n        if ii &gt;= n_packets:\n            print(\"Stopped before reaching end of file\")\n            break\n</pre> n_packets = 500000 timestamps = [] x1 = [] y1 = [] x2 = [] y2 = [] ii = 0 with open(filename_pos, \"rb\") as fileobj:     instr = fileobj.readline()     while instr != b\"\\n\":         print(instr)         instr = fileobj.readline()     for packet in iter(lambda: fileobj.read(12), \"\"):         if packet:             ts_ = struct.unpack(\"= n_packets:             print(\"Stopped before reaching end of file\")             break <pre>b'&lt;Start settings&gt;\\n'\nb'threshold: 199\\n'\nb'dark: 0\\n'\nb'clockrate: 30000\\n'\nb'camera resolution: 640x480\\n'\nb'pixel scale: 0 pix/cm\\n'\nb'Fields: &lt;time uint32&gt;&lt;xloc uint16&gt;&lt;yloc uint16&gt;&lt;xloc2 uint16&gt;&lt;yloc2 uint16&gt;\\n'\n</pre> In\u00a0[\u00a0]: Copied! <pre># we estimate large periods of inactivity as periods where the animal's estimated position did not move for at least 10 seconds\nminLength = 1800  # 30 seconds @ 60 fps\nbounds, _, _ = nel.utils.get_events_boundaries(\n    np.gradient(x1),\n    PrimaryThreshold=0,\n    SecondaryThreshold=0,\n    mode=\"below\",\n    minLength=minLength,\n    ds=1,\n)\n\nFS = 30000\nbounds_ts = np.zeros(bounds.shape)\nfor row in range(len(bounds)):\n    for col in range(2):\n        bounds_ts[row, col] = timestamps[bounds[row, col]]\n\nrest = nel.EpochArray(\n    bounds_ts / FS, domain=nel.EpochArray((timestamps[0] / FS, timestamps[-1] / FS))\n)\nsession_epochs = rest + ~rest\nsession_epochs._sort()\n</pre> # we estimate large periods of inactivity as periods where the animal's estimated position did not move for at least 10 seconds minLength = 1800  # 30 seconds @ 60 fps bounds, _, _ = nel.utils.get_events_boundaries(     np.gradient(x1),     PrimaryThreshold=0,     SecondaryThreshold=0,     mode=\"below\",     minLength=minLength,     ds=1, )  FS = 30000 bounds_ts = np.zeros(bounds.shape) for row in range(len(bounds)):     for col in range(2):         bounds_ts[row, col] = timestamps[bounds[row, col]]  rest = nel.EpochArray(     bounds_ts / FS, domain=nel.EpochArray((timestamps[0] / FS, timestamps[-1] / FS)) ) session_epochs = rest + ~rest session_epochs._sort() In\u00a0[14]: Copied! <pre>for ep in session_epochs:\n    print(ep.duration)\n</pre> for ep in session_epochs:     print(ep.duration) <pre>1:32:265 minutes\n18:10:642 minutes\n17:06:481 minutes\n20:08:130 minutes\n15:48:443 minutes\n</pre> In\u00a0[15]: Copied! <pre>rest\n</pre> rest Out[15]: <pre>&lt;EpochArray at 0x17b899b60f0: 3 epochs&gt; of duration 34:27:189 minutes</pre> In\u00a0[16]: Copied! <pre>~rest\n</pre> ~rest Out[16]: <pre>&lt;EpochArray at 0x17b899aa5f8: 2 epochs&gt; of duration 38:18:772 minutes</pre> In\u00a0[\u00a0]: Copied! <pre>pos = nel.AnalogSignalArray(\n    np.vstack((x1, y1)),\n    timestamps=np.array(timestamps) / FS,\n    support=(~rest).shrink(20),\n    fs=60,\n)\n</pre> pos = nel.AnalogSignalArray(     np.vstack((x1, y1)),     timestamps=np.array(timestamps) / FS,     support=(~rest).shrink(20),     fs=60, ) <pre>c:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\n</pre> In\u00a0[\u00a0]: Copied! <pre>ax = npl.plot2d(pos.simplify(n_points=20000), lw=1)\nax.set_aspect(\"equal\")\nax.set_ylim(90, 440)\nax.set_xlim(190, 540)\n</pre> ax = npl.plot2d(pos.simplify(n_points=20000), lw=1) ax.set_aspect(\"equal\") ax.set_ylim(90, 440) ax.set_xlim(190, 540) Out[\u00a0]: <pre>(190, 540)</pre> In\u00a0[\u00a0]: Copied! <pre># rough pixel-to-cm conversion:\npixels_per_cm = (475 - 250) / 70\n</pre> # rough pixel-to-cm conversion: pixels_per_cm = (475 - 250) / 70 In\u00a0[\u00a0]: Copied! <pre>with npl.FigureManager(show=True, figsize=(10, 4), nrows=1, ncols=2) as (fig, axes):\n    npl.utils.skip_if_no_output(fig)\n    ax0, ax1 = axes\n    npl.plot2d(pos[0].smooth(sigma=0.1), lw=0.5, color=\"0.2\", ax=ax0)\n    npl.plot2d(pos[1].smooth(sigma=0.1), lw=0.5, color=\"0.2\", ax=ax1)\n    for ii, ax in enumerate(axes):\n        ax.set_aspect(\"equal\")\n        ax.set_ylim(90, 440)\n        ax.set_xlim(190, 540)\n        npl.utils.clear_left_right(ax)\n        npl.utils.clear_top_bottom(ax)\n        ax.set_title(\"run session {}\".format(ii + 1))\n</pre> with npl.FigureManager(show=True, figsize=(10, 4), nrows=1, ncols=2) as (fig, axes):     npl.utils.skip_if_no_output(fig)     ax0, ax1 = axes     npl.plot2d(pos[0].smooth(sigma=0.1), lw=0.5, color=\"0.2\", ax=ax0)     npl.plot2d(pos[1].smooth(sigma=0.1), lw=0.5, color=\"0.2\", ax=ax1)     for ii, ax in enumerate(axes):         ax.set_aspect(\"equal\")         ax.set_ylim(90, 440)         ax.set_xlim(190, 540)         npl.utils.clear_left_right(ax)         npl.utils.clear_top_bottom(ax)         ax.set_title(\"run session {}\".format(ii + 1)) <pre>c:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\nc:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\n</pre> In\u00a0[21]: Copied! <pre>mat = nel.io.matlab.load(filename_spikes)\n</pre> mat = nel.io.matlab.load(filename_spikes) In\u00a0[\u00a0]: Copied! <pre># Epoch for which spikes were sorted\nsession_bounds = nel.EpochArray(mat[\"spikes\"][0][0][\"timerange\"].ravel()[0])\n</pre> # Epoch for which spikes were sorted session_bounds = nel.EpochArray(mat[\"spikes\"][0][0][\"timerange\"].ravel()[0]) In\u00a0[\u00a0]: Copied! <pre>spikes = []\nct = 0\nnum_array = 0\nfor ii, array in enumerate(mat[\"spikes\"]):\n    # If empty array, that particular tetrode was not sorted\n    if array.size &gt; 1:\n        for jj, subarray in enumerate(array):\n            if subarray.size != 0:\n                # Exclude tetrodes with no spikes\n                if len(subarray[\"time\"].ravel()[0]) != 0:\n                    spikes.append(subarray[\"time\"].ravel()[0])\n                    ct += 1\n    elif array.size == 1:\n        if len(array[\"time\"].ravel()[0]) != 0:\n            spikes.append(array[\"time\"].ravel()[0])\n            ct += 1\nprint(\"Found {} units total\".format(ct))\n</pre> spikes = [] ct = 0 num_array = 0 for ii, array in enumerate(mat[\"spikes\"]):     # If empty array, that particular tetrode was not sorted     if array.size &gt; 1:         for jj, subarray in enumerate(array):             if subarray.size != 0:                 # Exclude tetrodes with no spikes                 if len(subarray[\"time\"].ravel()[0]) != 0:                     spikes.append(subarray[\"time\"].ravel()[0])                     ct += 1     elif array.size == 1:         if len(array[\"time\"].ravel()[0]) != 0:             spikes.append(array[\"time\"].ravel()[0])             ct += 1 print(\"Found {} units total\".format(ct)) <pre>Found 25 units total\n</pre> In\u00a0[24]: Copied! <pre>st = nel.SpikeTrainArray(timestamps=spikes, support=session_bounds, fs=FS)\nst.support\n</pre> st = nel.SpikeTrainArray(timestamps=spikes, support=session_bounds, fs=FS) st.support Out[24]: <pre>&lt;EpochArray at 0x17b8a22bef0: 1 epoch&gt; of duration 1:11:46:831 hours</pre> In\u00a0[25]: Copied! <pre>rest.domain\n</pre> rest.domain Out[25]: <pre>&lt;EpochArray at 0x17b8a35c978: 1 epoch&gt; of duration 1:12:45:962 hours</pre> In\u00a0[\u00a0]: Copied! <pre>with npl.FigureManager(show=True, figsize=(30, 4)) as (fig, ax):\n    npl.utils.skip_if_no_output(fig)\n    npl.rasterplot(st, lw=0.5, ax=ax)\n    npl.epochplot(~rest, alpha=0.3)\n    ax.set_xlim(*session_bounds.time)\n</pre> with npl.FigureManager(show=True, figsize=(30, 4)) as (fig, ax):     npl.utils.skip_if_no_output(fig)     npl.rasterplot(st, lw=0.5, ax=ax)     npl.epochplot(~rest, alpha=0.3)     ax.set_xlim(*session_bounds.time) In\u00a0[\u00a0]: Copied! <pre>sigma_100ms = 0.1\nspeed2d = (\n    nel.utils.dxdt_AnalogSignalArray(pos, smooth=True, sigma=sigma_100ms)\n    / pixels_per_cm\n)\n</pre> sigma_100ms = 0.1 speed2d = (     nel.utils.dxdt_AnalogSignalArray(pos, smooth=True, sigma=sigma_100ms)     / pixels_per_cm ) <pre>c:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\nc:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\nc:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\nc:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\n</pre> In\u00a0[\u00a0]: Copied! <pre>run_epochs = nel.utils.get_run_epochs(\n    speed2d.smooth(sigma=0.5), v1=2, v2=1\n)  # original choice\n</pre> run_epochs = nel.utils.get_run_epochs(     speed2d.smooth(sigma=0.5), v1=2, v2=1 )  # original choice <pre>c:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\nc:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\n</pre> In\u00a0[31]: Copied! <pre>run_epochs\n</pre> run_epochs Out[31]: <pre>&lt;EpochArray at 0x17b89d6c470: 180 epochs&gt; of duration 15:20:877 minutes</pre> In\u00a0[\u00a0]: Copied! <pre>with npl.FigureManager(show=True, figsize=(64, 3)) as (fig, ax):\n    npl.utils.skip_if_no_output(fig)\n    plt.plot(pos.time, pos.asarray().yvals[0, :], lw=1, alpha=0.2, color=\"gray\")\n    plt.plot(pos.time, pos.asarray().yvals[1, :], lw=1, alpha=0.2, color=\"gray\")\n    npl.plot(pos[run_epochs], ax=ax, lw=1, label=\"run\")\n\n    plt.title(\"run activity\")\n</pre> with npl.FigureManager(show=True, figsize=(64, 3)) as (fig, ax):     npl.utils.skip_if_no_output(fig)     plt.plot(pos.time, pos.asarray().yvals[0, :], lw=1, alpha=0.2, color=\"gray\")     plt.plot(pos.time, pos.asarray().yvals[1, :], lw=1, alpha=0.2, color=\"gray\")     npl.plot(pos[run_epochs], ax=ax, lw=1, label=\"run\")      plt.title(\"run activity\") <pre>c:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_analogsignalarray.py:449: UserWarning: ignoring signal outside of support\n</pre> In\u00a0[33]: Copied! <pre>st_run = st[run_epochs]\n</pre> st_run = st[run_epochs] In\u00a0[\u00a0]: Copied! <pre>ds_run = 0.5  # 100 ms\nds_50ms = 0.05\n\n# smooth and re-bin:\nsigma = 0.3  # 300 ms spike smoothing\nbst_run = (\n    st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms)\n)\n\nsigma = 0.2  # smoothing std dev in cm\ntc2d = nel.TuningCurve2D(\n    bst=bst_run,\n    extern=pos,\n    ext_nx=50,\n    ext_ny=50,\n    ext_xmin=190,\n    ext_xmax=540,\n    ext_ymin=90,\n    ext_ymax=440,\n    sigma=sigma,\n    min_duration=0,\n)\n\nplt.matshow(tc2d.occupancy.T, cmap=plt.cm.Spectral_r)\nplt.gca().invert_yaxis()\n</pre> ds_run = 0.5  # 100 ms ds_50ms = 0.05  # smooth and re-bin: sigma = 0.3  # 300 ms spike smoothing bst_run = (     st_run.bin(ds=ds_50ms).smooth(sigma=sigma, inplace=True).rebin(w=ds_run / ds_50ms) )  sigma = 0.2  # smoothing std dev in cm tc2d = nel.TuningCurve2D(     bst=bst_run,     extern=pos,     ext_nx=50,     ext_ny=50,     ext_xmin=190,     ext_xmax=540,     ext_ymin=90,     ext_ymax=440,     sigma=sigma,     min_duration=0, )  plt.matshow(tc2d.occupancy.T, cmap=plt.cm.Spectral_r) plt.gca().invert_yaxis() <pre>c:\\etienne\\dropbox\\code\\nelpy\\nelpy\\core\\_spiketrain.py:361: UserWarning: unit tags have not yet been specified\n</pre> In\u00a0[\u00a0]: Copied! <pre>uu = 0\nuu += 1\nnpl.imagesc(tc2d.smooth(sigma=6.2).ratemap[uu, :, :], cmap=plt.cm.Spectral_r)\n</pre> uu = 0 uu += 1 npl.imagesc(tc2d.smooth(sigma=6.2).ratemap[uu, :, :], cmap=plt.cm.Spectral_r) Out[\u00a0]: <pre>(&lt;matplotlib.axes._subplots.AxesSubplot at 0x17b904a4978&gt;,\n &lt;matplotlib.image.AxesImage at 0x17b900a3588&gt;)</pre> In\u00a0[\u00a0]: Copied! <pre>ext_nx = 50\next_ny = 50\nx0 = 190\nxl = 540\ny0 = 90\nyl = 440\n\nxx_left = np.linspace(x0, xl, ext_nx + 1)\nxx_mid = np.linspace(x0, xl, ext_nx + 1)[:-1]\nxx_mid += (xx_mid[1] - xx_mid[0]) / 2\nyy_left = np.linspace(y0, yl, ext_ny + 1)\nyy_mid = np.linspace(y0, yl, ext_ny + 1)[:-1]\nyy_mid += (yy_mid[1] - yy_mid[0]) / 2\n\ntvals, vals = pos.asarray()\nxvals, yvals = vals\n\nsigma_tc = 7\nratemap = tc2d.smooth(sigma=sigma_tc).ratemap\n\nwith npl.palettes.color_palette(npl.colors.rainbow):\n    with npl.FigureManager(show=True, figsize=(30, 30), nrows=5, ncols=5) as (\n        fig,\n        axes,\n    ):\n        npl.utils.skip_if_no_output(fig)\n        for ii, ax in enumerate(axes.ravel()):\n            placefield = ratemap[ii]\n            npl.imagesc(x=xx_mid, y=yy_mid, data=placefield.T, cmap=plt.cm.hot, ax=ax)\n            ax.plot(xvals, yvals, lw=0.25, color=\"w\", alpha=0.4)\n            ax.set_title(\"unit {}\".format(tc2d.unit_ids[ii]))\n            npl.utils.clear_left_right(ax)\n            npl.utils.clear_top_bottom(ax)\n</pre> ext_nx = 50 ext_ny = 50 x0 = 190 xl = 540 y0 = 90 yl = 440  xx_left = np.linspace(x0, xl, ext_nx + 1) xx_mid = np.linspace(x0, xl, ext_nx + 1)[:-1] xx_mid += (xx_mid[1] - xx_mid[0]) / 2 yy_left = np.linspace(y0, yl, ext_ny + 1) yy_mid = np.linspace(y0, yl, ext_ny + 1)[:-1] yy_mid += (yy_mid[1] - yy_mid[0]) / 2  tvals, vals = pos.asarray() xvals, yvals = vals  sigma_tc = 7 ratemap = tc2d.smooth(sigma=sigma_tc).ratemap  with npl.palettes.color_palette(npl.colors.rainbow):     with npl.FigureManager(show=True, figsize=(30, 30), nrows=5, ncols=5) as (         fig,         axes,     ):         npl.utils.skip_if_no_output(fig)         for ii, ax in enumerate(axes.ravel()):             placefield = ratemap[ii]             npl.imagesc(x=xx_mid, y=yy_mid, data=placefield.T, cmap=plt.cm.hot, ax=ax)             ax.plot(xvals, yvals, lw=0.25, color=\"w\", alpha=0.4)             ax.set_title(\"unit {}\".format(tc2d.unit_ids[ii]))             npl.utils.clear_left_right(ax)             npl.utils.clear_top_bottom(ax) In\u00a0[\u00a0]: Copied! <pre>unit_id = 9\n\nax = npl.plot2d(pos, lw=0.5, c=\"0.8\")\nat = st.loc[:, unit_id][pos.support].time[0]\n_, pos_at_spikes = pos.asarray(at=at)\nax.plot(\n    pos_at_spikes[0, :], pos_at_spikes[1, :], \"o\", alpha=0.5, color=npl.colors.sweet.red\n)\nax.set_aspect(\"equal\")\nax.set_title(\"unit {}\".format(unit_id), y=1.03)\nax.set_ylim(90, 440)\nax.set_xlim(190, 540)\n</pre> unit_id = 9  ax = npl.plot2d(pos, lw=0.5, c=\"0.8\") at = st.loc[:, unit_id][pos.support].time[0] _, pos_at_spikes = pos.asarray(at=at) ax.plot(     pos_at_spikes[0, :], pos_at_spikes[1, :], \"o\", alpha=0.5, color=npl.colors.sweet.red ) ax.set_aspect(\"equal\") ax.set_title(\"unit {}\".format(unit_id), y=1.03) ax.set_ylim(90, 440) ax.set_xlim(190, 540) Out[\u00a0]: <pre>(190, 540)</pre> In\u00a0[\u00a0]: Copied! <pre>unit_id = 17\n\nax = npl.plot2d(pos, lw=0.5, c=\"0.8\")\nat = st.loc[:, unit_id][pos.support].time[0]\n_, pos_at_spikes = pos.asarray(at=at)\nax.plot(\n    pos_at_spikes[0, :], pos_at_spikes[1, :], \"o\", alpha=0.5, color=npl.colors.sweet.red\n)\nax.set_aspect(\"equal\")\nax.set_title(\"unit {}\".format(unit_id), y=1.03)\nax.set_ylim(90, 440)\nax.set_xlim(190, 540)\n</pre> unit_id = 17  ax = npl.plot2d(pos, lw=0.5, c=\"0.8\") at = st.loc[:, unit_id][pos.support].time[0] _, pos_at_spikes = pos.asarray(at=at) ax.plot(     pos_at_spikes[0, :], pos_at_spikes[1, :], \"o\", alpha=0.5, color=npl.colors.sweet.red ) ax.set_aspect(\"equal\") ax.set_title(\"unit {}\".format(unit_id), y=1.03) ax.set_ylim(90, 440) ax.set_xlim(190, 540) Out[\u00a0]: <pre>(190, 540)</pre>"},{"location":"tutorials/WMazeDemo/#w-maze-demo","title":"W maze demo\u00b6","text":""},{"location":"tutorials/WMazeDemo/#2-obtain-example-data","title":"2. Obtain example data\u00b6","text":""},{"location":"tutorials/WMazeDemo/#get-session-boundaries-and-trajectory-objects","title":"Get session boundaries and trajectory objects\u00b6","text":""},{"location":"tutorials/WMazeDemo/#load-sorted-spike-data","title":"Load sorted spike data\u00b6","text":""},{"location":"tutorials/WMazeDemo/#find-spike-times-of-sorted-cells","title":"Find spike times of sorted cells\u00b6","text":""},{"location":"tutorials/develop/","title":"develop","text":"<p>Here we will look at some common patterns and conventions to follow during the development of nelpy.</p> <p>Let's start with the required imports.</p> In\u00a0[1]: Copied! <pre>import warnings\n\nimport matplotlib.pyplot as plt\n\nimport nelpy as nel  # recommended import for nelpy\nimport nelpy.plotting as npl  # recommended import for the nelpy plotting library\n\n%matplotlib inline\n</pre> import warnings  import matplotlib.pyplot as plt  import nelpy as nel  # recommended import for nelpy import nelpy.plotting as npl  # recommended import for the nelpy plotting library  %matplotlib inline <p>In general warnings should not all be supressed. They are usually useful, and sometimes critical in order to make sure that you get the expected behavior from nelpy. Sometimes, however, it is best to suppress warnings. Consider the following example.</p> In\u00a0[2]: Copied! <pre>epocharray = nel.EpochArray([[3, 4], [5, 8], [10, 12], [16, 20], [22, 23]])\ndata = [3, 4, 2, 5, 2]\n\nfor epoch in epocharray:\n    print(epoch)\n</pre> epocharray = nel.EpochArray([[3, 4], [5, 8], [10, 12], [16, 20], [22, 23]]) data = [3, 4, 2, 5, 2]  for epoch in epocharray:     print(epoch) <pre>&lt;EpochArray at 0x167775e3f10: 1 epoch&gt; of length 1 seconds\n&lt;EpochArray at 0x16777662b90: 1 epoch&gt; of length 3 seconds\n&lt;EpochArray at 0x167771c2650: 1 epoch&gt; of length 2 seconds\n&lt;EpochArray at 0x167775e3f10: 1 epoch&gt; of length 4 seconds\n&lt;EpochArray at 0x1677680bf10: 1 epoch&gt; of length 1 seconds\n</pre> <p>We see that we can easily iterate over all the epochs in <code>epocharray</code>, but we also get two warnings as a result, namely that no sampling frequency has been specified, and that no meta data has been set. These are useful warnings when constructing an (important or critical) EpochArray object, but not when we simply want to iterate over the epochs, or for example, when we want to compute or plot something related to those epochs.</p> <p>The implementation of nelpy.plotting.core.plot (or simply npl.plot) looks similar to this:</p> In\u00a0[3]: Copied! <pre>for epoch, val in zip(epocharray, data):\n    plt.plot(\n        [epoch.start, epoch.stop],\n        [val, val],\n        \"-o\",\n        color=\"k\",\n        markerfacecolor=\"w\",\n        lw=1.5,\n        mew=1.5,\n    )\n</pre> for epoch, val in zip(epocharray, data):     plt.plot(         [epoch.start, epoch.stop],         [val, val],         \"-o\",         color=\"k\",         markerfacecolor=\"w\",         lw=1.5,         mew=1.5,     ) <p>But again, we notice those warnings showing up. When plotting this type of figure, we do not need to warn the user about not having access to meta data, or not having an explicitly set sampling rate. Those warnings should have been dealt with when the actual data objects were created. So for thos plotting code, we wrap it in an ignore-warnings piece of code like this:</p> In\u00a0[4]: Copied! <pre>with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for epoch, val in zip(epocharray, data):\n        plt.plot(\n            [epoch.start, epoch.stop],\n            [val, val],\n            \"-o\",\n            color=\"k\",\n            markerfacecolor=\"w\",\n            lw=1.5,\n            mew=1.5,\n        )\n</pre> with warnings.catch_warnings():     warnings.simplefilter(\"ignore\")     for epoch, val in zip(epocharray, data):         plt.plot(             [epoch.start, epoch.stop],             [val, val],             \"-o\",             color=\"k\",             markerfacecolor=\"w\",             lw=1.5,             mew=1.5,         ) <p>and we see that the warnings no longer show up.</p> <p>Another use case for supressing warnings is if we want to have pretty (without warnings!) output for a jupyter notebook that we want to put online, for example. In that case, we can suppress all warnings simply by calling</p> In\u00a0[5]: Copied! <pre>warnings.simplefilter(\"ignore\")\n</pre> warnings.simplefilter(\"ignore\") <p>after which no more warnings will show up during that session.</p> <p>We often need to assign default parameters, and ones that we personally think might look good. These can include colors, linewidths, text labels, axis limits, and more. However, we need to follow a strategy that will allow the user to customize these values as easily and flexibly as possible.</p> <p>Consider again the <code>nelpy.plotting.core.plot</code> function, which has default behavior as follows:</p> <pre><code>color: '0.3'  # gray\nlinewidth (lw): 1.5\nmarkerfacecolor (mfc): white\nmarkeredgecolor (mec): same as color\nmarkeredgewidth (mew): same as linewidth</code></pre> <p>We can look at the default behavior below:</p> In\u00a0[6]: Copied! <pre>ax = npl.plot(epocharray, data, color=\"0.3\")\n</pre> ax = npl.plot(epocharray, data, color=\"0.3\") <p>But what if we want to change the linewidth, and say, the color? It would be annoying if we have to change both the <code>lw</code> and <code>mew</code> parameters just to change the linewidth, and similarly if we have to change both <code>color</code> and <code>ec</code> to change the color. So instead, we use default values to infer those as follows:</p> <pre><code>if lw is None:\n    lw = 1.5\nif mew is None:\n    mew = lw\nif color is None:\n    color = 'k'\nif mec is None:\n    mec = ec\n    </code></pre> <p>so that if only <code>lw</code> is specified, then <code>mew</code> automatically defaults to <code>lw</code>, and if only color is specified, then <code>mec</code> automatically defaults to <code>color</code>. This gives us a nice and clean interface.</p> In\u00a0[7]: Copied! <pre>ax = npl.plot(epocharray, data, color=\"deepskyblue\")\n</pre> ax = npl.plot(epocharray, data, color=\"deepskyblue\") <p>In the above, we see that simply specifying the <code>color</code> argument, we affect both the line color, and the marker color. However, we are free to customize both independently:</p> In\u00a0[8]: Copied! <pre>ax = npl.plot(epocharray, data, color=\"deepskyblue\", mec=\"red\")\n</pre> ax = npl.plot(epocharray, data, color=\"deepskyblue\", mec=\"red\") <p>and the same is true for the <code>linewidth</code> and <code>markeredgewidth</code> arguments.</p> <p>However, it gets even better, because we can catch all of the other keywords that plt.plot might take, and pass those along. For example, the <code>ms</code> argument can be used to specify the marker size. We do not check for this keyword, nor do we have any logic dealing with it. We simply catch it in <code>**kwargs</code>, and pass it along to <code>plt.plot(**kwargs)</code>.</p> <p>Then the user has the flexibility to change our custom plots whichever way she chooses, with the familiar interface of matplotlib.</p> In\u00a0[9]: Copied! <pre>ax = npl.plot(\n    epocharray,\n    data,\n    color=\"k\",\n    lw=1,\n    marker=\"d\",\n    mew=3,\n    mec=\"orange\",\n    ms=14,\n    linestyle=\"dashed\",\n)\n</pre> ax = npl.plot(     epocharray,     data,     color=\"k\",     lw=1,     marker=\"d\",     mew=3,     mec=\"orange\",     ms=14,     linestyle=\"dashed\", ) <p>It is also best practice to not specify default parameters in the function definition, but rather to specify them as <code>arg=None</code> and then to describe the default values and / or behavior in the docstring, and to assign default values in the body of the function.</p> <p>There are some caveats to this approach, such as when <code>None</code> is a legitimate and perhaps deliberate choice by the user. One example of such a case might be if the user wants to hide a particular element of a plot by setting the color to <code>None</code>, in which case it would be undesirable to overwrite the parameter value with the default. I have not really found an elegant way of dealing with this yet.</p> <p>In our analysis scripts, we can also create dictionaries to pass to our functions, so that we don't need to re-write all of the parameters every time we call a function. For example,</p> In\u00a0[10]: Copied! <pre># create dictionary of frequently used parameters:\nkws = {\n    \"lw\": 5,\n    \"ms\": 0,\n    \"color\": \"orange\",\n}  # hide markers, set color to orange, and set linewidth to 5\n\n# pass keyword dictionary to npl.plot():\nax = npl.plot(epocharray, data, **kws)\nax = npl.plot(epocharray, data[::-1], linestyle=\"dashed\", **kws)\n</pre> # create dictionary of frequently used parameters: kws = {     \"lw\": 5,     \"ms\": 0,     \"color\": \"orange\", }  # hide markers, set color to orange, and set linewidth to 5  # pass keyword dictionary to npl.plot(): ax = npl.plot(epocharray, data, **kws) ax = npl.plot(epocharray, data[::-1], linestyle=\"dashed\", **kws)"},{"location":"tutorials/develop/#developer-notes-and-examples-for-nelpy","title":"Developer notes and examples for <code>nelpy</code>\u00b6","text":""},{"location":"tutorials/develop/#selective-suppression-of-warnings","title":"Selective suppression of warnings\u00b6","text":""},{"location":"tutorials/develop/#make-use-of-default-params-and-kwargs-to-maintain-flexibility","title":"Make use of default params and  <code>**kwargs</code> to maintain flexibility\u00b6","text":""},{"location":"tutorials/plotting/","title":"plotting","text":"<p>Here we will look at some of the different plot types and utilities in the <code>nelpy.plotting</code> module.</p> <p>Let's start with the required imports.</p> In\u00a0[1]: Copied! <pre>import matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport nelpy as nel  # recommended import for nelpy\nimport nelpy.plotting as npl  # recommended import for the nelpy plotting library\n\n%matplotlib inline\n</pre> import matplotlib.gridspec as gridspec import matplotlib.pyplot as plt import numpy as np  import nelpy as nel  # recommended import for nelpy import nelpy.plotting as npl  # recommended import for the nelpy plotting library  %matplotlib inline <p>Nelpy includes a powerful and flexible scalebar implementation based on an excellent gist by Dan Meliza (see https://gist.github.com/dmeliza/3251476). Nelpy extends the basic functionality in a number of ways, and formats the scalebar labels correctly.</p> In\u00a0[2]: Copied! <pre>fig = plt.figure(1, figsize=(12, 4))\nax = fig.add_subplot(241)\nnpl.add_scalebar(ax=ax, sizex=0.25, labelx=\"10 cm\")\nax.set_title(\"horizontal\")\nax = fig.add_subplot(242)\nnpl.add_scalebar(ax=ax, sizey=0.15, labely=\"5 cm\")\nax.set_title(\"vertical\")\nax = fig.add_subplot(243)\nnpl.add_scalebar(ax=ax, sizex=0.15, sizey=0.3, labelx=\"5 ms\", labely=\"100 $\\mu$V\")\nax.set_title(\"both\")\nax = fig.add_subplot(244)\nnpl.add_scalebar(\n    ax=ax, sizex=0.15, sizey=0.3, labelx=\"5 ms\", labely=\"100 $\\mu$V\", sep=16\n)\nax.set_title(\"sep labels from bars\")\nax = fig.add_subplot(245)\nnpl.add_scalebar(\n    ax=ax, sizex=0.15, sizey=0.3, labelx=\"5 ms\", labely=\"100 $\\mu$V\", pad=2\n)\nax.set_title(\"padding\")\nax = fig.add_subplot(246)\nnpl.add_scalebar(ax=ax, sizex=0.25, labelx=\"10 cm\", fc=\"red\", ec=\"deepskyblue\", loc=2)\nax.set_title(\"change colors; loc=2\")\nax = fig.add_subplot(247)\nnpl.add_scalebar(\n    ax=ax,\n    sizex=0.4,\n    labelx=\"10 cm\",\n    fc=\"red\",\n    ec=\"deepskyblue\",\n    lw=5,\n    fontsize=16,\n    sep=5,\n)\nax.set_title(\"linewidth and fontsize\")\nax = fig.add_subplot(248)\nnpl.add_scalebar(\n    ax=ax, sizex=0.25, labelx=\"10 cm\", matchy=True, fc=\"gray\", ec=\"gray\", hidey=False\n)\nax.set_title(\"autoscale; show axis\")\n</pre> fig = plt.figure(1, figsize=(12, 4)) ax = fig.add_subplot(241) npl.add_scalebar(ax=ax, sizex=0.25, labelx=\"10 cm\") ax.set_title(\"horizontal\") ax = fig.add_subplot(242) npl.add_scalebar(ax=ax, sizey=0.15, labely=\"5 cm\") ax.set_title(\"vertical\") ax = fig.add_subplot(243) npl.add_scalebar(ax=ax, sizex=0.15, sizey=0.3, labelx=\"5 ms\", labely=\"100 $\\mu$V\") ax.set_title(\"both\") ax = fig.add_subplot(244) npl.add_scalebar(     ax=ax, sizex=0.15, sizey=0.3, labelx=\"5 ms\", labely=\"100 $\\mu$V\", sep=16 ) ax.set_title(\"sep labels from bars\") ax = fig.add_subplot(245) npl.add_scalebar(     ax=ax, sizex=0.15, sizey=0.3, labelx=\"5 ms\", labely=\"100 $\\mu$V\", pad=2 ) ax.set_title(\"padding\") ax = fig.add_subplot(246) npl.add_scalebar(ax=ax, sizex=0.25, labelx=\"10 cm\", fc=\"red\", ec=\"deepskyblue\", loc=2) ax.set_title(\"change colors; loc=2\") ax = fig.add_subplot(247) npl.add_scalebar(     ax=ax,     sizex=0.4,     labelx=\"10 cm\",     fc=\"red\",     ec=\"deepskyblue\",     lw=5,     fontsize=16,     sep=5, ) ax.set_title(\"linewidth and fontsize\") ax = fig.add_subplot(248) npl.add_scalebar(     ax=ax, sizex=0.25, labelx=\"10 cm\", matchy=True, fc=\"gray\", ec=\"gray\", hidey=False ) ax.set_title(\"autoscale; show axis\") Out[2]: <pre>Text(0.5, 1.0, 'autoscale; show axis')</pre> <p>The options for the location (<code>loc</code>) parameter are listed below:</p> <pre><code>'upper right'  : 1,\n'upper left'   : 2,\n'lower left'   : 3,\n'lower right'  : 4, default\n'right'        : 5,\n'center left'  : 6,\n'center right' : 7,\n'lower center' : 8,\n'upper center' : 9,\n'center'       : 10.</code></pre> In\u00a0[3]: Copied! <pre>stdata1 = [1, 2, 4, 5, 6, 10, 20]\nstdata2 = [3, 4, 4.5, 5, 5.5, 19]\nstdata3 = [5, 12, 14, 15, 16, 18, 22, 23, 24]\nstdata4 = [5, 12, 14, 15, 16, 18, 23, 25, 32]\n\nep1 = nel.EpochArray([[0.8, 2.8], [4.5, 5]])\n# ep1 = EpochArray([[0.8, 2.8]])\n\nsta1 = nel.SpikeTrainArray(\n    [stdata1, stdata2, stdata3, stdata4, stdata1 + stdata4], fs=5\n)\nsta2 = nel.SpikeTrainArray(\n    [stdata1, stdata2, stdata3, stdata4, stdata1 + stdata4], support=ep1, fs=5\n)\nsta3 = nel.SpikeTrainArray([stdata1, stdata3], fs=5, unit_ids=[10, 13])\nplt.figure()\nax = plt.gca()\nnpl.rasterplot(sta1, cmap=plt.cm.Blues, ax=ax)\nnpl.rasterplot(sta2, cmap=plt.cm.gray, ax=ax)\nnpl.rasterplot(sta3, color=\"orange\", lw=4, ax=ax)\nnpl.epochplot(sta2.support, ax=ax)\n</pre> stdata1 = [1, 2, 4, 5, 6, 10, 20] stdata2 = [3, 4, 4.5, 5, 5.5, 19] stdata3 = [5, 12, 14, 15, 16, 18, 22, 23, 24] stdata4 = [5, 12, 14, 15, 16, 18, 23, 25, 32]  ep1 = nel.EpochArray([[0.8, 2.8], [4.5, 5]]) # ep1 = EpochArray([[0.8, 2.8]])  sta1 = nel.SpikeTrainArray(     [stdata1, stdata2, stdata3, stdata4, stdata1 + stdata4], fs=5 ) sta2 = nel.SpikeTrainArray(     [stdata1, stdata2, stdata3, stdata4, stdata1 + stdata4], support=ep1, fs=5 ) sta3 = nel.SpikeTrainArray([stdata1, stdata3], fs=5, unit_ids=[10, 13]) plt.figure() ax = plt.gca() npl.rasterplot(sta1, cmap=plt.cm.Blues, ax=ax) npl.rasterplot(sta2, cmap=plt.cm.gray, ax=ax) npl.rasterplot(sta3, color=\"orange\", lw=4, ax=ax) npl.epochplot(sta2.support, ax=ax) <pre>WARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nD:\\github\\nelpy\\nelpy\\plotting\\core.py:1296: UserWarning: Spike trains may be plotted in the same vertical position as another unit\n</pre> Out[3]: <pre>&lt;Axes: &gt;</pre> In\u00a0[4]: Copied! <pre>ep1 = nel.EpochArray([1200, 1400])\nep2 = nel.EpochArray([[1150, 1210], [1220, 1240], [1380, 1420], [1500, 1550]])\n\nfig = plt.figure(figsize=(12, 4))\ngs = gridspec.GridSpec(2, 3, hspace=0.3)\n\nax1 = plt.subplot(gs[0])\nnpl.epochplot(ax=ax1, epochs=ep1, height=1)\nax1.set_xlim([1100, 1600])\nax1.set_title(\"ep1\")\n\nax2 = plt.subplot(gs[1])\nnpl.epochplot(ax=ax2, epochs=ep2, height=1)\nax2.set_xlim([1100, 1600])\nax2.set_title(\"ep2\")\n\nax3 = plt.subplot(gs[2])\nnpl.epochplot(ax=ax3, epochs=ep1[ep2], height=1)\nax3.set_xlim([1100, 1600])\nax3.set_title(\"ep1[ep2]\")\n\nax4 = plt.subplot(gs[3])\nnpl.epochplot(ax=ax4, epochs=ep2[ep1], height=1)\nax4.set_xlim([1100, 1600])\nax4.set_title(\"ep2[ep1]\")\n\nax5 = plt.subplot(gs[4])\nnpl.epochplot(ax=ax5, epochs=ep2.intersect(ep1, boundaries=True), height=1)\nax5.set_xlim([1100, 1600])\nax5.set_title(\"ep2.intersect(ep1)\")\n\nax6 = plt.subplot(gs[5])\nnpl.epochplot(ax=ax6, epochs=ep1.intersect(ep2), height=1)\nax6.set_xlim([1100, 1600])\nax6.set_title(\"ep1.intersect(ep2)\")\n\nprint(\"==== ep1 ====\")\nfor ep in ep1:\n    print(ep)\nprint(\"==== ep2 ====\")\nfor ep in ep2:\n    print(ep)\nprint(\"== ep2[ep1] =\")\nfor ep in ep2[ep1]:\n    print(ep)\n</pre> ep1 = nel.EpochArray([1200, 1400]) ep2 = nel.EpochArray([[1150, 1210], [1220, 1240], [1380, 1420], [1500, 1550]])  fig = plt.figure(figsize=(12, 4)) gs = gridspec.GridSpec(2, 3, hspace=0.3)  ax1 = plt.subplot(gs[0]) npl.epochplot(ax=ax1, epochs=ep1, height=1) ax1.set_xlim([1100, 1600]) ax1.set_title(\"ep1\")  ax2 = plt.subplot(gs[1]) npl.epochplot(ax=ax2, epochs=ep2, height=1) ax2.set_xlim([1100, 1600]) ax2.set_title(\"ep2\")  ax3 = plt.subplot(gs[2]) npl.epochplot(ax=ax3, epochs=ep1[ep2], height=1) ax3.set_xlim([1100, 1600]) ax3.set_title(\"ep1[ep2]\")  ax4 = plt.subplot(gs[3]) npl.epochplot(ax=ax4, epochs=ep2[ep1], height=1) ax4.set_xlim([1100, 1600]) ax4.set_title(\"ep2[ep1]\")  ax5 = plt.subplot(gs[4]) npl.epochplot(ax=ax5, epochs=ep2.intersect(ep1, boundaries=True), height=1) ax5.set_xlim([1100, 1600]) ax5.set_title(\"ep2.intersect(ep1)\")  ax6 = plt.subplot(gs[5]) npl.epochplot(ax=ax6, epochs=ep1.intersect(ep2), height=1) ax6.set_xlim([1100, 1600]) ax6.set_title(\"ep1.intersect(ep2)\")  print(\"==== ep1 ====\") for ep in ep1:     print(ep) print(\"==== ep2 ====\") for ep in ep2:     print(ep) print(\"== ep2[ep1] =\") for ep in ep2[ep1]:     print(ep) <pre>==== ep1 ====\n&lt;EpochArray at 0x2b9ee1b5bd0: 1 epoch&gt; of length 3:20 minutes\n==== ep2 ====\n&lt;EpochArray at 0x2b9ee2398d0: 1 epoch&gt; of length 1:00 minutes\n&lt;EpochArray at 0x2b9edf67d90: 1 epoch&gt; of length 20 seconds\n&lt;EpochArray at 0x2b9ee2398d0: 1 epoch&gt; of length 40 seconds\n&lt;EpochArray at 0x2b9eb0bf510: 1 epoch&gt; of length 50 seconds\n== ep2[ep1] =\n&lt;EpochArray at 0x2b9ee358490: 1 epoch&gt; of length 10 seconds\n&lt;EpochArray at 0x2b9ee2663d0: 1 epoch&gt; of length 20 seconds\n&lt;EpochArray at 0x2b9eb0bf510: 1 epoch&gt; of length 20 seconds\n</pre> In\u00a0[5]: Copied! <pre>n_traces = 30\n\nfig, axes = plt.subplots(n_traces, 1, figsize=(3.5, 3))\n\nfor ax in axes:\n    data = np.random.randn(40)\n    ax.plot(data, color=\"k\")\n\nnpl.utils.clear_left_right(*axes)\nnpl.utils.clear_top_bottom(*axes)\n\nnpl.scalebar.add_scalebar(\n    ax=axes[0], sizex=5, sizey=10, labely=\"10 Hz\", labelx=\"500 ms\", pad=1, loc=3\n)\n</pre> n_traces = 30  fig, axes = plt.subplots(n_traces, 1, figsize=(3.5, 3))  for ax in axes:     data = np.random.randn(40)     ax.plot(data, color=\"k\")  npl.utils.clear_left_right(*axes) npl.utils.clear_top_bottom(*axes)  npl.scalebar.add_scalebar(     ax=axes[0], sizex=5, sizey=10, labely=\"10 Hz\", labelx=\"500 ms\", pad=1, loc=3 ) Out[5]: <pre>&lt;Axes: &gt;</pre>"},{"location":"tutorials/plotting/#package-nelpyplotting-examples","title":"Package <code>nelpy.plotting</code> examples\u00b6","text":""},{"location":"tutorials/plotting/#flexible-scalebar-implementation","title":"Flexible scalebar implementation\u00b6","text":""},{"location":"tutorials/plotting/#plotting-spike-rasters","title":"Plotting spike rasters\u00b6","text":""},{"location":"tutorials/plotting/#epocharray-intersections","title":"EpochArray intersections\u00b6","text":""},{"location":"tutorials/plotting/#multiple-traces-with-no-axis-borders","title":"Multiple traces with no axis borders\u00b6","text":""}]}